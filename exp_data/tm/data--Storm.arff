@relation 'technicalDebt'

@attribute Text string
@attribute class-att {negative,positive}

@data

'todo support multiple levels includes ',positive
'when all nimbodes down and one few them come unfortunately there might not exact way know which one contains the most updated blob ',positive
'todo this assumes key only from the one field not need have order fields ',positive
'the following code duplicated the constructor mqttpublisher reproduce here fail the client side ssl misconfigured rather than when the topology deployed the cluster ',positive
'should remove the third blob because the first has the reset timestamp ',positive
'get rid multiple url ',positive
'todo track file offsets instead line number ',positive
'todo support more configuration options for now were defaulting the hbasexml files found the classpath ',positive
'this bit ugly but works order maintain the same directory structure that existed before need have storm conf storm jar and storm code shared directory and need set the permissions for that entire directory but the tracking per item basis are going end running the permission modification code once for each blob that downloaded times this case because the permission modification code runs separate process are doing global lock avoid any races between multiple versions running the same time ideally this would per topology basis but that lot harder and the changes run fairly quickly should not big deal ',positive
'could not find fully compatible version look see there possibly compatible version right below ',positive
'resourceutilsjava not available the classpath let parse out the resources want ',positive
'filesmove with nonempty directory doesnt work well windows this not atomic but does work ',positive
'not thread safe have one instance per producer thread synchronize externally ',positive
'todo substitution implementation not exactly efficient kind memory ',positive
'todo need better way this ',positive
'todo need able set the tick tuple time the message timeout ideally without parameterization ',positive
'connection unavailable will drop pending messages and let atleastonce message replay kick another option would buffer the messages memory but this option has the risk causing oom errors especially for topologies that disable message acking because dont know whether the connection recovery will succeed not and how long the recovery will take ',positive
'todo add metrics ',positive
'todo consider adding shuffle grouping after the spout avoid much routing the argsreturninfo all over the place least until its possible just pack bolt logic into the spout itself ',positive
'this hack for nonras scheduler topology and worker resources ',positive
'todo move this logic the model class ',positive
'for some reason the new code ackers null get ',positive
'this bit hack list then component stream want format this componentstream ',positive
'todo dry this code with whats ',positive
'should fail the tuple kill the worker ',positive
'todo streams should uniquely identifiable ',positive
'todo would love standardize this ',positive
'fixme this class can moved webapp once porting done ',positive
'this isnt strictly necessary but doesnt hurt and ensures that the machine stays date even callbacks dont all work exactly right ',positive
'work around should get alternative working way ',positive
'read last this should cause evicted next put ',positive
'todo more collection content type checking ',positive
'fixme were using default config since cannot serialized still needs provide some options externally ',positive
'todo this can only ever null someone doing something odd with mocking should really fix the mocking and remove this ',positive
'todo put some better exception mapping todo move populatecontext filter ',positive
'should fail the batch kill the worker ',positive
'getting the exact response code bit more complex todo should use better client ',positive
'todo deduplicate this logic with the code nimbus ',positive
'todo should worker even take the topologyid input this should deducible from cluster state searching through assignments what about theres inconsistency assignments but nimbus should guarantee this consistency param conf storm configuration param context param topologyid topology param assignmentid assignment param supervisorport parent supervisor thrift server port param port port which the worker runs param workerid worker ',positive
'this bit ugly but shows how this would happen worker will use the same apis ',positive
'fixme stores map topoconf topologycontext and expose these derived classes ',positive
'original implementation doesnt actually check delete succeeded not ',positive
'doesnt seem like you should have this but java serialization wacky and doesnt call the default constructor ',positive
'this bit ugly the json are expecting should the form component resource value but because value coming from json going number and want double the goal through each entry and update accordingly ',positive
'may though unlikely lose metering here state transition too frequent less than millisecond ',positive
'want throw exception path doesnt exist ',positive
'todo better way this would great ',positive
'todo the future this might better common webapp location ',positive
'fixme should moved stormclient when can removed ',positive
'todo figure out how want deal with overrides users may want add streams even when overriding other properties for now just add them blindly which could lead potentially invalid topology ',positive
'more accurate that threadsleep but still not great ',positive
'todo ignore the master batch coordinator ',positive
'this test rather ugly but the only way see the error messages are working correctly ',positive
'todo need able replace existing fields with the function fields like cascading fieldsreplace ',positive
'todo wrap this set the stream name ',positive
'todo this isnt right its not the map anymore ',positive
'todo add method for drpc stream needs know how automatically return results etc too expensive batch per drpc request ',positive
'todo the inner join incrementally emitting the cross join with this tuple against all other sides todo only cross join least one tuple each side ',positive
'todo once everything java this should not possible any more ',positive
'heck for backward compatibility need pass hbase conf the conf instance instance persistentmap making copy ',positive
'todo handle regular rich spout without batches need lots updates support this throughout ',positive
'this hack allow zookeepermain called this command ',positive
'perhaps there better way this ',positive
'todo can optimize further only querying backing map for keys not the cache ',positive
'todo this causing issues ',positive
'todo need invoke hook provided the topology giving chance create user resources this would part the initialization hook need separate into workercontext and workerusercontext actually just via interfaces just need make sure hide setresource from tasks ',positive
'not implemented ',positive
'todo what would good test ensure that least somewhat defensively copied ',positive
'todo the future want way include this logic the spout itself make unnecessary having storm include metadata about which grouping tuple came from ',positive
'todo does this work well windows ',positive
'not enough guaranteed use the age the topology instead todo need good way only this once ',positive
'use parsewithexception instead parse can capture deserialization errors the log they are likely bugs the spout code ',positive
'todo add logging that not all tuples were received ',positive
'todo take away knowledge storms internals here ',positive
'todo should handle ref ',positive
'todo make sure test these two functions manual tests ',positive
'todo get this from type instead hardcoding nimbus establish clientserver transport via plugin ',positive
'there bug some versions that returns for the uptime ',positive
'need set active false before calling onkill current implementation does not return ',positive
'this kind unnecessary for clojure ',positive
'todo this class reserved for supporting messages with different schemas current only one schema the cache ',positive
'bad files dir config ',positive
'fixme can filter listkeys with local blobstore when storm going resolved workaround call getblobmeta for all keys ',positive
'min this really means something wrong even very slow node ',positive
'todo check for null grouping args ',positive
'todo perhaps can adjust the frequency later ',positive
'reestablish connection eventhub servers using the right offset tbd might optimized with cache ',positive
'todo handle the case where there may schema ',positive
'seems safer not follow symlinks since dont expect them here ',positive
'this because json doesnt allow numbers keys todo replace json with better form encoding ',positive
'todo optimize this computation perhaps inner loop can outside avoid rescanning tuples ',positive
'likely because bug try get another way ',positive
'while disabling retain the sampling pct ',negative
'hearbeat upon ',negative
'requestedmemonheap ',negative
'test environment variable substitution ',negative
'assignments ',negative
'verify that all messages are emitted ack all the messages ',negative
'executioncommand ',negative
'boolean track deactivated state ',negative
'track punctuation nonbatch mode that the punctuation acked after all the processors have emitted the punctuation downstream ',negative
'local executors and localtaskids running this worker ',negative
'only emit have declared fields ',negative
'all types files included ',negative
'want capture the full time range the target size had one bucket less then ',negative
'use batch size that matches the default credit size ',negative
'use the local setting for the login config rather than the topologys ',negative
'get the nimbodes with the latest version ',negative
'destination taskid ',negative
'component ',negative
'fail the last emitted tuple and verify that the spout wont retry because its above the emit limit ',negative
'generate null response then authentication has completed not warn and return without sending response back the server ',negative
'supervisor assignment idsupervisor ',negative
'the windowmanager scan loop early ',negative
'failed send the jms message fail the tuple fast ',negative
'set the first assignment ',negative
'root ',negative
'update the count the state here the first argument the initial value for the count and the second argument function that increments the count for each value received ',negative
'test write ',negative
'key ',negative
'topologyid ',negative
'start emitting right now ',negative
'format date worker logs ',negative
'map ',negative
'use different blobstore dir doesnt conflict with other test ',negative
'timestamp either milliseconds seconds which this metric occurred ',negative
'noop ',negative
'update the current count for this object ',negative
'ranked fourth since rack has alot memory but not cpu ',negative
'each controlmessage encoded code short each taskmessage encoded task short len int payload byte ',negative
'this call returns immediately ',negative
'for testing ',negative
'test test when more workers are available due topology worker max heap size limit but there memory still available wordspout going contain executors that needs scheduling each those executors has memory requirement the cluster contains free workerslots for this topolology each worker limited max heap size thus one executor not going able get scheduled thus failing the scheduling this topology and executors this ',negative
'set clean time really high doesnt kick ',negative
'set for nimbus only ',negative
'lag ',negative
'creates arraylargestid filled with nulls ',negative
'',negative
'close the input stream ',negative
'dependency uploading only makes sense for distributed mode ',negative
'read line and ack ',negative
'usedcpu ',negative
'failed tuples retried then send tuples from hdfs ',negative
'happens when part just rather than denoting some directory ',negative
'next verify that the blob store correct before start ',negative
'the connection ready once the channel active see ',negative
'test for keylist download ',negative
'tuples should available store before they are added window manager ',negative
'memory required topot memory required topot ',negative
'this dsl yaml etc topology ',negative
'check for required fields check for substruct validity ',negative
'periodically calls refreshload sec simulate worker load update timer ',negative
'remotetaskid jcqueue some entries maybe null emits those tasksids from this worker ',negative
'encoders ',negative
'commit frequency count ',negative
'check iterable ',negative
'set acl below that can shared other users well but allows only read ',negative
'emit any remaining messages ',negative
'autohdfs specified not attempt login using keytabs only kept for backward compatibility ',negative
'ack ',negative
'task ids should pulled first ',negative
'referenced from metric ',negative
'pulse ',negative
'add reference one and then remove reference again has newer timestamp ',negative
'writes multiple metric values into the database batch operation the tree map keeps the keys sorted ',negative
'get first and last block times for multiple runs and strategies ',negative
'the worker and running check for profiling requests ',negative
'the other config does not have set ',negative
'memoffheap ',negative
'capacity ',negative
'new line beginning each line instead end for better recovery from ',negative
'exceptions are captured and thrown the end the batch the executor ',negative
'this thread will send out messages destined for remote tasks other workers ',negative
'another conversion lets just make this all common ',negative
'store the time which the query started executing the sql standard says that functions such currenttimestamp return the same value throughout the query ',negative
'maintain backward compatibility for ',negative
'checkpoint the state every seconds ',negative
'cannot launch the container yet the resources may still updating ',negative
'use bootstrap tuple wait for topology running ',negative
'aze ',negative
'spout cpu gpu bolt cpu gpu bolt cpu gpu total cpu gpu this node has cpu gpu left ',negative
'this case will arise case nonsequential offset being processed the topic doesnt contain offset nextcommitoffset possible the topic compacted deleted the consumer should jump the next logical point the topic next logical offset should the first element after nextcommitoffset the ascending ordered emitted set ',negative
'topology can set resources terms cpu and memory for each component ',negative
'add with past ',negative
'when moving pacemaker workerbeats can leaked too ',negative
'seconds ',negative
'race condition with another thread and lost try again ',negative
'the producers are shut down first keep going until the queue empty ',negative
'dont allow any cluster wide configuration ',negative
'evicted metadata needs stored immediately metadata lookups count being the cache ',negative
'will have the correct settings that cannot overriden the submitter ',negative
'map describing which topologies are using which slots this node the format the map the following ',negative
'track how many times each supervisor slot has been listed bad ',negative
'when they are successfully processed ',negative
'all sent events are stored pending ',negative
'topo has large tasks ',negative
'ignore any exceptions might doing test for authentication ',negative
'first failure the initial delay not interesting ',negative
'boltspecific configuration for windowed bolts specify the window length time duration ',negative
'resourcesmap ',negative
'contains one tuple per stream being joined refs fields that will part output fields ',negative
'remote subject ',negative
'default parser uses for quoting identifiers switching dqid double quoted identifiers needed for array and map access arr etc work ',negative
'the second tuple should not ackd because the batch should cleared and this will ',negative
'need adjust the throughput accordingly that stays the same aggregate ',negative
'test scheduling does not cause negative resources ',negative
'return now without sending upstream here since client not authorized ',negative
'reader ',negative
'ranked first since rack has the most balanced set resources ',negative
'configs ',negative
'waiting returned ',negative
'validate path defined ',negative
'blostore launch command with topology blobstore map here are giving local name that can read from the file binstorm jar ',negative
'for some odd reason they are leaked ',negative
'get topology constraints ',negative
'ensure the second file has later modified timestamp the spout should pick the first file first ',negative
'launch heartbeat threads immediately that slowloading tasks dont cause the worker timeout ',negative
'tuple contains string object json format ',negative
'returns list tuple key val from table val from row ',negative
'bolt cpu gpu total cpu gpu this node has cpu gpu left ',negative
'the priority topology describes the importance the topology decreasing importance starting from the highest priority and the priority importance decreases the priority number increases recommended range but hard limit set there are not enough resources cluster the priority combination with how far over guarantees ',negative
'the second tuple used wait for the spout rotate its pending map ',negative
'does the actual ack when the state saved ',negative
'unknown version should treated current version which supports rpc heartbeat ',negative
'there are few possible files that would want clean basedir tmp basename basedir tmp basename current basedir basenameversion basedir basenamecurrent basedir basenameversion general always want delete the tmp files they are there ',negative
'check for case someone called their user user name this line contains the user name for the pid were looking example line user name ',negative
'need reverse the order elements delete files from oldest newest ',negative
'ignored the file did not match ',negative
'cleanup internal assignments ',negative
'jms options ',negative
'has been acked ',negative
'check exec satisfy spread ',negative
'event sent checkpoint shall created ',negative
'dynamic fields ',negative
'wait for more acks before proceeding ',negative
'get query filter ',negative
'allocate another array switched ',negative
'jump ',negative
'verify that offset was last committed offset since this the offset the spout should resume ',negative
'modprinc maxlife mins principal kadmin ',negative
'acquire lock file ',negative
'now selecting from the full set should cause the fourth task chosen ',negative
'node ',negative
'prevstatus ',negative
'configs ',negative
'the new partition should discovered and the message should emitted ',negative
'error message returned something went wrong ',negative
'the failed executions should not cause rotations and any new files ',negative
'producers ',negative
'only logset when theres been change the assignment ',negative
'singlerel ',negative
'only some data has arrived each input stream ',negative
'ack for message that failed once least and was reemitted then the record itself will use that determine the needs told about and then remove the record itself ',negative
'metric name ',negative
'bolt cpu gpu bolt cpu gpu bolt cpu gpu total cpu gpu this node has cpu gpu left ',negative
'componentexecutors ',negative
'the manually set config supervisor will overwrite resources assigned ',negative
'when this master not leader and get heartbeats report from supervisornode just ignore ',negative
'try locking another file the same time ',negative
'each time try schedule new component simulate taking second longer ',negative
'now need build the array ',negative
'file should moved archive ',negative
'create couple files consume ',negative
'',negative
'current state ',negative
'workers requested less than took then know some workers track died since have more workers than are supposed have ',negative
'returns void the event should continue false the event should not done ',negative
'user jerry submits another topology into full cluster ',negative
'doing nothing probably due oom issue and hoping will handle ',negative
'should remain unchanged ',negative
'just sure ',negative
'launchtimesecs ',negative
'there wont batchinfo for the success stream ',negative
'strategy determine the fetch offset the first realized the spout upon activation ',negative
'start accepting requests ',negative
'this could fail blob gets deleted mistake dont crash nimbus ',negative
'status ',negative
'create default pipeline implementation ',negative
'public object executecontext ',negative
'scheduling changed after killed all the processes ',negative
'this finds all active topologies blob keys from all local topology blob keys ',negative
'sasl transport ',negative
'for cgroups limit max long ',negative
'not available ',negative
'scheduled ',negative
'config mainmethods ',negative
'any errorexception thrown fetch from zookeeper ',negative
'initial state ',negative
'errorexception thrown just skip ',negative
'metricname ',negative
'verify the signature ',negative
'empty ',negative
'set really small will cleanup ',negative
'look for deleted log timeouts ',negative
'authz authn password ',negative
'',negative
'but that only there bug one the password providers ',negative
'numusedworkers ',negative
'bolt should also receive from checkpoint streams bolt bolt ',negative
'already points where want ',negative
'really make this work well ',negative
'mapping ',negative
'equivalent create command command line ',negative
'keyclassstring ',negative
'there are any abandoned files pick oldest one ',negative
'assigning internally ',negative
'topology priority ',negative
'totalsharedoffheap ',negative
'retry till least element drained ',negative
'schedule mid block ',negative
'key fields ',negative
'',negative
'request from authorized hosts and group should allowed ',negative
'now need free some resources ',negative
'all internal state except for the current buckets are ',negative
'update the isleader field for each nimbus summary ',negative
'has the the node its doing the partitioning ',negative
'',negative
'lets checkpoint that can get the last checkpoint when restarting ',negative
'allow requesting slots number bigger than available slots ',negative
'activation expired list should contain even the ones expired due ',negative
'not sure what could cause this ',negative
'specific ',negative
'flag indicating hivewriter was closed ',negative
'stream number cube pairs ',negative
'default ',negative
'run default scheduler nonisolated topologies ',negative
'',negative
'list all daemon logs ',negative
'already checked this ',negative
'add any autocredential expiry metrics from the worker ',negative
'this ensures that list values always written the same way regardless whether its java collection one clojures persistent collections which have different serializers doing this lets deserialize arraylist and avoid writing the class here ',negative
'create couple input files read ',negative
'follow the model service loaders even though not service ',negative
'should double ack same msg should still ',negative
'not process events beyond current ',negative
'generate some supervisors that are depleted one resource ',negative
'run spout ',negative
'await all results ',negative
'worker slot which was never back normal tolerance period will removed from cache ',negative
'attention whb can null ',negative
'waiting for this also ensures that the first tuple gets failed resettimeout doesnt work ',negative
'propagate interrupt ',negative
'save the metadata for all types strings matches ',negative
'populate node component assignments ',negative
'for serialization ',negative
'wildcard directory ',negative
'just output the word value with count ',negative
'send watermark event which should trigger three windows ',negative
'tests for case when subject null security turned off and ',negative
'both things are not expected and should not happen ',negative
'thenreturn always returns the same object which already consumed the time user tries getblob ',negative
'process all metadata ',negative
'update the keycurrent symlink first create tmp symlink and ',negative
'arbitrary message returned when scheduling done ',negative
'resource also present resources map will overwrite the above ',negative
'heartbeats stats ',negative
'can local shuffle ',negative
'request impersonate users from unauthroized groups should rejected ',negative
'lets reread the children storms the source truth and see new one was created the background ',negative
'naive implementation but might good enough ',negative
'this special case where the jar was not uploaded will not download already the classpath ',negative
'dependencyartifacts ',negative
'update the word counts the state here the first argument the initial value for the state and the second argument function that adds the count the current value the state ',negative
'parallelism same ',negative
'need search more not going help ',negative
'would only happen are for now ',negative
'test for replication ',negative
'direct ',negative
'lock dir config ',negative
'once every updaterateperiodns ',negative
'kill the container and restart ',negative
'avoid system dependent things ',negative
'for this part the test interleve the differnt rotation types ',negative
'guarantees list unused string ids exists once the list empty creates new list ',negative
'reached eof didnt read anything ',negative
'set the shard iterator for last fetched sequence number start from correct position shard ',negative
'dont wait for timetrigger fire since this could lead timing issues unit tests set large value and trigger manually ',negative
'bolt that subscribes the intermediate bolt and publishes jms topic ',negative
'just and try delte the others ',negative
'tuple payload serializer specified via configuration ',negative
'also called from ',negative
'load the first part entries ',negative
'future got interrupted exception want interrupt parent thread itself ',negative
'ignore changes scheduling while downloading the topology blobs dont support canceling the download through the future yet because pending blobs may shared multiple workers and cancel may lead race condition keep everything sync just wait for all workers ',negative
'doublearg ',negative
'node and supervisor are the same ',negative
'read line and see another log entry was made ',negative
'read initial lines file then check lock exists ',negative
'now lets get the creds for the topos can verify those well ',negative
'used recognize the pattern active log files may remove the current from this list ',negative
'',negative
'this class consists exclusively static factory mainmethods that create instances that are essential work with the jpmml library ',negative
'seconds passed not timing out ',negative
'its null one later transaction batch was emitted before this should skip this batch didnt exist and was created which case the stateinitializer was invoked and was emitted ',negative
'end executor summary ',negative
'test whether the integer power ',negative
'release things that dont need wait for finish downloading ',negative
'rocksdb should insert sorted key order ',negative
'updating file few times every seconds ',negative
'dropping the parallelism the bolts instead can find solution reasonable amount work when backtracking ',negative
'emit and ack the rest ',negative
'configs for memory enforcement done the supervisor not cgroups directly ',negative
'icredentialsrenewer ',negative
'only need keep track failed tuples commits kafka are controlled tuple acks which happens only for atleastonce processing semantics ',negative
'todo file rotation ',negative
'copy case want modify ',negative
'this read default value for other configurations ',negative
'this should trigger the scan find the next aligned window end but not produce any activations ',negative
'argslist ',negative
'consume both files ',negative
'jms topic spout ',negative
'set nimbusinfo ',negative
'what want ',negative
'gets nimbus subject with nimbusprincipal set ',negative
'include partition the file name that index for different partitions are independent ',negative
'with known best input ',negative
'too fast not reported ',negative
'record returned put the sequence number the emittedpershard tie back with ack fail ',negative
'for point for point ',negative
'null log any unhandled errors stderr ',negative
'targetloglevel ',negative
'creates mongoclient described uri ',negative
'empty still ',negative
'retire writers ',negative
'iprincipaltolocal ',negative
'sentencespout mybolt ',negative
'the resource aware scheduler used ',negative
'since user derek has exceeded his resource guarantee while user jerry has not topo topo could evicted because they have the same priority ',negative
'class offset ',negative
'the connection not sent unless response requested ',negative
'now that the root fine can start look the other paths under ',negative
'convenience data structure speedup lookups ',negative
'returns the recorded throughput since the last call since this meter was instantiated being called for fisrt time ',negative
'noop could add configs through the webxml wanted something stand alone here ',negative
'storm ',negative
'offset where processing will resume upon spout restart ',negative
'file offset and byte offset should always zero when searching multiple workers multiple ports ',negative
'grab lock ',negative
'gets collection ',negative
'blobs are not supported local mode return nothing ',negative
'filtered negative value ',negative
'noop dont actually want change log levels for tests ',negative
'transfer encoding should set jersey sets default ',negative
'',negative
'agg spout stats ',negative
'keep this constructor for backward compatibility ',negative
'noop purpose ',negative
'deleting closed file should return true ',negative
'stream name specified ',negative
'validate metrics aggregations found for for all agglevels when searching port ',negative
'assume topology ',negative
'with principals the subject acl should always set worldeverything ',negative
'errortimesecs ',negative
'shouldnt happen ',negative
'topoids ',negative
'generously adapted from avroserializerscala which has asl license ',negative
'just ignore the exception ',negative
'receives msgs from remote workers and feeds them local executors any receiving local executor under back pressure ',negative
'avoid reordering emits stop first failure ',negative
'didnt take gpus into account everything would fit under single slot but because there only gpu per node and each the spouts needs gpu has scheduled least nodes and hence slots because this all the bolts will scheduled single slot with one the spouts and the other spout its own slot everything that can shared shared ',negative
'producerfwdconsumer measurement ',negative
'try uploading second one and should failed throwing runtimeexception ',negative
'start the thread pool ',negative
'the spout must able reemit all retriable tuples even the maxpollrecords set low value compared ',negative
'stream metric value note that sidoutstats may contain both long and double values ',negative
'all the events should expired when the next watermark received ',negative
'test for subject with principals and acls set default ',negative
'copy the half the buffer the first half ',negative
'remove something randomly ',negative
'found the topology lets get the conf ',negative
'since made sys components visible the component map has all system components ',negative
'noop ',negative
'computes tumbling window average ',negative
'check lock file contents ',negative
'queryplanner streams mode configures the topology with compiled classes need add new classes into topology jar topology will serialized and sent nimbus and deserialized and executed workers ',negative
'all done can launch the worker now ',negative
'login and also update the subject field this instance ',negative
'map from stream name batch ',negative
'create symbolic link relative tar parent dir ',negative
'set the metrics sample rate force update the executor stats every time something happens this necessary because relies the executor emit stats accurate ',negative
'check node alive ',negative
'even the topology not valid still need remap all ',negative
'',negative
'translating the name this call happens different callback from validating the user name and password this has stateless though cannot save the password provider away sure got the same one that validated the password the password providers are written correctly this should never happen because they cannot read the name they would return null but the off chance that something goes wrong with the translation because mismatch try skip the bad one ',negative
'now rebalance and add new partition ',negative
'shutdown server process since could not handle thrift requests any more ',negative
'when worker bootup worker will start setup initial connections other workers when all connection ready will count down this latch and spout and bolt will activated assuming the topology not deactivated ',negative
'json parsing fail error received ',negative
'memoryusage ',negative
'object handling interaction with kinesis ',negative
'spout stats ',negative
'spout implementation ',negative
'emit ',negative
'boltobject ',negative
'use custom class loader set testing environment ',negative
'topo has one single huge task that can not handled the smallsuper ',negative
'user derek submits another topology into full cluster topo should not able scheduled initially but since topo has higher priority than topo topo will evicted that topo can scheduled ',negative
'throws ioexceptions for call next succeeds thereafter ',negative
'remove reverse lookup from map ',negative
'now rebalance ',negative
'scans the database look for metadata string and returns the metadata info ',negative
'numtasks ',negative
'deprecated favor nonthreaded rotatingmap ',negative
'trust that the file exists ',negative
'this test works because mocking spout splits the tuples evenly among the tasks ',negative
'mapping from storm tuple rocketmq message ',negative
'usedmem ',negative
'key transformers ',negative
'any events are scheduled sleep until event generation any recurring events are scheduled then will always through this branch sleeping only the exact necessary amount time give upper bound millis the sleeping time limit the response time for detecting any new event within secs ',negative
'dont want override the client there thrift server and running would not test any the actual thrift code ',negative
'each drpc request always single attempt ',negative
'add the callers cache cant add the stringmetadatacache since that could cause eviction database write which want only occur from the inserting thread ',negative
'this partition new and should start the committed offset ',negative
'assuming sidefields are preserving its order ',negative
'blocking call ',negative
'above gives extra empty string the end below removes that ',negative
'remote address ',negative
'far matches keep going ',negative
'fail are forced try again ',negative
'attempt find the string the database ',negative
'setup test message ',negative
'prior the orgapache change ',negative
'delete one worker failed from topo assignment enable actual schedule for testing ',negative
'actual ret mapstring mapstring longdouble ',negative
'time sleep between retries milliseconds ',negative
'ranked fifth since rack has not cpu resources ',negative
'time last flush this writer ',negative
'this attempt give all the streams equal opportunity emit something ',negative
'this the end key for whole scan ',negative
'transfers messages destined other workers ',negative
'make sure that have received least integer length ',negative
'name ',negative
'test write again ',negative
'will closed automatically when shutting down the dfs cluster ',negative
'should remove key ',negative
'batch spout then contains txid ',negative
'called with bad port not the config searching should done ',negative
'for partitionpersist ',negative
'the dir empty try delete may fail but that ',negative
'empty class ',negative
'seems this can fail returning false throwing exception convert false ret value exception ',negative
'found the next offset commit ',negative
'this only used for loggingmetrics dont crash the process over ',negative
'counter for spout wait strategy counter for back pressure wait strategy ',negative
'now lets create token and verify that can connect ',negative
'todo log ',negative
'this validates the structure the topology ',negative
'note that portdir active worker containing active logs ',negative
'may output many tuples for given input tuple ',negative
'now multiple ',negative
'the user wants explicitly set auto offset reset policy should respect but when the spout configured for atleastonce processing should default seeking the earliest offset case theres offset out range error rather than seeking the latest kafkas default this type error will typically happen when the consumer requests offset that was deleted ',negative
'hash ',negative
'bookkeeping ',negative
'instances this type are sent from nettyworker upstream workertransfer indicate backpressure situation ',negative
'assumption therere put and delete for same target parameter list ',negative
'default need this different user ',negative
'set the default heap memory size for supervisortest ',negative
'common ',negative
'this the first time initialize the resources ',negative
'need enforcement topology level not single worker level because for cgroups each page shared memory goes the worker that touched first may need make this more plugable the future and let the resource isolation manager tell what ',negative
'headers ',negative
'update set the second assignment ',negative
'create reader and some checks ',negative
'principal ',negative
'nonstatic impl mainmethods exist for mocking purposes ',negative
'topo evicted since user bobby dont have any resource guarantees and topo the next lowest priority for user bobby ',negative
'evaluator ',negative
'replicationcount ',negative
'delete file and retry creation ',negative
'givenwhen ',negative
'the latch not started yet start ',negative
'offsets emitted are void ',negative
'but topo was submitted earlier thus choose that one evict somewhat arbitrary ',negative
'dont include sys ',negative
'storm topology name ',negative
'defaults info level when the logger isnt found previously ',negative
'specific reason mock this one easiest ways make dummy instance ',negative
'remotetaskid truefalse indicates remote task under ',negative
'use the default server port ',negative
'play and ack tuple ',negative
'emit the averages downstream ',negative
'for reporting errors ',negative
'common aggregate ',negative
'bind and start accept incoming connections ',negative
'package access for unit test ',negative
'this test where are configured point right single artifact ',negative
'required required required required required required required required required ',negative
'assume filter choices have been made and since selection was made all levels are valid ',negative
'open sasl transport with the login credential ',negative
'let first lock ',negative
'min values ',negative
'validate search port ',negative
'merge with existing assignments ',negative
'initialize state for batch ',negative
'offset are pending failed but not retriable ',negative
'null istimedout means worker never reported any heartbeat ',negative
'rwrr ',negative
'one time scheduling ',negative
'regardless ticketrenewwindow setting above and the ticket expiry time thread will not sleep between refresh attempts any less than minute milliseconds minute ',negative
'and wait for the message get through the spout acks use the same path timeout resets ',negative
'subprocesses must send their pid first thing ',negative
'null for system bolt ',negative
'assume that get doesnt have any families defined this for not digging deeply ',negative
'the best way force backtracking change the heuristic the components are reversed hard find answer ',negative
'local mode there jar ',negative
'extract spout resource info ',negative
'class path entries that are neither directories nor archives zip jar files nor the asterisk wildcard character are ignored ',negative
'topologyversion ',negative
'convert sources json serializable format ',negative
'cpuguarantee ',negative
'toplogy worgenspout fieldsgrouping countbolt ',negative
'careful about adding additional tests the dfscluster will shared ',negative
'start metastore ',negative
'get transaction already exists and attempt greater than the attempt there ',negative
'type ',negative
'set committed looks like some messages have been committed each partition ',negative
'should not returned since this executor not part the topologys assignment ',negative
'required required required required required required optional optional optional optional optional optional optional optional optional optional optional ',negative
'jprofile start ',negative
'find retirement candidates ',negative
'committed offset the offset where processing will resume upon spout restart initially set fetchoffset ',negative
'allow freqsec expire ',negative
'schedule topo ',negative
'flatmapfunction aware prepare let handle preparation ',negative
'advance time and then trigger call kafka consumer commit ',negative
'verify that the tuple not emitted again ',negative
'use this instead storms built one that can specify without knowledge storms internals ',negative
'setup hfs bolt ',negative
'wait for seconds ',negative
'put global stream for spouts ',negative
'using monotonically increasing attempt downstream tasks can memory efficient clearing out state for old attempts soon they see higher attempt for transaction ',negative
'how partition for second stage aggregation ',negative
'required optional optional optional optional ',negative
'try the loader plugin configured ',negative
'bail out ',negative
'the user not set lets see what the request context ',negative
'being the position the fields this seq the remainder the seq the size ',negative
'notjump closed strict mode ',negative
'asserts that commitsync has been called once that there are only commits one topic and that the committed offset covers messagecount messages ',negative
'allow poll the partition not the limit ',negative
'when this ever null ',negative
'read error and input streams this would free the buffers ',negative
'test for replication with nimbus user ',negative
'',negative
'the below field declarations are also used commonclj define the event logger output fields ',negative
'result ',negative
'iautocredentials ',negative
'topologygetbolt aka sys tasks most specifically acker tasks ',negative
'windowtotransferred ',negative
'there most one schedule per message ',negative
'make sure that even though nexttuple doesnt receive valid data the offset will checkpointed after checkpointinterval seconds ',negative
'retrieve any existing aggregation matching this one and update the values ',negative
'private constructor ',negative
'taskids first ',negative
'keeps track flight tuples ',negative
'loggerinfoemit for partition partitiongetid offset offset ',negative
'',negative
'this because the tests are checking that hard cap maxpollrecords uncommitted offsets exists ',negative
'good far check are cgroup ',negative
'supervisor which was never back normal tolerance period will removed from cache ',negative
'maybe needs start phase where can retrieval update phase and then finish phase shouldnt really oneatatime interface since have all the tuples already tood used for the new values stream the list needed able get reduceragg and combineragg persistentaggregate for grouped streams working efficiently ',negative
'java classpath expanded all jarjar files the directory ',negative
'verify that offset was committed for the given topicpartition since processing should resume ',negative
'could not lock try another file ',negative
'use the hash index for prefix searches ',negative
'compacted away ',negative
'done ',negative
'check map ',negative
'tuples may ackedfailed after the spout deactivates have able handle this too ',negative
'zkhoststring for solr gettingstarted example ',negative
'not blocking call cannot emit will add tuple pendingemits and return false pendingemits can null ',negative
'waiting fetched ',negative
'stream number square pairs ',negative
'outputfields ',negative
'timers ',negative
'',negative
'not use canonical file name here are using symbolic links read file data and performing atomic move while updating files ',negative
'initiate connection remote destination ',negative
'assume that there could worker already the node that under the minworkercpu budget its possible could combine with lets disregard minworkercpu from the request and validate that cpu rough fit ',negative
'generating list random numbers and removing the ones that already are use ',negative
'client api invoke blob store api functionality ',negative
'delete everything hdfs ',negative
'not send upstream other handlers further action needs ',negative
'test class override the write directory ',negative
'retry once after minute ',negative
'update this only after writing hdfs ',negative
'ranked second since rack has balanced set resources but less than rack ',negative
'',negative
'comma separated list topics consumer group for which the offset needs calculated bootstrap brokers security protocol connect kafka properties file containing additional kafka consumer configs ',negative
'configs hdfs bolt ',negative
'aggregating metric did not exist dont look for further ones with smaller timestamps ',negative
'rotate files when they reach ',negative
'create authentication callback handler ',negative
'port testshuffleloadeven ',negative
'given ',negative
'consume file ',negative
'requested assigned guaranteedavailable ',negative
'jprofile dump ',negative
'',negative
'are launching now ',negative
'tests that isscheduled isready and are mutually consistent when there are multiple messages scheduled partition ',negative
'can updatestatebykey statequery processors ',negative
'these are mandatory parameters ',negative
'scan the entire window handle out order events the case time based windows ',negative
'reader type config ',negative
'windowtostats ',negative
'find number constraints per component keycomp value constraints ',negative
'set the inmemory filesystem ',negative
'whether find all documents according the query filter ',negative
'metrics are off verify null ',negative
'check that one message fails repeatedly the retry cap limits how many times the message can reemitted ',negative
'mock state store and receiver ',negative
'test commit second creates properly ',negative
'out order events should processed upto the lag ',negative
'process has not terminated check has completed not just destroy ',negative
'dont update unless there are tuples this helps out with things like global partition persist where multiple tasks may still exist for this processor only want the global one anything ',negative
'compute the stats for the different input streams ',negative
'because the simple topology was scheduled first want sure that didnt put anything the gpu nodes ',negative
'wait for available queue ',negative
'username ',negative
'make sure can store the worker tokens even creds are provided ',negative
'this bolt does not emit anything ',negative
'should only happen badly configured system ',negative
'ignores invalid usertopokey ',negative
'are cleared ',negative
'return false stop scan ',negative
'submit topology storm cluster ',negative
'prevent daemon log reads from pathing into worker logs ',negative
'totalnodeshared ',negative
'link and several other trident classes inherit from these classes are serialized out part the bolts and spouts topology often for each boltspout the topology the following are marked transient because they are never used after the topology created keeping them around just wastes space the serialized topology ',negative
'read once since the first file empty the spout should continue with file ',negative
'fail supervisor ',negative
'update blob interface ',negative
'default aways distributed but here local cluster being used ',negative
'this cheating bit since maxpollrecords would normally spread this across multiple polls ',negative
'spy object that tries mock the real object store ',negative
'should not show files outside log root ',negative
'workerheartbeats ',negative
'zeroout the half prevent accidental matches ',negative
'sleep bit between emits ensure that dont reach the cap too quickly since this spout used test time based windows ',negative
'transferred totals ',negative
'consume file ',negative
'test listkeys ',negative
'',negative
'congested contract much more quickly ',negative
'setup broker ',negative
'bolt aggregate ',negative
'memory requirement large enough that two executors can not fully assigned one node ',negative
'',negative
'now lets load setting gets and should still get valid map back ',negative
'disablelogincache indicates whether not use the logincache exclude from the keystring ',negative
'commands contains one more null value spout compiled with lower version stormkafkaclient ',negative
'namedloggerlevel ',negative
'note the queue has thread safe ',negative
'access foo make most recently used ',negative
'other scenario covers when maxseqnumber and nimbus seq number are equal ',negative
'compute the word counts the last two second window ',negative
'dispatches the right join method innerleftrightouter based the joininfojointype ',negative
'partition info does not change eventhub ',negative
'timestamp ',negative
'seconds ',negative
'add all fetched records the set failed records they are present failed set ',negative
'used ',negative
'create multiple copies test topology ',negative
'now need map them all back again ',negative
'now compile ',negative
'eventlogport ',negative
'sync assignmentsidinfo local ',negative
'ensure fields per tuple and null fields ',negative
'offset committed are failed maxpollrecords are emitted fail the last tuples only offset not failed advance time the failed tuples become ready for retry and check that the spout will emit retriable tuples for all the failed tuples that are within tuples the committed offset ',negative
'assume key the first field ',negative
'use redis for state persistence ',negative
'found existing token and not going expire any time soon dont bother adding new token ',negative
'spout notified that message returned for retrying was actually emitted hence remove from set and wait for its ack fail but still keep counts map retry again failure remove ack ',negative
'current version supports rpc heartbeat ',negative
'certain states might only accept onetuple keys those should just throw error ',negative
'set the size case are recovering already downloaded object ',negative
'find primary key from constructor ',negative
'sort executors based component constraints ',negative
'should ',negative
'config settings ',negative
'test launch topo together should able use either mem cpu resource due exact division ',negative
'file offset should zero since searcharchived false ',negative
'access ',negative
'the spout should now commit all the offsets since all offsets are either acked were missing when retrying ',negative
'servers should rotated before the old client removed from clientforserver race with getwriteclient could cause put back the map ',negative
'now test regular updateblob ',negative
'recancel current thread also interrupted ',negative
'also fail the last tuple from partition two since the failed tuple beyond the limit should not retried until earlier messages are acked ',negative
'test launch topo only both mem and cpu should exactly used ',negative
'field name does not match static field test matches dynamic field ',negative
'sleep prevent race conditions ',negative
'weak supervisor node ',negative
'default ordered ',negative
'ensure the loggers are configured the workerxml before trying use them here ',negative
'release lock file and check ',negative
'also put generic resource with value the resources list verify that doesnt affect the sorting ',negative
'offheapnode ',negative
'its newbyteoffset negative this normal are out bytes read from small file ',negative
'dont bother blocking full queue just drop metrics case cant keep ',negative
'update scenario and explain the code logic written here especially when nimbus crashes and comes after and before update ',negative
'means ',negative
'save the evicted keyvalue the database immediately ',negative
'should and ',negative
'localorshuffle ',negative
'the sym link are pointing ',negative
'cpu memory not set the values stored and ',negative
'initialize serverside sasl functionality havent yet which case are looking the first sasl message from the client ',negative
'resend status case prev notification was missed reordered ',negative
'slots will null while supervisor has been removed from cached supervisors ',negative
'just return the first metric meet ',negative
'boolean cache for local mode decision ',negative
'when authnid and authzid are not equal authnid attempting impersonate authzid ',negative
'kinesis spout kinesisconfig object ',negative
'spouts ',negative
'add new ports cached supervisor need modifiable set allow removing ports later ',negative
'binaryarg ',negative
'jms producer ',negative
'for failed message add failed set will retried otherwise ack remove from emitted either way ',negative
'there are trigger values earlier attempts this new batch emit pending triggers ',negative
'when ',negative
'this code logic covers the update scenarios when the nimbus goes down before syncing the blob nimbus and update happens seqnum for nimbus and maxseqnumber then next sequence number maxseqnumber ',negative
'should done now ',negative
'getters ',negative
'trigger the window ',negative
'offset offset offset offset offset offset offset offset ',negative
'non blocking call cannot emit destination immediately such tuples will added pendingemits argument ',negative
'renewal threads main loop exits from here thread will exit ',negative
'validate ',negative
'shutdownable mainmethods ',negative
'new supervisor cache ',negative
'need get acl from meta ',negative
'test scheduling new topology does not disturb other assignments unnecessarily ',negative
'windowtoemitted ',negative
'the encryption key must hexadecimal ',negative
'execute and process latency ',negative
'there should now maxpollrecords emitted all ',negative
'first just set the keys null then flag remove them beginning next commit when know the current and last value are both null ',negative
'without fragmentation the cluster would able schedule both topologies each node lets call each node with both topologies scheduled scheduled schedule the cluster blocks topologies measuring the time schedule the blocks the first middle and last blocks attempt schedule the following the last block has number scheduling failures due cluster fragmentation and its time dominated attempting evict topologies timing results for scheduling are noisy result multiple runs and use median values for firstblock and lastblock times somewhere statistician crying the ratio lastblock firstblock remains fairly constant took firstblock lastblock ratio firstblock lastblock ratio firstblock lastblock ratio took firstblock lastblock ratio firstblock lastblock ratio firstblock lastblock ratio took firstblock lastblock ratio firstblock lastblock ratio firstblock lastblock ratio took firstblock lastblock ratio firstblock lastblock ratio firstblock lastblock ratio took firstblock lastblock ratio firstblock lastblock ratio firstblock lastblock ratio choose the median value the values above ',negative
'find the number bytes with nonleading zeros ',negative
'check avoids multiple log msgs when idle loop ',negative
'schedule left over system tasks ',negative
'this doesnt follow symlinks which what want ',negative
'build put query ',negative
'window partitions ',negative
'take ownership stale lock ',negative
'are capturing exceptions thrown blitzers child threads into this data structure that can properly passfail this test the reason that blitzer doesnt report exceptions which known bug blitzer ',negative
'set size cleanup another one ',negative
'should not seek the paused partition ',negative
'verify digest rejected ',negative
'system bolt not part backpressure ',negative
'check just the one port ',negative
'add with current ',negative
'required ',negative
'value the metric ',negative
'overflowq size the time the last bpstatus was sent ',negative
'tests download topology blobs local mode topology without resources folder ',negative
'retry locking and verify ',negative
'default ',negative
'contents the key starts with nimbus host port information ',negative
'context not used the default implementation but included the interface case useful subclasses ',negative
'get more than one stateful operation need process the current group that have one stateful operation per stateful bolt ',negative
'nothing for conf now ',negative
'add our messages and verify metrics are recorded ',negative
'looks like usage might not supported ',negative
'ensure nimbus assigns topologies quickly possible ',negative
'put legacy values ',negative
'try get the topology conf from nimbus can reuse ',negative
'funcargs ',negative
'use instead for field delimiter ',negative
'complexity that linear scan treemap ',negative
'nextoffset the last offset from last batch ',negative
'spout aggregate ',negative
'use storms zookeeper servers not specified ',negative
'clean memory ',negative
'configs are present generic map and legacy the legacy values will overwritten ',negative
'for each isolated topology compute even distribution executors workers the number workers specified for the topology compute distribution workers machines determine host list slot topology executors iterate through hosts and machine good only running workers from one isolated topology all workers running match one the distributions executors for that topology matches one the workers blacklist the good hosts and remove those workers from the list need assigned workers otherwise unassign all other workers for isolated topologies assigned ',negative
'now check that the spout will not emit anything else since nothing has been committed ',negative
'',negative
'none ',negative
'the metrics store not critical the operation the cluster allow nimbus come ',negative
'check the credential our principal ',negative
'system bolt doesnt call reporterror ',negative
'join against diff stream compared ',negative
'parsed topology definition ',negative
'compile parameters ',negative
'check for all ports ',negative
'streaid indicates where tuple came from ',negative
'for the node dont know have another one unless look the contents ',negative
'workerhooks ',negative
'validate search stream ',negative
'debugoptions ',negative
'storm support launch workers older version the config comes from the older version replace the package name ',negative
'update the watermark this timestamp ',negative
'create spouts ',negative
'need wait until sasl channel also ready ',negative
'perf critical path would nice avoid iterator allocation here and below ',negative
'all column families ',negative
'generate some that has alot memory but little cpu ',negative
'',negative
'userspout jdbcbolt ',negative
'all things are from dependencies ',negative
'nothing ',negative
'test exist with nonexistent key ',negative
'testing whether acls are set worldeverything ',negative
'iterate through all executor heartbeats ',negative
'there are free slots that can take advantage now ',negative
'need more data ',negative
'port ',negative
'register call back for blobstore ',negative
'taskstart ',negative
'tests for case when subject null security turned and acls for the blob are set worldeverything ',negative
'restart ',negative
'performs projection the tuples based projectionfields ',negative
'are running should recover the blobs ',negative
'something happened and couldnt find the file ignore for now ',negative
'setup spout ',negative
'callback caller ',negative
'testing whether acls are set worldeverything here the acl should not contain worldeverything because the subject neither null nor empty the acl should however contain usereverything user needs have ',negative
'inputs ',negative
'make sure lines from each file were read all ',negative
'topology ',negative
'look for public instance variable ',negative
'need one large set all and then clean via lru ',negative
'all other cases check for the latest update sequence the blob the nimbus and assign the appropriate number check all are have same sequence number ',negative
'local worker heartbeat can null cause some errorexception ',negative
'because local mode its not separate process supervisor will register this case returns false then distributed mode ',negative
'compare contents files ',negative
'true this offsetmanager has made least one commit kafka ',negative
'minimum version supporting storm would ',negative
'assignedcpu ',negative
'sleep before cleaning files ',negative
'',negative
'state query added the existing stateful bolt ',negative
'attempts lookup the unique for string that may not exist yet returns ',negative
'only used for timedrotationpolicy ',negative
'bolt ',negative
'error occurs but assignment has not changed ',negative
'ensure there only the default stream when configuring the spout should safe ignore the parameter here ',negative
'batch played with tuples initially ',negative
'save try again later ',negative
'this was byte test ',negative
'pendingprepare has entries ',negative
'static state ',negative
'got least one getpulseresponse message ',negative
'overrides are disabled wont replace anything that already exists ',negative
'fall through purpose ',negative
'mock the supervisor failed supervisor ',negative
'restart topology with different topology ',negative
'set null and get the old value ',negative
'test with integer value ',negative
'schedule nimbus code sync thread sync code from other nimbuses ',negative
'assignment has changed ',negative
'pretend storm ',negative
'required required required ',negative
'now verify that when switched can recover ',negative
'doesnt manipulate tuples lists stuff that things like aggregating into cassandra cleaner dont need lists everywhere just store the single value there ',negative
'wrapper hold global and window averages ',negative
'start the part the log file are interested ',negative
'windows should set this false cause symlink compressed file doesnt work properly ',negative
'set closing true prevent any further reconnection attempts ',negative
'there are some different levels accuracy here and want deal with all them ',negative
'the spout should have emitted the tuple and must have committed before emit ',negative
'populate with existing assignments ',negative
'ignore not tupleimpl type faster than checking and then casting ',negative
'store can null during testing when mocking utils ',negative
'output the sum all the known counts for this key ',negative
'read the short field ',negative
'filesystem path the resource ',negative
'topohistory ',negative
'should rewrite this file move ',negative
'tests subclassing ',negative
'send response client ',negative
'get trigger values only they have more than zero ',negative
'all messages except the first acked message should have been emitted ',negative
'theres enough bytes the buffer read ',negative
'get host all assignable worker slots for nonblacklisted machines assigned not assigned will then have list machines that need assigned machine topology list list executors match each spec machine who has the right number workers free everything else that machine and assign those slots one topology time blacklist all machines who had production slots defined log isolated topologies who werent able get enough slots machines run default scheduler isolated topologies that didnt have enough slots nonisolated topologies remaining machines set blacklist what was initially ',negative
'noop prepare should have already been called ',negative
'demonstrate that the spout doesnt ack pending tuples when skipping compacted tuples the pending tuples should allowed finish normally ',negative
'failed get anything from artifactory try get from our local cache ',negative
'test when worker fails ras does not alter existing assignments healthy workers ',negative
'keep track how many times see each taskid ',negative
'make sure that have received least short ',negative
'previous code used this method generate the string ensure the two match ',negative
'should acked once ',negative
'window length ',negative
'need add empty string else nto added query param ',negative
'storm regression test verify that remote worker can handle many tasks one executor ',negative
'attempt schedule multiple copies different topologies topot and topot blocks without fragmentation possible schedule all topologies but fragmentation causes topologies not schedule for the last block ',negative
'two seconds tumbling window ',negative
'this verifies that partitions cant prevent each other from retrying tuples due the limit ',negative
'another thread could writing out the metadata cache the database ',negative
'duplicate case ',negative
'needed keep simplefileobject constructor happy ',negative
'constraints and spreads ',negative
'remove executor details assigned the failed worker ',negative
'',negative
'executorinfo ',negative
'otherwise dont bother them ',negative
'aggregate matching metrics over bucket timeframes well process starting with the longest bucket the metric for this does not exist dont have ',negative
'this the topology page know the user authorized ',negative
'',negative
'the offset the last available message ',negative
'call firemessageread since the client allowed perform this request the clients request will now proceed the next pipeline component namely stormclienthandler ',negative
'with exhibitor ',negative
'creating nimbus hosts containing latest version blob ',negative
'jsonconf ',negative
'check lock file presence ',negative
'interval which commit offsets milliseconds ',negative
'remove the slot from the existing assignments ',negative
'spout with parallel instances ',negative
'the topology zookeeper authentication configuration unset ',negative
'just ignore these for now are going throw away anyways ',negative
'holds remaining streams ',negative
'class that has the logic handle tuple failure ',negative
'sets link solrfieldsmapper use the default solr collection there one defined ',negative
'adds the serialized and base file the credentials map string with the filename the key ',negative
'works emitting null the collector since the planner knows this add node with new output fields just passes the tuple forward ',negative
'get existing tuples and pendingunsuccessful triggers for this and add them windowmanager ',negative
'consumer rebalance listener the same thread the caller ',negative
'truststore file assume the truststore the keystore ',negative
'needed could make config for update thread pool size ',negative
'updating blacklist file periodically with random words ',negative
'all failed events are put toresend which sorted events offset ',negative
'anonymous user ',negative
'error ',negative
'optimizes relnode with ruleset ',negative
'kill newly submit ',negative
'task under backpressure initially ',negative
'enable metrics ',negative
'partitions evicted window state ',negative
'for each owner get resources configs and aggregate ',negative
'',negative
'get mapping components executors ',negative
'indexed ',negative
'configs topo parallelism ',negative
'next scheduled refresh sooner than now mintimebeforelogin ',negative
'fileoffset one past last scanned file ',negative
'this partition was previously assigned the consumer position shouldnt change ',negative
'catch any runtime exceptions caused eviction ',negative
'this ignored javac currently but usejavautilzip should ',negative
'required required required optional optional optional optional optional optional optional ',negative
'make sure the property was actually set ',negative
'boltspecific configuration for windowed bolts specify the sliding interval time duration ',negative
'sanity check the provided test data ',negative
'split the sentences words ',negative
'otherwise poll see any new event was scheduled this essence the response time for detecting any new event schedulings when there are scheduled events ',negative
'lock file has been updated since last time then leave this lock file alone ',negative
'records that have been polled and are queued emitted the nexttuple call one record emitted per nexttuple ',negative
'meta ',negative
'sends the same tuple list scoredpredicted values all the declared streams ',negative
'always check recq acking enabled ',negative
'package access for unit tests ',negative
'jcqueue spoutq new jcqueuespoutq jcqueue ackq new jcqueueackq final ackingproducer ackingproducer new ackq final acker acker new ackerackq spoutq acker ',negative
'task ids experiencing can null task ids longer experiencing can null ',negative
'instantiate the hdfsbolt ',negative
'this method should return sequential numbers starting ',negative
'statespouts ',negative
'have copied and pasted some the needed mainmethods here with few changes logging ',negative
'gsidtoinputstats ',negative
'only place fall though the loop over again ',negative
'thread died before could get the info skip ',negative
'should not have flushed file system yet ',negative
'make sure the worker down before try shoot any child processes ',negative
'not assign the highest sequence number ',negative
'slotsused origrequest ',negative
'max heap size for worker used topology ',negative
'tmpdir will handled separately ',negative
'that store passed windowstateupdater remove them after committing the batch ',negative
'this arbitrary choice make the result consistent with calculatemin any value would valid here becase there are nonzero resources the total set resources were trying average values ',negative
'the component system component and are hiding them keep going ',negative
'errors ',negative
'metastoreuri ',negative
'ack the tuple and commit since the tuple more than max poll records behind the most recent emitted tuple the consumer wont catch this poll ',negative
'load drivermanager first avoid any race condition between drivermanager static initialization block and specific driver classs static initialization block phoenixdriver should take this workaround since prepare method synchronized but worker can initialize multiple abstractjdbcbolt instances and they would make race condition just need ensure that drivermanager class always initialized earlier than provider below line should called first than initializing provider ',negative
'class directinserter ',negative
'keys sorted descending order ',negative
'unpin the last partition ',negative
'now check for autocreds that are missing from the command line but only the command line used ',negative
'store inprocess triggers batch store for batch retries for any failures ',negative
'deleting this early does not hurt anything ',negative
'request from hosts that are not authorized should rejected ',negative
'ignoredexpected ',negative
'the user from the token bob verify that the name was set correctly ',negative
'package level for unit tests ',negative
'case ',negative
'topology ',negative
'gets database ',negative
'server ',negative
'hive principal stormhivewitzencom storm hive keytab hivemetastoreuris ',negative
'sharedmemonheap ',negative
'ignore file names config ',negative
'normal create update sync scenario returns the greatest sequence number the set ',negative
'for jackson ',negative
'heartbeat ensure its longer stale and read back the heartbeat data ',negative
'this cannot happen since were using standard charset ',negative
'how often poll exhibitor cluster millis ',negative
'intended behavior ',negative
'expected ',negative
'index null memory cpu ',negative
'should the topology active inactive ',negative
'get mapping execs components ',negative
'play tuple ',negative
'expect the bolt log exactly one decorated line per emit ',negative
'this must defensively copied because bolt probably has only one rotation policy object ',negative
'test that the setloggerlevel function was not called ',negative
'required optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional ',negative
'',negative
'cluster forgets about its previous status scheduled just leave ',negative
'try again empty assignment has been nulled ',negative
'lastly the default servlet for root content always needed satisfy servlet spec ',negative
'',negative
'read lines file ',negative
'object associated with json field already json ',negative
'required required required required required ',negative
'check component declared for spreading ',negative
'first need some configs ',negative
'trigger occurred create aggregation and keep them store ',negative
'check can run topology with that version storm ',negative
'repartitioning involved does perpartition aggregate key before emitting the results downstream ',negative
'deletes metadata strings before the provided timestamp ',negative
'assume that within the minimum congestion still fine not congested grow but slowly ',negative
'the worker orphan the worker that fails the healthy worker ',negative
'same txid can prepared again but the next txid cannot prepared when previous one not committed yet ',negative
'topology executor ids component stats ',negative
'merge contents config into topology config ',negative
'convenience method for registering combinedmetric ',negative
'map from topology set sets executors ',negative
'make key list download ',negative
'totalworkers ',negative
'all tuples acked ',negative
'update the size the objects ',negative
'should ',negative
'completelatencyms ',negative
'copy the data ',negative
'get metric name ',negative
'there are remote outbound tasks dont start the thread ',negative
'download missing blobs from potential nimbodes ',negative
'just ignore any errorexception ',negative
'avoid allocating spoutackinfo obj not necessary ',negative
'not going timeout for while ',negative
'try emit all messages ensure only are emitted ',negative
'only try reading once ',negative
'jpmml evaluator ',negative
'for each partition the spout allowed retry all tuples between the committed offset and ahead not allowed retry tuples past that limit this makes the actual limit per partition maxpollrecords reached the tuple the limit the earliest retriable tuple the spout tuple below the limit and receives full maxpollrecords tuples the poll ',negative
'submit topology ',negative
'poll metrics every minute then kill topology after specified duration ',negative
'validate least two agg level none metrics exist ',negative
'allow the failed record retry ',negative
'listens for all metrics dumps them text configured hostport use add this your topologys configuration java hostport edit the stormyaml config file yaml class argument examplecom parallelismhint ',negative
'for now not make transaction when removing topology assignment from local overdue assignment may left local disk should check the local disk assignment valid when initializing topology files does not exist the workerpossibly alive will reassigned timedout topology files exist but the topology invalid just let supervisor make sync topology files exist and topology files valid recover the container ',negative
'node under which commit the sequence number messages ',negative
'just few polls check that nothing more emitted ',negative
'topo should evicted since its been the longest ',negative
'ack few ',negative
'',negative
'clean the profiler actions that are not being processed ',negative
'cassandra doesnt actually shut down until jvm shutdown need wait for that first ',negative
'authenticate removed after authentication completes ',negative
'get sorted list unassigned executors based number constraints ',negative
'now schedule gpu but with the simple topology place ',negative
'dont have anything ',negative
'targetsize bytes ',negative
'spout settings ',negative
'defaults ',negative
'used for reporting used ports when heartbeating ',negative
'any change this code must serializable compatible there will problems ',negative
'aggregate the count ',negative
'noop ',negative
'make sure the timestamp the metadata has the latest time ',negative
'some components might have different resource configs ',negative
'iterate again ',negative
'just quit ',negative
'additional safety check make sure that topologysubmitter going valid value ',negative
'root password ',negative
'dont exit not running unless error ',negative
'unknown should only happen during compilation some unit tests ',negative
'convert targets json serializable format ',negative
'stream words ',negative
'insert child inbetween parent and its current child nodes ',negative
'shuffle ',negative
'check that only the tuple the currently assigned partition retried ',negative
'optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional ',negative
'just ignore the exception ',negative
'from localizertest ',negative
'execstats ',negative
'first should have expired due expire events threshold ',negative
'seconds passed timed out ',negative
'topic compaction enabled kafka sometimes need commit past gap deleted offsets since the kafka consumer should return offsets order can assume that message acked then any prior message will have been emitted least once see acked message and some the offsets preceding were not emitted they must have been compacted away and should skipped ',negative
'since made sys components hidden the component map empty for this worker ',negative
'nodehost ',negative
'methods for validating confs ',negative
'also add support for worker tokens ',negative
'build phase segregate tuples the window into streams first streams tuples into probe rest into hashmaps hashedinputs ',negative
'now check for overlap the node ',negative
'the window partition that holds the events ',negative
'number retry attempts for ',negative
'same txid can committed again but the txid committed must the last prepared one ',negative
'make every attempt sync the data have cant done then kill the bolt with runtime exception the filesystem presumably very bad state ',negative
'fall through not supported ',negative
'when tuple tracking enabled the spout must not commit acked tuples atmostonce mode because they were committed before being emitted ',negative
'avoid locking will through the map twice should small probably not big deal ',negative
'emit the count for the words that occurred atleast five times the last two seconds ',negative
'for backwards compatability ',negative
'nothing expires until threshold hit ',negative
'default seconds cache the size cheap ',negative
'ack all emitted messages and commit them ',negative
'normally this noop ',negative
'perf critical path dont use iterators ',negative
'nothing ',negative
'read the partitions transaction ids and offsets from the old stormkafka user path ',negative
'max number events cached memory ',negative
'get document ',negative
'required required optional optional ',negative
'read from state store not found use startingoffset ',negative
'validate args ',negative
'the match this candidate offset failed start over with the next candidate byte from the buffer ',negative
'ensure always same order for registrations with treemap ',negative
'initialize spout using the same populated data same kafkaunitrule ',negative
'refresh the ticket granting ticket tgt periodically how often refresh determined the tgts existing expiry date and the configured for testing and development you can decrease the interval expiration tickets for example minutes running ',negative
'populate worker comp assignments ',negative
'non error scenarios for the azure data lake store file system adl the output stream must closed before the file associated with deleted for adlfs deleting the file also removes any handles the file hence outclose will fail ',negative
'wildcard file ',negative
'groups username command return nonconsistent across different unixes ',negative
'cached topology executor ids used for deciding timeout workers heartbeatscache ',negative
'create windowed stream five seconds duration ',negative
'spout should ack failed messages after they hit the retry limit ',negative
'take ones complement ',negative
'environment variable substitution ',negative
'dont modify the original ',negative
'nothing ',negative
'the frequency reporting ',negative
'sleep for seconds ',negative
'early detectionearly fail ',negative
'ensures overflowcount overflowlimit set disables overflow limiting ',negative
'initialize the hashedinputs data structure ',negative
'the current lat and count buckets are protected different lock from the other buckets this reduce the lock contention when doing complex calculations never grab the instance object lock ',negative
'',negative
'found ',negative
'emit and fail the same tuple until weve reached retry limit ',negative
'dont prorate anything all approximate extra bucket not that bad ',negative
'parts empty for cgroups else what mapped that are looking for ',negative
'wrapping makes mutable ',negative
'test deep equal ',negative
'save the current user help with recovery ',negative
'taskid jcqueue initialized after local executors are initialized ',negative
'shard iterator not present for this message get ',negative
'this triggers cannot convert null int ',negative
'this also tracks how many times worker transitioning out state ',negative
'storm blobstore update file blacklisttxt key ',negative
'note that delete the return value ',negative
'waitsecs ',negative
'get the list keys from blobstore ',negative
'the following are required were defining core storm topology dag yaml etc ',negative
'use this weird wrapper pattern temporarily for mocking clojure test ',negative
'archive dir config ',negative
'remove ',negative
'track resources user resourceset concurrenthashmap explicitly used everywhere this class because uses locks guarantee atomicity for compute and computeifabsent where concurrentmap allows for retry the function passed and would require the function have side effects ',negative
'write the mocking backwards the actual method not called the spy object ',negative
'user will decide which topologies are run and which ones are not ',negative
'topo evicted lowest priority ',negative
'the retry service returns message that not failed set then ignore should never happen ',negative
'the key the map the worker and the value the corresponding workerslot object ',negative
'specified via boltselect used declaring output fields protected string fieldnames xyz format stream name used for naming output fields ',negative
'check lock file gone ',negative
'map from batchgroupid coordspec ',negative
'test another topology getting blob with updated version should update version now ',negative
'make sure all workers scheduled rack the favored nodes would have put different rack but because that rack does not have free space run the topology falls back this rack ',negative
'base for exponential function seconds for retrying for second third and failures ',negative
'topologyid workerid executors ',negative
'timestamp decide when commit again ',negative
'nimbus being newly elected leader change recurring pattern addresses these problems ',negative
'authorize ',negative
'wait for locks expire ',negative
'should have created blobdir ',negative
'free slot ',negative
'test launch topo and they together request little more mem than available one the topos will not ',negative
'executemsavg ',negative
'customserialized ',negative
'class pair ',negative
'try modify the list which should fail ',negative
'default batch size ',negative
'add default user acl when only empty user acl not present ',negative
'totalspoutlag ',negative
'stats ',negative
'make the consumer return single message for each partition ',negative
'adding metadata avoid null pointer exception ',negative
'add endofbatch indicator ',negative
'works out ',negative
'rebalance ',negative
'wait and check for expiry again ',negative
'the maximum number states that will searched looking for solution the constraint solver strategy ',negative
'length parts should greater than ',negative
'call prepare with our available taskids ',negative
'consume the iterator traversing and thus emptying ',negative
'stream cannot null ',negative
'taken care finally block ',negative
'submit topology wait for few min and terminate ',negative
'since user jerry has enough resource guarantee ',negative
'abandon file ',negative
'this filter makes sure receive only key row but not values associated with those rows ',negative
'without exhibitor ',negative
'for backward compatibility ',negative
'calc cidsidinputstats ',negative
'emit message each partition and revoke the first partition ',negative
'acls are not set for the blob default ',negative
'metrics related ',negative
'required required required optional optional optional optional optional optional optional optional ',negative
'imeta ',negative
'routine level ',negative
'this should not happen because checked for public isfieldallowed ',negative
'bolts ',negative
'container collection tuples ',negative
'the limit should not enforced since only meaningful atleastonce mode ',negative
'need more data ',negative
'while ',negative
'spouts that use them only one uses they can created inline with the add ',negative
'havent found string keep searching ',negative
'prevent issue when the implementation fieldnames not serializable returns calcite pair which not serializable ',negative
'the jsonconf populated topologybuilder ',negative
'group node ',negative
'acl ',negative
'heartbeat here that worker process dies this fails its important that worker heartbeat supervisor asap that supervisor knows ',negative
'uptimesecs ',negative
'hdfs keytabprincipal have been supplied login otherwise assume they are logged already running insecure hdfs ',negative
'this directory file ',negative
'producer ',negative
'might need set the number acker executors and eventlogger executors the estimated number workers ',negative
'hbase has some entries ',negative
'will adjust weights based off the minimum load ',negative
'tried all the slots and none them worked ',negative
'need block the task run the executor safe run even after metrics are closed ',negative
'componentdebug ',negative
'should throw ',negative
'nimbus admin ',negative
'component overrides ',negative
'let add the kerbclientprinc and kerbticket need clone the ticket because assumes tgt unique for each subject sharing tgt with multiple subjects can cause expired tgt never refresh ',negative
'stop emitting certain point because log rolling breaks the tests ',negative
'shard iterator corresponding position shard for new messages ',negative
'level longer idling with threadsleep ',negative
'this does not have atomic worst case update when one not needed ',negative
'the windows should persisted state ',negative
'define our taskids and loads ',negative
'test for supervisor admin ',negative
'read another line and see another log entry was made ',negative
'control cpu usage ',negative
'emitted offsets list ',negative
'there acked tupled after compaction gap the spout should commit immediately ',negative
'main ',negative
'commit solr and ack according the commit strategy ',negative
'spouts ',negative
'even though normally bolts not need care about thread safety this particular bolt different maintains static field that prepopulated before the topology starts written into the topology and then read from after the topology completed all this potentially different threads ',negative
'activate deactivate close declare outputs ',negative
'this will only ever grow need worry about falling off the end ',negative
'',negative
'update streamstate based stateupdates ',negative
'equivalent update command command line ',negative
'the partition revocation hook must called before the new partitions are assigned the consumer ',negative
'new topology needs scheduled ',negative
'small control algorithm adjust the amount time that sleep make more accurate ',negative
'cached globalstreamid ',negative
'special cases for storm ',negative
'',negative
'partition state path ',negative
'everything should fit single slot ',negative
'last fetched sequence number corresponding position shard ',negative
'sorted acked sequence numbers needed figure out what sequence number can committed ',negative
'check that null meta makes the spout seek latest and that the returned meta correct ',negative
'tuple ',negative
'todo jsonify stormtopology the minimum should send source info ',negative
'timer ',negative
'dont allow topoconf override various clusterspecific properties specifically adding the cluster settings the topoconf here will make sure these settings also override the subsequently generated conf picked locally the classpath will dealing with confs the submitted topoconf created here the combined classpath conf with the topoconf added top the nimbus conf with conf above added top first forcing the topology conf contain the nimbus settings guarantee all three confs ',negative
'switch the new assignment even localization hasnt completed empty state ',negative
'calcite ensures that the value structurized the table definition hence can use index directly elaborate table bar defined integer name varchar deptid integer and query like insert into bar select name from foo executed calcite makes the projection name null the value before insert ',negative
'clearing assignments ',negative
'with earliest the spout should also resume where left off rather than restart the earliest offset ',negative
'load fully loaded load fully loaded ',negative
'emit results ',negative
'setup timer for commit elapse time tracking ',negative
'this additional check and download for nimbus high availability case you have more than one nimbus ',negative
'write the tuple ',negative
'get the most recent artifact string and then parse the yaml ',negative
'proposedrefresh too far the future its after ticket expires simply return now ',negative
'cached curatorframework mainly used for blobstore ',negative
'construct message containing the sasl response and send the server ',negative
'now check that the spout will emit another messages ',negative
'processlatencyms ',negative
'thread safety assumes collectoremit calls are externally synchronized needed ',negative
'test for user having write admin privileges change replication blob ',negative
'message was acked after being retried clear the state for that message ',negative
'reset commit timer such that commit happens next call nexttuple ',negative
'the retry schedules for two messages should unrelated ',negative
'boltspecific configuration for windowed bolts specify the time interval for generating watermark events watermark event tracks the progress time when tuple timestamp used this config effective only link specified ',negative
'group field ',negative
'stream words emitted the queryspout used the keys query the state ',negative
'add supervisors that might have crashed but workers are still alive ',negative
'put owner stormbaselist mapping ownertobasesmap this owner the input parameter null add all the owners with stormbase and guarantees else add only this owner the input paramter the map ',negative
'the system low memory cannot kind and need shoot something ',negative
'the child processes typically exit sec mins later they are still around something wrong ',negative
'only log the leader has changed not interesting otherwise ',negative
'race happened and probably not running ',negative
'required required optional ',negative
'authorize client allowed dorequest and only the client ',negative
'this spout added test purpose just failing fast doesnt hurt much ',negative
'minimum version supporting storm would ',negative
'advance the time and replay the failed tuple ',negative
'boltspecific configuration for windowed bolts specify the sliding interval count number tuples ',negative
'only need keep track acked tuples commits kafka are controlled tuple acks which happens only for atleastonce processing semantics ',negative
'not close this inputstream method will used from jetty server ',negative
'partition ids ',negative
'the current assignment already running new assignment will never promoted currassignment because timer not being compared equals equivalent meaning newassignment always equals currassignment ',negative
'this happens the min too small ',negative
'config ',negative
'should not show files outside worker log root ',negative
'valid delete before whats been committed since those batches will never accessed again ',negative
'client should not sending otherthansasl messages before saslserverhandler has removed itself from the pipeline such nonsasl requests will denied the authorize channel handler the next handler upstream the server pipeline sasl authentication has not completed ',negative
'window system state ',negative
'used internally merge values groupbykeyandwindow ',negative
'',negative
'register the newly established channel ',negative
'when storetuplesinstore false then the given windowstorefactory only used store triggers and ',negative
'obtain serializer object ',negative
'verify correct unwrapping partitions and delegation assignment ',negative
'meter declared here can registered any daemon and currently used supervisor ',negative
'send flush tuple all local executors ',negative
'include the topology name worker port the file name that multiple event loggers can log independently ',negative
'another kafkaspout instance this topology already committed therefore does not apply ',negative
'adjust the divisor for the average account for any skipped resources those where the total was ',negative
'bolt overrides ',negative
'rate ',negative
'schedule the heartbeat one thread pool ',negative
'ensure the commit timer has expired ',negative
'didnt get the group just return empty list ',negative
'parts for cgroup else maps hierarchy proccgroups ',negative
'yes are putting config that not the same type pulled out ',negative
'replicationfactor ',negative
'cleanup ',negative
'max number window events memory ',negative
'non blocking returns count how many inserts succeeded ',negative
'concurrent deletion only one thread should succeed ',negative
'now lets update but not advance time should get old map again ',negative
'remove the unneeded entries from the graph want keep all our nodes and the nodes that they are connected directly parents and children ',negative
'have enough resources now ',negative
'dont emit anything allow configured spout wait strategy kick ',negative
'was scheduled for retry and reemitted remove from schedule ',negative
'onheap ',negative
'filter configured allow anyone ',negative
'setup spout with mock consumer can get the rebalance listener ',negative
'note variable used lambda expression should final effectively final will cause compilation error and variable type should implement the serializable interface isnt primitive type will cause not serializable exception ',negative
'hosts this user authorized impersonate from ',negative
'worker crashes the states all workers are rolled back and initstate message sent across the topology that crashed workers can initialize their state the bolts that have their state already initialized need not reinitialized ',negative
'enable ',negative
'note that sometimes the tuples active may less than maxspoutpending maxspoutpending active acked there wont commit for because isnt committed yet and there wont batch for because theres maxspoutpending active ',negative
'multivalue field split non default token match dynamic fields the form txt this field wont indexed solr ',negative
'map consisting all workers the node ',negative
'had timeout but the timeout longer active ',negative
'sample fielddescriptor streamxyz ',negative
'this odd case for rolling upgrade where the user the old assignment may null but not the new one although all other ways they are the same this happens want use the assignment with the owner ',negative
'dont override the host name everything looks like nimbus ',negative
'',negative
'this also helpful optimization that state implementations dont need manually ',negative
'sigar uses jni and does not work local mode ',negative
'for mesos this hostnametopologyid ',negative
'writablebytechannel channel which implements closeable hence although declared autocloseable superclose here should only throws ioexception rethrow conform the signature ',negative
'retire them ',negative
'create factory class ',negative
'assignmentid ',negative
'assignedmemoffheap ',negative
'topology with multiple spouts ',negative
'populate metric values using the provided key ',negative
'done separately like with setting the ',negative
'since the largest unpinned entry before loaded ',negative
'commits offsets during deactivation ',negative
'reason try execute previous attempt than weve already seen ',negative
'pipeline component ',negative
'should have been reemitted ',negative
'',negative
'add the event logger details ',negative
'support topologies older version run might have loose the constraints that the configs older version can pass the validation ',negative
'there race backpressure too ',negative
'free the error stream buffer ',negative
'name ',negative
'partitioned example case emitter task receives later transaction than its emitted far when sees the earlier txid should know emit nothing ',negative
'some times bolt spout will have some memory that shared between the instances these are typically caches but could anything like static database that memory mapped into the processes these can declared separately and added the bolts and ',negative
'creates mongouri from the given string ',negative
'verify that only committed the message the assigned partition ',negative
'',negative
'',negative
'this can happen when topology first coming its thrown the blobstore code ',negative
'fall back string which already set ',negative
'there can more then one line cgroups are mounted more then one place but assume the first good enough ',negative
'the resource that not used should count being used ',negative
'could block ',negative
'one tuple and one rotation should yield one file with data ',negative
'null acks every tuple ',negative
'explicit delete for ephemeral node ensure this session creates the entry ',negative
'the spout must respect even some tuples have been acked but not committed ',negative
'projection ',negative
'search all metadata strings ',negative
'this should never happen because only the primary nimbus active but just case declare the race safe even lose ',negative
'should remove the blob since cache size set really small ',negative
'newreader true and tuple null then empty reader ',negative
'populate user password map with jaas configuration entries from the server section usernames are distinguished from other options prefixing the username with user prefix ',negative
'read remaining lines file then ensure lock gone ',negative
'havent received the entire object yet return and wait for more bytes ',negative
'',negative
'schedule the simple topology first ',negative
'first offset this batch ',negative
'this tuple should removed from emitted only inside the ack method this ensure that the offsetmanager for that topicpartition updated and allows commit progression ',negative
'add element and bar should drop out ',negative
'below calls shouldnt propagate any exceptions ',negative
'private string timestampfield ',negative
'need able lookup bolts then switch based ',negative
'parse the output clear the input stream buffer ',negative
'treat like jar ',negative
'this very private and does not need exposed ',negative
'fetch records from kinesis starting sequence number for message passed argument any other messages fetched and are the failed queue will also ',negative
'its possible for target have multiple tasks reads multiple sources ',negative
'tests for case when subject null security turned and ',negative
'make sure samplingpct within bounds ',negative
'will fail since doesnt implement extend dnstoswitchmapping ',negative
'port range ',negative
'try grab another lock while dir locked ',negative
'schedule tasks that are not part components returned from topologygetspout ',negative
'function call when timer killed ',negative
'setup const spout ',negative
'now schedule all the topologies that need scheduled ',negative
'critical path dont use iterators ',negative
'good ',negative
'this default ensures things expire most past the expiration time ',negative
'this should the load metrics there will usually only one message but there are multiple only process the latest one ',negative
'should remove the second blob first ',negative
'get close enough ',negative
'comma separated offsets ',negative
'close file and retry creation ',negative
'track serialized size messages ',negative
'map the value ',negative
'string does not exist ',negative
'sync the filesystem after every tuples ',negative
'for kryo compatibility ',negative
'log median ratios for different strategies ',negative
'grouping assignment node see the nodes diff then notify nodessupervisors synchronize its owned assignment because the number existing assignments small for every scheduling round ',negative
'metric value ',negative
'removed since that what going trigger the retry for cleanup ',negative
'level waiting ',negative
'uncompressfalse ',negative
'this partition was previously assigned this spout ',negative
'add new records kafka and check that the next batch contains these records ',negative
'scheduler histogram ',negative
'precondition the new and current assignments must equivalent ',negative
'submit storm cluster ',negative
'for example have three nodessupervisor supervisor supervisor cluster slots before sort supervisor supervisor supervisor supervisor supervisor supervisor supervisor supervisor supervisor slots after sort supervisor supervisor supervisor supervisor supervisor supervisor supervisor supervisor supervisor ',negative
'ignore ',negative
'this shouldnt throw check because nothing configured yet ',negative
'drain element and ensure relieved trypublish succeeds ',negative
'save the private worker key away can test too ',negative
'found spout spout ',negative
'taskidindexingbase queue list all recvqs local this worker ',negative
'spoutobject ',negative
'the short term the goal not shoot anyone unless really need the heap should limit the memory usage most cases reasonable amount someone using way more than they requested this bug and should not allow ',negative
'for native protocol below all variables must bound with native protocol above variables can left unset which case they will ignored server side tombstones will generated ',negative
'location the file the artifactory archive also used name file cache ',negative
'int short ',negative
'migrate the coordinator currtx currattempts and meta directories the new spout expects the list topic partitions coordinator meta ',negative
'implementations ',negative
'emit messages and fail all them then ensure that the spout will retry them when the retry backoff has passed ',negative
'generate sasl response but only actually send the response ',negative
'overloading the readint method accomodate subject order check for authorization security turned ',negative
'get and set the start time before getting current time order avoid potential race with the gauge ',negative
'make sure dont process too frequently ',negative
'fields ',negative
'hadoop ',negative
'boltmsgqueue should have least one entry the moment ',negative
'intermediate bolt subscribes jms spout anchors tuples and autoacks ',negative
'map track number failures for each kinesis message that failed ',negative
'transform the stream words stream word pairs ',negative
'yes this should topo name but makes this simpler ',negative
'create transport factory that will invoke our auth callback for digest ',negative
'for the given processor node received punctuation from all tasks its parent windowed streams ',negative
'test for nimbus admin ',negative
'max lag ',negative
'bolt stats ',negative
'statespoutobject ',negative
'database ',negative
'emit second batch ',negative
'when there input field then the whole tuple considered for comparison ',negative
'for each executor nodeport pair ',negative
'publish retained message the broker ',negative
'this distinguish from transactionattempt ',negative
'just and try delete the others ',negative
'created lets chmod properly ',negative
'stop searching the message known ready for retry ',negative
'daemon common mainmethods ',negative
'bobby has guarantee topo and topo evicted ',negative
'validate search topology and executor ',negative
'hard coded max number states search ',negative
'consumer sets topology that reads the given kafka spouts and logs the received messages ',negative
'this support things like persisting off drpc stream which inherently unreliable and wont have attempt ',negative
'convert nodeport nodeinfo again ',negative
'effectively disable commits based time ',negative
'current and version not match roll back the version file match what current pointing ',negative
'any other failure result the assumption that the strategy set the status ',negative
'shutdownable via ',negative
'this since concat null string will actually concat null which not the expected ',negative
'storm config ',negative
'test for replication using supervisor access ',negative
'its possible string used multiple types metadata strings ',negative
'this avoids race condition with canceltimer ',negative
'resources missing from used are using none that resource ',negative
'idx index the type this field the fieldtype list ',negative
'build get query ',negative
'delete and all tables ',negative
'acls for the blob are set default empty acl list only for localfsblobstore ',negative
'',negative
'the output field the spout lambda provided the boltmessagefield that this gets written out the message the kafka topic the tuples have key field the messages are written kafka without key ',negative
'memfree buffers cached memfree buffers cached ',negative
'nothing ',negative
'client ',negative
'right now this only used for sending metrics nimbus but may want combine with the heartbeattimer some point ',negative
'default cache size converted bytes ',negative
'should never happened since apply uuid ',negative
'checkpoint spout should been added ',negative
'locate expired lock files any try take ownership oldest lock first ',negative
'this may may not reported depending when process exits ',negative
'sort available slots size from large small ',negative
'case plugin icontext class ',negative
'able delete the blob without checking metas acl skip checking everything and continue deleting local files ',negative
'ignored cgroups not setup dont anything with ',negative
'the spout must reemit failed messages waiting for retry even not allowed poll for new messages due being exceeded ',negative
'check see have enough slots before trying get them ',negative
'the supervisor ',negative
'off blobstore and get assume dir passed exists and has correct permission ',negative
'share some common metadata strings validate they not get deleted ',negative
'simulate worker loss ',negative
'topology with two unconnected partitions ',negative
'inputfields can equal outfields but multiple aggregators cannot have intersection outfields ',negative
'run only once ',negative
'acknowledge all changing blobs futures ',negative
'the resources are already normalized ',negative
'used recognize the pattern some meta files worker log directory ',negative
'name ',negative
'just output the word value with count the hbasebolt will handle incrementing the counter ',negative
'have not moved java worker yet ',negative
'update the latest timestamp and add the string cache ',negative
'schedule first block ',negative
'close the state force flush ',negative
'when partitions are reassigned the spout should seek with the first poll offset strategy for new partitions previously assigned partitions should left alone since the spout keeps the emitted and acked state for those ',negative
'offheapworker ',negative
'jms topic provider ',negative
'class dirlockingthread ',negative
'still need return the first pending list ',negative
'timestamp used for sharditeratortype attimestamp can null ',negative
'get factory class name ',negative
'blocking call that can interrupted using threadinterrupt ',negative
'download updated blobs from potential nimbodes ',negative
'consumer ',negative
'tell cassandra where the configuration files are use the test configuration file ',negative
'maximum uncommitted records count has reached dont emit any new records and return ',negative
'min ',negative
'check lock creationdeletion and contents ',negative
'always have space between ',negative
'windowstate table should already created with cftuples column ',negative
'archive passed must contain symlink named tmptestsymlink not zip file ',negative
'bolt ',negative
'topologies ',negative
'join the squares and the cubes stream within the window the values the squares stream having the same key that the cubes stream within the window will joined together ',negative
'acquire lock file and verify worked ',negative
'scheduling changed ',negative
'please pick small artifact which has small transitive dependency and lets mark ignore want run test even without internet maven central often not stable ',negative
'last evaluated and last expired message ids per task stream source taskid streamid ',negative
'for some reasons can not get supervisor port info supervisor shutdown just skip for this scheduling round ',negative
'need set after setting memorylimitinbytes error might occur ',negative
'cpu ',negative
'required required optional optional optional optional optional optional optional optional optional optional optional optional optional optional ',negative
'save little typing ',negative
'',negative
'the events should put window when the first watermark received ',negative
'change this template choose tools templates and open the template the editor ',negative
'only one the multireducers will receive the tuples ',negative
'merge stdout and stderr ',negative
'capacity for spout ',negative
'this spout owns partitions and ',negative
'cpuusage ',negative
'wait for the process finish and check the exit code ',negative
'the old trident kafka spout always returns true like this ',negative
'utility mainmethods ',negative
'late tuple stream ',negative
'when tuple tracking enabled the spout must not replay tuples guarantee mode ',negative
'memory the constraining resource ',negative
'tuple values are mapped with metric timestamp value map tagktagv respectively ',negative
'adds addressedtuple destination not full else adds pendingemits its not null ',negative
'then facing backpressure ',negative
'hidden sys component ',negative
'sleep bit avoid hogging the cpu ',negative
'the position behind the committed offset this can happen some cases message failed lots more than maxpollrecords later messages were acked and the failed message then gets acked the consumer may only part way through catching where was when went back retry the failed tuple skip the consumer forward the committed offset ',negative
'for ',negative
'info ',negative
'jprofilestart not used when you see jprofilestop start profiling and save away stop when timeout happens ',negative
'register metrics ',negative
'serverport ',negative
'try maintain rolling upgrade compatible with releases ',negative
'extract the field from tuple field may nested field xyz ',negative
'try append open file ',negative
'until ctrlc ',negative
'for the topology which wants rebalance specified the scratchtopoid exclude its assignment meaning that all the slots occupied its assignment will treated free slot the scheduler code ',negative
'not blacklist then add and set the resume time according config ',negative
'first topology should get evicted for higher priority lower value second topology successfully schedule ',negative
'strong supervisor node ',negative
'topology state transitions ',negative
'all events since last clear ',negative
'keys ',negative
'deny unsupported operations ',negative
'flush disk ',negative
'events were found the previous window interval scan through the events the queue find the next window intervals based event ',negative
'null then use zookeeper used storm ',negative
'the other tuples are used reset the first tuples timeout ',negative
'builds json list ',negative
'this store used only for storing triggered aggregated results but not tuples storetuplesinstore set false int below call ',negative
'graph with kinds nodes operation partition spout all operations have finishbatch and can optionally committers ',negative
'emit all remaining messages failed tuples retry immediately with current configuration need wait ',negative
'are the same process cannot recover anything ',negative
'sorted failed sequence numbers needed figure out what sequence number can committed ',negative
'skip special case storm killworkers already invoked ',negative
'windowtoacked ',negative
'there race logconfig where they can leaked some versions storm ',negative
'expected ',negative
'common stats ',negative
'allow searching when startbyteoffset filelen doesnt blow length files ',negative
'noop need create links local mode ',negative
'dont need take care sync cause were always updating heartbeat ',negative
'producers this just get some data kafka normally you would getting this data from elsewhere ',negative
'get startend indices for blocks ',negative
'there security are done ',negative
'log file permissions ',negative
'security off admin allowed group allowed user ',negative
'optional optional optional optional optional optional optional ',negative
'prevent filename from pathing into worker logs outside daemon log root ',negative
'weight ',negative
'test commit creates properly ',negative
'ignored ',negative
'get the nodes ',negative
'some tests rely reading the worker log there are too many emits and too much logged the log might roll breaking the test ensure the time based windowing tests can emit for minutes ',negative
'totalresources ',negative
'taskidindexingbase queue some entries can null outbound for this executor instance ',negative
'topologystats ',negative
'batch replayed with tuples ',negative
'want update longest scheduling time real time case scheduler get stuck get current time before starttime avoid potential race with schedulers timer ',negative
'atomically decrement the count its greater than threshold and return the event should evicted ',negative
'ran out buffer for the search ',negative
'returns true there was change the situation ',negative
'proxy timeout ',negative
'set topology name that sample trident topology can use stream name ',negative
'look for available port ',negative
'storeslength users bengaluru ',negative
'add messages ',negative
'always make sure clean everything else before worker directory ',negative
'races are okay this just avoid extra work for each page load ',negative
'partition should not have been evicted ',negative
'note could add support for setting the replication factor ',negative
'delete anything older than hour ',negative
'only have set amount time can wait for before looping around again ',negative
'look local blobstore ',negative
'events with past should expire ',negative
'ifn ',negative
'the list all executors preferably sorted make assignments simpler ',negative
'this for backwards compatibility ',negative
'see locktimeoutsec time has elapsed since last selected the lock file ',negative
'the supervisor the node down add orphaned slot hold the unsupervised worker ',negative
'not set recoverpartial not set recoverpartial ',negative
'can use the task index starting from the partition ',negative
'read the length field ',negative
'authorizationid not set set authenticationid ',negative
'the first transaction the new batch ',negative
'end include processing ',negative
'reusing tupleinfo object directly call executorackspoutmsg are not sending msgs perf critical ',negative
'ack tuple ',negative
'verify that the commit logic can handle offset voids due log compaction ',negative
'the maximum number state search before stopping ',negative
'kafka must able return more messages than that order for the tests meaningful ',negative
'log the user and get the tgt ',negative
'longarg ',negative
'make sure theres enough bytes the buffer ',negative
'check see there are any existing files already localized ',negative
'generate random storm ',negative
'null value string value acceptable ',negative
'since this processor type committer this occurs the commit phase ',negative
'ignore the future ',negative
'get the worker count back can assert each test function ',negative
'',negative
'seconds passed still not timing out ',negative
'are going skip over cpu and memory because they are captured elsewhere ',negative
'',negative
'this code here ensures that only one attempt ever tracked for batch when ',negative
'discard the pending records that are already committed ',negative
'will not follow sym links ',negative
'commit solr and ack every tuple ',negative
'local node ',negative
'calls this before actually killing the worker locally sends task finished update ',negative
'keep track free slots ',negative
'should pass ',negative
'optional optional optional optional optional ',negative
'there are more pending consumed messages and storm delivered ack for all ',negative
'timer null only the processing guarantee atmostonce ',negative
'give hook chance alter the clock ',negative
'populating request context ',negative
'allow poll there are retriable tuples within the limit ',negative
'build cluster and connect ',negative
'check log file contents ',negative
'create thread process insertion all metrics ',negative
'deleting open file should return true ',negative
'most clojure tests currently try access the blobs using getblob since checks for updating the correct version the blob part nimbus before performing any operation there necessity stub several test cases ignore this method valid trade off return nimbusdetails which include the details the current nimbus host port data are not initialized part the test moreover this applies only local blobstore when used along with nimbus ',negative
'get spread components ',negative
'shared off heap node memory ',negative
'reemit second batch ',negative
'thread safe same instance can used across multiple threads ',negative
'topology source class that can produce stormtopology thrift object ',negative
'for the case that ackfail message arrives before ackinit ',negative
'properties file substitution ',negative
'number spout executors ',negative
'jdk tries automatically drain the input streams for when the process exits but since close not synchronized creates race close the stream first and the same recycled the stream draining thread will attempt drain that may block oom cause bizarre behavior see issue fixed build ',negative
'initialstatus ',negative
'solo ',negative
'this sink and result emit ',negative
'yes its just test purpose ',negative
'retrieving encapsulated retrieval interface ',negative
'memoryguarantee ',negative
'dont really care too much about the scheduling topologygpu because was scheduled ',negative
'ignored will with default timeout ',negative
'where state stored zookeeper only for batch spout types ',negative
'the blobstore good now lets get the list all topo ids ',negative
'method which initializes nimbus admin ',negative
'case control message ',negative
'then exception ',negative
'numerrchoice ',negative
'kinesis stream name read from ',negative
'the spout must respect after committing set records ',negative
'other builder functions not exposed onfactory registerwith mbeanserver mapstringtimeunit specificrateunits ',negative
'restart topology with the same topology which mimics the behavior partition reassignment ',negative
'everything scheduled correctly need search any more ',negative
'nodes the descending order priority processornode has higher priority than partition and window nodes that the topological order iterator will group many processor nodes together possible has higher priority than statequeryprocessor that statequeryprocessor can mapped the same statefulbolt that part ',negative
'abandoned files then pick oldest file sourcedirpath lock and rename ',negative
'ack not process the record again restart and move next message ',negative
'then ',negative
'validate cpu settings ',negative
'release both locks ',negative
'children only ever null does not exist this happens during unit tests and because nonexistant directory definition clean are ignoring ',negative
'last partition not evicted ',negative
'',negative
'have something schedule ',negative
'support for worker tokens similar iautocredentials implementation ',negative
'unexpected error ',negative
'numexecutors ',negative
'blank result communicates that there are more blobs ',negative
'filesmove with nonempty directory doesnt work well windows not atomic ',negative
'play tuple ',negative
'each thread will have its own request context ',negative
'the current processor preserves the key and already partitioned key skip the repartition ',negative
'test for nimbus itself user ',negative
'register most recent relogin attempt ',negative
'decoder ',negative
'this allowed because the committed message brings the below the cap ',negative
'retnumsupervisors ',negative
'only enable cleanup blobstore nimbus ',negative
'disregard first line because has header already read ',negative
'sleep for mins ',negative
'imperative not run the function inside the timer lock otherwise possible deadlock the deals with other locks like the submit lock ',negative
'idtoboltaggstats ',negative
'lets use the number actually scheduled workers way bridge ras and nonras ',negative
'obtained empirical testing see comment block above ',negative
'verify workertokenmanager recognizes the expired workertoken ',negative
'good increment tasks this component being executed ',negative
'order executors scheduled ',negative
'does not exist ',negative
'metadata information commit kafka unique per spout instance ',negative
'number times had backtrack ',negative
'null tuple not configured emitted should marked emitted and acked immediately allow its offset commited kafka ',negative
'non impersonating request should permitted ',negative
'set the number workers the same partition number the idea have spout and partial count bolt coexist one worker avoid shuffling messages across workers storm cluster ',negative
'topology being null used for tests probably should fix that some point but not trivial ',negative
'helper classes ',negative
'stop searching soon passed current time ',negative
'register imetric instance storm will then call getvalueandreset the metric every and the returned value sent all metrics consumers you must call this during iboltprepare ispoutopen return the imetric argument unchanged ',negative
'expired ',negative
'commit polled records immediately ensure delivery atmostonce ',negative
'nonblacklisted supervisor ',negative
'distributed mode send heartbeat directly master local supervisor goes down ',negative
'class batchinserter ',negative
'pause other topicpartitions only poll from current topicpartition ',negative
'map assignments organized node with the following format ',negative
'earliest start ',negative
'can regular nodes static state processor nodes ',negative
'has closed the writer its safe remove the writer from the map here ',negative
'ensure that the first three tasks have been selected before ',negative
'swapping two arrays ',negative
'the streamid defined use for the grouping otherwise assume storms default stream ',negative
'groups ',negative
'print the results stdout ',negative
'verify that bobs token has expired ',negative
'just make note the oldest expired lock now and check its still unmodified after locktimeoutsec ',negative
'take the batched metric data and write the database ',negative
'assume message immediately acked nonack mode ',negative
'executors ',negative
'debug only once have confidence can lose this ',negative
'enable acking ',negative
'required required required required ',negative
'identify the join field for the stream and look tuple field can nested field outerkeyinnerkey ',negative
'just skip when any error happens wait for next round assignments reassign ',negative
'requestedcpu ',negative
'theres enough bytes the buffer read ',negative
'returns false are done with this section rows ',negative
'for others using too much really question how much memory free the system ',negative
'partition ids ',negative
'topology may deployed deactivated mode wait for activation ',negative
'random number generator ',negative
'junit ensures that the temporary folder removed after the test finishes ',negative
'instead iterating again would possible commit and update the state for each topicpartition the prior loop but the multiple network calls should more expensive than iterating twice over small loop ',negative
'source dir config ',negative
'read initial lines file then check lock exists ',negative
'consider all events for the initial window ',negative
'initialize assignment map ',negative
'save metric keyvalue batched ',negative
'signature ',negative
'allow blacklist scheduler cache the supervisor ',negative
'invoke setter ',negative
'done for requests ',negative
'this class assumes that there most one retry schedule per message this set time ',negative
'remove the executors cache let recompute ',negative
'numfails ',negative
'check annotation one our ',negative
'return false cant increment anymore ',negative
'thread polling every seconds update the wordset seconds which used filterwords bolt filter the words ',negative
'records ',negative
'for tests reader will not null ',negative
'fullclassname ',negative
'acked messages sorted ascending order offset ',negative
'advance time and then trigger first call kafka consumer commit the commit must progress offset ',negative
'get topology info ',negative
'make logic simple assumes that all the tables have one which should extended support composed key ',negative
'are not really running anything make this simple check for ',negative
'select tasks once more than the number tasks available ',negative
'iautocredentials icredentialsrenewer iprincipaltolocal ',negative
'remove any configs that are specific host that might mess with the running topology ',negative
'subscribe callback implementation ',negative
'removing self not create deadlock where nimbus trying download missing blob from itself ',negative
'the states recovered ',negative
'topo evicted since user bobby dont have any resource guarantees and topo the lowest priority for user bobby ',negative
'always provide mocked hivewriter ',negative
'use cannot calculate assume that bad ',negative
'report messagesizes metric enabled nonnull ',negative
'nodes ',negative
'perf critical check avoid unnecessary allocation ',negative
'not symmetric difference performing aentryset bentryset ',negative
'and provides consumer bolt ',negative
'the elements having the same key within the window will grouped together and the corresponding values will merged the result pairstreamstring iterabledouble with stock symbol the key and stock prices for that symbol within the window the value ',negative
'returns nil doesnt exist ',negative
'these triggers will retried part batch retries ',negative
'emit and ack some tuples ensure that some polled tuples remain cached the spout emitting less than maxpollrecords ',negative
'nature join field for the current stream field for the other stream ',negative
'new document should inserted there are matches the query filter ',negative
'schedule nimbus inbox cleaner ',negative
'setup topology ',negative
'key has not been created yet and the first time being created ',negative
'cleanup thread killing topology assignment and starting the topology ',negative
'offset commits have ever been done for this consumer group and topicpartition start the beginning end depending ',negative
'the current executor are trying schedule ',negative
'totaltasks ',negative
'metricvalue ',negative
'find the smallest offset toresend list ',negative
'reached far add the set messages waiting retried with next retry time based how many times failed ',negative
'end test ',negative
'onheap and offheap memory requirement ',negative
'emit the messages ',negative
'wrapper class handy for the client code use the json parser build use with json parser ',negative
'executordetails task mapstring type resource mapstring type that resource double amount ',negative
'remove from failedpershard anyway ',negative
'serializedparts ',negative
'remove contiguous elements from the head the heap ',negative
'merge and push unions rules ',negative
'test read ',negative
'find the smallest offset pending list ',negative
'create root directory not exist ',negative
'shard iterator type based kinesis api beginning time latest timestamp are only supported ',negative
'log writer command ',negative
'componenttonumtasks ',negative
'jsonaware not working for nested element map write json format from here ',negative
'join the streams order streamjoinorder ',negative
'explicitly anchor emits corresponding input tuples only default window anchoring will anchor them all tuples window ',negative
'the following tests are run for both hdfs and local store test the ',negative
'covers scenarios expalined scenario when nimbus holding the latest update goes down before downloaded nimbus nimbus gets elected leader ',negative
'end test ',negative
'can null for things like partitionpersist occuring off drpc stream ',negative
'key shouldnt iterator since its marked deleted ',negative
'defaults seconds ',negative
'the combination the lock and the finished flag ensure that never timed out has been finished ',negative
'note istrategyclass enforced daemonconf error will thrown nimbus topology submission and not the client prior submitting the topology ',negative
'convert the state back stream and print the results ',negative
'make sure weve handled all supervisors the host before break ',negative
'execsummary ',negative
'will used instead ',negative
'workers ',negative
'other members ',negative
'worker command ',negative
'reset all the weights ',negative
'when nid and zid are not equal nid attempting impersonate zid ',negative
'offset was previously committed for this consumer group and topicpartition either this another topology ',negative
'the user here from the jaas conf bob impersonation done verify that ',negative
'tar not native windows use simple java based implementation for tests and simple tar archives ',negative
'this means are pointing file ',negative
'user configurable ',negative
'wildcard given file ',negative
'details ',negative
'check adding reference local resource with topology same name ',negative
'supervisor health check ',negative
'',negative
'check exec can worker based user defined component exclusions ',negative
'first verify that something has high load its distribution will drop over time ',negative
'daemons can only nimbus supervisor worker ',negative
'ignore nonodeexists exceptions because when sync may populate curr with stale data since zookeeper reads are eventually consistent ',negative
'spout internals ',negative
'get tasks the user authorized for this topology ',negative
'check for substruct validity ',negative
'this takes care setting coord streams for spouts and bolts ',negative
'note dont return from this method parseexception avoid triggering the spout wait strategy due emits instead back into the loop and generate tuple from next file ',negative
'this happens when the key not found the cache loader returns null and this exception thrown because the cache cannot store null ',negative
'rack list host names that rack ',negative
'first reemit any previously failed tuples from retrylist ',negative
'nimbus metrics distribution ',negative
'authz authn ',negative
'check the user allowed read this ',negative
'now parse and return the map ',negative
'after this resources should contain all the kinds resources can count for the group see kind resource another node not resourceskeyset well throw ',negative
'when using the guarantee mode the spout must commit tuples periodically regardless whether theyve been acked ',negative
'expect notify supervisors almost the same time ',negative
'mastercodedir ',negative
'increment the fail count started with ',negative
'when reading the conf nimbus want fall back our own settings ',negative
'may null worker tokens are not supported the thrift transport ',negative
'read line ',negative
'expected ',negative
'todo ',negative
'construct the final assignments adding starttimes etc into ',negative
'sharedmemory ',negative
'this just case supervisor down that disk doesnt fill shouldnt take supervisor seconds between listing dir and reading ',negative
'components ',negative
'for testing careful doesnt clone ',negative
'the write failed try sync anything already written ',negative
'schema change should have forced rotation ',negative
'these can chained like with setting the cpu requirement ',negative
'getter mainmethods ',negative
'add the nid the real user reqcontexts subject which will used during authorization ',negative
'always empty processing guarantee none atmostonce ',negative
'set keep blobs each size ',negative
'seek next offset after last offset from previous batch ',negative
'this not atomic something bad happens the middle need able recover ',negative
'the unique topology for the topology that created this metadata ',negative
'locate oldest expired lock file any and take ownership ',negative
'populating request context ',negative
'check last block scheduling time does not get significantly slower ',negative
'null wasnt sampled ',negative
'expecting inside remoteexception ',negative
'now pending toresend ',negative
'cant leave choices empty initiate similar shufflegrouping ',negative
'minheap ',negative
'retries management ',negative
'close all the created htable instances ',negative
'the service must able remove retry schedules for unnecessary partitions ',negative
'the default false the default false ',negative
'use redis based state store for persistence ',negative
'',negative
'ackersnull when ackercount not explicitly set the topology ',negative
'whether they are ibasicbolt irichbolt instances ',negative
'this really should impossible because off the min load and inc anything within but just sure never issue especially with float rounding etc ',negative
'therefore the timer newassignment wont invoked ',negative
'performs hashjoin constructing hash map the smaller set iterating over the larger set and finding matching rows the hash map ',negative
'all down which unlikely hence there might need update the blob all down ',negative
'watermark interval ',negative
'cant move this outside without breaking backward compatibility ',negative
'tuples that were successfully ackedemitted these tuples will committed periodically when the commit timer expires ',negative
'inner join core implementation ',negative
'this should throw because auth failed ',negative
'ipersistentmap ',negative
'race condition with delete ',negative
'the tick should cause tuple ackd ',negative
'the consumer should not seeking retry the failed tuple should just continuing from the current position ',negative
'interrupted thrown when are shutting down just ignore for now ',negative
'worker launched through external commands hence count their exceptions toward shell exceptions ',negative
'jstack dump ',negative
'the result aggregation forwarded the redisstorebolt the forwarded tuple keyvalue pair word count with key value being the field names ',negative
'waiting for spout tuples isnt strictly necessary since also wait for bolt emits but anyway allow two minutes for topology startup then wait for most the time should take produce windows ',negative
'noop the events are acked execute ',negative
'list files ',negative
'workerchildopts validates ',negative
'ignore workers that are still bound slot which monitored supervisor ',negative
'all events are sent successfully return last sent offset ',negative
'metric timestamp value map tagktagv respectively ',negative
'',negative
'the first field the batch ',negative
'stats ',negative
'the writer must closed before removed from the map failed might lose some data ',negative
'realm ignored ',negative
'clocks sync ',negative
'some cases the new localassignment may equivalent the old but not equal those cases want update the current assignment the same the new assignment ',negative
'validate single task return ',negative
'now also check that more tuples are polled for since both partitions are their limits ',negative
'force send error ',negative
'overloading the method accomodate subject order check for authorization ',negative
'null worker means generate one ',negative
'there race credentials where they can leaked some versions storm ',negative
'play all tuples ',negative
'this will fail the test since user derek does not have entry for memory ',negative
'',negative
'messageid ',negative
'rebalanceoptions ',negative
'for most scenes avoid inner array resizing ',negative
'early return shard assigned probably because number executors number shards ',negative
'acquire another lock file and verify failed ',negative
'commit offsets ',negative
'catching and logging because there subsequent update and delete the nonleader nimbodes might throw exception ',negative
'nimbus compatibility ',negative
'returns either the source component name the stream name for the tuple ',negative
'want able select the measurement interval reporting window dont need different reports want able specify format and configs specific the format with perhaps defaults overall ',negative
'modifies justassignedkeys ',negative
'this called async lets assume that something care about ',negative
'remove uploaded jars blobs not artifacts since theyre shared across the cluster note that dont handle texception delete jars blobs because its safer leave some blobs instead topology not running ',negative
'get storm values and emit ',negative
'complete the send ',negative
'initial delay for the commit and assignment refresh timers ',negative
'todo enable setstatespout gets implemented testexpected ',negative
'here dont set the tuples context and emit unanchored the checkpoint tuple will trigger checkpoint the receiver with the emitted tuples ',negative
'storm configuration ',negative
'looks for files the directory with current suffix ',negative
'this test sends broadcast all connected clients from the server need wait until the server has registered the client connected before sending load metrics its not enough wait until the client reports that the channel open because the server event loop may not have finished running channelactive for the new channel send metrics too early the server will broadcast one waiting for the response here ensure that the client will registered the server before send load metrics ',negative
'checks for assertion when turn security ',negative
'listener implementation ',negative
'not required not required ',negative
'this will only get updated once ',negative
'determine how long sleep from looking tickets expiry should not allow the ticket expire but should take into consideration will not sleep less than unless doing would cause ticket expiration ',negative
'try append closed file ',negative
'convert thrift stats java maps ',negative
'sharedresources ',negative
'make the spout commit any acked tuples ',negative
'users ',negative
'heartbeat for this one should ',negative
'clear the kerberos state but the tokens are not cleared per the java kerberos login module code only the kerberos credentials ',negative
'copy constructor ',negative
'writes the offsets the new format the user partitions paths ',negative
'specify configuration object used ',negative
'the spout should emit most one message per call nexttuple this necessary for storm able throttle the spout according maxspoutpending ',negative
'reserved for future ',negative
'same set events part three windows ',negative
'',negative
'stream stock quotes ',negative
'mapfunction aware cleanup let handle cleaning ',negative
'stream name unspecified ',negative
'topo has large tasks ',negative
'success ',negative
'set watermark interval high value and trigger manually fix timing issues ',negative
'fail tuple ',negative
'tuples that have been emitted but that are the wire pending being acked failed ',negative
'ensure nimbus has leadership otherwise topology submission will fail ',negative
'ignore ',negative
'this latch closed need create new instance ',negative
'actually mapstring mapstring mapstring longdouble ',negative
'objects are absent they were zero both this iteration and the last only this one need report zero ',negative
'with realm hdfswitzendcom ',negative
'get new random number and seed make sure that runs are consistent where possible ',negative
'tuple arrives from spout can passed otherwise the value the first field the tuple ',negative
'totaltopologies ',negative
'simulate the time trigger setting the reference time and invoking ontrigger manually ',negative
'returns paused topicpartitions ',negative
'the free pool never has anything running ',negative
'the time now twice the message timeout the second tuple should expire since was not acked ',negative
'define our taskids ',negative
'wrap ',negative
'user class supplied this also provides bridge trident ',negative
'trigger manually avoid timing issues ',negative
'executorid ',negative
'create links artifacts dir ',negative
'other classes from config ',negative
'means delegate batch size trident batch size ',negative
'initial delay for the assignment refresh timer ',negative
'names ',negative
'wait for lock expire ',negative
'that fails use config ',negative
'verify simple rejected ',negative
'filter executorsummarys with empty stats ',negative
'backward compatibility ',negative
'verify recorded messages size metrics ',negative
'first partition second partition ',negative
'ranked third since rack has lot cpu but not lot memory ',negative
'fragmentedcpu ',negative
'last batch meta null but this not the first batch emitted for this partition this emitter instance this replay the first batch for this partition use the offset the consumer started ',negative
'dont start new requests there exception ',negative
'require there both topology and component this case parse out such ',negative
'there one task inside one executor for each worker the topology taskid always therefore you can only sendunanchored tuples colocated systembolt this bolt was conceived export worker stats via metrics api ',negative
'now scan all metadata and remove any matching string ids from this list ',negative
'ignored ',negative
'save the memory limit can enforce less strictly ',negative
'executor resources ',negative
'could not recover container will null ',negative
'returns null its not drpc group ',negative
'pump more msgs than size verify msg count expexted ',negative
'check negative resource count ',negative
'ack tuple ',negative
'log any info sent the error stream ',negative
'fill the half with new bytes from the stream ',negative
'drpc token only works for the invocations transport not for the basic thrift transport ',negative
'mergewith accstats compkey cidstatknum ',negative
'map the key needed ',negative
'setup devnull bolt ',negative
'failedwithexitcode were mimicing hadoops health checks treat nonzero exit codes indicators that the scripts failed execute properly not that the system unhealthy which case dont want start killing things ',negative
'fail both emitted tuples ',negative
'theres not enough bytes the buffer ',negative
'oneproducerconsumer twoproducerconsumer producerfwdconsumer ',negative
'should allowed retry times addition original try ',negative
'check allowedworkers only the scheduler not the resource aware scheduler ',negative
'check that trypublish tryoverflowpublish work expected ',negative
'put tuple cause the first tuple acked ',negative
'failures happen you dont get explosion memory usage the tasks ',negative
'httpserverdestroy ',negative
'seek directly the earliest retriable message for each retriable topic partition ',negative
'hide the deadports from the allports these deadports can reused next round assignments ',negative
'resets the last access time for key ',negative
'action ',negative
'singleton instance allows mock delegated static mainmethods our ',negative
'when click link the logviewer expect the match line somewhere near the middle the page subtract half the default page length from the offset which found the match ',negative
'generate topologies ',negative
'allowing keytab based login for backward compatibility ',negative
'owner ',negative
'map tag value pairs ',negative
'just for testing purpose after the migration testingclj this class could removed ',negative
'only rack use the second rack with gpus the first rack with gpus ',negative
'used worker only keep latch ',negative
'throws parseexception effectively produces lines from each file read ',negative
'ras resource aware scheduler ',negative
'starting empty ',negative
'ack return the first pending list ',negative
'storm configuration ',negative
'',negative
'highest sequence number that can committed for this shard ',negative
'the batch size can larger than half the full recvqueue size avoid contention issues ',negative
'exact variable time that added the current bucket ',negative
'completemsavg ',negative
'events should not scanned all since timeevictionpolicy lag should break ',negative
'get from cluster statezookeeper every time collect the stats may replace with other statestore later ',negative
'note that only uses the supervisordetails the rest the arguments are there satisfy the inimbus interface ',negative
'the key and value txids are guaranteed converted utf encoded string ',negative
'clear workers off all hosts that are not blacklisted ',negative
'creating blob again before launching topology ',negative
'assignedmemonheap ',negative
'commit ',negative
'user jerry submits topo ',negative
'optional optional optional optional optional optional ',negative
'records mbs memory footprint the worst case ',negative
'this updated the worker and the topology has shared access ',negative
'need release resources associated with the worker event loop group ',negative
'assertion ',negative
'noop ',negative
'protected using the object lock ',negative
'distribution should even for all nodes when loads are even ',negative
'implementation for converting kinesis record storm tuple ',negative
'callback that does nothing ',negative
'case deactivate was called before ',negative
'close the socket which releases connection has created any ',negative
'remove any entries the cache ',negative
'accstats compkey boltstatsspoutstats ',negative
'update current key list inside the blobstore the version changes ',negative
'lock protects against multiple topologies being submitted once and ',negative
'windowtofailed ',negative
'one more column families ',negative
'supervisorid ',negative
'wait for all locks expire then heart beat locks ',negative
'create sequence format instance ',negative
'blob key not specified use file ',negative
'lock log entry every tuples effectively disable commits based time ',negative
'todo finish ',negative
'this prevent the potential bug that the login cache enabled and then disabled and then enabled again and the logincachekey remains unchanged will use the login cache from which could wrong because the tgt cache well the principle could have been changed during ',negative
'some other form unix ',negative
'element sojourn time milliseconds ',negative
'suppressing exceptions dont care for errors abort ',negative
'need get the next node iterator ',negative
'creating blacklist file read from the disk ',negative
'treeset uses compareto instead equals for the set contract ensure that can save two retry schedules with the same timestamp ',negative
'thread object thread will null refresh thread not needed ',negative
'encoder ',negative
'dont ack tick tuples ',negative
'dont let the user set who launch ',negative
'acls for the blob are set worldeverything ',negative
'create another input file and reverify same behavior ',negative
'setup default producer ',negative
'pregenerate commonly used keys for scans ',negative
'sasl authentication disabled saslchannelready initialized true otherwise false ',negative
'number values must odd compute median below ',negative
'noop windows gets support for run user will need find way support this ',negative
'wordspout countbolt redisbolt ',negative
'',negative
'storm blobstore create file blacklisttxt acl orwa key ',negative
'setup topology ',negative
'emitted ',negative
'the body the message message currentoffset message ',negative
'object capturing all related information for storing committed sequence numbers ',negative
'theres race condition with delete either blobstore this should thrown the caller indicate that the key invalid now ',negative
'hostname ',negative
'stop services without killing the process instead ',negative
'path should defined most systems ',negative
'setup ',negative
'backoff for test retry service just check that messages will retry immediately ',negative
'just need ',negative
'javac option remove these when the javac zip impl fixed httpbissueid ',negative
'both assignments are null just wait ',negative
'for each partition the spout allowed retry all tuples between the committed offset and ahead must retry tuples within that limit even more tuples were emitted ',negative
'for local cluster ',negative
'check that null meta makes the spout seek earliest and that the returned meta correct ',negative
'messagesnext null can happen lost the connection and subsequently reconnected timed out ',negative
'emits sliding window and global averages ',negative
'topologies that were deemed invalid ',negative
'addition add all the owners with guarantees ',negative
'after consumer rebalance during closedeactivate always empty processing guarantee none atmostonce ',negative
'for local test ',negative
'window should compacted and events should expired ',negative
'add more events with current ',negative
'the manually set config supervisor will overwrite ',negative
'left join core implementation ',negative
'returns true pendingemits empty ',negative
'now ack msg and check ',negative
'race with delete not here the replication ',negative
'ignore ',negative
'this needs thread safe ',negative
'the worst case will return serialized name after password provider said that the password was okay that case the acls are likely prevent the request from going through anyways ',negative
'enable blobstore acl validation ',negative
'nodeinfo ',negative
'metrics ',negative
'tests that isscheduled isready and are mutually consistent when there are messages from multiple partitions scheduled ',negative
'this might partial key grouping ',negative
'make sure resources dir created ',negative
'make sure support different user reading same blob ',negative
'validate search metric ',negative
'map node ids node objects ',negative
'pass cases ',negative
'construct transport plugin ',negative
'reference key ',negative
'initialize slots for this node ',negative
'quoting javadoc filter returns code null this abstract pathname does not denote directory error occurs ',negative
'were making mock ignoring ',negative
'expire the token ',negative
'create links blobs ',negative
'maps transaction ids jms message ids ',negative
'testing whether acls are set worldeverything here are testing only for localfsblobstore the hdfsblobstore gets the subject information the local system user and behaves always authenticated ',negative
'define our taskids the test expects these incrementing one from zero ',negative
'for kryo ',negative
'static ensure eventhough the class created using reflection can still get the topology actions ',negative
'convenience method for registering reducedmetric ',negative
'one instance per executor avoids false sharing cpu cache ',negative
'functionname ',negative
'theres race condition with delete blobstore this should thrown the caller indicate that the key invalid now ',negative
'deletes metrics matching the filter options ',negative
'must specify column schema when providing custom query ',negative
'case task message ',negative
'populate metric ',negative
'the false parameter ensures overwriting the version file not appending ',negative
'since the last tuple the partition more than maxpollrecords ahead the failed tuple shouldnt emitted here ',negative
'acls have two user acls for empty user and principal discard empty user acl ',negative
'perform scan given filter options and return results either metric raw data ',negative
'function called timer reset log levels last set debug ',negative
'oneproducerconsumer measurement twoproducerconsumer measurement measurement ',negative
'set operator sets the value field document ',negative
'schedule last block ',negative
'submit storm cluster ',negative
'batch replayed with tuples ',negative
'add the authnid the real user reqcontexts subject which will used during authorization ',negative
'return null its not single emit ',negative
'mergewith partial mergewith sumor accout spoutout ',negative
'config spout log progress lock file for each tuple ',negative
'defaults ',negative
'the value follows pre executed alltime split default split default split default split default executelatencies alltime split default split default split default split default pre ',negative
'these should match the test resources ',negative
'write string array nework int followed int byte array compressed strings handles also null arrays and null values could generalised using introspection ',negative
'numworkers ',negative
'wait for ready channel connected and maybe authentication ',negative
'required for instantiation via reflection must call prepare thereafter ',negative
'this will best effort flushing since the linger period was set creation ',negative
'second predicate for condition uses the fact that long addition over the limit circles back ',negative
'yes eat the exception ',negative
'this technically does not conform with rfc but should work long you dont have any really odd names your kdc ',negative
'invalidate the iterator ',negative
'only put this owner the map ',negative
'example spout generate random strings bolt get the first part string bolt output the tuple ',negative
'and try renew the ticket ',negative
'reload from cached file ',negative
'seconds ',negative
'just throw away local mode ',negative
'fail cases ',negative
'first off want verify that root good ',negative
'max outstanding tuples ',negative
'nimbus groups admin ',negative
'almost all cases these should the same but warn the user just case something goes wrong ',negative
'fail all emitted messages except the last one try commit ',negative
'public timestampfield thistimestampfield timestampfield return this ',negative
'some cases users will want drop retrying old batches the topology should start over from scratch the ignores committed offsets should not retry batches for old topologies the batch retry should skipped entirely ',negative
'run until ctrlc ',negative
'automatically turn into batch spout should take parameters how much batch public stream newstreamirichspout spout node new null spout spoutnode spouttypebatch return addnoden ',negative
'now login ',negative
'will only serialize amqpvalue type ',negative
'necessary that this produce deterministic assignment based the key seed the random from the key ',negative
'given for this iteration ',negative
'check for required fields ',negative
'global grouping fields with empty list ',negative
'spout ',negative
'offset and messageid are used interchangeably ',negative
'olog ',negative
'msec ',negative
'print metrics every sec kill topology after min ',negative
'this histogram reflects the data distribution across only one clustersummary data distribution across all entities type data from all nimbustopologies one moment hence use half the cachingwindow time ensure retains only data from the most recent update ',negative
'blocking call under the hood must invoke after launch cause some services must initialized ',negative
'start the threads ',negative
'the inputwindow gives view all the events the window events that expired since last activation the window events that newly arrived since last activation the window ',negative
'memonheap ',negative
'the old token could not deserialized this bad but are going replace anyways just keep going ',negative
'then for this iteration ',negative
'case didnt fill enough ',negative
'cycle spout activation ',negative
'check for latest sequence number key inside zookeeper and return nimbodes containing the latest sequence number ',negative
'need download temp file and then unpack into the one requested ',negative
'add identity partitions between groups ',negative
'acking tuples for partitions that are longer assigned useless since the spout will not allowed commit them ',negative
'read few lines from file dont ack ',negative
'check for blobstore with authentication ',negative
'key val val key val val ',negative
'setup hdfs spout ',negative
'read lines dont ack commit pos should remain same ',negative
'playing from the repl and defining functions file wont exist ',negative
'under ras the number workers determined the scheduler and the settings the conf are ignored confsetnumworkers ',negative
'',negative
'check for null which can exist because race condition which nimbus nodes may have been removed when connections are reconnected after getting children the above line ',negative
'specificstats ',negative
'register file cleanup after jvm shutdown ',negative
'todo conditionally load properties from file our resource ',negative
'dont need sleep here because the blocked call its fine call this function tight loop ',negative
'triggers when assignment should refreshed ',negative
'clocks are sync then simply take ownership the oldest expired lock ',negative
'first sync assignments local then sync processes ',negative
'samplingpct ',negative
'check for new file every often ',negative
'waited for second loop around and try again ',negative
'topo should not able scheduled ',negative
'create new config make additive true inherit parents appenders ',negative
'prints the total with low probability ',negative
'this task containing worker will killed assignments sync tasktonodeport will empty map which refreshed workerstate ',negative
'all ',negative
'class joininfo ',negative
'supervisor metrics distribution ',negative
'inner join age and gender records field ',negative
'with uncommitted earliest the spout should pick where left off when reactivating ',negative
'test ras spreads executors across multiple workers based the set limit for worker used the topology ',negative
'filter supervisor ',negative
'not include the success stream part the batch should not trigger coordination tuples and just metadata tuple assist cleanup should not trigger batch tracking ',negative
'load pmml model from file ',negative
'this code here handles case where previous commit failed and the partitions changed since the last commit this clears out any state for the removed partitions for this txid make sure only single task ever does this were also guaranteed that its impossible for there another writer the directory for that partition because only single commit can happening once this because order for another attempt the batch commit the batch phase must have succeeded between hence all tasks for the prior commit must have finished committing whether successfully not ',negative
'shell ',negative
'clean some things the user should not set not security issue just might confuse the topology ',negative
'prevent timer check heartbeat based last thing before activate ',negative
'data ',negative
'only holds msgs from other workers via workertransfer when recvqueue full ',negative
'heap dump ',negative
'wait for all tasks complete ',negative
'ack rest ',negative
'check that only two message ids were generated ',negative
'login will sleep until time from last refresh tickets expiry has been reached which time will wake ',negative
'above forloop has closed all the writers its safe clear the map here ',negative
'get executor heartbeat ',negative
'name list empty return empty map ',negative
'include sys should not matter ',negative
'detected mainq full try adding overflow ',negative
'finds the metadata string that matches the string and type provided the string should exist ',negative
'validate search time ',negative
'checks the tasks which had back pressure are now free again sends update other workers ',negative
'submit topology storm cluster ',negative
'fragmentedmem ',negative
'distributed mode ',negative
'need read new one ',negative
'sidtooutputstats ',negative
'return the smaller pending and toresend ',negative
'',negative
'the spout must respect when requestingemitting tuples ',negative
'day values ',negative
'',negative
'get numexecutors ',negative
'default this noop ',negative
'normalize state ',negative
'javaobject ',negative
'for acked message add acked set and remove from emitted and failed ',negative
'can track when was last used for later deletion database cleanup ',negative
'avoid buffering ',negative
'the newtimeouts map now contains logger timeout ',negative
'try locking again ',negative
'construct thshaserver ',negative
'version ',negative
'the acked message was emittedpershard that means need remove from the emittedpershard which ',negative
'serializes java object json ',negative
'resume polling the last committed offset the first offset that not marked processed ',negative
'invalid key remove from blobstore ',negative
'thriftify stats mainmethods ',negative
'wordspout countbolt mongoupdatebolt ',negative
'size identifier ',negative
'calc sidoutputstats ',negative
'noop ',negative
'ranked last since rack has neither cpu nor memory available ',negative
'flatmapfunction aware cleanup let handle cleaning ',negative
'construct message containing the sasl response and send the ',negative
'this bolt does not emit tuples ',negative
'dont try move the jar file local mode does not exist because was not uploaded ',negative
'nothing scheduled here throw away all the profileactions ',negative
'loggerinfoemitted new batches listeventssize ',negative
'not used placeholder for gui etc ',negative
'extend the config with defaults and the command line ',negative
'setting value any nonnull string ',negative
'any stop profile actions that hadnt timed out yet should restart after the worker running again ',negative
'remove existing schedule for the message ',negative
'totals ',negative
'asserts that commitsync has been called once that there are only commits one topic and that the committed offset covers messagecount messages ',negative
'stormassignment ',negative
'find homedir ',negative
'delete the current index file and rename the tmp file atomically replace the index file orphan tmp files are handled gettxnrecord ',negative
'validate search host ',negative
'avoid case different blob version when blob not downloaded first time download ',negative
'iterate the tuples ',negative
'create thread delete old metrics and metadata ',negative
'want register topo directory getchildren callback for all workers this dir ',negative
'',negative
'expecting this exception ',negative
'case backpressurestatus ',negative
'metriclist ',negative
'validates ',negative
'give the topology time come without using wait for the spouts complete ',negative
'include sys ',negative
'sometimes external things used with testing dont shut down all the way ',negative
'key supervisor key value supervisor ports ',negative
'run aggregator compute the result ',negative
'this noop ',negative
'repartitioning involved does perpartition reduce key before emitting the results downstream ',negative
'configs from get configs from conf ',negative
'password ',negative
'set acl user doesnt have read access ',negative
'scans from key start the key before end calling back until callback indicates not process further ',negative
'unsuccesful fail the pending tuples ',negative
'this test two phases the first phase fills the buckets with tuples each ',negative
'reset property ',negative
'version ',negative
'pulseids ',negative
'this method enables the metrics accessed from outside the jcqueue class ',negative
'verify lock file location verify lock filename ',negative
'pass for the case running tons tasks single executor ',negative
'acked message should not failed since fails and gets reemitted moves emittedpershard from failedpershard defensive coding ',negative
'all null tuples should commited meaning they were considered emitted and acked ',negative
'the start index positioned find any possible occurrence search string that did not quite fit the buffer the previous read ',negative
'generate another rack supervisors with less resources ',negative
'scheduling changed while running ',negative
'load from state ',negative
'boolean indicate whether timer active ',negative
'valid javac option which another bug ',negative
'map the worker the components the worker able enforce constraints ',negative
'any receive call after exceeding max pending messages results null ',negative
'invoke service handler ',negative
'finally delete any basenameversion files that are not pointed the current version ',negative
'choosing atmost words update the blacklist file for filtering ',negative
'assume the recvqueue stable which the arrival rate equal the consumption rate this assumption does not hold the calculation sojourn time should also consider departure rate according queuing theory ',negative
'serializedjava ',negative
'generate sasl response server using channellocal sasl client ',negative
'gets nimbus subject with nimbusprincipal set ',negative
'topologystatus ',negative
'take the max the default and whatever the user put here each nodes resources can the sum several operations the simplest thing get the max the situation want avoid that the user sets low resources one node and when that node combined with bunch others the sum still that low resource count any component isnt set want use the default right now this code does not check that just takes the max the summed resource counts for simplicitys sake could perform some more complicated logic more accurate but the benefits are very small and only apply some very odd corner cases ',negative
'indexed ',negative
'unblock downloading accepting the futures ',negative
'this finds all dependency blob keys from active topologies from all local blob keys ',negative
'nimbuses ',negative
'emit all messages and check that they are emitted ack the messages too ',negative
'rest jerrys running topologies ',negative
'since asked for tuples starting seekoffset some retriable records must have been compacted away ack the first offset received the record not already acked currently the topology ',negative
'always retain resources use ',negative
'update worker tokens needed ',negative
'consumer ',negative
'not blocking call cannot emit will add tuple pendingemits and return false pendingemits can null ',negative
'cannot since can doublemaxvalue should not return infinity that case ',negative
'the failed tuples are ready for retry make appear like and were compacted away ',negative
'helpful for debugging tests ',negative
'workerresources ',negative
'else use the exponential backoff logic and handle long overflow ',negative
'complete access the blob ',negative
'streams ',negative
'reasonable size for simple class ',negative
'order avoid going over maxnodes may need steal from myself even though other pools have free nodes figure out how much each group should provide ',negative
'factory methods declaring modeloutputs default stream ',negative
'persist the window state ',negative
'hdfs related settings ',negative
'race condition with delete ',negative
'timesecs ',negative
'hour values ',negative
'noop ',negative
'gets minmax task pairs executors ',negative
'tick should have flushed ',negative
'test for subject with principals and acls set worldeverything ',negative
'create empty files filesdir ',negative
'initialize worker slot for every port even there assignment ',negative
'explicit delete for ephmeral node ensure this session creates the entry ',negative
'for global cleanup for active workers dir make sure for the last ',negative
'repartition that state query fields grouping works correctly this can optimized further ',negative
'the logwriter turn launches the actual worker ',negative
'the config consists single key config its values are used instead this means that the same config files can used with flux and the ',negative
'offset was not committed this topology therefore applies only when the topology first deployed ',negative
'this likely happen when try commit something that was cleaned this expected and acceptable ',negative
'credupdatelock not needed here because creds are being added for the first time ',negative
'required required optional optional optional optional optional optional optional optional ',negative
'get pmml model from blobstore ',negative
'boltspecific configuration for windowed bolts specify the name the field the tuple that holds the message this used track the windowing boundaries and avoid reevaluating the windows during recovery ',negative
'create metric for memory ',negative
'kafka ',negative
'delete and recreate lock file returns false somebody else already deleted take ownership ',negative
'maps storm tuple redis key and value ',negative
'this will fail since jerry doesnt have either cpu memory entries ',negative
'removeclean changed requests that are not for ',negative
'bolt that subscribes the intermediate bolt and autoacks ',negative
'delete absent file should return false ',negative
'transferred ',negative
'test with dummy testsubject for cases where subject null security turned ',negative
'should synchronize supervisor doesnt launch anything after being down optimization ',negative
'default parallelism its omitted the topology will still function ',negative
'pendingcommit has entries ',negative
'query the streamstate for each input task stream and compute recoverystates ',negative
'required optional required ',negative
'reset for next run ',negative
'that fails fall back the file ',negative
'back off ',negative
'settable ',negative
'first retry then retry time current time initial delay ',negative
'windows the host process still holds lock the logfile ',negative
'each evicted partition has ',negative
'the response should empty since you should not able list files outside the worker log root ',negative
'totalexecutors ',negative
'try create the parent directory may not work ',negative
'simulate time starts out are going just leave here ',negative
'optional optional ',negative
'required optional optional optional ',negative
'disable log every sec ',negative
'handles tuple events emit ack etc ',negative
'boolval ',negative
'only log accesses that fetched something ',negative
'make sure that the error thread exits ',negative
'look hdfs blobstore again ',negative
'check avoids multiple log msgs when spinning idle loop ',negative
'add events ',negative
'create test dnstoswitchmapping plugin ',negative
'for the currently tested assignment map the node the components able enforce constraints ',negative
'otherwise tuples were emitted directly ',negative
'expiry before next scheduled refresh ',negative
'the system still has some free memory give them grace period ',negative
'tuple contains string object json format tuple contains java object that must serialized json solrjsonmapper ',negative
'topo has small tasks whose mem usage does not exactly divide nodes mem capacity ',negative
'bolt bolt should also receive from checkpoint spout ',negative
'cache the msgs grouped destination node ',negative
'add unique identifier each tuple which helpful for debugging ',negative
'the redis bolt sink ',negative
'since this tumbling window calculation use all the tuples the window compute the avg ',negative
'respectively ',negative
'sleep for seconds ',negative
'create array the right type ',negative
'storm will try get metrics from the spout even while deactivated the spout must able handle this ',negative
'initialization only complete after the first call ',negative
'mock failure ',negative
'update nextoffset ',negative
'based how java handles the classpath ',negative
'ordered partition keys ',negative
'this should mean that were pointed directory ',negative
'renames files and returns the new file path ',negative
'leave the acked offsets and consumer position they were resume where left off ',negative
'test for user having read write admin access read replication for blob ',negative
'instead the scheduler lets you set the maximum heap size for any worker ',negative
'test time schedule large cluster scheduling with fragmentation ',negative
'when for this iteration ',negative
'can happen during shutdown drpc while topology still ',negative
'mapstreamname joininfo ',negative
'size the resource ',negative
'leadership coordination may incomplete when launchserver called previous behavior did one time check which could cause nimbus not process transitions similar problem exists for ',negative
'should promote only fetch storm bases topologies that need scheduling ',negative
'for faster insertion rocksdb ',negative
'mapstreamname mapkey listtuple ',negative
'move tmp current that the operation atomic ',negative
'supervisorsummaries ',negative
'this needs appropriately large drown out any time advances performed during topology boot ',negative
'not scheduled never failed never emitted scheduled and ready retried ',negative
'the node does not exist then the version must ',negative
'wait interfal for retrying after first failure ',negative
'only add topologies that are not sharing nodes with other topologies ',negative
'read and ack remaining lines ',negative
'create the blobstores ',negative
'seqable ',negative
'new topology needs scheduled topo should evicted even though topo from user jerry older topo will not evicted ',negative
'with auth ',negative
'warm seconds ',negative
'number evicted events ',negative
'add spouts groups can get parallelisms ',negative
'setup kafka spout ',negative
'offset management ',negative
'have the new credentials pass the logincontext constructor ',negative
'check see the cgroup mounted all ',negative
'emit crossjoin all emitted tuples ',negative
'copy everything from local blobstore hdfs ',negative
'check that the spout will reemit all failed tuples and other tuples ',negative
'every executor has instance this class ',negative
'customobject ',negative
'test substitution where the target type list ',negative
'implementation for handling the failed messages retry logic ',negative
'the kafkaconsumer commitsync api docs the committed offset should the next message your application will consume ',negative
'should not throw ',negative
'log the connection error only once ',negative
'try get blobmeta this will check the key exists and the subject has authorization ',negative
'todo batch updating ',negative
'configured for achieving max throughput single worker mode empirically found for reference numbers taken macbook pro mid acker millsec batchszk recvqsizek millsec batchsz recvqsizek acker millsec lat microsec batchsz boltwaitparkmicros acker millsec lat micros batchsz receivebuffersizek boltwait bpwait progressivedefaults acker millsec lat micros batchsz boltwaitparkmicros ',negative
'insure that keytab used only one login per process executed ',negative
'means ',negative
'test the happy path emit batches sequence ',negative
'print the values stdout ',negative
'wait strategy when the netty channel not writable ',negative
'the following come from the jvm specification table ',negative
'called flushtupletimer thread ',negative
'ack received for message then add the ackedpershard treeset treeset because while committing need figure out what the ',negative
'get trigger count value from store ',negative
'passed workers local clusters exposed thrift server distributed mode ',negative
'topology will not able successfully scheduled config largest memory requirement component the topology ',negative
'',negative
'netty timertask already defined and hence fully qualified name ',negative
'executor form starttaskid endtaskid ',negative
'get per task components ',negative
'bolts ',negative
'verify that some ticks are received the interval between ticks validated the bolt too few and the checks will time out too many and the bolt may crash not reliably but the test should become flaky ',negative
'measurement ',negative
'add more events with gap ',negative
'error should not leaked according the code but they are not important enough fail the build ',negative
'supervisors ',negative
'end test ',negative
'copy cluster that can modify but does not get committed back cluster unless scheduling succeeds ',negative
'the jitter allows the clients get the data different times and avoids thundering herd ',negative
'ranked last since rack has not cpu resources ',negative
'read line and ack ',negative
'filecontext supports atomic rename whereas filesystem doesnt ',negative
'seconds milliseconds ',negative
'use default stormgenerated file names ',negative
'nothing expired yet ',negative
'should pass now ',negative
'filtered out ',negative
'sometimes leader election indicates the current nimbus leader but the host was recently restarted and currently not leader ',negative
'show progress bar know were not stuck especially slow connections ',negative
'end test ',negative
'todo timestamps ',negative
'ignore ',negative
'the timeout thread handling ',negative
'slots schedule for some reason skip ',negative
'end test ',negative
'parallelismhint ',negative
'commit offsets that are ready committed for every topic partition ',negative
'its likely that bolt shutting down need throw runtimeexception just ignore ',negative
'create empty file ',negative
'testing whether acls are set worldeverything here the acl should not contain worldeverything because the subject neither null nor empty the acl should however contain usereverything user needs have complete access the blob ',negative
'this assumes that infields and outfields are the same for combineragg assumption also made above ',negative
'converts metadata string into unique integer updates the timestamp the string ',negative
'there will legacy values they will the outer conf ',negative
'wait until all workers supervisors and nimbus waiting ',negative
'construct groups mapping for the fixedgroupsmapping class ',negative
'this test where are configured point right artifact dir ',negative
'cond prevents staying stuck with consuming overflow ',negative
'the elements having the same key within the window will grouped together and their values will reduced using the given reduce function here the result pairstreamstring double with stock symbol the key and the maximum price for that symbol within the window the value ',negative
'secretversion ',negative
'get the directory put uncompressed archives ',negative
'expecting failcount ',negative
'now know for sure that this bad ',negative
'redis has chunk but more ',negative
'file order calculation significant sorting done unixformat names casefolded order get platformindependent sort and calculate the same all platforms ',negative
'resultcreate states that you must ensure that the keyvalues are already sorted ',negative
'one group subscribes the same stream with same partitioning multiple times merge those together otherwise can end with many output streams created for that partitioning need split into multiple output streams because same input having different partitioning the group ',negative
'global variables only used internally class ',negative
'shard iterator corresponding position shard for failed messages ',negative
'acquire lock dir ',negative
'find the most recent child and load that ',negative
'atmostonce mode must commit tuples before they are emitted the topology ensure that spout crash wont cause replays ',negative
'cached supervisor doesnt show ',negative
'else leader noop ',negative
'val xor value ',negative
'check log file content line count tuples emitted ',negative
'return the first message retried from the set will return the message with the earliest retry time current time ',negative
'amount data written and rotation policies ',negative
'this possibly lossy the case where value deleted because has received messages over the metrics collection period and new messages are starting come this because dont want the overhead synchronize just have the metric absolutely perfect ',negative
'locate login configuration ',negative
'the key was removed should delete too ',negative
'namely the two eds the orphaned worker and the healthy worker ',negative
'instantiation ',negative
'compute the stats for these and save them ',negative
'not for this topology skip ',negative
'generate some that have neither resource verify that the strategy will prioritize this last ',negative
'notjump open not strict mode ',negative
'',negative
'split gzipmagic into readable bytes ',negative
'check log file content line count tuples emitted ',negative
'after which nimbus comes back and read update performed ',negative
'get existing assignment just the map default filter out ones which have executor timeout figure out available slots cluster add that the used valid slots get total slots figure out how many executors should each slot only keep existing slots that satisfy one those slots for rest reassign them across remaining slots edge case for slots with executor timeout but with supervisor timeout just treat these valid slots that can reassigned worst comes worse the executor will timeout and wont assign here next time around ',negative
'',negative
'first block ',negative
'end metrics ',negative
'any errorexception thrown just ignore ',negative
'start threads after metadata cache created ',negative
'node for basepath nothing remove ',negative
'test with long value ',negative
'create stream word pairs ',negative
'hbase tokens are not renewable always have get new ones ',negative
'task run ',negative
'class ',negative
'initialize with storm configuration ',negative
'kafka throws their own type exception when interrupted throw new java ensure storm can recognize the exception reaction interrupt ',negative
'metric win value win metric value ',negative
'else local noop ',negative
'return null mount options string that not part cgroups ',negative
'past limit quit ',negative
'the whole bytes were not received yet return null ',negative
'executor summaries ',negative
'metrics rpc ',negative
'netclsns not supported ubuntu ',negative
'returns map from task componentid ',negative
'very stream name matches stream name was specified ',negative
'build new metadata based emitted records ',negative
'drop the change notifications are not running anything right now ',negative
'lets build topology ',negative
'iterate over tuples the current window ',negative
'ignored ',negative
'supervisor admin ',negative
'while holding currentlock avoid deadlocks ',negative
'obtain context object ',negative
'protected using lock this counter ',negative
'again dont want exit because logging issues ',negative
'validate search topology ',negative
'canonically the metrics data exported time bucketed when doing counts convert the absolute values here into time buckets ',negative
'simulate lock file lease expiring and getting closed hdfs ',negative
'tuple values are mapped with ',negative
'refresh interval msec last time the command was performed env for the command execution ',negative
'max rounds scanning the dirs ',negative
'configs from loader try read from ',negative
'requestid ',negative
'service off now just interrupt ',negative
'fail tuple and call nexttuple then fail tuple ',negative
'this bolt dosent emit downstream bolts ',negative
'check that reemit emits exactly the same tuples the last batch even kafka returns more messages ',negative
'executelatencyms ',negative
'each context will have single client channel worker event loop group ',negative
'try find way merge this code with whats already done tridentboltexecutor ',negative
'component metric value note that input may contain both long and double values ',negative
'common fields ',negative
'compacted away ',negative
'stream words ',negative
'this fine still have watch from the successful exists call ',negative
'now lets advance time ',negative
'ioexception from reading the version files ignored ',negative
'does not block ',negative
'check that nonnull meta makes the spout seek according the provided metadata and that the returned meta correct ',negative
'class mockcollector ',negative
'the global stream this the from component must part ',negative
'invalidate the cache something the node changed ',negative
'suppressing exceptions dont care for errors connection close ',negative
'ignored ',negative
'stormid ',negative
'list array conversion ',negative
'locate our thrift transport plugin ',negative
'switch cachedgauge this starts hurt performance ',negative
'declare separate punctuation stream per output stream that the receiving bolt can subscribe this stream with all grouping and process the punctuation once receives from all upstream tasks ',negative
'sync sending will return sendresult ',negative
'acked ',negative
'not change unless add state not stored the parent class ',negative
'storm rotation policy other than timedrotationpolicy causes npe cleanup ',negative
'user with impersonation acl should reject ',negative
'this method expected thread safe ',negative
'are done nothing that short going work here ',negative
'less really per seconds ',negative
'precommit can invoked during recovery before the state initialized ',negative
'not set lot things are not really going work all that well ',negative
'distributed mode ',negative
'add special pathspec static content mapped the homepath ',negative
'',negative
'ack both emitted tuples ',negative
'resetloglevel ',negative
'use jsonserializer the default serializer ',negative
'call firechannelread since the client allowed perform this request the clients request will now proceed the next pipeline component namely stormclienthandler ',negative
'sliding interval ',negative
'drpc spout then contains function ',negative
'partial writes prior lines ',negative
'deletes the state inside the zookeeper for key for which the ',negative
'ilookup ',negative
'disconnects dont fail ',negative
'nimbus itself ',negative
'make the new assignments for topologies ',negative
'late tuple emitted ',negative
'was trycause but looked the code around this and key not found not wrapped runtime not needed ',negative
'user jerry submits another topology ',negative
'key shouldnt iterator ',negative
'blobstore directory private ',negative
'the merged configs are only for the reset logic ',negative
'now see can create new token for bob and try again ',negative
'create socket with server ',negative
'remove offsetmanagers for all partitions that are longer assigned this spout ',negative
'need have slots separate hosts the topology needs gpus memory and cpu the bolt instances must separate nodes because they each need gpus the bolt instances must the same node they each need gpu this assumes that are packing the components avoid fragmentation the bolt and spout instances fill the rest ',negative
'when empty ',negative
'remove any expired keys after possibly inserting new ones ',negative
'were either going empty starting fresh blob download either way the changing blob notifications are outdated ',negative
'verify that the following acked now committed tuples are not emitted again since the consumer position was somewhere the middle the acked tuples when the commit happened this verifies that the spout keeps the consumer position ahead the committed offset when committing ',negative
'flushed the buffers completely ',negative
'next file ',negative
'subscribe parents punctuation stream ',negative
'set the current timestamp the reference time for the eviction policy evict the events ',negative
'resources ',negative
'cannot connect there client section the jaas conf ',negative
'metrics ',negative
'skip any resources where the total the percent used for this resource isnt meaningful fall back prioritizing cpu memory and any other resources ignoring this value ',negative
'initialcapacity set since its the default inital capacity ',negative
'raw input data scored predicted pmml model read from file null using blobstore pmml model downloaded from blobstore null using file ',negative
'poll ',negative
'measurement ',negative
'cant find the resources directory resources jar the classpath just create empty resources directory this way can check later that the topology jar was fully downloaded ',negative
'acquire locks filefilefile ',negative
'successfully decoded frame ',negative
'well assume the metadata was recently used still the cache ',negative
'number events per windowpartition ',negative
'',negative
'reuse the retrieved iterator ',negative
'add cassandra cluster contact points ',negative
'fail all emitted messages except the first commit the first ',negative
'string does not exist create using unique string and add cache ',negative
'this class should combined with ',negative
'keeping for backward compatibility ',negative
'required required optional optional optional ',negative
'use servers backup they exist ',negative
'its more likely file read exception here dont differentiate ',negative
'select new file one not open already ',negative
'some cases might limiting memory the supervisor and not the cgroups ',negative
'possible that this component already scheduled this node worker when backtrack cannot remove ',negative
'test can reemit the second batch ',negative
'outputsfields can empty this bolt acts like sink topology ',negative
'iterating multiple times should produce same events ',negative
'executor resides one one worker one executor and executor another worker the other node ',negative
'',negative
'optional ',negative
'are pretending nimbus here ',negative
'wait for leader elected topology submission can rejected ',negative
'stateful processor immediately follows window specification ',negative
'user not authorized ',negative
'stormversion ',negative
'validator definitions ',negative
'should fail ',negative
'batch sync sending ',negative
'dont emit anything allow configured spout wait strategy kick ',negative
'downloading all blobs finished this the precondition for all codes below ',negative
'initial timeout second workers commit suicide after this ',negative
'since read last should evicted and should exist ',negative
'run ',negative
'nummatchessought nummatchesfound ',negative
'validate memory settings ',negative
'set principal rebalanceoptions nil because users are not suppose set this ',negative
'add tick tuple each second force acknowledgement pending tuples ',negative
'has successfully authenticated with this server ',negative
'state found for this shard then set the sequence number ',negative
'failing tuples for partitions that are longer assigned useless since the spout will not allowed commit them they later pass ',negative
'required required required required required optional optional optional optional optional optional ',negative
'errors bit special because older versions storm the worker created the parent directories lazily because this means need auto create least the topoid directory for all running topos ',negative
'look impt hdfs timestamp may not reflect recent appends double check the timestamp last line file see when the last update was made ',negative
'clear cache ',negative
'throwing exception ',negative
'set upper limit how much cpu can used all workers running supervisor node this done that some cpu cycles will remain free run the daemons and other miscellaneous operations ',negative
'this gets called repeatedly for apparent reason dont anything ',negative
'join users and stores city name ',negative
'for testing only invoked via reflection ',negative
'first streams data goes into the probe ',negative
'fail fast ',negative
'hostport workersummary ',negative
'files are worldwide readable and owner writable ',negative
'storm tuple redis keyvalue mapper ',negative
'add configs from resources like hdfssitexml ',negative
'maybe apersistentset which dont support addall ',negative
'since made user not authorized component map empty ',negative
'create stateupdater with the given windowstorefactory remove triggered aggregation results form store ',negative
'get all metadata from the cache put into the database use new map prevent threading issues with writer thread ',negative
'avro strings are stored using special avro utf type instead using java primitives ',negative
'dont error timer shut down happens when the elector closed ',negative
'with tuples per second ',negative
'comma separated list connect strings connect zookeeper localhost ',negative
'lose the race but doesnt matter ',negative
'fall throw purpose ',negative
'yes can delete something that not because races but that for metrics ',negative
'remove old connections atomically ',negative
'enum conversion ',negative
'noop ',negative
'dont permit path traversal for calls intended read from the daemon logs ',negative
'put bolt message first then put task ids ',negative
'make sure the parent directory there and ready ',negative
'nothing assigned yet should emit nothing ',negative
'filter sys streams necessary ',negative
'calling choosetasks should finished before refreshing ring adjusting might needed with really slow machine allow race condition between refreshing ring and choosing tasks will not make exact even distribution though diff expected small given that all threadtasks are finished before refreshing ring ',negative
'there could collisions keytostring returns only part result ',negative
'object handling interaction ',negative
'search for the remaining bucket metrics ',negative
'gainleadership system event dont log issue ',negative
'only keep important conf keys ',negative
'javaxjms objects ',negative
'get the last successfully committed state from state store ',negative
'since only give leadership were waiting for blobs sync makes sense wait full sync cycle before trying for leadership again ',negative
'keys shouldnt appear twice ',negative
'filter ',negative
'any errorexception thrown report directly nimbus ',negative
'all time ',negative
'this should not happen localhost but does are still ',negative
'may the task restarted the middle and the state needs initialized fail fast and trigger recovery ',negative
'async sending ',negative
'enables avoid projection unless the final stream being joined ',negative
'when topology was launched ',negative
'lock timeout ',negative
'helper mainmethods ',negative
'something odd happened try again later ',negative
'groups this user authorized impersonate ',negative
'session timeout milliseconds ',negative
'stream overrides ',negative
'evicted enough topologies have hope scheduling try now and dont evict more than needed ',negative
'error ',negative
'attempt find the string cache ',negative
'validate search executor ',negative
'datasize ',negative
'intended not guarding with trywithresource since otherwise test will fail ',negative
'since using stringwriter not need close ',negative
'waiting for spout tuples isnt strictly necessary since also wait for bolt emits but anyway ',negative
'stormcore needed here for backwards compatibility ',negative
'eventloghost ',negative
'below regex uses negative lookahead not split the middle json objects json arrays this needed parse valid json objectarrays passed options via stormcmd windows this not issue while using stormpy since urlencodes the options and the below regex just does split the commas that separates each option note this regex handles only valid json strings and could produce invalid results the options contain unencoded invalid json strings with unmatched can replace below code with split once stormcmd fixed send urlencoded options ',negative
'node topologyid double ',negative
'return object ',negative
'the outputstream done ',negative
'',negative
'level parknanosl ',negative
'create framed transport ',negative
'wait until time out ',negative
'clean for resource isolation enabled ',negative
'devnull bolt ',negative
'release the reference all blobs associated with this topology ',negative
'lasterror ',negative
'check that blob can deleted when temporary file exists the blob directory ',negative
'fields with embedded commas doublequote characters ',negative
'this means but not ',negative
'messageblob ',negative
'noop for zookeeper implementation ',negative
'max number files delete for every round ',negative
'print lookup result with low probability ',negative
'only off the topology for now ',negative
'never rollback ',negative
'create another spout take over processing and read few lines ',negative
'delay delay current time are bigger than long max value ',negative
'this case are ignoring the coord stuff and only looking ',negative
'maximum number retries ',negative
'setup idbolt devnullbolt ',negative
'nodeid topologyid workerid execs ',negative
'all the jms unacked messages are going redelivered clear the pendingacks ',negative
'add all the owners the map ',negative
'prefer local tasks target tasks possible ',negative
'latest offset ',negative
'first failure add the count map ',negative
'handle input ',negative
'user authorized ',negative
'drop back down ',negative
'imperative that dont emit any tuples from here since output factory cannot gotten until preparation done therefore receivers wont ready receive tuples yet cant emit tuples from here anyway since its not within batch context which only startbatch execute and finishbatch ',negative
'topology tracking topology dir and resources ',negative
'skip uploading first one since dont test upload for now ',negative
'this can happen the supervisor crashed after launching worker that never came ',negative
'generate some that has alot cpu but little memory ',negative
'loop through the arguments hit list that has convered array perform the conversion ',negative
'the second part the test the rate doubles per second but the rate tracker increases its result slowly push the tuples per second buckets out and relpace them ',negative
'process stream definitions ',negative
'failed ',negative
'validate port ',negative
'not configurable prevent change between topology restarts ',negative
'topologies running noop ',negative
'for windowed bolt windowed output collector will the anchoringacking ',negative
'inject output bolt ',negative
'non blocking returns truefalse indicating successfailure fails full ',negative
'scansetcaching ',negative
'fail only the last tuple ',negative
'required required required required required required required optional optional optional optional optional optional optional optional optional optional optional ',negative
'consume file partially ',negative
'generate sasl response but only actually send the response its nonnull ',negative
'generate random numbers ',negative
'nothing except validate heartbeat for now ',negative
'retrieve authentication configuration ',negative
'send expire command for hash only once expires key itself entirely use with caution ',negative
'retry forever ',negative
'check minor optimization ',negative
'all writessyncs will fail this should cause runtimeexception ',negative
'now create params map put our conf ',negative
'convention each share corresponds cpu core core full time the guaranteed number approximately should ',negative
'optional optional optional optional ',negative
'returns messageids order emission ',negative
'required required required required required optional ',negative
'fail count greater than maxretries discard ack for for maxretries failures are allowed maximum ',negative
'eventhubs bolt configurations partition mode with partitionmodetrue you need create the same number tasks the number eventhubs partitions and each bolt task will only send data one partition the partition the task the bolt event format the formatter convert tuple bytes for eventhubs null the default format common delimited tuple fields ',negative
'guard npe ',negative
'replaces normal aggregation here with global grouping because needs consistent across batches ',negative
'add our collection ',negative
'login our principal ',negative
'find document from mongodb ',negative
'msg ',negative
'noop ',negative
'login our user ',negative
'first check that respected when emitting from scratch ',negative
'perform join ',negative
'only host should returned given filter ',negative
'verify correct wrappingunwrapping partition and delegation partition assignment ',negative
'for chainedaggregator ',negative
'have crummy cache show off shared memory accounting ',negative
'key already exists while creating the blob else update ',negative
'ensure that serializations are same for all tasks matter whats ',negative
'connection timeout milliseconds ',negative
'the following are required for backwards compatibility with clojure code ',negative
'spawn tar utility untar archive for full fledged unix behavior such resolving symlinks tar archives ',negative
'the contract rankablecopy returns rankable value not ',negative
'unauthorized ',negative
'create already existing open file override flag ',negative
'could also make this static but not due mock ',negative
'blobstore state ',negative
'key dir needs number number buckets choose one know where look ',negative
'compute window length adjustment delta account for time drift ',negative
'processmsavg ',negative
'backtracking algorithm does not take into account the ordering executors worker reduce traversal space ',negative
'ignore ',negative
'spoutbolt object ',negative
'sampling metrics ',negative
'inputstream done ',negative
'check processor has specific priority first ',negative
'skip uploading first one since want test rollback not upload ',negative
'bits ',negative
'bolt implementation ',negative
'read all the topologies ',negative
'required optional optional optional optional optional optional ',negative
'split the stream numbers into streams even and odd numbers the first stream contains even and the second contains odd numbers ',negative
'has been emitted and pending ack fail ',negative
'value greater than longmaxvalue truncates longmaxvalue ',negative
'use jsonscheme the default scheme ',negative
'build components that may referenced spouts bolts etc the map will string object where the object fully constructed class instance ',negative
'mapfunction aware prepare let handle preparation ',negative
'two constructor signatures used initialize validators one constructor takes input map arguments the other doesnt take any arguments default constructor validator has constructor that takes map argument call that constructor ',negative
'connect ',negative
'configure for achieving max throughput single worker mode empirically found expect millsec millsec with batchsz millsec lat microsec with acker batchsz ',negative
'return taskmessage object ',negative
'static fields ',negative
'metrics ',negative
'maxretries dont retry and return false per interface contract ',negative
'find the number data bytes length byte ',negative
'sharedmemoffheap ',negative
'add new connections atomically ',negative
'setup stringgen spout ',negative
'get numtasks ',negative
'watermark events are not added the queue ',negative
'some mount options and relatime type are not cgroups related ',negative
'bolts ',negative
'update histograms based the new summary most common implementation reporter reports gauges before histograms because derivativegauge will trigger cache refresh upon reporters query histogram will also updated before query ',negative
'employed when incoming data employed when outbound path congested ',negative
'update the offsetmanager for each committed partition and update ',negative
'create keyspace not supported the current datastax driver ',negative
'suppressing exceptions dont care for errors heartbeats ',negative
'schema changes should have forced file rotations ',negative
'test when supervisor and worker fails ras does not alter existing assignments ',negative
'finish reading the file ',negative
'',negative
'passing factory rather than the actual object avoid enforcing the strong requirement having have modelrunner serializable ',negative
'script ',negative
'initial setup ',negative
'string does not exist database ',negative
'dont use the classpath part this just empty list ',negative
'treat reassign remove and add ',negative
'get latest sequence number the blob present the zookeeper possible refactor this piece code ',negative
'throttle this spout bit avoid maxing out cpu ',negative
'whatever remains the tab are non matching left rows ',negative
'explicit anchoring emits corresponding input tuples only default window anchoring will anchor them all tuples window ',negative
'scheduling status set failother eviction policy will attempted make space for this topology ',negative
'banner ',negative
'didnt find but there are races want check again after sync ',negative
'empty factoryargs ',negative
'make the assignment null let slot clean the disk assignment ',negative
'authentication client complete will also send saslcomplete message the client ',negative
'race condition with another thread and lost try again ',negative
'convenience alternative prepare for use tests ',negative
'use while loop try decode more messages possible single call ',negative
'windows should aligned the slide size starting firstwindowendtime windowsec because all windows are aligned the slide size can partition the spout emitted timestamps which window they should fall this checks that the partitioned spout emits fall the expected windows based the logs from the spout and bolt ',negative
'shouldnt propagate any exceptions ',negative
'get one message time for backward compatibility behaviour ',negative
'seconds from now ',negative
'spout ',negative
'update the shard iterator next one case this fetch does not give the message ',negative
'local mode just get the local nimbus instance and set the heartbeats ',negative
'ensure that choice and choice are not the same task ',negative
'need extract not what are looking for ',negative
'queries the state and emits the matching key value results the stream state returned the updatestatebykey passed the argument statequery ',negative
'keep committing when topology deactivated since ack and fail keep getting called deactivated topology ',negative
'heartbeats related ',negative
'join helper concat fields the record ',negative
'convert all strings numeric ids for the metric key and add the metadata cache ',negative
'once storm baselined java can use charset instead which obsoletes this method ',negative
'make sure that defined key string case wrong stuff got put into configjava ',negative
'writing random words blacklisted ',negative
'more acls can added here ',negative
'wait for all locks expire then heart beat locks and verify stale lock ',negative
'idtospoutaggstats ',negative
'emit all messages and fail the first one while acking the rest ',negative
'aggregation stats mainmethods ',negative
'parse cmd line args ',negative
'schedule topology ',negative
'singleton instance allows mock delegated static mainmethods our tests subclassing ',negative
'purposely simulate second bucket size the rate will always per second ',negative
'wordspout lookupbolt ',negative
'stringarg ',negative
'you cant define topologysource and dsl topology the same time ',negative
'can null nested field xyz becomes stringxyz either streamxyz xyz depending whether stream name present ',negative
'task not used just pick static value ',negative
'make sure all workers scheduled rack ',negative
'requestedmemoffheap ',negative
'for testing ',negative
'parallelism double ',negative
'two distinct schema should result only two files ',negative
'initialize serverside sasl functionality havent yet which case are looking the first sasl message from the ',negative
'this spout owns partitions ',negative
'then get and return the file string ',negative
'sliding windows should produce one window every slidesize tuples wait for the spout emit least enough tuples get minboltemit windows and least one full window ',negative
'add all the components ',negative
'monitor for assignment changes often possible shutdown happens fast possible ',negative
'user defined callback ',negative
'let worker tokens work insecure ',negative
'were rescheduled while waiting for the resources updated but the container already not running ',negative
'nodehost not null here newconnections only nonempty assignment was not null above host port ',negative
'supervisor port should only presented worker which supports rpc heartbeat ',negative
'update the value state ',negative
'the first seek offset for each topic partition the offset this spout instance started processing ',negative
'cassandra daemon calls systemexit windows which kills the test ',negative
'rotate once per timeout period that has passed since last time this was called this necessary since this method may called arbitrary intervals ',negative
'timer discarded after the initial launch assignment ',negative
'check resources ',negative
'later this will joined back with returninfo and all the results ',negative
'based transactional topologies ',negative
'threadsleep ',negative
'jms queue provider ',negative
'remove resources ',negative
'this verify that low maxpollrecords does not interfere with reemitting failed tuples ',negative
'the node does not exist fineexpected ',negative
'default sliding window count ',negative
'verify that failed offsets will only retry the corresponding message exists when log compaction enabled kafka possible that tuple can fail and then impossible retry because the message kafka has been deleted the spout needs quietly ack such tuples allow commits progress past the deleted offset ',negative
'boolarg ',negative
'distribution should exactly even ',negative
'attempt find callers cache ',negative
'benchmark label ',negative
'messages ',negative
'merge the old credentials creds nimbus created are not lost and case the user forgot upload something important this time ',negative
'generated code will not compilable since return type myplus and type are different ',negative
'examine the response message from server ',negative
'force triggers building ring ',negative
'nothing here could not get message ',negative
'required ',negative
'try use zookeeper secret ',negative
'check for uncaught exceptions during the execution the trigger and fail fast the uncaught exceptions will wrapped executionexception and thrown when futureget invoked ',negative
'need relogin some other thread might have logged into hadoop using their credentials autohbase might also part nimbu auto creds ',negative
'when this master not leader and get sync request from node just return nil which will cause clientnode get unknown error the nodesupervisor will sync timer task ',negative
'size matches check the streams are expected ',negative
'emulate the call withlatetuplestream method ',negative
'set the stddev the skewed keys the length but then use the absolute value that everything skewed towards ',negative
'storm hbase keytab ',negative
'there are windowedbatched processors would ack immediately ',negative
'the spout should not emit any more tuples ',negative
'helper static vars for each platform ',negative
'taskend ',negative
'not change this sysout ',negative
'try adding main queue its overflow not empty ',negative
'bolt bolt bolt ',negative
'new assignment ',negative
'generally used compare how files were actually written and compare expectations based total ',negative
'retries the connect fails ',negative
'then stop running ',negative
'commonstats ',negative
'find the task the events from this component route ',negative
'handle ctrlc ',negative
'the empty partition was not invalidated flush but evicted from cache ',negative
'componentexecutors maybe apersistentmap which dont support put ',negative
'possible get asked about eviction before have context due this case should hold all the events when the first watermark received the context will set and the events will reevaluated for eviction ',negative
'add tuple the batch state ',negative
'redis config parameters for the redisstorebolt ',negative
'this fine because the only time this shared when its local context ',negative
'usedports ',negative
'schedulermeta ',negative
'the oldest pqsize files this directory will placed with the newest the root ',negative
'required optional ',negative
'make sure all workers scheduled rack ',negative
'read remaining lines file then ensure lock gone ',negative
'get currently unused unique string ',negative
'use streamid source component name field tuple distinguish incoming tuple streams ',negative
'the spout must reemit retriable tuples even they fail out order the spout should able skip tuples has already emitted when retrying messages even those tuples are also retries ',negative
'business logic ',negative
'user jerry submits another topology but this one should scheduled since has higher priority than than the ',negative
'the last slot fill ',negative
'operation level exceptions ',negative
'extract bolt resource info ',negative
'executed ',negative
'sort port from small large ',negative
'sort comps number constraints ',negative
'build new metadata based the consumer position want the next emit start the current consumer position make meta that indicates that position the last emitted offset this helps avoid cases like storm and simplifies the seek logic ',negative
'emit empty batches simulating new records being present kafka ',negative
'atmostonce mode the offsets are committed after every poll and not periodically controlled the timer ',negative
'test topology already partially scheduled one rack ',negative
'',negative
'total tuples should recovered batch batch replayed batch ',negative
'its possible that permissions were not set properly the directory and the user who supposed own the dir does not this case try the delete the supervisor user ',negative
'create the directory matter what this can check was downloaded the future ',negative
'busy wait ',negative
'short ',negative
'check that the emitter can handle emitting empty batches new partition the spout configured seek latest the partition empty the initial batches may empty ',negative
'string metastoredblocation ',negative
'this here avoid overridable call the constructor ',negative
'pending empty return the smallest toresend ',negative
'attempt scheduling both topologies this triggered negative resource event the second topology incorrectly scheduled with the first place ',negative
'ignored the file system are does not support this dont ',negative
'kill process when timeout ',negative
'find login file configuration from storm configuration ',negative
'compareandset added because ',negative
'test for subject with principals ',negative
'connect callback implementation ',negative
'other case just set this trigger resync later ',negative
'force process terminated ',negative
'acquire slot ',negative
'returns list list slots reverse sorted number slots ',negative
'play and fail tuple ',negative
'delete ',negative
'doing below because its affecting storm metrics most likely had make tick tuple mandatory argument since its positional ',negative
'thread safety mostly around acl acl needs updated changed atomically more then one thread may trying update time but that because the change atomic ',negative
'could replaced when metrics support remove all functions ',negative
'required optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional ',negative
'else assume the same teh superacl which covered creatorall ',negative
'topo has small tasks should able exactly use both the cpu and mem the cluster ',negative
'ensure that the original batch discarded and new one persisted ',negative
'was not there before need run ',negative
'stream random sentences ',negative
'reference archive ',negative
'default all the values are ',negative
'object representing information paramaters use while connecting kinesis using kinesis client ',negative
'read lines file ',negative
'noop grouper ',negative
'release things that dont need wait for ',negative
'trigger manually avoid timing issues ',negative
'actual value mapstring longdouble win stream value ',negative
'file should read ',negative
'check see any shard has already fetched records waiting emitted which case dont fetch more ',negative
'for testing ',negative
'local mode the main process should never exit its own ',negative
'first delete everything that longer exists ',negative
'killoptions ',negative
'additional tests that beyond what this test primarily about ',negative
'old buckets and their length are only touched when rotating gathering the metrics which should not that frequent such all access them should protected synchronizing with the ratetracker instance ',negative
'null tuple should added the ack list since definition direct ack ',negative
'dont know better way validate that failed ',negative
'the tick should not cause any acks since the batch was empty except for acking itself ',negative
'have three mutually disjoint treesets per shard any given time keep track what sequence number can committed zookeeper emittedpershard ackedpershard and failedpershard any record starts entering emittedpershard ack moves from emittedpershard ackedpershard and fail retry service tells retry then moves from emittedpershard failedpershard the failed records will move from failedpershard emittedpershard when the failed record emitted again retry logic for deciding what sequence number commit find the highest sequence number from ackedpershard called such that there sequence number emittedpershard failedpershard that satisfies for ackedpershard emittedpershard and failedpershard then can only commit and not because still pending and has failed ',negative
'use exhibitor servers ',negative
'reserved for saving topology data ',negative
'map records that were fetched from kinesis result failure and are waiting emitted ',negative
'this can things like have simple drpc that doesnt need use batch processing ',negative
'stop iterating the middle the partition ',negative
'componenttype ',negative
'configure the server ',negative
'assign partitions the spout ',negative
'its return null ',negative
'should assert file size ',negative
'approvedworkers ',negative
'jms queue spout ',negative
'first make the cache dir ',negative
'emit some more messages ',negative
'just case get something are confused about can continue processing the rest the tasks ',negative
'need add the new futures the existing ones ',negative
'logging exception while client connecting ',negative
'the scheduler generally will try pack executors into workers until the max heap size met but this can vary depending the specific scheduling strategy selected the reason for this try and balance the maximum pause time might take which larger for larger heaps against better performance because not needing tuples ',negative
'wordspout countbolt mongoinsertbolt ',negative
'isleader ',negative
'unknown error just skip ',negative
'factory methods declaring modeloutputs multiple streams ',negative
'session ',negative
'executornodeport ',negative
'index and fieldindex are precomputed delegates built over many operations using persistent data structures ',negative
'validation only configs some configs inside configjava may reference classes dont want expose stormclient but still want validate that they reference valid class allow this happen part the validation the client side with annotations static final members the config class and other validations here avoid naming them the same thing because clojure code walks these two classes and creates clojure constants for these values ',negative
'pulses ',negative
'common ',negative
'required for jackson serialization ',negative
'timestamp millisecond means disabling filter ',negative
'todo batch finding ',negative
'perf critical loop dont use iterators ',negative
'componentid ',negative
'commit frequency seconds ',negative
'there race here that can still lose ',negative
'jprofile stop ',negative
'class ',negative
'aggworkerstats tests ',negative
'break the code out sync thrift protocol ',negative
'server ',negative
'status scheduling the topology success fail ',negative
'with realm stormwitzendcom ',negative
'calc sidoutputstats ',negative
'',negative
'the removed writer should have been closed ',negative
'jmock see for more information ',negative
'convert each record into hashmap using fieldnames keys ',negative
'used for local cluster heartbeating ',negative
'servicetype ',negative
'apply new log settings just received ',negative
'being null means the source spout node ',negative
'look hdfs blobstore ',negative
'the first tuple should acked and should not have failed ',negative
'this section simply put the formatted log filename and corresponding port the matching ',negative
'topoowner ',negative
'filter configured should fail all users ',negative
'validating class implementation could fail nonnimbus daemons nimbus will catch the class not found startup and log error message just validating this string for now ',negative
'clocks are not sync ',negative
'jdkversion ',negative
'read record from file emit collector and record progress ',negative
'this should always set digest ',negative
'were rescheduled while waiting for the worker come ',negative
'get from and add lookup cache ',negative
'list all files for this topology ',negative
'examine the response message from server ',negative
'alice has digest jaas section all ',negative
'test basic substitution ',negative
'monotonically increasing for correlating sentrecvd msgs restarts from crash ',negative
'for unit tests ',negative
'just mkdir stormzookeeperroot dir ',negative
'nothing else should emitted all tuples are acked except for the final tuple which pending ',negative
'topologyname ',negative
'ensure checkpoint interval not less than sane low value ',negative
'end topology state transitions ',negative
'get metric name ',negative
'matches and matchcount not changed ',negative
'but throughput the same ',negative
'without auth ',negative
'creds ',negative
'tracks order which msg came ',negative
'the first tuple wil used check timeout reset ',negative
'hbase principal stormhbasewitzencom ',negative
'write the tuple jms destination ',negative
'call updatemetricfromrpc with params ',negative
'this here only for testing ',negative
'nothing ',negative
'preserve interrupt status ',negative
'how many states searched far ',negative
'noop need create links local mode ',negative
'usage let and then explicitly terminate metrics will printed when application terminated ',negative
'propagate that task gets canceled and the exception can retrieved from executorfutureget ',negative
'allow blacklist scheduler cache the supervisor with added port ',negative
'flushes local and remote messages ',negative
'allow the revocation hook commit offsets for the revoked partitions ',negative
'roll next ',negative
'topology will scheduled ',negative
'ensure filename doesnt contain parts ',negative
'reinit cache and partitions ',negative
'boilerplate overrides cast result from base type joinbolt user doesnt have down cast when invoking these mainmethods ',negative
'case nonicontext plugin must have makecontexttopoconf method that returns icontext object ',negative
'metrics has just been collected lets also log ',negative
'not exposed withclockclock ',negative
'validate search component ',negative
'remove any previously created cache instance ',negative
'release earliest blacklist but release all supervisors given blacklisted host ',negative
'stop counting when past current time ',negative
'this can happen when multiple clients doing mkdir same time ',negative
'remove uploaded jars blobs not artifacts since theyre shared across the cluster ',negative
'creates brand new tuples with brand new fields ',negative
'topology metrics distribution ',negative
'its method with zero args ',negative
'not call default constructor ',negative
'its nonnull ',negative
'storm ',negative
'done capturing topology information ',negative
'next tuple ',negative
'the metrics processor not critical the operation the cluster allow supervisor come ',negative
'add more events with past ',negative
'force create windowed bolt with identity nodes that dont have stateful processor inside windowed bolt ',negative
'intarg ',negative
'configs kafka spout ',negative
'its likely that bolt shutting down need die just ignore and loop will terminated eventually ',negative
'get all the tuples batch and add ',negative
'not process current timestamp since tuples might arrive while the trigger executing ',negative
'when tuple tracking enabled the spout must not replay tuples atmostonce mode ',negative
'there are records not call flush ',negative
'add null for missing fields usually case outer joins ',negative
'dont want run the test cgroups are not setup ',negative
'initialize the network topography ',negative
'read remaining lines ',negative
'resolve references ',negative
'for completeness ',negative
'the topology are scheduling ',negative
'the node already deleted nothing ',negative
'should still fail ',negative
'factories ',negative
'test when supervisor fails ras does not alter existing assignments ',negative
'start trigger once the initialization done ',negative
'reference key ',negative
'backtracking ever get here there really isnt lot hope that will find scheduling ',negative
'kafka consumer configuration ',negative
'ensure instance per producer thd ',negative
'',negative
'ignored not set ',negative
'now fail tuple partition one and verify that allowed retry because the failed tuple below the limit ',negative
'ack the tuple and commit the waiting emit list should now cleared and the next emitted tuple should the last tuple the partition which hasnt been emitted yet ',negative
'kept memory avoid going kinesis again for retry ',negative
'counted ',negative
'isset assignments ',negative
'the supervisors this also allows you declare the serializations sequence ',negative
'should not show files outside daemon log root ',negative
'class filelockingthread ',negative
'closing the channel and reconnecting should done before handling the messages ',negative
'all internal state except for the count the current bucket are ',negative
'workers requested more than currently have ',negative
'streamid ',negative
'path ',negative
'now add some more events and new watermark and check that the previous events are expired ',negative
'tasks figure out what tasks talk looking topology runtime ',negative
'instead iterating over all the tuples the window compute the sum the values for the new events are added and old events are subtracted similar optimizations might possible other windowing computations ',negative
'answer when ask for private key ',negative
'fail ',negative
'schedstatus ',negative
'pick worker mock failed ',negative
'batch ',negative
'data ',negative
'offset committed are failed but not retriable the spout should now emit another maxpollrecords messages ',negative
'only start those requested ',negative
'set use the default resource aware strategy when using the ',negative
'boltspecific configuration for windowed bolts specify the window length count number tuples the window ',negative
'topologyconf ',negative
'check format ',negative
'remove the port from the supervisor and make sure the blacklist scheduler can remove the port without ',negative
'create wrap transport factory that could apply user credential during connections ',negative
'component stats ',negative
'invalidate after releasing the lock the parition pinned before could invalidate will get invalidated the next flush when the entry gets evicted from the cache ',negative
'link the components together ',negative
'mark the current buffer position before reading tasklen field because the whole frame might not the buffer yet will reset the buffer position the marked position ',negative
'attempt find the string cache this will update the lru ',negative
'kafka spout configuration ',negative
'json record doesnt need columns the same order table hive ',negative
'the interval between retries exhibitor operation ',negative
'value fields ',negative
'running daemon mode would pass error calling thread ',negative
'batch ',negative
'resources assigned ras resource aware scheduler ',negative
'',negative
'found good result are done ',negative
'pending still empty ',negative
'informs other workers about back pressure situation runs the nettyworker thread ',negative
'sleep for mins ',negative
'legacy resource parsing ',negative
'required required ',negative
'last offset this batch ',negative
'given processornodes and static state nodes ',negative
'create the blobstore ',negative
'the window boundaries are windowstart windowend ',negative
'batch ',negative
'',negative
'storm ',negative
'could make this configurable the future ',negative
'sub process used execute the command ',negative
'that worker running and moves ',negative
'map track next retry time for each kinesis message that failed ',negative
'executorstats ',negative
'rwx ',negative
'long crctuple tuple ',negative
'spout ',negative
'need more data ',negative
'wait all events expire ',negative
'create new session the user gave empty session string this the use case when the user wishes list blobs starting from the beginning ',negative
'host ',negative
'called after all executor objects the worker are created and before this object used ',negative
'redis has three chunks which last chunk only has entries ',negative
'read property file for extra consumer properties ',negative
'debug logging turned should just log the leader all the time ',negative
'lock file has expired then own ',negative
'iterate workersummary and find the one with the port ',negative
'now can calculate percentage ',negative
'queue records per shard fetched from kinesis and are waiting emitted ',negative
'change the change this minutes ',negative
'timestamp ',negative
'spout overrides ',negative
'merge with existing statuses ',negative
'least ',negative
'sorted set records retrued based retry time earliest retrytime record comes first ',negative
'check that only subscribed one componentstream for statespout setsubscribedstate appropriately ',negative
'factory mainmethods ',negative
'get sequence number details from latest sequence number the blob ',negative
'create stream random numbers from spout that emits random integers extracting the tuple value index ',negative
'schedule topology history cleaner ',negative
'will commit progress into lock file commit threshold reached ',negative
'create and configure topology ',negative
'map hold partition level and topic level metrics ',negative
'setup some filesdirs emulate supervisor restart ',negative
'the versions are different roll back whatever current ',negative
'only for test ',negative
'supervisor contains bad slots ',negative
'couldshould use ',negative
'save the current state for recovery ',negative
'sub directories store either files uncompressed archives respectively ',negative
'dependencyjars ',negative
'ksec ',negative
'the subset earliest retriable offsets that are pollable partitions ',negative
'convert millis ',negative
'workersummaries ',negative
'class configs ',negative
'call firechannelread since the client allowed perform this request the clients request will now proceed the next ',negative
'enabledisable acking ',negative
'tgt not bother with ticket management ',negative
'fileoffset the index last scanned file ',negative
'the failed tuples are ready for retry make appear like and the first partition were compacted away this case the second partition acts control verify that only skip past offsets that are longer present ',negative
'worker dirs ',negative
'this point there nothing all likelihood any filesystem operations will fail the next tuple will almost certainly fail write andor sync which force rotation that will give rotateandreset chance work which includes creating fresh file handle ',negative
'required required required required required required ',negative
'final stormtopology topology ',negative
'loglevel ',negative
'which case its noop ',negative
'verify that the poll started the earliest retriable tuple offset ',negative
'now check memory only ',negative
'continue without idling ',negative
'init the writer once the cache setup ',negative
'this because cant currently merge splitting logic into spout not the most kosher algorithm here since the grouper indexes are being trounced via the adding nodes random groups but ',negative
'load pmml model from blob store ',negative
'add nimbuses ',negative
'heap memory used help calculate the heap the java process for the worker off heap memory for things like jni memory allocated off heap when using the shellbolt shellspout this case the off heap just example are not using ',negative
'expectedowner being null means that security disabled which why are uploading credentials with security disabled ',negative
'allowedid null the constructor can assign what needsetc ',negative
'verify that more tuples are emitted and all tuples are committed ',negative
'emitted messages for partitions that are longer assigned this spout cant acked and should not retried hence remove them from emitted collection ',negative
'treat supervisor bad only all its slots matched the cached supervisor track how many times cached supervisor has been marked bad ',negative
'spouts ',negative
'acked once ',negative
'get load from the cache optionally pinning the entry that wont get evicted from the cache ',negative
'key for hdfs kerberos configs ',negative
