@relation 'technicalDebt'

@attribute Text string
@attribute class-att {negative,positive}

@data

'fixme probably this should also integrated with issame logics ',positive
'wait for block just sure not ideal ',positive
'hive not sure this correct dont think gets wrapped udftointeger ',positive
'expect this never happen practice can pool paths even have angled braces ',positive
'todo this does not work correctly none the partitions created but the folder for the first two created because when going through the partitions the first two are already put and started the thread pool when the exception occurs the third one when the exception occurs the finally part but the map can empty depends the progress the other threads the folders wont deleted pathtablelocation year ',positive
'todo remove this after stashing only rqd pieces from opconverter ',positive
'todo should try make giant array for one cache call avoid overhead ',positive
'forward the row reducer discard the row vectorized may forward the row not sure yet ',positive
'handle any move session requests the way move session works right now sessions get moved destination pool there capacity destination pool there capacity destination pool the session gets killed since cannot pause query todo future this the process killing can delayed until the point where session actually required could consider delaying the move when destination capacity full until there claim src pool may change command support delayed move etl which will run under src cluster fraction long ',positive
'not threadsafe ',positive
'would better had errorcode field ',positive
'todo this can probably replaced with much less code via dynamic dispatch andor templates ',positive
'todo handle this properly ',positive
'todo need maxlength checking ',positive
'todo prewarm and update can probably merged ',positive
'deprecated favour link removed hive ',positive
'todo disabling this test tez publishes counters only after task completion which will cause write side counters not validated correctly dag will completed before validation testtimeout ',positive
'todo might want increase the default batch size viable gets oom too high ',positive
'todo remove when method relmdsize ',positive
'store all caches variables change the main one based config this not thread safe between different split generations and wasnt anyway ',positive
'todo once shuffle out this can use apis convert between app and job ',positive
'todo does this need the finaldestination ',positive
'todo add limit instead count should more efficient ',positive
'todo not support listing resources command beeline list jar ',positive
'note scheduler will call this based lack sources schedule time and set this true theres easy way work around this need better classes ',positive
'todo will this also fix windowing try ',positive
'but lets make little bit more explicit ',positive
'todo this should inherit from volcanocost and should just override isle method ',positive
'todo check for duplicates assume clause values present ndv which may not correct range check can find assume values ndv set uniformly distributed over col values account for skewness histogram ',positive
'todo didnt care about the column order could switch join sides here ',positive
'todo could remember its unsupported and stop sending calls although might bad idea for hsstandalone metastore that could updated with support maybe should just remember this for some time ',positive
'deprecated favour link removed hive ',positive
'todo why this like that ',positive
'todo escape handling may changed follow the largest issue which are treated statement terminators for the cli once the cli fixed this code should reinvestigated ',positive
'delete data fail table doesnt exist need results back ',positive
'todo this does not work correctly none the partitions created but the folder for the first two created because when going through the partitions the first two are already put and started the thread pool when the exception occurs the third one when the exception occurs the finally part but the map can empty depends the progress the other threads the folders wont deleted ',positive
'note assume that this isnt already malformed query dont check for that here will fail later anyway ',positive
'todo not clear should cache these not for now dont bother ',positive
'todo this needs removed see comments ',positive
'bug this will not work remote mode hive ',positive
'todo change this method make the output easier parse parse programmatically ',positive
'todo remove ',positive
'wouldnt make more sense return the first element the list returned the previous call ',positive
'better logic would find the alias ',positive
'need enforce the size here even the types are the same ',positive
'this method may not safe can throw npe key value null ',positive
'todo this ugly hack because tez plugin isolation does not make sense for llap plugins are going register threadlocal here for now that the scheduler initializing the same thread after the communicator will pick the other way around ',positive
'todo mssplit uncomment once move eventmessage over ',positive
'follow hives rules for type inference oppose calcites for return type todo perhaps should this for all functions not just ',positive
'todo hive while getting stats from metastore currently only get one col time this could improved get all necessary columns advance then use local todo hive aggregations could done directly metastore hive over mysql ',positive
'todo see planindexreading this not needed here ',positive
'todo why stored both table and dpctx ',positive
'todo use threadpool for more concurrency todo checkset all files only directories ',positive
'todo hive handle additional reasons like launch failed ',positive
'todo handle agg func name translation correct add func ',positive
'todo both these are texception why need these separate clauses ',positive
'todo two hss start exactly the same time which could happen during coordinated restart they could start generating the same ids should store the starttime ',positive
'todo given the specific data and lookups perhaps the nested thing should not map fact cslm has slow singlethreaded operation and one file probably often read just one few threads much more simple with locking might better lets use cslm for now since its available ',positive
'todo ideally this should done independent whether setup not ',positive
'this not modeled before because needs parameterized pertest there better way this should ',positive
'todo hive ',positive
'make sure nullvalued confvar properties not override the hadoop configuration note comment out the following test case for now until better way test found this test case cannot reliably tested the reason for this that hive does overwrite fsdefaultname hiveconf the property set system properties coresitexml null coresitexml ',positive
'deprecated favour link removed hive ',positive
'todo due value this probably should throw exception ',positive
'deprecated favour link removed hive ',positive
'hmm not good the only type expected here struct which maps hcatrecord anything else error return null the inspector ',positive
'todo should these rather arrays ',positive
'todo change all this based regular interface instead relying the proto service exception signatures cannot controlled without this for the moment ',positive
'will implemented later ',positive
'todo actually need this reader the caller just extracts child readers ',positive
'todo hive what mergework and why not part the regular operator chain the call further down can block and will not receive information about abort request ',positive
'todo hive handle cases where nodes down and come back the same port historic information ',positive
'todo most protocol exceptions are probably unrecoverable throw ',positive
'todo also need remove the mapjoin from the list rss children ',positive
'todo why there tezsession execdriver ',positive
'replace insert overwrite merge equivalent rewriting here need this complex ast rewriting that generates the same plan that merge clause would generate because cbo does not support merge yet todo support merge first class member cbo simplify this logic ',positive
'todo current implementation replication will result droppartition under replication scope being called perpartition instead multiple partitions however robust must still handle the case multiple partitions case this assumption changes the future however this assumption changes will not very performant fetch each partition onebyone and then decide inspection whether not this candidate for dropping thus need way push this filter the metastore allow drop partition not depending predicate the parameter key values ',positive
'todo should pass curr instead null ',positive
'really not sure this should here will have see how the storage mechanism evolves ',positive
'todo force file setup staging dir confsetfsdefaultfs file tmp ',positive
'todo why not just use getrootdir ',positive
'todo lossy conversion ',positive
'todo remove the copy after orc and orc ',positive
'todo the name doesnt match with the metadata from dump then need rewrite the original and expanded texts using new name currently refers the source database name ',positive
'todo remove this constructor ',positive
'todo add the ability extractfiletail read from multiple buffers ',positive
'todo instead simply restricting message format should eventually move jdbcdriverstype registering message format and picking message factory per event decode for now however since all messages have the same factory restricting message format effectively guard against older leftover data that would cause problems ',positive
'todo when hive moves java make updatetimezone default method ',positive
'not strictly necessary noone will look ',positive
'exponent part scaling down while scale scaling now its tricky unscaledvalue significand scale twoscaledown ',positive
'todo cat fairly certain that most calls this are error this should only used when the database location unset which should never happen except when new database being created once have confirmation this change calls this getdatabasepath since does the right thing also merge this with duplicates much the logic ',positive
'todo should this the concern the mutator ',positive
'todo provide support for reporting errors this should never happen server always returns valid status success ',positive
'todo ideally when the splits udf made proper api coordinator should not managed global should create and then pass around ',positive
'todo convert sqlstate etc ',positive
'todo include externalpreemption this list ',positive
'workaround avroutf ',positive
'todo needs initializeinput part inputjobinfo ',positive
'note that this logic may drop some the tables the database even the drop database fail for any reason todo fix this ',positive
'some walkers extending defaultgraphwalker forwardwalker not use opqueue and rely uniquely the towalk structure thus store the results produced the dispatcher here todo rewriting the logic those walkers use opqueue ',positive
'this corner case where have extract time unit like daymonth pushed extraction todo the best way fix this add explicit output druid types calcite extraction functions impls ',positive
'workaround for hadoop remove when hadoop longer supported ',positive
'could pass the number nodes that expect instead also single concurrent request per node currently hardcoded ',positive
'todo strangely the default parametrization ignore missing tables ',positive
'fixme should clean testpath but not doing now for debuggings sake ',positive
'this will throw error close pout early ',positive
'not very clean way and should modified later due compatibility reasons user sees the results json for custom scripts and has way for specifying that right now hardcoded the code ',positive
'todo this test passes fine locally but fails linux not sure why ',positive
'todo eventually calls hadoop method that used buggy and also anyway just does copy for direct buffer copy here ',positive
'todo expand other functions needed what about types other than primitive ',positive
'set footer cache for current split generation see field comment not thread safe ',positive
'todo the below seem like they should just combined into partitiondesc ',positive
'callablewithndc inherits from ndc only when call invoked callablewithndc has extended provide access its ndcstack that cloned during creation until then will use reflection access the private field fixme hive follow remove this reflection ',positive
'todo this brittle who said everyone has upgrade using upgrade process ',positive
'check whether task can run completion may end blocking its sources this currently happens via looking source state todo eventually this should lookup the hive processor figure out whether its reached state where can finish especially cases failures after data has been fetched return true the task can finish false otherwise ',positive
'undone how look for all nulls multikey let nulls through for now ',positive
'note this can still conflict with parallel transactions not currently handle parallel changes from two admins design ',positive
'todo time good enough for now well likely improve this may also work something the equivalent pid thrid and move nanos ensure uniqueness ',positive
'fixme somehow place pointers that reexecution compilation have failed the query have been successfully compiled before ',positive
'undone havent finished isrepeated ',positive
'hack instead figure out way get the paths ',positive
'todo support complex types for complex type simply return ',positive
'todo the control flow for this needs defined hive supposed threadlocal ',positive
'todo should the top the file ',positive
'get rid tokselexpr ',positive
'clear the work map after build todo remove caching instead ',positive
'donotupdatestats supposed transient parameter that only passed via rpc want avoid this property from being persistent note this property set table property will remove which incorrect but cant distinguish between these two cases this problem was introduced hive better approach would pass the property ',positive
'this using the payload from the corresponding inputname ideally should using its own configuration class but that ',positive
'like cannot used with this todo ',positive
'deprecated favour link removed hive ',positive
'todo once hbase completed use that api switch using mapreduce version the apis rather than mapred copied from hbases tablemapreduceutil since not public api ',positive
'todo fetch partitions batches todo threadpool process partitions ',positive
'todo simple wrap rethrow for now clean with error codes ',positive
'todo add method only get current skip history more efficient ',positive
'simple wrapper object with objectinspector todo need redefine the hashcode and equals methods that can put into hashmap key this class also serves facility for function that returns both object and objectinspector ',positive
'the ordering types here used determine which numeric types are commonconvertible one another probably better rely the ordering explicitly defined here than assume that the enum values that were arbitrarily assigned primitivecategory work for our purposes ',positive
'something else wrong ',positive
'todo comments rexshuttlevisitcall mention other types this category need resolve those together and preferably the base class rexshuttle ',positive
'todo verify this needed why cant always nullempty ',positive
'the outer join that saw most recently right outer join ',positive
'todo this relies hdfs not changing the format assume could get inode this still going work otherwise file ids can turned off later should use public utility method hdfs obtain the inodebased path ',positive
'todo should rather prefix ',positive
'todo this method temporary ideally hive should only need pass tez the amount memory requires the map join and tez should take care figuring out how much allocate adjust the percentage memory reserved for the processor from tez based the actual requested memory the map join return the adjusted percentage ',positive
'hack verify that authorization check passed exception can thrown cause the functions are not being called with valid params verify that exception has come from objectstore code which means that the ',positive
'todo should this use ',positive
'todo move this calcite ',positive
'todo add this metadatareader orc metadata buffer not just metadata ',positive
'todo this should come from type system currently there definition ',positive
'todo the moment theres way knowing whether query running not race possible between dagcomplete and registerfragment where the registerfragment processed after dagcompletes may need keep track completed dags for certain time duration avoid this alternately send explicit dag start message before any other message processed multiple threads communicating from single gets the way this ',positive
'should allow writing nontransactional tables explicit transaction the user may issue rollback but these tables wont rollback can this checking determine whether its readingwriting any non acid and raise appropriate error driveracidsinks and can used any acid the query ',positive
'todo implement ',positive
'todo what this checking ',positive
'think this wrong the drop table statement should come the table topic not the topic alan ',positive
'todo rather tez sessions should not depend sessionstate ',positive
'todo support tezvectorization ',positive
'todo read this somewhere useful like the task scheduler ',positive
'todo add support for serialization values here ',positive
'todo convert genincludedcolumns and setsearchargument use typedescription ',positive
'todo when this code little less hot change most logs debug will determine what under lock and then stuff outside the lock the approach statebased consider the task have duck when have decided give one the sends below merely fix the discrepancy with the actual state may add the ability wait for llaps positively ack the revokes future the procedural approach requires that track the ducks traveling network concurrent terminations etc while more precise its much more complex ',positive
'todo why doesnt this use context ',positive
'note this redundant with types ',positive
'the following lines are exactly what mysql does todo why this ',positive
'temporary variable for testing this added just turn off this feature case bug deployment has not been documented hivedefaultxml intentionally this should removed ',positive
'todo need session handle ',positive
'this kind hacky the read entity contains the old table whereas the write entity contains the new table this needed for rename both the old and the new table names are passed ',positive
'undone for now dont add more small keys ',positive
'todo can this ever happen ',positive
'todo theres potential problem here some table uses external schema like avro with very large type name seems like the view does not derive the serde from the table wont able just get the type from the deserializer like the table does wont able properly store the type the rdbms metastore ',positive
'todo this should depends input format and map something ',positive
'todo can actually consider storing all the delta encoded row offsets not lot overhead compared the data itself and with row offsets could use columnar blocks for inconsistent splits are not optimizing for inconsistent splits for now ',positive
'todo should this just use physical ids ',positive
'todo wtf why this this method this has nothing with anything ',positive
'this class isnt used and suspect does totally the wrong thing its only here that can provide some output format the tables and partitions create actually write ',positive
'todo does arg need type cast ',positive
'what ',positive
'todo not sure this the right exception ',positive
'need convert the hive type the sql type name todo this would better handled enum ',positive
'todo npe should not thrown ',positive
'consider allocate larger initial size ',positive
'good way find out may even have app ',positive
'hardcode sasl here zkdtsm only supports none sasl and never want none ',positive
'support listobject setobject and object have differently ',positive
'still null after just having initialized bail out somethings very wrong ',positive
'todo convert this assertfail once hive fixed ',positive
'todo should able enable caches separately ',positive
'for dynamic partitioned hash join the big table will also coming from reducesinkoperator check for this condition todo use indexof parentrsgettag ',positive
'todo following hivesubqueryfinder has been copied from since there bug there calcite once calcite fixed should get rid the following code ',positive
'todo extend rule can applied for these cases ',positive
'todo support case date ',positive
'todo figure out better way set repeat for binary type ',positive
'todo are with breaking compatibility existing party storagehandlers this method could moved the hivestoragehandler interface ',positive
'todo this global lock may not necessary all concurrent methods are synchronized ',positive
'todo should this make sure set jobs ',positive
'builder for relational expressions todo note that this copied from calcites relbulder because calcite hasnt been fixed yet this should deleted and replaced with relbuilder subqueryremoverule once calcite fixed edit although calcite has been fixed and released but hive now has special handling join gets flag see semi join created not still can not replace this with calcites relbuilder pcode relbuilder does not make possible anything that you could not also accomplish calling the factory methods the particular relational expression but makes common tasks more straightforward and concise pcode relbuilder uses factories create relational expressions default uses the default factories which create logical relational expressions link link and forth but you could override those factories that say code filter creates instead code hivefilter pit not threadsafe ',positive
'todo setting autocommit should not generate exception long set false ',positive
'todo precision and scale would practically invalid for string conversion ',positive
'todo should this really default fetchnext ',positive
'todo not sure about the use this should instead use workeridentity sessionid ',positive
'deprecated favour link removed hive ',positive
'todo trace ranges here between data cache and incomplete cache ',positive
'fixme retain old error create new one ',positive
'dont remove the work from the sparkwork here the removal done later ',positive
'todo need move from python java for the rest the script ',positive
'check input pruning possible todo this code buggy relies having one file per bucket support design ',positive
'todo strictly speaking you can commit empty txn thus conjunct wrong but only possible for for multistmt txns ',positive
'view ddl alter view add partition does not work because the nature implementation the ddl hive hive will internally invoke another driver the select statement and hcat does not let select statement through cannot find way get around without modifying hive code just leave unsupported ',positive
'todo test these need link against libs maven profiles table tflat int int stored orc table tflattext int int stored textfile ',positive
'note this hacky and this section code fragile depending code varnames its likely stop working some time the future especially upgrade versions actively need find better way make sure the leak doesnt happen instead just clearing out the cache after every call ',positive
'todo should never happen ',positive
'undone dont know why hive causes this return assertequals readergetprogress ',positive
'deprecated favour link removed hive ',positive
'not support windows will revisit this really need for windows ',positive
'pass empty list columns will written the file todo should able make this work for update ',positive
'todo throw exception ',positive
'todo move dynamicserde when its ready ',positive
'todo replace with direct call progresshelper when reliably available ',positive
'todo this the only place that uses keeptmpdir why ',positive
'todo use when the typo fixed ',positive
'indicates whether node had recent communication failure this primarily for tracking and logging purposes for the moment todo some point treat task rejection and communication failures differently ',positive
'todo move other protocols use this too ',positive
'todo move the following properties out configuration constant ',positive
'creates size estimators for java objects the estimators attempt most the reflection work initialization time and also take some shortcuts minimize the amount work done during the actual estimation todo clean ',positive
'todo move base class ',positive
'since the mapjoin has had all its other parents removed this point would bad here tries anything ',positive
'todo should this done for use with ',positive
'todo this should changed evaluated lazily especially for single segment case ',positive
'todo disable blacklisting tez when using llap until this properly supported blacklisting can cause containers move terminating state which can cause attempt marked failed this becomes problematic when set allowedfailures todo hive what happens when try scheduling task node that tez this point thinks blacklisted ',positive
'todo should this check conformtoacid ',positive
'todo split count not same buckets ',positive
'todo cols that come through ptf should retain virtualcolumness ',positive
'todo verify any quoting needed for keys ',positive
'todo this should check that the job actually completed and likely use completion time ',positive
'todo move this logicalequals ',positive
'todobr could use combined instead list use column processing from why not use expr ',positive
'todo temporary for debugging doesnt interfere with mtt failures unlike logdebug ',positive
'todo deprecation reason does not seem reflect the config the ordering important case keys which are also deprecated unset will unset the deprecated keys and all its variants ',positive
'grpset col already part input todo cant just copy the exprnodedesc from input need explicitly set table alias null false ',positive
'todo dataoperationtype set conservatively here wed really want distinguish updatedelete and insertselect and resource that written acid not ',positive
'todo gap ctas may currently broken used work see the old code and why isctas isnt used ',positive
'todo add partitioned table that needs conversion mmacid ',positive
'todo really need all this nonsense ',positive
'this bad but have sort the keys the maps order commutative ',positive
'todo this seems the same really need both ',positive
'todo npe should not thrown ',positive
'tests for the worker thread and its jobs todo most delta files this test suite use txn range nnm that means that they all look like they were created compaction streaming api delta files created sql should have range and suffix and later need change some these have better test coverage ',positive
'not good reach here this was initialized setmetastorehandler time this means handlergetwh returning null error out ',positive
'the processor context for partition pruner this contains the table alias that being currently processed todo this class may not useful ',positive
'deprecated favour link removed hive ',positive
'todo enable this for production debug switching between two small buffers new caslog ',positive
'helper class isolate newer hbase features from users running against older versions hbase that dont provide those features todo remove this class when its okay drop support for earlier version hbase ',positive
'note for uniform hash bucketspartitions when the key empty will use the instead ',positive
'todo verify this needed ',positive
'todo this class completely unnecessary mapping with parent ',positive
'moving across different filesystems differnent encryption zone need file copy instead rename todo consider need this for different file authority throws hiveexception ',positive
'todo support propagation for windowing ',positive
'named columns join todo can also the same for semi join but seems that other dbms does not support yet ',positive
'helper class that generates sql queries with syntax specific target todo why throw metaexception ',positive
'can continue todo need check that this the same that are rebuilding ',positive
'guess commonjoinresolver will work commonjoinresolver may convert join operation correlation optimizer will not merge that join todo for joinoperator that has both intermediate tables and query input tables input tables should able guess this joinoperator will converted mapjoin based ',positive
'todo could log from ticket cache instead good method ugi right now ',positive
'todo support for binary spec presumably wed parse somewhere earlier ',positive
'redundant todo callers this often get partvals out name for reason ',positive
'todo mssplit for now keep copy hiveconf around need call other methods with this should changed configuration once everything that this calls that requires hiveconf moved the standalone metastore ',positive
'closes the client releasing any link imetastoreclient meta store connections held does not notify any open transactions todo perhaps should ',positive
'todo will this work ',positive
'dont have the entire part copy both whatever intended cache and the rest allocated buffer could try optimize bit have contiguous buffers with gaps but its probably not needed ',positive
'todo hack fix until hive addressed nonexact type shouldnt promoted exact type might this corrects that ',positive
'fixme old implementation returned null exception maybe ',positive
'optimize the scenario when there are grouping keys only reducer needed ',positive
'todo decimal exact numeric approximate numeric ',positive
'todo why doesnt this use one the existing options implementations ',positive
'ensure theres threadlocal dont expect one dont ever want create key paths with world visibility why that even option ',positive
'todo when job complete should print the msgcount table log ',positive
'todo explain should use fetchtask for reading ',positive
'this detail not desired ',positive
'todo use the other hdfsutils here ',positive
'first check the registry has been updated since the error and skip the error have received new valid registry info todo externally add grace period for this ',positive
'todo calculate from cached values ',positive
'array size not big enough ',positive
'undone needed longtest ',positive
'todo reduce the number lookups that happen here this shouldnt hdfs for each call ',positive
'data read for this stripe check have some included indexonly columns todo there may bug here could there partial filtering indexonly column ',positive
'fixme the right thing luke ',positive
'todo make can randomize the column order ',positive
'todo right now treat each slice stripe with single and never bother with indexes phase need add indexing and filtering ',positive
'todo pattern from curator better error handling ',positive
'thread pool listening used for unhandled errors for now todo remove ',positive
'now need look for any values that the user set that metastoreconf doesnt know about todo commenting this out for now breaks because the conf values arent getting properly interpolated case variables see hive ',positive
'fixme including this the signature will almost certenly differ even the operator doing the same there might conflicting usages logicalcompare ',positive
'multimrinput may not fix once tez resolved ',positive
'preempt only there are pending preemptions the same host when the premption registers the request the highest priority will given the slot even the initial preemption was caused some other task todo maybe register which task the preemption was for avoid bad nonlocal allocation ',positive
'todo this wrong this test sets dummy txn manager and cannot create acid tables this used work accident now this works due test flag the test needs fixed also applies for couple more tests ',positive
'todo why this needed could just save the host and port ',positive
'todo there needs mechanism figure out different attempts for the same task delays could potentially changed based this ',positive
'its not wrong take all delete events for bucketed tables but its more efficient only take those that belong the bucket assuming trust the file name unbucketed table get all files ',positive
'determine initial input vector expression note may have convert later from decimal regular decimal ',positive
'note this little bit confusing the special treatment stripelevel buffers because run the columnstreamdata refcount one ahead specified above may look like this would release the buffers too many times one release from the consumer one from below and one here however this merely handling special case where all the batches that are sharing the stripe level stream have been processed before got here they have all decrefed the csd but have not released the buffers because that extra refcount this essentially the consumer refcount being released here ',positive
'this doesnt always work since some jdbc drivers oracles return blank string from gettablename ',positive
'todo can this moved out the main callback path ',positive
'todo add more expected test result here ',positive
'todo mssplit for now have construct this reflection because imetastoreclient cant moved until after hivemetastore moved which cant moved until this moved ',positive
'todo need test where actually have more than file ',positive
'here comes the ugly part ',positive
'removed hive ',positive
'optimization for later ',positive
'there way provide char length here might actually long there object inspector with char length receiving this value ',positive
'this command exists solely output this message todo can error ',positive
'todo add support for and clauses under clauses firstcut takes known minimal tree and others expr expr and ',positive
'todo set this tree instead flat list ',positive
'operator factory for predicate pushdown processing operator graph each operator determines the pushdown predicates walking the expression tree each operator merges its own pushdown predicates with those its children finally the tablescan operator gathers all the predicates and inserts filter operator after itself todo further optimizations multiinsert case create filter operator for those predicates that couldnt pushed the previous operators the data flow merge multiple sequential filter predicates into that plans are more readable remove predicates from filter operators that have been pushed currently these pushed predicates are evaluated twice ',positive
'hive call currently ineffective ',positive
'note location check the buffer always locked for move here ',positive
'todo mssplit for now cannot load the default class since its from load the which just throws this allows existing hive instances work but also allows instantiate the metastore stand alone for testing not sure this the best long term solution ',positive
'todo this very brittle given that hive supports nested directories the tables the caller should pass flag explicitly telling the directories the input are data parent data for now retain this for backward compat ',positive
'todo these appear always called under write lock they need sync ',positive
'todo could perhaps reuse the same directory for hiveresources ',positive
'theres mismatch between static and object name mismatch between vector and nonvector operator name the optimizer doenst work correctly ',positive
'not really sure how refer this can todo could find different from branch for the union that might have alias could add alias here refer but that might break other branches ',positive
'todo instantiating objects are generally costly refactor ',positive
'todo when upgraded method should used here ',positive
'all the code paths below propagate nulls even neither arg nor arg have nulls this reduce the number code paths and shorten the code the expense maybe doing unnecessary work neither input has nulls this could improved the future expanding the number code paths ',positive
'deprecated favour link removed hive ',positive
'todo remove this once calcite can take rule operand ',positive
'todo could fall back trying one one and only ignore the failed ones ',positive
'deprecated favour link removed hive ',positive
'first try temp table todo cat think the right thing here always put temp tables the current catalog but dont yet have notion current catalog well have hold until ',positive
'todo set other table properties needed ',positive
'dont pass the pool set not thread safe the user trying force use nonexistent pool want fail anyway will fail later during get ',positive
'todo since operationlog moved package oahhqlsession may add enum there and map fetchorientation ',positive
'todo ideally this only needs called the result type will also change however since that requires support from type inference rules tell whether rule decides return type based input types for now all operators will recreated with new type any operand changed unless the operator has builtin type ',positive
'todo ideally should store shortened representation only the necessary fields hbase will probably require custom sarg application code ',positive
'this massive hack the compactor threads have access packages such acidinputformat depends metastore cant directly access those deal with this the compactor thread classes have been put and they are instantiated here dyanmically this not ideal but avoids massive refactoring hive packages wrap the start the threads catch throwable loop that any failures dont doom the rest the metastore ',positive
'fixme there were afterclass methodsi guess this the right ordermaybe not ',positive
'todo hive get all these properties from the registry this will need take care different instances publishing potentially different values when support changing configurations dynamically ',positive
'todo replace with withtimeout after get the relevant guava upgrade ',positive
'todo need turn rules thats commented out and add more necessary ',positive
'todo handle renaming files somewhere ',positive
'deprecated favour link removed hive ',positive
'todo this doesnt include superclass ',positive
'fixme move this colstat related part ',positive
'todo not sure that this the correct behavior doesnt make sense create the partition without column info this should investigated later ',positive
'note implies but but here not sufficient enough info set ',positive
'fixme this should changeto valueof that will also kill that fallback none which think more like problem than feature ',positive
'todo setup set threads process incoming requests make sure requests for single dagquery are handled the same thread ',positive
'todo add alter database support hcat ',positive
'todo add tests for partitions other catalogs ',positive
'todo suspect could skip much the stuff above this the function the case update and delete but dont understand all the side effects the above code and dont want skip over yet ',positive
'todo expireafteraccess locks cache segments put and expired get doesnt look too bad but find some perf issues might good idea remove this are probably not caching that many constructors note that weakkeys causes used for key compare this will only work for classes the same classloader should this case ',positive
'todo why the queue name set again has already been setup via setqueuename only one the two ',positive
'todo create and init session sets queue isdefault but does not initialize the configuration ',positive
'deprecated favour link string removed hive ',positive
'todo handle query hints currently ignore them ',positive
'todo currently only support equal operator two references might extend the logic support other orderpreserving udfs here ',positive
'the right was the left side right outer join ',positive
'todo why does this only kill nondefault sessions nothing for workload management since that only deals with default ones ',positive
'todo should this use thats what other code uses ',positive
'todo fix this ',positive
'fixme doing the multiline handling down here means higherlevel logic never sees the extra lines for example script being saved wont include the continuation lines this logged sfnet bug ',positive
'review jhyde oct this rule nonstatic depends the state members reldecorrelator and has sideeffects the decorrelator this breaks the contract planner rule and the rule will not reusable other planners ',positive
'can loop thru all the tables check they are acid first and then perform cleanup but its more efficient unconditionally perform cleanup for the database especially when there are lot tables ',positive
'todo implement this ',positive
'todo allow using unsafe optionally bounds check first trigger bugs whether the first byte matches not ',positive
'todo this doesnt appear used anywhere ',positive
'for auto convert mapjoins not safe dedup here todo ',positive
'were scanning tree from roots leaf this not technically correct demux and mux operators might form diamond shape but will only scan one path and ignore the others because the diamond shape always contained single vertex the scan depth first and because remove parents when pack pipeline into vertex will never visit any node twice but because that might have situation where need connect work that comes after the work were currently looking also note the concept leaf and root reversed hive for historical ',positive
'the following parameters are not supported yet todo add support ',positive
'todo this should unique ',positive
'todo concurrent insertupdate same partition should pass ',positive
'deprecated favour link removed hive ',positive
'todo this invalid for acid tables and cannot access acidutils here ',positive
'after each major compaction stats need updated each column the tablepartition which previously had stats create bucketed orc backed table orc currently required acid populate partitions with data compute stats insert some data into the table using streamingapi trigger major compaction which should update stats check that stats have been updated throws exception todo add nonpartitioned test add test with sorted table ',positive
'hive delta file names changed deltaxxxxyyyyzzzz prior that the name was deltaxxxxyyyy want run compaction tests such that both formats are used since new code has able read old files ',positive
'todo should call ',positive
'todo could try get superclass generic interfaces ',positive
'todo this actually calls the metrics system and getmetrics that may expensive for now looks like should thread ',positive
'todo get rid the builders they serve purpose just call ctors directly ',positive
'todo verify having not separate filter shouldnt introduce derived table ',positive
'why dont lock the snapshot here instead having client make explicit call whenever chooses want rely locks for transaction scheduling must get the snapshot after lock acquisition relying locks pessimistic strategy which works better under high contention ',positive
'todo temporary need expose from orc utils note the difference null checks ',positive
'todo maybe use stack estobj pairs instead recursion ',positive
'todo shortcut for last col below length ',positive
'temporary order avoid new version storageapi the conversion here ',positive
'todo these constraints should supported for partition columns ',positive
'have nested setcolref process that and start from scratch todo use stack ',positive
'todo this correct based the same logic hive ',positive
'todo refactor the following into the pipeline ',positive
'todo change fileinputformat field after mapreduce ',positive
'todo ever use this endpoint for anything else refactor cycling into separate class ',positive
'something seriously wrong this happening ',positive
'todo ordering seems affect the distinctness needs checking disabling ',positive
'todo avoid put working directly outstream ',positive
'cant fetch prefix colqual must pull the entire qualifier todo use iterator the filter serverside ',positive
'these tests inherently cause exceptions written the test output logs this undesirable since you might appear someone looking the test output logs something failing when isnt not sure ',positive
'return the above line should have been all the implementation that need but due bug that impl which recognizes only singledigit columns need another impl here ',positive
'deprecated favour link removed hive ',positive
'todo verify need use unwrap data ',positive
'generate relnode for lateral view todo support different functions not only inline with lateral view join ',positive
'todo this actually not adding anything since lockcomponent uses trie promote lock except accident when have partitioned target table have readentity and writeentity for the table mark readentity and then delete writeentity replace with partition entries dbtxnmanager skips read lock the readentity inputnolockneeded ',positive
'todo cleanup once parquet support timestamp type annotation ',positive
'todo buffers are accounted for allocation time but ideally should report the memory overhead from the java objects memory manager and remove when discarding file ',positive
'todo cleanup this ',positive
'todo hive need maxlength checking ',positive
'addbatchtowriter have passed the batch both orc and operator pipeline neither ever changes the vectors wed need set vectors batch write todo for now create this from scratch ideally should return the vectors from ops could also have the orc thread create for its spare time ',positive
'todo enforce max length ',positive
'todo would make sense return buffers asynchronously ',positive
'todo the best solution support nan expression reduction ',positive
'todo ideally this should moved outside hivemetastore shared between all the rawstores right now theres method create pool ',positive
'todo hive abort handling ',positive
'todo update search once hive done ',positive
'note type param not available here ',positive
'todo when function privileges are implemented they should deleted here ',positive
'todo perhaps move orc instream ',positive
'todo should not throw different exceptions for different hms deployment types ',positive
'todo clean all the other paths that are created ',positive
'todo ideally when colstatsaccurate stuff stored some sane structure this should retrieve partstoupdate single query checking partition params java ',positive
'undone missing datetime interval data types ',positive
'deprecated favour link hcattablegetcols removed hive ',positive
'todo refactor this out ',positive
'todo this needs looked map map map made concurrent for now since split generation can happen parallel ',positive
'todo fix the expressions later ',positive
'deprecated favour link createhcattable removed hive ',positive
'todo handle ',positive
'push down semi joins todo enable this later ',positive
'todo add txnscomment filed and set aborted system due timeout easier read logs ',positive
'todo riven switch this back package level when can move into riven ',positive
'todo change this over just store local dir indices instead the entire path far more efficient ',positive
'todo might revisit this createdroprecreate cases needs some thinking ',positive
'todo why this text formatter ',positive
'note this rule replicated from calcites subqueryremoverule transform that converts exists and scalar subqueries into joins todo reason this replicated instead using calcites calcite creates null literal with null type but hive needs properly typed psubqueries are represented link rexsubquery expressions subquery may may not correlated subquery correlated the wrapped link relnode will contain link rexcorrelvariable before the rewrite and the product the rewrite will link correlate the correlate can removed using link reldecorrelator ',positive
'truncate todo posixfallocate ',positive
'note this code tries get all keyvalue pairs out the map its not very efficient the more efficient way should let mapoi return iterator this currently not supported mapoi yet ',positive
'undone why need specify binarysortableserde explicitly here ',positive
'todo last param bogus why this hardcoded ',positive
'todo move this common method note this gets ids name assume indices dont need adjusted for acid ',positive
'ends decorrelating expressions corcol value generator not generated further simplified false this wrong and messes the whole tree prevent this visitcall overridden rewritesimply such predicates not null also need take care that this only for correlated predicates and not user specified explicit predicates todo this code should removed once calcite fixed and there support not equal ',positive
'todo need proper clone meanwhile lets least keep this horror one place ',positive
'note that pass job config the record reader but use global config for llap todo add tracing serde reader ',positive
'deprecated favour link removed hive ',positive
'fixme support template types currently has conflict with ',positive
'this rule copy link that regenerates hive specific aggregate operators todo when calcite completed should able remove much this code and just override the relevant methods planner rule that reduces aggregate functions link simpler forms prewrites liavgx rarr sumx countx listddevpopx rarr sqrt sumx sumx sumx countx countx listddevsampx rarr sqrt sumx sumx sumx countx case countx when then null else countx end livarpopx rarr sumx sumx sumx countx countx livarsampx rarr sumx sumx sumx countx case countx when then null else countx end ',positive
'todo handle more than inputs for setop ',positive
'todo separate model needed for compressedoops which can guessed from memory size ',positive
'todo should convert multijoin child hivejoin ',positive
'todo should doing security check here users should not able see each others locks ',positive
'handle anything but for now again need real client for this api todo handle and return new connection nothing for now ',positive
'todo close basically resets the object bunch nulls should ideally not reuse the object because its pointless and errorprone ',positive
'todo assumes its isoriginal why ',positive
'todo required due why wasnt required before ',positive
'todo change this not serialize the entire configuration minor ',positive
'todo for object inspector fields assigning for now better estimate the memory size every object inspectors have implement memoryestimate interface which lot change with little benefit compared ',positive
'undone add support for date timestamp intervalyearmonth intervaldaytime ',positive
'todo there was code here create guessestimate for collection wrt how usage changes when removing elements however its too errorprone for anything involving preallocated capacity was discarded ',positive
'extract the buckedid from pathfilesmap this more accurate method however may not work certain cases where buckets are named after files used while loading data such case fallback old potential inaccurate method the accepted file names are such copy ',positive
'this bit hackish fix mismatch between sarg and hive types for timestamp and date todo move those types storageapi ',positive
'todo hive move using loop and timed wait once tez fixed ',positive
'todo add configurable option skip the history and just drop ',positive
'todo refactor this into utility llap tests use this pattern lot ',positive
'todo make aliases unique otherwise needless rewriting takes place ',positive
'derive additional attributes rendered explain todo this method relied upon custom input formats set jobconf properties this madness this hive storage handlers ',positive
'connects the link imetastoreclient meta store that will used manage link transaction lifecycles also checks that the tables destined receive mutation events are able the client should only hold one open transaction any given time todo enforce this ',positive
'todo this should have option for directory inherit from the parent table including bucketing and list bucketing for the use compaction when the latter runs inside transaction ',positive
'undone parameterize for implementation variation ',positive
'todo ifexists could moved metastore fact already supports that check for now since get parts for output anyway can get the error message earlier get rid output can get rid this ',positive
'fixme moved default value herefor now think this features never really used from the command line ',positive
'todo should moved out ',positive
'todo types need checked ',positive
'todo handle multi joins ',positive
'todo ugly hack because java doesnt have dtors and tez input hangs shutdown ',positive
'todo make script output prefixing configurable had disable this since results lots test diffs ',positive
'todo also support filekey splits like orcsplit does ',positive
'todo when txn stats are implemented use writeids determine stats accuracy ',positive
'hive pretty simple read stupid writing out values via the serializer were just going through matching indices hive formats normally handle mismatches with null dont have that option instead well end throwing exception for invalid records ',positive
'todo handle negations ',positive
'todo handle task container map events case hard failures ',positive
'todo partition names are not case insensitive ',positive
'todo this should configured serde ',positive
'todo have put the support for clause ',positive
'ideally should use hiverelnode convention however since volcano planner throws that case because druidquery does not implement the interface set bindable currently not use convention hive hence that should fine todo want make use convention while directly generating operator tree instead ast this should changed ',positive
'call getsplit the inputformat create hcatsplit for each underlying split when the desired number input splits missing use default number denoted zero todomalewicz currently each partition split independently into desired number however want the union all partitions split into desired number while maintaining balanced sizes input ',positive
'todo need handle the this what mysql does here ',positive
'deprecated favour link and removed hive ',positive
'todo watches the output dirs need cancelled some point for now via the expiry ',positive
'todo use the hard link feature hdfs once done ',positive
'todo allow port per host ',positive
'note this for generating the internal path name for partitions users should always use the metastore api get the path name for partition users should not directly take partition values and turn into path name themselves because the logic below may change the future the future its add new chars the escape list and old data wont corrupt because the full path name metastore stored that case hive will continue read the old data but when creates new partitions will use new names edit there are some use cases for which adding new chars does not seem backward compatible partition was created with name having special char that you want start escaping and then you try dropping the partition with hive version that now escapes the special char using the list below then the drop partition fails work ',positive
'todo can more precise than stringstring ',positive
'todo this wrong this test sets dummy txn manager and cannot create acid tables change use proper txn manager the setup for some tests hangs this used work accident now this works due test flag the test needs fixed create table ',positive
'todo currently way test alter partition hcatclient doesnt support ',positive
'note some these scenarios could handled but they are not supported right now the reason that bind query appuser using the signed token information and dont want bother figuring out which one use case ambiguity use case ambiguous user ambiguous user ambiguous user ambiguous app ',positive
'only support bulkload when hfilefamilypath has been specified todo support detecting cfs from column mapping todo support loading into multiple cfs time ',positive
'todo why this changed from the default hiveconf ',positive
'todo need set catalog parameter ',positive
'todo should have check the server side embedded metastore throws remote throws ttransportexception ',positive
'todo this will probably send message that needed here ',positive
'theres full hash code stored front the key could check that first keylength obviously doesnt make sense less bytes check key then theres match check vain but what the proportion matches for writes could all keys are unique for reads hope its really high then theres mismatch what probability there that key mismatches bytes just checking the key faster ',positive
'given rexcall tablescan find max nulls currently picks the col with max nulls todo improve this param call param return ',positive
'note beelineopts uses reflector extensive way call getters and setters itself you want add any getters setters this class but not have interfere with saved variables beelineproperties careful use this marker needed also possible get this naming these functions obtainblah instead getblah and but that not explicit and will likely surprise people looking the code the future better explicit intent ',positive
'todo should this currentdirs ',positive
'todo what else required this environment map ',positive
'fixme hive should probably move this method somewhere else ',positive
'todo these bytes should versioned ',positive
'todo minicluster slow this test times out make work ',positive
'send dropped table notifications subscribers can receive these notifications for dropped tables listening topic hcat with message selector string value value todo datanucleus currently used the hivemetastore for persistence has been found throw npe when serializing objects that contain null for this reason override some fields the storagedescriptor this notification this should fixed after hive upgrade datanucleus from resolved ',positive
'this utility designed help with upgrading hive ondisk layout for transactional tables has changed and require preprocessing before upgrade ensure they are readable hive some transactional tables identified this utility require major compaction run them before upgrading once this compaction starts more updatedeletemerge statements may executed these tables until upgrade finished additionally new type transactional tables was added insertonly tables these tables support acid semantics and work with any inputoutputformat any managed tables may made insertonly transactional table these tables dont support updatedeletemerge commands this utility works modes preupgrade and postupgrade preupgrade mode has have hive jars the classpath will perform analysis existing transactional tables determine which require compaction and generate set sql commands launch all these compactions note that depending the number tablespartitions and amount data them compactions may take significant amount time and resources the script output this utility includes some heuristics that may help estimate the time required script produced action needed for compactions run instance standalone hive metastore must running please make sure sufficiently high this specifies the limit concurrent compactions that may run each compaction job mapreduce job may used set yarn queue ame where all compaction jobs will submitted postupgrade mode hive jarshivesitexml should the classpath this utility will find all the tables that may made transactional with ful crud support and generate alter table commands will also find all tables that may not support full crud but can made insertonly transactional tables and generate corresponding alter table commands todo rename files execute option may supplied both modes have the utility automatically execute the equivalent the generated commands location option may supplied followed path set the location for the generated scripts ',positive
'srcs new filestatus why this needed ',positive
'todo should probably throw exception here ',positive
'convert agg args and type args calcite todo does hql allows expressions aggregate args can only ',positive
'todo note that the token not renewable right now and will last for weeks default ',positive
'todo should this also topdown ',positive
'todo most the code this class ripped from zookeeper tests instead redoing should contribute updates their code which let more easily access testing helper objects xxx copied from the only used class qtestutil from hbasetests ',positive
'its constant constant column column cant fetch any ranges todo can try smarter and push the value some node which ',positive
'todo implement ',positive
'todo implement implicit asyncrddactions conversion instead jcmonitor todo how handle stage failures ',positive
'todo should check isassignablefrom ',positive
'for replication addptns need follow insertifnotexist alterifexists scenario todo ideally should push this mechanism the metastore because otherwise have choice but iterate over the partitions here ',positive
'todo should create the batch from vrbctx and reuse the vectors like below future work ',positive
'this hacky way doing the quotes since will match any these hello this something split would considered quoted ',positive
'todo the type checking the expressions ',positive
'make tree out the filter todo this all pretty ugly the only reason need all these transformations maintain support for simple filters for hcat users that query metastore forcing everyone use thick client out the question maybe could parse the filter into standard hive expressions and not all this separate tree ',positive
'not safe continue for rsgbygbylim kind pipelines see hive for more ',positive
'hack for tables with columns treat table with single column called col ',positive
'todo expect one dir why dont enforce ',positive
'post serialization separators are automatically inserted between different fields the struct currently there not way disable that the work around here pad the ',positive
'only support limited unselected column following order todo support unselected columns genericudtf and windowing functions examine the order this query block and adds column needed order select list ',positive
'todo want explicit about this dump not being replication dump can uncomment this else section but currently unneeded will require lot golden file regen ',positive
'todo refactor this hive ',positive
'todo and not null can potentially folded earlier into noop ',positive
'todo remove this for now adding because has assertion about column counts that not true for semijoins ',positive
'todo this seems indicate that priorities change too little perhaps need adjust the policy ',positive
'note need srcfs rather than because possible that the files lists files which are from different filesystem than the where the files file itself was loaded from currently possible for repl load hdfsipdir and for the files contain hdfsname entries andor viceversa and this causes errors might also possible that there will mix them given files file todo revisit close the end replv dev see our assumption now still holds and not optimize ',positive
'note with some trickery could add logic for each type confvars for now the potential spurious mismatches and for float should easy work around ',positive
'fixme hiveserversiteurl not settable ',positive
'the following check only guard against failures todo knowing which expr constant gbys aggregation function arguments could better done using metadata provider calcite check the corresponding expression exprs see literal ',positive
'todo remove hive this required only support the deprecated interfaces ',positive
'todo partitions are loaded lazily via the iterator then will have avoid conversion everything here defeats the purpose ',positive
'figures out the aliases for whom safe push predicates based ansi sql semantics the join conditions are left associative right outer join left outer join inner join interpreted right outer join left outer join inner join for inner joins both the left and right join subexpressions are considered for pushing down aliases for the right outer join the right subexpression considered and the left ignored and for the left outer join the left subexpression considered and the left ignored here aliases and are eligible pushed todo further optimization opportunity for the case and and are first joined and then the result with but the second join currently treats and separate aliases and thus disallowing predicate expr containing both tables and such such predicates also can pushed just above the second join and below the first join param join operator param row resolver return set qualified aliases ',positive
'todo not stopping umbilical explicitly some taskkill requests may get scheduled during querycomplete which will using the umbilical hive should fix this until then leave umbilical open and wait for closed after max idle timeout default ',positive
'todo gap design noone seems use tables they will work but not convert its possible work around this recreating and reinserting the table ',positive
'todo should moved out ',positive
'todo threadpool could here one thread per stripe for now linear ',positive
'optionally some filtering rows undone ',positive
'todo should have check the server side embedded metastore throws remote throws ',positive
'fixme support pruning dynamic partitioning ',positive
'fixme sideeffect will leave the last query set the session level ',positive
'todo use faster nonsync inputstream ',positive
'todo should this rather use threadlocal for numa affinity ',positive
'todo this object created once call one method and then immediately destroyed its basically just roundabout way pass arguments static method simplify ',positive
'todopc implement max ',positive
'todo make sure this method eventually used find the prep batch scripts ',positive
'this doesnt throw any exceptions because dont want the compaction appear failed stats gathering fails since this prevents cleaner from doing its job and there are multiple failures auto initiated compactions will stop which leads problems that are much worse than stale stats todo longer term should write something this binary field need figure out the msg format and how surface show compactions etc ',positive
'todo transitive dependencies warning ',positive
'are get the token locally todo coordinator should passed hive must initialized for now ',positive
'todo ideally acidutils class and various constants should common ',positive
'hack actually need but that not available ',positive
'todo hive handling dummyops and propagating abort information them ',positive
'todo this least for the session pool will always the hive user how does doas above this affect things ',positive
'todo this currently broken need set memory manager bogus implementation avoid problems with memory manager actually tracking the usage ',positive
'todo for oneblock case could move notification for the last block out the loop ',positive
'todo figure out better data structure for node list ',positive
'why isnt ppd working working but storage layer doesnt row level filtering only row group level ',positive
'note currently this implementation does not fall back regular copy distcp tried and fails depend upon that behaviour cases like replication wherein distcp fails there good reason not plod along with trivial implementation and fail instead ',positive
'todo this never used ',positive
'todo versions could also picked build time ',positive
'previous solution based tablealias and colalias which unsafe esp when cbo generates derived table names see hive for correctness purpose only trust colexpmap assume that cbo can the constantpropagation before this function called help improve the performance unionoperator limitoperator and filteroperator are special they should already columnposition aligned ',positive
'hack refactor once the metadata apis with types are ready ',positive
'todo this cannot evict enough will spin infinitely terminate some point ',positive
'but not convert the join ',positive
'todo ideally should have test for session itself ',positive
'todo needed verify that recordschema entry for fieldname matches appropriate type ',positive
'fixme null value treated differently the other endwhen those filter will ',positive
'todobr change the output colexprnodecolumn names external namesbr verify need use the keyvalue cols switch external names possiblebr exprnode columninfo the specified differently for different gbrs pipeline remove the different treatments virtualcolmap needs maintained ',positive
'replace the entire current diskrange with new cached range case inexact match either the below may throw not currently support the case where the caller requests single cache buffer via multiple smaller subranges that happens this may throw noone does now though todo should actively assert here for cache buffers larger than range ',positive
'todo fill when partitiondoneevent supported ',positive
'todo for now this affects non broadcast unsorted cases well make use the edge property when its available ',positive
'todo most other options are probably unrecoverable throw ',positive
'this temporary hack fix things that are not fixed the compiler ',positive
'should fixed accumulo and ',positive
'this hackery but having hivecommon depend standalonemetastore really bad because will pull all the metastore code into every module need check that arent using the standalone metastore are should treat the same ',positive
'todo fix this has run since tables may unbucketed ',positive
'not sure why this method doesnt throw any exceptions but since the interface doesnt allow well just swallow them and move this okish since releaselocks only called for roac queries would really bad eat exceptions here for write operations ',positive
'todo implement this when tez upgraded tez ',positive
'todo there should better way this code just needs modified ',positive
'todo session reuse completely disabled for doastrue always launches new session ',positive
'todo case failure heartbeat tasks for the specific dag should ideally killed ',positive
'todo check all required tables are allowed get from cache ',positive
'todo only the qualified name should left here ',positive
'fixme this secret contract reusein getaggrkey creates more closer relation the statsgatherer ',positive
'note its not quite clear why this done inside this seems like should the top level ',positive
'todo why this synchronized ',positive
'todo local cache created once the configs for future queries will not honored ',positive
'support for dynamic partitions can added later the following not optimized insert overwrite table tds select key value from where where and are bucketed the same keys and partitioned ',positive
'fixme using real scaling newold ration might yield better results ',positive
'todo try this with acid default seem making table acid listener too late ',positive
'context class for operator tree walker for partition pruner todo this class may not useful ',positive
'thrift cannot write readonly buffers well todo actually thrift never writes the buffer could use reflection unset the unnecessary readonly flag allocationcopy perf becomes problem ',positive
'todo should replaced cliserviceclient ',positive
'deprecated favour link builderhcattable boolean removed hive ',positive
'todo change exprnodeconverter independent partition expr ',positive
'are not going verify for each partition just verify for the table todo need verify the partition column instead ',positive
'todo doesnt the right thing hive ',positive
'todo can columns retain virtualness out union ',positive
'todo why invent our own error path top the one from futureget ',positive
'todo there more correct way get the literal value for the object ',positive
'todo this need review thread safety various places see callsers link pass sessionstate forked threads currently looks like those threads only read metadata but this fragile also maps sessionstate where tempt table metadata stored are concurrent and any putget crosses memory barrier and does using most code javautilconcurrent the readers the objects these maps should have the most recent view the object but again could fragile ',positive
'arguments then can use more efficient form ',positive
'todo something preventing the process from terminating after main adding exit hacky solution ',positive
'todo write error the channel theres mechanism for that now ',positive
'todo reuse columnvectors hasbatch save the array column take apart each list ',positive
'todo hold onto this predicate that dont add the filter operator ',positive
'todo ideally remove elements from this once its known that tasks are linked the instance all deallocated ',positive
'todo can blindly copy sort trait what inputs changed and are now sorting different cols ',positive
'todo this works different remote and embedded mode embedded mode exception happens ',positive
'undone need copy the object ',positive
'did remove those and gave cbo the proper ast that kinda hacky ',positive
'fixme consider other operator info wellnot just conf ',positive
'check todo add option skip this number partitions checks done triggers via counter ',positive
'vertex started but not complete ',positive
'todo need the description how these maps are kept consistent ',positive
'undone need copy the object ',positive
'todo filterexpr todo functioncache todo constraintcache todo need nested copy todo string intern todo monitor event queue todo initial load slow todo size estimation ',positive
'todo not sure about this this call doesnt set the compression type the conf file the way gethiverecordwriter does orc appears read the value for itself not sure this correct not ',positive
'todo refactor there upcoming patch that refactors this bit code currently the idea the following default replcopywork will behave similarly copywork and simply copy along data from the source destination the flag readsrcasfileslist set changes the source behaviour this copytask and instead copying explicit files this will then fall back behaviour wherein files read from the source and the files specified the files are then copied the destination this allows lazycopyonsource and pullfrom destination semantic that want use from replication ',positive
'scratch variable created here this could optimized the future perhaps using threadlocal storage allocate this scratch field ',positive
'ideally there should better way determine that the followingwork contains dynamic partitioned hash join but some cases createreducework looks like the work must createdconnected first before the gentezproccontext can updated with the mapjoinwork relationship ',positive
'future thought this may expensive consider having thread pool run parallel ',positive
'number rows processed between checks for factor todo there overlap between and checkinterval ',positive
'todo llapnodeid just hostport pair could make this class more generic ',positive
'note assume here that plan has been validated beforehand dont verify ',positive
'implement future needed ',positive
'todo move this into ctor would need create cachewriter then ',positive
'after spark only use jobmetricslistener get job metrics todo remove when the new api provides equivalent functionality ',positive
'todo could this only the actually used ',positive
'todo may add app name etc later ',positive
'todo across catalogs ',positive
'todo this does not work because materialized views need the creation metadata updated case tables used were replicated different database runcreate materialized view dbname matview select from dbname ptned where driver verifysetupselect from dbname matview ptndata driver ',positive
'fixme oss seems contain duplicates ',positive
'todo danger stack overflow needs retry limit ',positive
'cannot drop because uses one its tables todo error message coming from metastore currently not very concise foreign key violation should make easily understandable ',positive
'todo ideally querytracker should have fragmenttoquery mapping ',positive
'todo wtf the old code seems just drop the ball here ',positive
'hack initialize cache with expiry time causing return new hive client every time otherwise the cache doesnt play well with the second test method with the client gets closed the teardown the previous test ',positive
'deprecated favour link removed hive ',positive
'todo hive make use progress notifications once hive starts sending them out progressnotified ',positive
'todo evil need figure out way remove this sleep ',positive
'todo all the extrapolation logic should moved out this class ',positive
'fixme this add seems suspicious lines below the value returned this method used betterds ',positive
'this hack for now handle the group case ',positive
'all other distinct keys will just forwarded this could optimized ',positive
'this workaround for derby and oracle bug pretty horrible ',positive
'todo this fetching all the rows once from broker multiple historical nodes move use scan query avoid back pressure the nodes ',positive
'todo strictly speaking there bug here heartbeat commits but both heartbeat and checklock are the same retry block checklock throws heartbeat also retired ',positive
'deprecated favour link removed hive ',positive
'wow somethings really wrong ',positive
'undone does this random range need high ',positive
'todo java support using string with switches but ides dont all seem know that casing fine for now but should eventually remove this also didnt want create another enum just for this ',positive
'have found invalid decimal value while enforcing precision and scale ideally would replace with null here which what hive does however need plumb this thru somehow because otherwise having different expression type ast causes the plan generation fail after cbo probably due some residual state saqb for now will not run cbo the presence invalid decimal ',positive
'code initially inspired google objectexplorer todo roll the directonly estimators from fields various other optimizations possible ',positive
'dirty hack this will throw away spaces and other things find better ',positive
'todo should local cache also fileid preserve the original logic for now ',positive
'todo need get child ',positive
'todo refactor with cache impl has the same merge logic ',positive
'dirty hack set the environment variables using reflection code this method for testing purposes only and should not used elsewhere ',positive
'todo decorelation subquery should done before attempting partition pruning otherwise expression evaluation may try execute corelated sub query ',positive
'todo issamplingpred sampledesc issortedfilter ',positive
'todo will nice refactor ',positive
'todo this should not throw todo this should take comment parameter set ccmetainfo provide some context for the failure ',positive
'todo replace this with map ',positive
'todo remove some these fields needed ',positive
'todo this not valid function names for builtin udfs are specified functionregistry and only happen match annotations for user udfs the name what user specifies creation time annotation can absent ',positive
'todo verify that this correct ',positive
'todo could try get the declaring object and infer argument stupid java ',positive
'closedestroy used seq coupling most the time the difference either not clear not relevant remove ',positive
'todo this doesnt check compaction already running even though initiator does but ',positive
'this little bit weird well the call outside the lock our caller calls under lock wed preserve the lock state for them their finally block will release the ',positive
'todo really need some comments explain exactly why each these removed ',positive
'fixme this broken for multiline sql ',positive
'not implemented ',positive
'slice boundaries may not match split boundaries due torn rows either direction this counter may not consistent with splits this also why increment requested bytes here instead based the split dont want the metrics inconsistent with each other matter what determine here least well account for both the same manner ',positive
'this kinda hacky know these are llaserdedatabuffers ',positive
'todo perhaps can made more efficient creating byte directly ',positive
'this not strictly accurate but type cannot null ',positive
'todo for non columnar dont need this might well update all stats ',positive
'todo this should ideally not create addpartitiondesc per partition ',positive
'todo should also whitelist input formats here from ',positive
'not public since must have the deserialize read object ',positive
'todo checking children useless compare already does that ',positive
'todo why does the original code not just use datastream that passes stream ',positive
'this bogus hack because copies the contents the sql file intended for creating derby databases and thus will inexorably get out date with open any suggestions how make this read the file build friendly way ',positive
'this ifelse chain looks ugly the inner loop but given that will the same for given operator branch prediction should work quite nicely recordupdateer expects get the actual row not serialized version thus ',positive
'todo this has find better home its also hardcoded default hive would nice ',positive
'deprecated favour link removed hive ',positive
'todo add upstream ',positive
'todo should have check the server side embedded metastore throws remote throws metaexception ',positive
'load the list partitions and return the list partition specs todo followup hive should refactor use get the list full partspecs after that check the number dps created not exceed the limit and iterate over and call loadpartition here the reason dont inside hive the latter large and ',positive
'there are options for this conditionaltask merge the partitions move the partitions dont merge the partitions merge some partitions and move other partitions merge some partitions and dont merge others this case the merge done first followed the move prevent conflicts todo are not dealing with concatenate ddl should not create mergemove path ',positive
'todo this would more flexible doing sql select statement rather than using inputformat directly see link long long int string string param numsplitsexpected return throws exception ',positive
'deprecated favour link hcattablegetdbname removed hive ',positive
'todo hive differentiate between ',positive
'todo pointless ',positive
'this kind not pretty but this how detect whether buffer was cached would always set this for lookups put time ',positive
'todo move these test parameters more specific places theres need have them here ',positive
'todo checksum only available hdfs need find solution for other local etc ',positive
'fixme possible alternative move both into under some class nested ones and that way this factory level caching can made transparent ',positive
'neither expired nor olderthan criteria selected this better not attempt delete tokens ',positive
'fixme hadoop made the incompatible change for while spark still using hadoop spark requires hive support hadoop first then spark can start working hadoop support remove this after spark supports hadoop ',positive
'variables used llap daemons todo eventually autopopulate this based prefixes the conf variables will need renamed for this ',positive
'todo setfilemetadata could just create schema called two places clean later ',positive
'deprecated favour link hcattablecomment removed hive ',positive
'this workaround for hadoop libjars are not added classpath the ',positive
'todo need some sort validation phase over original ast make things user friendly for example original command refers column that doesnt exist this will caught when processing the rewritten query but the errors will point locations that the user cant map anything values clause must have the same number values target table including partition cols part cols last select clause insert select todo care preserve comments original sql todo check identifiers are propertly escapedquoted the generated sql its currently inconsistent look does unescape unparse todo consider when not matched source then update set targettablecol sourcetablecol what happens when source empty this should runtime error maybe not the outer side roj empty the join produces rows supporting when not matched source then this should runtime error ',positive
'todo wtf this doesnt anything ',positive
'todo why this needed ',positive
'note this whole logic replicated from calcites reldecorrelator and exteneded make suitable for hive should get rid this and replace with calcites reldecorrelator once that works with join project etc instead join project this point this has differed from calcites version significantly cannot get rid this reldecorrelator replaces all correlated expressions corexp relational expression relnode tree with noncorrelated expressions that are produced from joining the relnode that produces the corexp with the relnode that references ptodop lireplace code corelmap constructor parameter with relnode limake link currentrel immutable would require fresh reldecorrelator for each node being decorrelatedli limake fields code corelmap immutableli limake subclass rules static and have them create their own decorrelatorli ',positive
'deprecated favour link string removed hive ',positive
'concern leaking scratch column ',positive
'todo this ugly hack see the same for discussion ',positive
'derby and oracle not interpret filters ansiproperly some cases and need workaround ',positive
'todo currently not testing the following scenarios multidb whlevel repl load need add that insert into tables quite few cases need enumerated there including dyn adds ',positive
'todo ppd needs get pushed param scanrel return ',positive
'convert nonacidorctbl acid table todo remove transprop after hive ',positive
'fixme possibly the distinction between tablepartition not need however was like this beforewill change later ',positive
'dont take directories into account for quick stats todo wtf ',positive
'fixup the children and parents new vector child add new vector child the vector parents children list copy and fixup the parent list the original child instead just assuming relationship when the child mapjoinoperator will have extra parent for the mapjoinoperators small table needs fixed too ',positive
'todo the copy data unnecessary but there workaround ',positive
'get the list partitions that need update statistics todo should reuse the partitions generated compile time since getting the list partitions quite expensive return list partitions that need update statistics throws hiveexception ',positive
'todo this test should removed once acid tables replication supported ',positive
'todo there easy and reliable way compute the memory used the executor threads and onheap cache ',positive
'todo normally the result not necessary might make sense pass false ',positive
'remove newalloc flag first use full unlock after that would imply forcediscarding this buffer acceptable this kind ugly compact between the cache and ',positive
'only attempt this cmd was successful fixme would probably better move this afterexecution ',positive
'todo cleanup pending tasks etc that the next dag not affected ',positive
'todo does this include partition columns ',positive
'work bytescolumnvector output columns ',positive
'two reducesinkoperators are correlated means that they have same sorting columns key columns same partitioning columns same sorting orders and conflict the numbers reducers todo should relax this condition todo need handle aggregation functions with distinct keyword this case distinct columns will added the key columns ',positive
'fixme move testjsonserde from hcat serde ',positive
'todo confirm this safe ',positive
'todo execute errors like this currently dont return good error ',positive
'this not working workaround set part java opts dusertimezoneutc ',positive
'simply get the next day and back half day this not ideal but seems work ',positive
'todo maybe should throw this asis too thriftcliservice currently catches exception the combination determines what would kill the executor thread for now lets only allow oom propagate ',positive
'todo may possible finer grained locks ',positive
'todo this check somewhat bogus the maxjvmmemory xmx parameters see annotation llapservicedriver ',positive
'todo check maximum size compatible with ',positive
'deprecated favour link hcattableescapechar ',positive
'the sorted columns superset bucketed columns store this fact can later used optimize some groupby queries note that the order does not matter long the first ',positive
'fixme replace with hive copy once that copied ',positive
'todo revisit the fence ',positive
'todo how handle collisions should cloning columninfo not ',positive
'optimize the scenario when there are grouping keys and distinct mapreduce jobs are not needed ',positive
'join key expression likely some expression involving functionsoperators there actual table column for this but the reducesink operator should still have output column corresponding this expression using the columninternalname todo does tablealias matter for this kind expression ',positive
'todo why does tez api use object for this ',positive
'todo this log statement looks wrong ',positive
'relying task succeeding reset the exponent theres notifications whether task gets accepted not that would ideal reset this ',positive
'need override these methods due difference nullability between hive and calcite for the return types the aggregation particular for count and sum todo should close the semantics gaps between hive and calcite for nullability aggregation calls return types this might useful trigger some additional rewriting rules that would remove unnecessary predicates etc ',positive
'cancel job the monitor found job submission timeout todo the timeout because lack resources the cluster should ideally also cancel the app request here but facilities from spark yarn its difficult hive side alone see hive ',positive
'todo should checked server side embedded metastore throws remote metastore throws ttransportexception ',positive
'todo ever need the port could just away with nodeid altogether ',positive
'metastore related options that the initialized against when conf var this list changed the metastore instance for the cli will recreated that the change will take effect todo suspect the vast majority these dont need here but requires testing before just pulling them out ',positive
'originals split wont work due mapreduce issue fileinputformat ',positive
'todo doesnt support map array now the value should updated after support these data types ',positive
'todo there are more fields perhaps there should array class ',positive
'some columns select are pruned this may happen those are constants todo the best solution hook the operator before with the select operator see smbmapjoinq for more details ',positive
'todo ',positive
'todo this seems wrong following what hive regular does ',positive
'todo were asking the metastore what its configuration for this var may want revisit pull from client side instead the reason have this way because the metastore more likely have reasonable config for this than arbitrary client ',positive
'todo can this result crossthread reuse session state ',positive
'this conditioncheck could have been avoided but honour the old default not calling wasnt set retain that behaviour todocleanup after verification that the outer isnt really needed here ',positive
'todo support only non nested case ',positive
'deprecated favour link removed hive ',positive
'todo get rid this ',positive
'todo this not remotely accurate you have many relevant original files ',positive
'todo could remove extra copy for isuncompressed case copying directly cache ',positive
'todo hive its possible for bunch tasks come around the same time without the actual executor threads picking any work this will lead unnecessary rejection tasks ',positive
'conflict when loaded some issue with framework which needs relook into later ',positive
'deprecated favour link hcattablefileformat removed hive ',positive
'dangerous lets explicitly add incomplete ',positive
'this not complete list barely make information schema work ',positive
'todo perhaps should also summarize the triggers pointing invalid pools ',positive
'todo could instead get from path here and add normal files for every ugi ',positive
'cannot get root tablescan operator likely because there join groupby between topop and root tablescan operator dont handle that case and simply return ',positive
'review are supposed applying the getreadcolumnids ',positive
'todo this should accept file table names exclude from nonacid acid conversion todo change script comments preamble instead footer how does rename script work hadoop oldname newname and what what about how does this actually get executed all other actions are done via embedded jdbc ',positive
'todo the global lock might coarse here ',positive
'todo enums that have both field name and value list ',positive
'the result not boolean and not all partition agree the result dont remove the condition potentially can miss the case like where todo handle this case making result vector handle all constant values ',positive
'need remove this static hack but this the way currently get session ',positive
'todo code section copied over from serdeutils because nonstandard json production there should use quotes for all field names should fix this there and then remove this copy see for details trying enable jackson ignore that doesnt seem workcompilation failure when attempting use that feature having change the production itself ',positive
'assume should have the exact same object todo could also compare the schema and serde and pass only those the call instead most the time these would the same and llap can handle that ',positive
'todo hive sortpartitionedge ',positive
'todo allow the branch specified parameter ptest rather than requiring separate property file ',positive
'note that the tableexists flag used auth kinda hack and assumes only table will ever imported this assumption broken repl load however weve not chosen expand this map tablesetc since have expanded how auth works with repl dump repl load simply require admin privileges rather than checking each object which quickly becomes untenable and even more costly memory ',positive
'todo change after hive for now theres rack matching ',positive
'only small set operations allowed inside explicit transactions dml acid tables ops persistent side effects like use database show tables etc that rollback meaningful todo mark all operations appropriately ',positive
'todo once multistatement txns are supported add test run next statements single txn ',positive
'todo hivequeryid extraction parsing the processor payload ugly this can improved once tez fixed ',positive
'undone why dont these methods take decimalplaces ',positive
'serializes decimal the maximum bit precision decimal digits note major assumption the fast decimal has already been bounds checked and least has precision not bounds check here for better performance ',positive
'todo refactor this and more object oriented manner ',positive
'some data missing from the stream for ppd uncompressed read because index offset relative the entire stream and only read part stream rgs are filtered unlike with compressed data where ppd only filters cbs always get full and index offset relative take care the case when uncompressedstream goes seeking around its incorrect relative partial stream index offset will increase the length our and also account for buffers see creatediskrangeinfo index offset now works long noone seeks into this data before the why would they everything works this hacky stream shouldnt depend having all the data ',positive
'todo hive ideally sort these completion time once that available ',positive
'generate temporary path for dynamic partition pruning spark branch todo longer need this use accumulator param basepath param return ',positive
'todo probably temporary before hive after that may create one per session ',positive
'fixme managers endofbatch threadlocal can deleted ',positive
'todo most the time theres inmemory use array ',positive
'todo could tell the policy that dont care about these and have them evicted could just deallocate them when unlocked and free memory handle that eviction for now just abandon the blocks eventually theyll get evicted ',positive
'single concurrent request per node currently hardcoded the node includes port number different ams the same host count different nodes only have one request type and not useful send more than one parallel ',positive
'todo mssplit switch this back once hivemetastoreclient moved ',positive
'deprecated favour link removed hive ',positive
'currently map type not supported add back when arrow released ',positive
'todo why cas the result not checked ',positive
'todo shouldnt propgate col from tab all ',positive
'todo readencodedcolumns not supposed throw errors should propagated thru consumer potentially holding locked buffers and must perform its own cleanup also currently readencodedcolumns not stoppable the consumer will discard the data receives for one stripe could probably interrupt checked that ',positive
'review jvs oct shouldnt also incorporating the flavor attribute into the description ',positive
'requirements for bucket bucketed their keys both sides and fitting memory obtain number buckets todo incase non bucketed splits would computed based data sizemax part size ',positive
'note later may able set multiple things together except like ',positive
'may null tests todo see javadoc ',positive
'xxx makes sense for possibly not needed anymore ',positive
'the indices should line fixed ',positive
'the method for altering table props may set the table nonmm not affect todo all such validation logic should param tbl object image before alter table command null not retrieved yet param props prop values set this alter table command ',positive
'todo verify skipping charset here okay ',positive
'todo support dynamic partition for ctas ',positive
'todo some extra validation can also added this user provided parameter ',positive
'todo add time abort which not currently tracked ',positive
'todo dump the end wrapping around ',positive
'todo currently ignores gby and ptf which may also buffer data memory ',positive
'the queryid could either picked from the current request being processed generated the current request isnt exactly correct since the query done once return the results generating new one has the added benefit working once this moved out udtf into proper api setting this the generated appid which unique despite the differences taskspec the vertex spec should the same ',positive
'workaround for bug postgres ',positive
'necessary divide and multiply get rid fractional digits ',positive
'todo use common thread pool later ',positive
'register all permanent functions need improvement ',positive
'todo perhaps this could use better implementation for now even the hive query result set doesnt support this assume the user knows what hes doing when calling ',positive
'this our problem means the configuration was wrong ',positive
'used clients serviceregistry todo this unnecessary ',positive
'todo should this also handle acid operation etc seems miss lot stuff from hif ',positive
'todo will these checks work some other user logs isnt doas check required somewhere here well should doas check happen here instead after the user test with hiveserver who the incoming user terms ugi the hive user itself the user who actually submitted the query ',positive
'todo get rid deepcopy after making sure callers dont use references ',positive
'todo this fishy init object inspectors based first tag should either init for each tag rowinspector doesnt really matter then can create this ctor and get rid firstrow ',positive
'snapshot was outdated when locks were acquired hence regenerate context txn list and retry todo lock acquisition should moved before analyze this bit hackish currently acquire snapshot compile the query wrt that snapshot and then acquire locks snapshot still valid continue usual ',positive
'todo option allow converting orc file insertonly transactional ',positive
'todo calculate this instead just because were writing the location doesnt mean that itll always wanted the meta store right away ',positive
'todo propagate this error tezjobmonitor somehow without using killquery ',positive
'all users belong public role implicitly add that role todo mssplit change this back hivemetastorepublic once hivemetastore has moved standalone metastore mrole publicrole new hivemetastorepublic ',positive
'create bare needed because writables require defaultconstructed instance hydrate from the datainput todo remove once hbase fixed ',positive
'todo better with handling types exception here ',positive
'todo returns json string should recreate object from ',positive
'todo these methods really need deepcopy ',positive
'todo numtaskstopreempt currently always ',positive
'todo optimization add check see theres any capacity available point ',positive
'wait while for existing tasks terminate xxx this will wait forever ',positive
'todo replace below with jodatime which supports timezone ',positive
'todo why this inconsistent with what get names ',positive
'not implemented ',positive
'todo this should returning class not just int ',positive
'there are more distincts distinct not count todo may the same countdistinct key countdistinct key todo deal with duplicate count distinct key ',positive
'todo use nonzero index check for offset errors ',positive
'deprecated favour link removed hive ',positive
'todopc need enhance this with complex fields and gettypeall function ',positive
'hive adds the same mapping twice wish could fix stuff like that ',positive
'todo this should moved inner class readerwrite that the only place used ',positive
'harfilesystem has bug where this method does not work properly the underlying hdfs see mapreduce for more information this method from filesystem ',positive
'todo clean uprefactor assumptions ',positive
'todo this executor seems unnecessary here and tezchild ',positive
'todo verify having not seperate filter shouldnt introduce derived table ',positive
'this the testperformance cli driver for integrating performance regression tests part the hive unit tests currently this includes support for running explain plans for tpcds workload nonpartitioned dataset scaleset todo support for partitioned data set use hbase metastore instead derbythis suite differs from testclidriver wrt the fact that modify the underlying metastoredatabase reflect the dataset before running the queries ',positive
'hack note different split strategies return differently typed lists yay java this works purely magic because know which strategy produces which type ',positive
'todo should called here code too fragile move around ',positive
'stores binary keyvalue sorted manner get topn keyvalue todo rename topnheap ',positive
'fixme isnull not updated which might cause problems ',positive
'todo should use what expr udf ',positive
'todo should really probably throw keep the existing logic for now ',positive
'its column level parquet reader which used read batch records for list column todo currently list type only support non nested case ',positive
'todo not clear why dont the rest the cleanup dagclient not created jobclose will called fail after dagclient creation but before ',positive
'todo hive implement similar feature like hive spark ',positive
'todo currently not expose any runtime info for nonstreaming tables future extend this add more information regarding table status total size segments druid loadstatus table historical nodes etc ',positive
'this rather obscure the end last row cached precisely the split end offset the split the middle the file lrr would read one more row after that therefore unfortunate have onerow read however for that have happened someone should have supplied split that ends inside the last row few bytes earlier than the current split which pretty unlikely what more likely that the split and the last row both end the end file check for this ',positive
'todo this should also happen any error right now this task will just fail ',positive
'todo null can also mean that this operation was interrupted should really try recreate the session that case ',positive
'todo should wait for the entry actually deleted from hdfs would have poll the reader count waiting for reach which point cleanup should occur ',positive
'fixme for ctas this still needed because location not set sometimes ',positive
'todo consolidate this code with tezchild ',positive
'todo this check even needed given what the caller checks ',positive
'fixme file paths strings should changed either file path anything but string ',positive
'todo why isacidiudoperation needed here ',positive
'get all simple fields for partitions and related objects which can map oneonone will this queries use different existing indices for each one not get table and name assuming they are the same are using filter todo might want tune the indexes instead with current ones mysql performs poorly esp with order index large tables even the number actual results small query that returns out partitions can sec sec just adding partid filter that doesnt alter the results probably ',positive
'tez session relies threadlocal for open are some nonsession thread just use the same sessionstate used for the initial sessions technically given that all pool sessions are initially based this state shoudlnt also set this all times and not rely external session stuff should probably just get rid the thread local usage tezsessionstate ',positive
'hive input format doesnt handle the special condition paths split correctly ',positive
'not external itself that the case why ',positive
'todo this stopgap fix really need change all mappings unique node least this case track the latest unique for llapnode and retry all ',positive
'fixme druid storage handler relies queryid maintain some staging directories expose queryid session level ',positive
'todo need speed this for the normal path where all partitions are under the table and dont have stat every partition ',positive
'this function frequently used need optimize this ',positive
'this ugly two ways assume that has nullwritable first parameter since are using java and not say programming language theres way check ignore the fact that arg completely incompatible vrb writable because vectorization currently works magic getting vrb from with nonvrb value param just cast blindly and hope for the best which obviously what happens ',positive
'todo two possible improvements right now kill all the queries here could just kill qpdelta after the queries are killed queued queries would take their place could somehow restart queries could instead put them the front ',positive
'this method inefficient its only used when something crosses buffer boundaries ',positive
'todo why doesnt this check class name rather than tostring ',positive
'todo modify thrift idl generate export stage needed ',positive
'workaround for testing since tests cant set the env vars ',positive
'todo the memory release could optimized could release original buffers after are fully done with each original buffer from disk for now release all the end doesnt increase the total amount memory hold just the duration bit this much simpler can just remember original ranges after reading them and release them the end few cases where its easy determine that buffer can freed advance remove from the map ',positive
'todo perhaps add counters for separate things and multiple buffer cases ',positive
'todo this might only applicable try moving there ',positive
'todo cat number these need updated dont bother with deprecated methods this just internal class wait until were ready move all the catalog stuff into ',positive
'case outer joins need pull records from the sides still need produce output for apart from the big table for full outer join todo this reproduces the logic the loop that was here before assuming ',positive
'todo not clear why this check and skipseek are needed ',positive
'todo this safe assumption name collision external names ',positive
'todo cast function calcite have bug where infer type cast throws ',positive
'todo fix this actually not need this anymore ',positive
'deprecated favour link removed hive ',positive
'based todo use proper method after can depend hadoop ',positive
'todo isnt there prior impl isdirectory utility pathfilter users dont have write their own ',positive
'get rid trivial case first that can safely assume nonnull ',positive
'get the tmp uri path will hdfs path not local mode todo gap this doesnt work however this only the path for writer and reader mismatch dump the sidetable for tag load back hashtable file ',positive
'todo this duplicates method orc but the method should actually here ',positive
'todo will this work correctly with acid ',positive
'filter columns may have index which could partition column sarg todo should this then ',positive
'todo this interface ugly the two implementations are far apart featurewise ',positive
'deprecated favour link removed hive ',positive
'validate false default enable the constraint todo constraint like not null could enabled using alter but validate remains false such cases ideally validate should set true validate existing data ',positive
'todo this needs enhanced once change management based filesystem implemented ',positive
'todo would nice check the contents the files could use orcfiledump has methods print supplied stream but those are package private ',positive
'why null and class checks with the new design windowingspec must contain windowfunctionspec todo cleanup datastructs ',positive
'lock ensures have consistent view the file data which important given that generate stripe boundaries arbitrarily reading buffer data itself doesnt require that this lock held however everything else stripes list does todo make more granular only care that each one reader sees consistent boundaries could shallowcopy the stripes list then have individual locks inside each ',positive
'todo wtf ',positive
'todo avoid reading this from the environment ',positive
'todo wish could cache the hive object but its not thread safe and each threadlocal cache would need reinitialized for every query this huge pita hive object will cached internally but the compat check will done every time inside get ',positive
'todo can merge neighboring splits dont init many readers ',positive
'dont break might find better match later ',positive
'stats exist for this key add new object the cache todo get rid deepcopy after making sure callers dont use references ',positive
'todo the current impl triggers are added for tez pool triggers mapping between trigger name and pool name will exist which means all triggers applies tez for llap pool triggers has exist for attaching triggers specific pools for usability provide way for triggers sharinginheritance possibly with following modes only only pool inherit child pools inherit from parent ',positive
'todo using might wrong might need walk down find the ',positive
'todo this should come through relbuilder the constructor opposed ',positive
'todo the contains message check fragile should refactor semanticexception queriable for error code and not simply have message note ifexists might also want invoke this but theres good possibility that ifexists stricter about table existence and applies only the ptn therefore ignoring ifexists here ',positive
'todo were discussing iter interface and also lazytuple change this when plans for that solidifies ',positive
'todo add method udfbridge say cast func ',positive
'todo unregister the task for state updates which could turn unregister the node ',positive
'note assume length split correct given now lrr interprets offsets reading extra row should instead assume chars and add for isunfortunate ',positive
'todo maybe add the yarn url for the app ',positive
'note this particular bit will not work for tables there can multiple directories for different ids could put the path here that would account for the current being written but will not guarantee that other ids have the correct buckets the existing code discards the inferred data when the ',positive
'todo not having aliases for path usually means some bug should give ',positive
'todo get rid mapoutputinfo possible ',positive
'todo this object created once call one method and then immediately destroyed its basically just roundabout way pass arguments static method simplify ',positive
'todo this should passed the taskattemptcontext instead ',positive
'fixme use different exception type ',positive
'this little complicated first look for our own config values this those arent set use the hive ones but hive also has multiple ways this need look both theirs well cant use theirs directly because they wrap the codahale reporters their own and not ',positive
'todo should replace with once hive removes support for the hadoop series ',positive
'the import statement specified that were importing external table seem doing the following dont allow replacement unpartitioned preexisting table dont allow replacement partitioned preexisting table where that table external todo does this simply mean dont allow replacement external tables they already exist soie the check superfluous and wrong this can simpler check not then what seem saying that the only case allow allow import into external table the statement destination partitioned table exists long actually ',positive
'todo handle insert overwrite well hive ',positive
'todo hive include information about pending requests and last allocation time once yarn service provides this information ',positive
'todo this point dont know the slot number the requested host cant rollover next available ',positive
'will this true here dont create new object are already out memory ',positive
'todo replace this with exceptionhandler shutdownhook ',positive
'note this can called outside without calling setuppool basically should able handle not being initialized perhaps should get rid the instance and ',positive
'rowresolver outer query this used resolve correlated columns filter todo this currently will only able resolve reference parent querys column this will not work for references grandparent column ',positive
'todo use stripe statistics jump over stripes ',positive
'deprecated favour link removed hive ',positive
'todo why this needed doesnt represent any cols ',positive
'todo this invalid for smb keep this for now for legacy reasons see the other overload ',positive
'todo fix this ',positive
'this probably should not happen but does least also stop the consumer ',positive
'this seems like very wrong implementation ',positive
'todo should make aborttxns write something into txnstxnmetainfo about this ',positive
'registry again just case todo maybe should enforce that ',positive
'todo lossy conversion distance considered seconds similar timestamp ',positive
'todo this fraught with peril ',positive
'todo not the best way share the address ',positive
'this maps split path offset index based the number locations provided locations not change across jobs the intention map the same split the same node big problem when nodes change added removed temporarily removed and readded etc that changes the number locations position locations and will cause the cache almost completely invalidated todo support for consistent hashing when combining the split location generator and the serviceregistry ',positive
'the list empty too many concurrent operations spurious failure list drained and recreated concurrently same for the other list spurious todo the fact that concurrent recreation other list necessitates full stop not ideal the reason that the list not being recreated still uses the list being recreated for boundary check needs the old value the other marker however nodelta means the other marker was already set new value for now assume concurrent recreation rare and the gap before commit tiny ',positive
'the record count from these counters may not correct the input vertex has edges more than one vertex since this value counts the records going all destination vertices ',positive
'todo why this copypasted from hiveinputformat ',positive
'negative length should take precedence over positive value ',positive
'todo use diskrangelist instead ',positive
'todo shouldnt ignoreemptyfiles set based executionengine ',positive
'all errorandsolutions that errorheuristic has generated for the same error they should the same though its possible that different file paths etc ',positive
'different paths running locally remote filesystem ideally this difference should not exist ',positive
'todo should this not passed the ctor ',positive
'currently deserialization complex types not supported ',positive
'todo what about partitions not the default location ',positive
'todo currently put task info everywhere before submit and know the real node therefore are going store this separately ideally should roll uniqueness ',positive
'would nice could return typeinfo ',positive
'todo replace with storagehandler ',positive
'deprecated favour link ',positive
'todo this line can removed once precommit jenkins jobs move java ',positive
'recovery not implemented yet for ppd path ',positive
'will only check that hadoopauth not simple does not guarantee kerberos ',positive
'find the class that has this method note that may not work here because the method ',positive
'deprecated favour link removed hive ',positive
'removed hadoop but hive users older hadoop versions may still see this exception have reference name ',positive
'this dumb hiveoperation not always set see hivehive ',positive
'todo once hive should able retrieve writeidlist from the conf cachedwriteidlist ',positive
'this nonpool session get rid ',positive
'todo remove this ',positive
'not implemented ',positive
'todo only ever use one row these time why need cache multiple ',positive
'getdeserializer get the deserializer for table param conf hadoop config param table the table return returns instantiated deserializer looking class name deserializer stored storage descriptor passed table also initializes the deserializer with schema table exception metaexception any problems instantiating the deserializer todo this should move somewhere into serdejar ',positive
'not pretty but need way get the size ',positive
'see ctor comment todo should get rid this ',positive
'addition that druid allow numeric dimensions now this check not accurate ',positive
'converted true todo should check convert type string and set true ',positive
'todo ideally should make special form insert overwrite that could use fast merge path for orc and didnt have create table ',positive
'todo policy deserialization errors ',positive
'todo need keep track colnametoposmap for every ',positive
'creating new querystate unfortunately causes all qout change this separate ticket sharing querystate between generating the plan and executing the query seems bad ',positive
'todo currently pass null counters because this doesnt use llaprecordreader create counters for nonelevatorusing fragments also ',positive
'todo not sure that this the correct behavior doesnt make sense create the partition with column with invalid type this should investigated later ',positive
'todo use currently not available ptftranslator ',positive
'todo might well kill the this point how that from here ',positive
'bitsets cant correctly serialized kryos default serializer ',positive
'todo ideally move some the other cleanup code from resetcurrentdag over here ',positive
'todopc remove application logic separate interface ',positive
'todo should this for table ',positive
'todo this limitation the ast rewriting approach that will not able overcome till proper integration full multiinsert queries with calcite implemented the current rewriting gather references from insert clauses and then updates them with the new subquery references however insert clauses use tab cannot resolve the columns that are referring thus just bail out and those queries will not currently optimized calcite example such query from left join aid bid insert overwrite table joinresult select insert overwrite table joinresult select ',positive
'todo delete tablesdatabases ',positive
'todo improve this ',positive
'todo this only applies current thread its not useful all ',positive
'tracks tasks which could not allocated immediately tasks are tracked the order requests come different priority levels todo hive for tasks the same priority level may worth attempting schedule tasks with ',positive
'fixme current objective keep the previous outputsbut this possibly bad ',positive
'assume the enabled the daemon default cannot reasonably check here ',positive
'todo since hive not done minor compact compacts insert delta well should not linefile rsgeti ',positive
'send dropped partition notifications subscribers can receive these notifications for particular table listening topic named dbnametablename with message selector string value value todo datanucleus currently used the hivemetastore for persistence has been found throw npe when serializing objects that contain null for this reason override some fields the storagedescriptor this notification this should fixed after hive upgrade datanucleus from resolved ',positive
'nice error message should given user ',positive
'smbjoin possible need correct order ',positive
'lot these methods could done more efficiently operating the text value directly rather than converting hivechar ',positive
'its only for gby which should forward all values associated with the key the range limit new value should attatched with the key but current implementation only one values allowed with mapaggreagtion which true default this not common case just forward new keyvalue and forget that todo ',positive
'review oops somebody left the last command unterminated should fix for them complain for now nice and fix ',positive
'checks whether given url valid format the current uri format jdbchivehostport jdbchive run embedded mode jdbchivelocalhost connect localhost default port jdbchivelocalhost connect localhost port todo write better regex decide uri format ',positive
'dont fail would better actually compute range inf ',positive
'todo this pre post upgrade todo can different tables different filesystems ',positive
'bloating partinfo with inputjobinfo not good ',positive
'not need generate the again but rather use directly ',positive
'todo ideally wed register tezcounters here but seems impossible before registertask ',positive
'todo have cache for table objects need move that cache elsewhere and use from places like this ',positive
'todo change the indexcache guava loading cache rather than custom implementation ',positive
'todo fix comment hive ',positive
'todo some sessionstate internals are not thread safe the compiletime internals are synced via sessionscope global compile lock the runtime internals work magic they probably work because races are relatively unlikely and few tools run parallel queries from the same session operationstate should refactored out sessionstate and made threadlocal ',positive
'fixme extract the right info type ',positive
'todo should this configurable via annotation extending runwith annotation ',positive
'needs more explanation here xmx not the max heap value jdk you need subtract the survivor fraction from this get actual usable memory before goes into ',positive
'todo unionq the tab alias not properly propagated down the operator tree this happens when union all used sub query hence even column statistics are available the tab alias will null which will fail get proper column statistics for now assume worst case which denominator ',positive
'optional feature not implemented ',positive
'todo dont support this but should since users may create empty partition and then load data into ',positive
'todo backward compat for hive can removed later ',positive
'deprecated favour link removed hive ',positive
'fixme implement consolidateevent similar dumpeventevevroot ',positive
'todo support checking multiple child operators merge further ',positive
'according calcite going removed before calcite todo handle correlationid ',positive
'todo theres versioningetc will come here for now rely external locking ordering calls this should potentially return future for that ',positive
'todo should this done for use with ',positive
'settings borrowed from testjdbcwithminihs ',negative
'used for sending notifications about vertex completed for canfinish ',negative
'which should interpreted instant semantics ',negative
'unwrap the bag ',negative
'initialize dfs ',negative
'piggybacking import logic for now ',negative
'this version hadoop does not support filesystemaccess ',negative
'over the associated fields and look the dependencies ',negative
'singlecolumn long specific repeated lookup ',negative
'all bigtable input columns key expressions are isrepeating then calculate key once lookup once ',negative
'jdk ',negative
'dont fail the query just because any lineage issue ',negative
'this shouldnt really happen byte array ',negative
'add new filter ',negative
'execute sync mode ',negative
'add plugin module jars demand ',negative
'cleanup vectorexprargtype ',negative
'update the lastinputfile with the currentinputfile ',negative
'attach the predicate and group the return clause ',negative
'sortcols ',negative
'',negative
'this should just generate one strategy with splits for base and insertdeltas ',negative
'the following methods are for java serialization use only ',negative
'normal case convert all parameters ',negative
'thread generate the separate rows beside the normal thread ',negative
'day hrs mins secs day hrs mins days day hrs mins ',negative
'only second stripes will satisfy condition and hence single split ',negative
'how much can write current write buffer out what need ',negative
'comparison methods ',negative
'given subquery checks see what the aggegate function ',negative
'number rows received ',negative
'llap summary ',negative
'metadata ',negative
'the predicate numeric column and specifies open range key not support conversion negative values are lexicographically stored after positive values and thus they would returned ',negative
'submit few queries ',negative
'however allow the partition comments change ',negative
'add sparkhadoop prefix for yarn properties sparkconf only accept properties started with spark prefix spark would remove sparkhadoop prefix lately and add its hadoop configuration ',negative
'only fetch the table actually have listener ',negative
'worst case ',negative
'technically for ifnotexists case could insert one and discard the other because the first one now exists but seems better report the problem upstream such command doesnt make sense ',negative
'key ',negative
'buddy block free and the same free list have locked take out for merge ',negative
'now that have made sure that the argument primitive type can get the primitive category ',negative
'interrupted ',negative
'return size ',negative
'rewrite the above plan correlaterelleft correlation condition true leftinputrel projecta rexnode aggregate groupby agg agg projectb references covar rightinputrel correlated reference ',negative
'number elements average elements average elements times the covariance ',negative
'happens when using and the line cmds does not end with ',negative
'make sure zero trimming doesnt extend into the integer digits ',negative
'traverse all the joins and convert them necessary ',negative
'will this point indicating failure ',negative
'state should not null future proofing ',negative
'link lockid queryid ',negative
'replace each the position alias orderby with the actual column ',negative
'nothing default ',negative
'note this sets loadfiletype incorrectly for acid that relevant for import see setloadfiletype and setisacidiow calls elsewhere for example ',negative
'describe how serialize data out user script ',negative
'two dots ',negative
'make sure map task environment points ',negative
'stop reserve order start ',negative
'setup resolve make connections ',negative
'update job state with the childjob ',negative
'exists aleady ',negative
'this partition the only partition under partdate ',negative
'verify ',negative
'can the mapjoin present converted bucketed mapjoin ',negative
'skip druid properties which are used druidserde since they are also updated after serdeinfo properties are copied ',negative
'note that permanent functions can only properly checked from the session registry permanent functions are read from the metastore during hive initialization the jars are not loaded for the udfs during that time and hive unable instantiate the udf classes add the persistent functions set once permanent udf has been referenced session its functioninfo should registered the session registry and persistent set updated can looked there ',negative
'middle item ',negative
'there writeentity with for target table replace with writeentity for each partition ',negative
'the underlying sslsocket object bound hostport with the given sotimeout ',negative
'since want display all the met and not met conditions explain determine all information first ',negative
'partition root ',negative
'new movework was created then should link all dependent tasks from the movework link ',negative
'note that the code updating the state the task does when its out the queue the priorities the queue should correct the top task not killable then task the queue would killable ',negative
'create new grouping key for grouping dummy grouping added runtime the group operator creates rows per input row where the number grouping sets ',negative
'because local inpath doesnt delete source files ',negative
'deal with dynamic partition columns convert exprnodedesc type string ',negative
'try arg version ',negative
'replacing any previous mapping from old input ',negative
'salestxt will need added resource ',negative
'this illustration not functioning example ',negative
'there should new directory base ',negative
'remainder always positive ',negative
'when converting from char the value should stripped any trailing spaces ',negative
'check may julian day ',negative
'this should not start the actual tez ',negative
'for each dir get the inputformat and getsplits ',negative
'safety valve for extreme cases ',negative
'note that have regexes below ',negative
'case unions mapjoins possible that the file has already been seen ',negative
'theory this should not happen the original copy the udf had this data should able set the udf copy with this same settabledata ',negative
'parse validtxnlist ',negative
'inputsaddnew readentitypartn this needed all ',negative
'test table without portion ',negative
'record the final counters ',negative
'stop hiveserver ',negative
'firstname alan ',negative
'skipnulls true and there are rows valuechain all rows partition are null far add null ',negative
'expect query successfully completed now ',negative
'old view has partitions could not replaced ',negative
'oracle specific parser ',negative
'base case ',negative
'asynch write completed the semaphore ',negative
'override this method you want customize how the node dumps out its children ',negative
'analyze table pageview ',negative
'does same thing with except that this replaces original keyoi with which create hbasekeyfactory provided serde property for hbase ',negative
'dont set perms groups for default dir ',negative
'setup timer task check for hearbeat timeouts ',negative
'start concurrent txn ',negative
'also throws ioexception when binary detected ',negative
'some nasty examples that show how log format broken and test the regex these are all sourced from genuine logs text sample new imgzemantacom apr aacdfececdccaaec ffcfeadc restgetobject pixygif get http mozilla compatible msie windows net clr net clr text sample new imgzemantacom apr aacdfececdccaaec dbe restgetobject pixygif get pixygifxidbabacdab http zhuaxiacom text sample new staticzemantacom apr aacdfececdccaaec eeeffebfea restheadobject head http accessdenied mozilla compatible msie windows ',negative
'call the function ',negative
'loginfohar location harlocn ',negative
'nonjavadoc see javalangstring ',negative
'windowframe may contain just the start boundary the between style expressing windowframe both boundaries are specified ',negative
'the partitioning columns the child are more specific than those the parent ',negative
'sample the first batch processed for variable sizes ',negative
'phase create objects ',negative
'update for ',negative
'count ',negative
'case ',negative
'fail transactional set true and the table bucketed but doesnt use orc ',negative
'use the expressions from reduce sink ',negative
'want the values which are not skewed ',negative
'only specified nodes these types will walked empty set means all the nodes will walked ',negative
'valid txn list might generated for query compiled using this command thus need reset ',negative
'run the vectorized mode ',negative
'this point one task and taskp running ask for another task ',negative
'write files inside the subdirectory ',negative
'subdirectory then recursively list the files ',negative
'for thread safe ',negative
'otherwise add the join specs ',negative
'the target hash table disk spill this row disk well processed later ',negative
'this contrived example practice this query would course fail after drop table ',negative
'view ',negative
'not propagated ',negative
'create the final reduce sink operator ',negative
'from precision max scale max ',negative
'let the processor control start for broadcast inputs ',negative
'since are adding the user name the scratch dir not need give more permissions here ',negative
'the schema version already checked then ahead and use this metastore ',negative
'child can expr alias expr ',negative
'leftinputrel contains unique keys each row distinct and can group all the left ',negative
'store the inputs hashmap since cant get readentity from inputs since implemented setreadentity used the key that the hashmap has the same behavior equals and hashcode ',negative
'this operator has been visited already the rule not need apply the optimization ',negative
'the oldsplit may null during the split phase ',negative
'first create the archive tmp dir that the job fails the ',negative
'close the session before have wait again ',negative
'deltes cant raw format ',negative
'operationid ',negative
'shortcircuited clientside verifying that its empty object not null ',negative
'read the newly added via cachedstore ',negative
'multikey specific declarations ',negative
'the number files for the table should same number buckets ',negative
'get the join keys from parent reducesink operators ',negative
'whether all files are not sufficient reach sizeleft ',negative
'not vrb mode the new cache data ready should use ',negative
'directory removal will handled cleanup the sessionstate level ',negative
'use only reducer reduce keys ',negative
'only set when setting the secure config for ',negative
'repl load ',negative
'make room for remainder ',negative
'resolve storage handler any ',negative
'restore default cost model ',negative
'test that input name does not change iocontext returned and that each thread gets its own ',negative
'ignore exceptions from destroy ',negative
'this always nonnull when conversion completed ',negative
'true both are null ',negative
'utility methods used store pairs ints long ',negative
'the nullindicator ',negative
'for groupby values with same key topk should forwarded flag used control how topn handled for ptfwindowing partitions ',negative
'get default value stored metastore ',negative
'add new key add value existing key ',negative
'this one can pushed ',negative
'there already predicate this src ',negative
'expr sanity test strict mode the presence order limit must ',negative
'adding columninfo the rowschema signature ',negative
'there are two areas exploration for optimization here were serializing the schema with every object assume the schema provided the table always correct dont need this and and can just send the serialized bytes tofrom bytes immediately may save some time but doing this lazily but until theres evidence this useful ',negative
'get input data ',negative
'master node will serialize readercontext and will make available slaves ',negative
'files size for splits ',negative
'arrowallocatorlimit ignored allocator was previously created ',negative
'the grouping sets data consumed the current job ',negative
'completion txnidtxnidselect ',negative
'write value the column vector and return back the byte buffer used ',negative
'when reading the file for first time get the orc tail from the orc reader and cache the footer cache subsequent requests will get the orc tail from the cache file length and modification time not changed and populate the split info the split info object contains the orc tail from the cache then can skip creating orc reader avoiding filesystem calls ',negative
'obviously different expressions ',negative
'assert the actual stack traces are exactly equal the written ones and are contained stacktraces list the submission order ',negative
'map from newinput ',negative
'retain only the largest value for register index ',negative
'probe space should least equal the size our designated wbsize ',negative
'skewed columns stuff ',negative
'key ',negative
'deal with nonpartition columns ',negative
'dummy instantiation make sure any staticctor code blocks that driver are loaded and ready ',negative
'the name the dag what displayed the amjob ',negative
'note that this not real double hashing since have consistent hash top ',negative
'cant vectorize ',negative
'read the value and key length for the first record ',negative
'list conf entries not turn into env vars ',negative
'limit greater than available rows then not update statistics ',negative
'this the bad part the vectorized udf returns the right result ',negative
'nonjavadoc see javaioinputstream ',negative
'actual list deser its values ',negative
'create new agg function calls and rest project list together ',negative
'nonjavadoc see ',negative
'partition walker working ',negative
'instantiate new factory instance only current one not valid ',negative
'extra columns difference between referenced columns needed columns the difference could partition columns ',negative
'check this user has grant privileges for this privileges this object ',negative
'for each predicate does refer one many aliases one add the filterforpushing list that alias many add filter from merging trees ',negative
'cannot entries while currently hold read lock keep track them delete later ',negative
'definitely short most shorts fall here ',negative
'use the serialization scale and format the string with trailing zeroes round the decimal necessary ',negative
'report the row its the first row ',negative
'serialize using the serde then below deserialize using deserializeread ',negative
'original hashcode with low bits ',negative
'return evaluator depending the return type ',negative
'assume each partition has unique ',negative
'fieldschemas ',negative
'check the output fixacidkeyindex should indicate the index was fixed ',negative
'nothing ',negative
'export trivially undoable that nothing needs doing undo ',negative
'this returns the row because this formatter only called when the was used serialize the rows thriftable objects ',negative
'serialization ctor ',negative
'query that should fetch one column ',negative
'set the default the created date null there was ',negative
'referenced the current expression node ',negative
'descriptor not defined because takes variable number arguments with different data types ',negative
'note that toolrunner this expected local path see ',negative
'generate selection operator for groupby keys only ',negative
'nonjavadoc see javasqldate ',negative
'change the type back int that the same alter can attempted from connection ',negative
'test lazybinarymap ',negative
'this will turn setugi both client and server processes the test ',negative
'leave for clean ',negative
'the last block that add should append the current block where the cursor ',negative
'regrettable that have wrap the ioexception into runtimeexception but throwing the exception the appropriate result here and hasnext signature will only allow runtimeexceptions iteratorhasnext really should have allowed ioexceptions ',negative
'verify that updates the notificationevent with the new event ',negative
'spacing ',negative
'give ',negative
'extract partition spec file name part from path ',negative
'retrieve all partitions generated from partition pruner and partition ',negative
'test with doastrue ',negative
'validate database ',negative
'were merging the same location can avoid some metastore calls ',negative
'nonjavadoc see ',negative
'logdebugrunning hive query sql ',negative
'the location input and table output for altertable add partitions ',negative
'sort the ngram list frequencies descending order ',negative
'this case should expect the test have failed the very last read check ',negative
'',negative
'only offer these when the input file format not the fast vectorized formats ',negative
'that case will select but the rowoi need not amended ',negative
'nullsafe issame ',negative
'this path intermediate data ',negative
'start the server ',negative
'nonjavadoc see javautilcalendar ',negative
'are caching the include the cache ',negative
'use longs because dont have unsigned ints ',negative
'the input column can either string list integer values ',negative
'integer too large cannot recover trimming fractional digits ',negative
'ast has two children the child could partition spec columnname ',negative
'typically alter table concatenate run only one partitionone table ',negative
'first call fileutilsmkdir make sure that destf directory exists not creates ',negative
'add the new the old ',negative
'this the case when have mapside smb join one input the join treated dummy vertex ',negative
'',negative
'need count the current one map table then ',negative
'expected result ',negative
'',negative
'use the short user name for the request ',negative
'changes the owner role and verify the change ',negative
'partial test init ',negative
'optional tblname was specified ',negative
'update stats but dont update ndv will not change ',negative
'for every sampled alias figure out splits sampled and add them return list ',negative
'cleanup the synthetic predicate the tablescan operator and filter replacing with true ',negative
'test that exclusive partition locks coalesce one ',negative
'hiveserver global init file location ',negative
'example code test specific scenarios lowlevelcacheimpl cache new lowlevelcacheimpl new dummycachepolicy new dummyallocator true cleanup thread final int file gaps fbs prioritynormal null null gaps fbs prioritynormal null null gaps fbs prioritynormal null null diskrangelist mutatehelper new mutatehelperdr drinsertafterdr drinsertafterdr booleanref new booleanref mhnext testfactory null ',negative
'through the root tasks and verify the input format the map reduce tasks ',negative
'bucket the second field the record ',negative
'update the info sel operator based the pruned reordered columns ',negative
'case cols are null ',negative
'get the udtf path ',negative
'the table output and location input ',negative
'try nonnull path ',negative
'get the value length ',negative
'makes spilling prediction ismemoryfull too defensive which results unnecessary spilling ',negative
'since division has occurred dont format with decimal point ',negative
'build the bitset with not null columns ',negative
'dont set inputs and outputs the locks have already been taken its pointless ',negative
'requests before started ',negative
'create the local work for this plan ',negative
'needed until there junit release with beforeparam afterparam junit then should remove our own copy ',negative
'empty values except first column ',negative
'add arraystruct the list columns ',negative
'this also resets sessionstateget ',negative
'get the create table statement for the table and populate the output ',negative
'replace each the position alias orderby with the actual column name ',negative
'get columnnames from the first parent ',negative
'create the walker the rules dispatcher and the context ',negative
'check the objectinspector ',negative
'hiveparsekwif hiveparsekwleft hiveparsekwright ',negative
'shutdown existing session manager ',negative
'hits the index return match range ',negative
'this will used hadoop only unsecurenon kerberos mode ',negative
'null values ',negative
'create the context for the walker ',negative
'build the type property string not supplied ',negative
'whether contains common join contains return this common join ',negative
'struct ',negative
'should never happen just case ',negative
'add task insert delete materialized view from registry needed ',negative
'using async could also reserve one one ',negative
'here must deltaxy insert events only must compacting ',negative
'connect parent filesinkop ',negative
'run sql inserts concurrently ',negative
'test binary mode ',negative
'getacidstate smart not return any deltas current there base that covers them they were compacted but not yet cleaned this means rechecking compaction needed should cheaper ',negative
'its possible that old metadata still refers decimal column type precisionscale this case the default assumed thus nothing here ',negative
'the case where agg count countcorvar changed countnullindicator note any nonnullable field from the rhs can used the indicator however true field added the projection list from the rhs for simplicity avoid searching for nonnull fields projecta all gby keys rewritten nullable projexpr aggregate groupbyall left input refs countnullindicator other aggs projectb all left input refs plus the rewritten original projected exprs joinreplace corvar input ref from leftinputrel leftinputrel project everything from rightinputrel plus the nullindicator true rightinputrel ',negative
'the new port invalid find one starting with the default client port the default client port not specified starting with random port the random port selected from the range between these ports cannot registered with iana and are intended for dynamic allocation see httpbitlydynports ',negative
'and not etc ',negative
'this position aggregation store onerow only the aggregate results need adjust the correct position there are keys the gby operator ',negative
'subtract from self ',negative
'set isnull before calls case tney change their mind ',negative
'maxretries code retries until batch decays zero ',negative
'should fail try get info out the params ',negative
'execution fall back row mode ',negative
'optionally read current values big length big value len big value bytes since this the first record the valuerefword directs ',negative
'avro requires nullable types defined unions some type and null this annoying and were going hide from the user ',negative
'verify the raw object thats been created ',negative
'will create subquery expression boolean type ',negative
'todo fix throws exception ',negative
'this valid yarn service name parse out ',negative
'default value empty string which case properties will inherited ',negative
'reset the hivesitexml values for following param ',negative
'create the ungrouped splits ',negative
'not using the gauge avoid races ',negative
'all data intitializations ',negative
'any table empty inner join involving the tables should yield rows ',negative
'keep the arguments for reference want all the nonnumeric arguments the same ',negative
'trying using the cardinality from the value range ',negative
'through all joins should only contain selects and filters between ',negative
'currently getprocedurecolumns always returns empty resultset for hive ',negative
'can null incase junit tests skip reset reset thread name release time ',negative
'skipped like other ops should all work long the types are right ',negative
'arithmetic with type date longcolumnvector storing epoch days and type intervaldaytime storing ',negative
'convert the accumulo token hadoop token ',negative
'this expr udaf invocation does imply windowing return implies neither implies aggregation implies count implies windowing ',negative
'here txn was not found expected state ',negative
'change different fields and verify they were altered ',negative
'create session domain not present ',negative
'put the new mapping ',negative
'create fake directory throw exception ',negative
'assume and table names are the same for all partition provided arguments ',negative
'bgenjjtree extends ',negative
'found the union ',negative
'build conditions for join and filter and start adding ',negative
'bytes key hash set optimized for vector map join this the abstract base for the multikey and string bytes key hash set implementations ',negative
'optional byte array ',negative
'nonjavadoc see javalangobject ',negative
'table itself doesnt exist metastore nothing validate ',negative
'msck called add missing paritions into metastore and there are missing partitions ',negative
'cache pathexpr ',negative
'ignore shutdown errors since there are negative tests ',negative
'otherwise enabled deserialize rows using regular serde and add the object inspectable object row vectorizedrowbatch the vectormapoperator ',negative
'count number digits the value ',negative
'note here create fake directory along with fake files original directoriesfiles ',negative
'return for ',negative
'skip ',negative
'private static final string testtablename autopurgetesttable ',negative
'deserialization keys here just get reference bytes ',negative
'validate the metadata for the getcolumns result set ',negative
'bigger addition ',negative
'first set back the backup task with its children task ',negative
'authenticate using delegation tokens via the digest mechanism ',negative
'write stats objs persistently ',negative
'todo plumb progress info thru the reader can get metadata from loader first ',negative
'hivemetastore keys reserved for updating listenerevent parameters this key used check listener event run inside current transaction boolean value used for active true active false ',negative
'',negative
'hadoop vars ',negative
'boolean store information about whether valid txn list was generated ',negative
'need determine different type needed for dummy partitions ',negative
'ignore zerodivide cases ',negative
'replication scope allows replacement and does not require empty directories ',negative
'',negative
'this the last time well see the table objects for views add the inputs now isinsideview will tell this view embedded another view ',negative
'for bootstrap load the create function should always performed ',negative
'determine which rows are non matches determining the delta between inputselected and current batch selected ',negative
'create the hive catalog ',negative
'use druid default need configured user ',negative
'these represent the bucketed columns ',negative
'reuse existing connection ',negative
'note that will throw anonymous mode not allowed username not query string the request this ensures that the context webhcat allows anonymous even though webhcat itself will throw cant figure out username ',negative
'alphabet needed ',negative
'write bunch random rows that will used for read benchmark ',negative
'end user transaction timeout milliseconds ',negative
'first entry existence ',negative
'remove old temp table entry and add new entry list temp tables ',negative
'frequently occurring error ',negative
'normalize significand even ',negative
'version with seq version with rcf ',negative
'append the path substring since previous match ',negative
'dont bother with aggregation this case will probably invalid ',negative
'failure help ',negative
'optional bool isguaranteed ',negative
'second time ',negative
'',negative
'null for non partitioned table ',negative
'after all the rows are processed continue generate results for the rows that results havent generated for the case following and following process first results and then insert nulls for the case preceding and following process results ',negative
'setting empty collection ranges will unexpectedly scan all data ',negative
'skip the big table pos ',negative
'first search from the postovertex ',negative
'ppd for multiinsert query not yet implemented assume that nothing can pushed beyond this operator ',negative
'havent fixed the tmp path for this mapper yet ',negative
'nonjavadoc see ',negative
'test that two different tables dont collide their locks ',negative
'note this assumes both paths are qualified which they are currently ',negative
'varchar ',negative
'lazycaching the version ',negative
'note that use the same vectors both batches clever very clever ',negative
'ast specific data ',negative
'build rel for where clause ',negative
'delete the temp file the jvm terminate normally through hadoop job kill command caveat wont deleted jvm killed kill ',negative
'important signum given the firstbyte not bytekeep ',negative
'entry will null due zerodivide ',negative
'these have been localized already ',negative
'lists recursively compare the list element types ',negative
'clear out the union set dont need anymore ',negative
'note this called someone who has ensured the buffer not going moved ',negative
'get the field objectinspector fieldname and the field object ',negative
'get agg ret type calcite ',negative
'ptf handling ',negative
'update has failed wont try low pri task ',negative
'using vint instead bytes ',negative
'create case sensitive columns list ',negative
'needed kyro ',negative
'only set command needs updating the configuration stored client side ',negative
'the field that passed not primitive and either the field not declared schema was given initialization the field declared primitive initialization serialize the data json string otherwise serialize the data the ',negative
'assert ',negative
'bucket count ',negative
'the first query happens have full batches ',negative
'the primitives ',negative
'change the resource plan resize and down and remove remapping users everything will killed and wont change will start one more query from the queue and the query queued will requeued and started the fractions will also all change ',negative
'test getlogicallength side file ',negative
'its skip failed message the target has changed back the old value ',negative
'check that the values the older list are also newer lists should already sorted ',negative
'created april change this template choose tools template manager and open the template the editor ',negative
'store token the cache ',negative
'versions dont match return false ',negative
'the list families that have been added the scan ',negative
'empty maps ',negative
'initialize the constants for the grouping sets that they can reused for ',negative
'merge bytes ',negative
'files size for splits ',negative
'keep backward compatibility ',negative
'files the partition ',negative
'repeating then expression ',negative
'generate map reduce plans ',negative
'swsr lock are examining shared read ',negative
'trim the bins down the correct number bins ',negative
'lookup byte array key the hash map param keybytes byte array containing the key within range param keystart the offset the beginning the key param keylength the length the key param hashmapresult the object receive small table values information match for spill has information where spill the big table row return whether the lookup was match match spill the partition with the key currently spilled ',negative
'found exact match ',negative
'are missing section the end the part copy the start noncached ',negative
'needs least instances ',negative
'validate the effective window frames with the rules link validatewindowframe ',negative
'partition level column statistics test ',negative
'this lock used make sure removallistener wont close client that being contemplated for returning get ',negative
'thread local filesystem stats private and cannot cloned make copy new class ',negative
'nonjavadoc see ',negative
'fully copy over the contents the new into the old ',negative
'view just being created already created view being loaded ',negative
'each range checks for overflow first then moves digits ',negative
'since this can set make the system generate old style deltaxxxxyyyy file names this primarily needed for testing make sure code can still read files created older code also used comactor ',negative
'now release another session the thread that gave reuse can proceed ',negative
'see autocolumnstatsq under ',negative
'merge cost ',negative
'this currently only happens tests see getfiledata comment the interface ',negative
'reg info changes add notifications ignore errors and update alloc ',negative
'say table has partition are generating select where group rowid ',negative
'there avoid clashes ',negative
'extract the bits num into bytes from right left ',negative
'when are doing row deserialization these are the regular deserializer partition object inspector and vector row assigner ',negative
'optional int ',negative
'several hive classes depend the metastore apis which not included hiveexecjar these are the relatively safe ones operators classes ',negative
'revert the projected columns back because vrg will reused ',negative
'sure that null produced ',negative
'object map ',negative
'make copy the sourcetable would done across classloaders ',negative
'child this expression uses materialized view then decrease its cost certain factor this useful for partial rewritings where part plan does not use the materialization but still want decrease its cost chosen instead the original plan ',negative
'the bigdecimal class recommends not converting directly from double bigdecimal convert through string ',negative
'whatever remains take totake blocks splitting the block offset ',negative
'name ',negative
'select constant and casts can allowed without threshold check ',negative
'there were more batches and this either the first batch weve used the current batch buffer goto return false ',negative
'virtual columns needed ',negative
'unable find stats for this statstype return null can build stats ',negative
'acc short for accumulator its used build the row before forwarding ',negative
'rowid ',negative
'change the location position alias process here ',negative
'keys are always primitive ',negative
'have evaluate the input format see vectorization enabled not set right here ',negative
'update the cache add this new aggregate node ',negative
'find most significant bit with starting index ',negative
'just run our value expressions over input batch ',negative
'test getboolean rules nonboolean columns ',negative
'block ',negative
'gather input works operators ',negative
'keyvalue the index removed retrieve memory usage ',negative
'todo convert semantic exception ',negative
'first try use the blocks half size every arena ',negative
'give the cloned work different name ',negative
'originals written before table was converted acid considered written writeid which always committed there need check wrt invalid write ids but originals written load data for example can basex deltaxx must check committed not evn rowid not needed the operator pipeline ',negative
'field expression ',negative
'unless this overridden does nothing ',negative
'',negative
'return something ',negative
'given ast node this method recursively goes over checkexpr ast finds node type toksubqueryexpr throws error ',negative
'under exceptional load hadoop may not able look status finished jobs because has purged them from memory from hives perspective its equivalent the job having failed raise meaningful exception ',negative
'perform major compaction ',negative
'second pass process operator tree two steps first process the extra trees generated the first pass then process the main tree and the result task will depend ',negative
'create scratch dir ',negative
'case when mvdeptno null and mvdeptname null then else sources mvs end ',negative
'singlecolumn string outer get key ',negative
'since left has longer digit tail and doesnt move will shift the right digits our addition into the result ',negative
'are secure mode ',negative
'return previous block ',negative
'the primitive object inspector the source data type for any column being converted otherwise null ',negative
'print out all the stages that have childstages ',negative
'assume ',negative
'make sure the token made into the ugi ',negative
'hashstruct ',negative
'test with txncommit ',negative
'now finish task which will make capacity for task run nothing coming out the delayed queue yet ',negative
'reset the result for the evicted index ',negative
'genericudfbridge udfbridge genericudfbridge exprgetgenericudf ',negative
'drop any functions before dropping ',negative
'apply the group and permissions the leaf partition and files need not bother case hdfs permission taken care setting umask ',negative
'for cases where the table temporary ',negative
'not hbase row key this should either prefix individual qualifier ',negative
'tests that different threads get the same object per attempt per input and different between attemptsinputs that attempt inherited between threads and that clearing the attempt produces different result ',negative
'get privileges for this user and its role this object ',negative
'make certain the target directory exists ',negative
'the following test uses query that returns group and user entry the ldap atn should use the groupmembershipkey identify the users for the returned group and the authentication should succeed for the users that group well the lone user this case ',negative
'pass the request the responder ',negative
'this should never thrown ',negative
'columnname ',negative
'give our final state uiapi requests any ',negative
'well assume that there may nulls the input nonulls true for input vector this more forgiving errors loading the vectors properlywritten vectorized iterator will make sure that isnull set nonulls and isrepeating are true for the vector ',negative
'wait for failover close sessions ',negative
'for this particular file how many columns will actually read ',negative
'foreignkeycols ',negative
'second value ',negative
'verify serializationutils first ',negative
'pushed the whole thing down ',negative
'avoid timing issues with notifications and given that hdfs check anyway the authoritative one dont wait infinitely for the notifier just wait little bit ',negative
'nonjavadoc see list ',negative
'time actually run the dag actual dag runtime ',negative
'fktablename ',negative
'find functions which name contains tofind hidden the default database ',negative
'create argument capturer class variable cast this generic generic class ',negative
'only user belonging admin role can drop existing role ',negative
'optional int keyid ',negative
'expected fail due canceled token ',negative
'todo make use this config configure fetch size ',negative
'this should use but its protected currently ',negative
'need create the correct table descriptor for keyvalue ',negative
'exclude trailing comma ',negative
'add dist udaf args reduce keys ',negative
'evaluate the key expressions just this first batch get the correct key ',negative
'need iterate through all children even one found not candidate case the other children could individually pushed ',negative
'try more tolerant the input invalid instead incomplete well hit exception here again ',negative
'cookies manager used cache cookie returned service the goal avoid doing kdc requests for every request ',negative
'find all targets recursively ',negative
'leverage the nice batching behaviour async loggersappenders can signal the file manager that needs flush the buffer disk the end batch from users point view this means that all log events are always available the log file without incurring the overhead immediateflushtrue ',negative
'exists primarily allow for easier unit tests ',negative
'register two tasks same query but different ',negative
'for completed instances ',negative
'print out the cbo info ',negative
'interface design tablefunction provides interfaces execution batch and streaming batch mode the contract partition partition out streaming mode the contract stream processrow calls each which may return more rows partition not just batch rows enables more than single iteration the data multiple passes arbitrary access input rows relative navigation between rowsfor leadlag fns most ptfs will work batch mode the streaming mode gives the capabilities partitions for the benefit smaller footprint and faster processing window function processing this when there are only ranking functions each row needs accessed once the order provided hence there need hold all input rows partition the pattern any time you want only enhanceenrich input row streaming mode the right choice this the fundamental difference between ranking fns and udafs ranking functions keep the original data intact whereas udaf only return aggregate information finally have provided mixed mode where non streaming tablefunction can provide its output iterator far can tell this special case for windowing handling windowing the only last tablefunction chain makes sense collect the output rows into output partition justify the pollution the api the observation that windowing very common use case ',negative
'its valid case partition ',negative
'finally remove the expression from the tree ',negative
'these delims passed serde params ',negative
'tracks all instances including ones which have been disabled the past ',negative
'login from the keytab ',negative
'serialize the struct into mutation ',negative
'init input ',negative
'includebitset ',negative
'prefer date type arguments over other method signatures ',negative
'idempotent function add various intermediate files the source for the union the plan has already been created ',negative
'',negative
'',negative
'blindly add this union type containing int and double should sufficient for the test case ',negative
'the second operator has more than one child stop gathering ',negative
'the filter already top tablescan ',negative
'find the first ancestor this movetask which some form map reduce task either standard local merge ',negative
'construct expressionnodedesc representing join condition ',negative
'array for the values pass evaluator ',negative
'end master thread state ',negative
'implementation kvsource that can handle key and value byteswritable objects ',negative
'return single arraylist where the first element the number histogram bins and subsequent elements represent histogram pairs ',negative
'used for dynamic partitioning ',negative
'with the integer type range checking need know the hive data type ',negative
'abort all remaining txns ',negative
'files size for splits ',negative
'close file streams avoid resource leaking ',negative
'for now this can simply fetched from single registry instance ',negative
'there clause the output schema will keyvalue ',negative
'',negative
'note other standard ones include clientuser and clienthostname but dont need them for now ',negative
'get keyvaluesreaders from the logicalinput and add them priority queue ',negative
'notify listeners the changed value ',negative
'nonjavadoc should ideally not modify the tree traverse however since need walk the tree any time when modify the operator might well here ',negative
'there should delta dirs plus base dirs the location ',negative
'now deprecated ',negative
'adjust the memory have account for what have just evicted ',negative
'ownertype ',negative
'hash table loading happens server side llapdecider could kick out some fragments run outside llap flip the flag runtime case are running outside llap ',negative
'how many ways each block splits into target size how many targetsized blocks remain from last split the header index for the beginning the remainder ',negative
'very simple counter keep track join entries for key ',negative
'this operator check whether unary binary operator ',negative
'divide operations are not checked because the output always the type double ',negative
'only the pattern valuecol should handled ',negative
'construct ',negative
'for some strange reason bigdecimal can have scale not support that ',negative
'convenience method that makes the intended owner for the delegation token request the current user ',negative
'also determine any nulls are present since for join that means match ',negative
'remove the context words from the end the list ',negative
'stripergs should have been initialized this time with empty array ',negative
'create the reloading folder place jar files not exist ',negative
'look for passthru case where inputfileformat has and reads vectorizedrowbatch row ',negative
'fix the case where parent expressions output data type physical variations decimal whereas least one its children decimal some expressions like for example only accepts decimal for and this time there only both and has decimal ',negative
'must get statementid from file name since acid doesnt write into bucketproperty ',negative
'nulls right nulls left ',negative
'ndvproduct then column stats state must partial and are missing ',negative
'validate that the set partition columns found custom path must match ',negative
'todo unpause fetching ',negative
'copy jar change management fails fail the metastore transaction since the user might delete the jars hdfs externally after dropping the function hence having ',negative
'fails the finally clause will remove the lock ',negative
'well encode the absolute value sign separate ',negative
'java timezone has mention thread safety use thread local instance safe ',negative
'',negative
'reset the array null values ',negative
'this removes orderby only expressions from the projections ',negative
'partitions bail early ',negative
'make the columns list for the temp table input data file ',negative
'inputstream open the given sequence file broken rcfile ',negative
'current implementation will never happen but leave here make the logic complete ',negative
'try get the session quickly ',negative
'',negative
'here some query rewrite first get the new fetchrn which sum offset and fetch then push through creating new branchsort with the new fetchrn but offset ',negative
'optional string containeridstring ',negative
'check the restricted configs that the users cannot set ',negative
'insert some data this will again generate only insert deltas and delete deltas delta ',negative
'expected exception embedded metastore ',negative
'the user has specified location external not check the user ',negative
'need copy the data byte byte only case the outputlength length which means there least one escaped ',negative
'the view inside another view should have least one parent ',negative
'read database via cachedstore ',negative
'tez ',negative
'generate the partition columns from the parent input ',negative
'test incorrect totals dont normalize just make sure dont under overshoot ',negative
'the table containing the partition not yet loaded cache ',negative
'collect the hiveconflist and hivevarlist separately that they can ',negative
'job properties are only relevant for nonnative tables for native tables leave null avoid cluttering ',negative
'output final result the aggregation ',negative
'username ',negative
'create some cover the result ',negative
'metastore always support concurrency but certain acid tests depend this being set ',negative
'verify droppartition recycle part files ',negative
'isrepeating and nulls ',negative
'move common logic that can reused ',negative
'the expression identify the partition dropped ',negative
'this method gets called only the scope that destination table already exists were validating the table appropriate destination import into ',negative
'with trimfalse parsing cannot handle spaces ',negative
'read all credentials into the credentials instance stored jobconf ',negative
'nonjavadoc see ',negative
'this the case when the big table subquery and probably already bucketed the join column say group operation ',negative
'the fractional digits are gone when rounding clear remaining round digits and add ',negative
'hive need add project top join since semijoin with join its right input ',negative
'varchar columns should have correct display sizeprecision ',negative
'',negative
'logger jobconf ',negative
'plan needs complete before execute and not modify while execution the driver ',negative
'well dont recurse but make sure all children are initialized ',negative
'overall information this vectorized map operation ',negative
'fetch namespace ',negative
'topn query results ',negative
'read the first batch the first batch itself was null close the reader ',negative
'set recursive reads for subdirectories ',negative
'returns immutable map with the identity count count ',negative
'used for statistics ',negative
'add the expression partition specification ',negative
'vvavacavbbca ',negative
'optionally the next values small length could integer the values information ',negative
'the set tablescanoperators for pruning trees ',negative
'native vectorization not supported ',negative
'establish mapping from the output column the input column ',negative
'left full outer join and the left record did not produce results need take that record replace the right side with null values and produce the records ',negative
'todo logging currently goes hivelog ',negative
'optional string vertexname ',negative
'read from arrow stream batchbybatch ',negative
'passes null configuration into the serde shouldnt fail immediately this case ',negative
'the bottom operator not synthetic and does not contain limit ',negative
'skip query hints ',negative
'the default value clean ',negative
'show column level privileges ',negative
'pattern remove the timestamp and other infrastructural info from the out file ',negative
'have decimal after enforce precision and scale will become null ',negative
'max can even when ndv larger clause than column stats ',negative
'authenticate using keytab ',negative
'continue read move the secondary ',negative
'build reduceside graph ',negative
'cant null cant null cant null ',negative
'intentional fall through ',negative
'normal deduplication ',negative
'copy table level hcat keys the partition ',negative
'again done want exit because logging issues ',negative
'timeout occurs ',negative
'replace this with valueof ',negative
'first merge all the adjacent bitvectors that could merge and derive new partition names and index ',negative
'this qtest lets order the params map lexicographically key this get consistent param ordering between java and java ',negative
'clean txntowriteid table for entries under minuncommittedtxn referred any open txns ',negative
'worst case when there are column statistics ',negative
'set isnull before call case changes mind ',negative
'setup for actual notifications not already done for previous task ',negative
'obtain second lock this shouldnt block cleaner was acquired after the initial ',negative
'expression therefore scan the whole table ',negative
'init and run are both potentially long and blocking operations synchronization with the abort operation will not work since they end blocking monitor which does not belong the lock the abort will end getting blocked both these method invocations need handle the abort call their own ',negative
'rethrow the exception ioexception ',negative
'build hash map from colname object for old columnstats ',negative
'first traverse the batch evaluate and prepare the keywrappers ',negative
'project ',negative
'batch batch ',negative
'serializes decimal the maximum bit precision decimal digits ',negative
'for the first grandkid replace the original parent ',negative
'get one the default separators avoid having set custom separator ',negative
'optional bool isguaranteed default false ',negative
'otherwise heuristics ',negative
'were folding multiple masked lines into one ',negative
'leftsemijoin found match and not have any additional predicates skipping the rest the rows the rhs table the semijoin ',negative
'zero reducers ',negative
'only the minimum cast for decimals other types are assumed safe fix needed also dont anything for nonprimitive children maybe should assert ',negative
'since timestamps are averaged with double dont need partial class and since timestamps are output double for avg dont need final class either vectorudafavgmerge partial vectorudafavgmerge final ',negative
'startedtime ',negative
'the branch represented the list ',negative
'case ',negative
'this time completes adding just foreign key constraints for table ',negative
'perform repldumpload ',negative
'seen the users ',negative
'open client session ',negative
'checking for null the forloop condition prevents nullptr exception and allows fail more gracefully with parsing error ',negative
'also the location field partition ',negative
'check that find all expected columns ',negative
'used based stats collector ',negative
'use full partition path for error case ',negative
'hiveexception expected ',negative
'create paritioned table ',negative
'default this will same that super class but need obtain again ',negative
'',negative
'finally try reuse with something the queue due fairness this wont work ',negative
'the following repeatedx values will set any the columns are repeating ',negative
'testlazyhbaseobject test for the lazyhbasexxx classes ',negative
'check the partitions exist the desttable ',negative
'extra value that can return while reading ahead ',negative
'this string constant will persisted metastore indicate whether corresponding ',negative
'tblpatterns ',negative
'mark the start the sync write sync update lastsyncpos ',negative
'perform major compaction there should extra base dir now ',negative
'defining partition names unsorted order ',negative
'find any referenced resources ',negative
'abstract method overridden for task execution ',negative
'contain the ',negative
'add keys this grouping set ',negative
'this map which vectorized row batch columns are the value columns ',negative
'simulate insert into partitions ',negative
'for last batch row group adjust the batch size ',negative
'yarntez job dont have the kerberos credentials anymore use the delegation token ',negative
'need loop here handle the case where consumer goes away ',negative
'that test doesnt block ',negative
'have deleterecordid currrecordidinbatch must now move find next the larger deleterecordid that can possibly match anything the batch ',negative
'',negative
'total blocks total elements the rowcontainer temporary file holding the spilled blocks ',negative
'value the configuration object ',negative
'the below group fields pools etc can only modified the master thread ',negative
'serdetype ',negative
'methods should really protected but some places have use this field ',negative
'the column map can not generated ',negative
'find the value matched column ',negative
'this used communicate over the not related tokens used talk llap daemons itself via the securit work ',negative
'note that recent metastore stores decimal string ',negative
'write out header for the payload ',negative
'timeseries query ',negative
'value columns ',negative
'ptf input that represents source the overall query this could table subquery ptf chain requires execution multiple ptf operators then the original invocation object decomposed into set component invocations every component invocation but the first one ends ptfqueryinputspec instance during the construction the operator plan ptfqueryinputspec object the chain implies connect the ptf operator the input has been generated far ',negative
'start hive server ',negative
'create mapworks and add them the sparkwork ',negative
'optional bool isguaranteed ',negative
'check noscan command ',negative
'not available nothing ',negative
'and use set remember which virtual columns were actually referenced ',negative
'schema evolution will insert the acid columns row schema for acid read ',negative
'verify that when have kerberos credentials pull the serialized token ',negative
'set output isrepeating true make sure gets overwritten similarly with nonulls ',negative
'foreigndbname ',negative
'same primitive category ',negative
'then need create metastore client that proxies that user ',negative
'user takes precendence over groups unless ordered explicitly ',negative
'optional optional optional ',negative
'equalscheck true and the inputoi the same the outputoi ',negative
'space usually ',negative
'since the udtf operator feeds into lvj operator that will rename all the internal names can just use field name from the udtfs the internal name ',negative
'validatecstr ',negative
'another quick path ',negative
'this pair defined the index build new mutation keyvalue cqrowkey cvcolumnvisibility value ',negative
'digit int all lowest decimal digit longword ',negative
'standard case ',negative
'pull the table schema out the split info ',negative
'despite string being primitive cant serialized binary ',negative
'reserve spaces for the byte size the list which integer and takes four bytes ',negative
'testing negative substring index start index should yield the last characters the string ',negative
'backwardforward compatible ',negative
'creating dummy table control the event truncate not ',negative
'ignore the preupgrade script errors ',negative
'over here should have some checks the deserialized object against the orginal object ',negative
'remove from src pool ',negative
'should error since exists ',negative
'for partitionless table initialize partvalue null ',negative
'insert some rows into table ',negative
'long they are still the same stream and are not already released ',negative
'invoke the outputformat entrypoint ',negative
'file name ',negative
'add added jars ',negative
'ideally want specify the different arguments updatelocation separate argnames however did that helpformatter swallows all but the last argument note that this know issue with the helpformatter class that has not been fixed specify all arguments with single argname workaround this helpformatter bug ',negative
'the context along ',negative
'case test with just originals single split strategy with two splits ',negative
'have found match insert this distinct clause head ',negative
'wait exception happens otherwise retry immediately ',negative
'for conditional task next task list should return the children tasks each task which contained the conditional task ',negative
'note the xff just way convert unsigned bytes signed integer ',negative
'read from hive test ',negative
'dont sync ',negative
'this duplicate invocation function dont add windowingspec ',negative
'objs ',negative
'used only for debugging testing purposes ',negative
'fields belong one the next entries ',negative
'repeating null ',negative
'the map should now empty ',negative
'nothing ',negative
'for each big tables bucket call the start forward ',negative
'normal case active session was removed from the pool session was restarted out bounds any userside handling should ignored ',negative
'txns table should have atleast one entry because just inserted the newly opened txns ',negative
'need set this only for replication tasks ',negative
'read count ',negative
'this will only get called once since compactrecordreader only returns one record the input split based the split were passed instantiate the real reader and then iterate until finishes since there way parametrize instance class ',negative
'merge join work ',negative
'set authorization mode ',negative
'fill forwardcache with skipvector ',negative
'begin conversion ',negative
'first take look any fieldschemas contain comma ',negative
'key ',negative
'make sure get correct number sessions each queue and that dont crash ',negative
'this path only potentially encountered during setup otherwise specific partxxxx file name generated and passed ',negative
'should not filter any object ',negative
'now tell launchmapper which files should add hadoopclasspath ',negative
'based the errormsg set hiveexception ',negative
'',negative
'data columns partition columns ',negative
'acquire txn batch ',negative
'all privilege expanded these not needed here ',negative
'hasnulls true then this array contains true the value null otherwise false the array always allocated batch can reused later and nulls added ',negative
'cache has found old buffer for the key and put into array instead our new one ',negative
'extract the record type ',negative
'initialize all children first ',negative
'this either the first batch weve used the current batch buffer ',negative
'register running task into the runningtasks structure ',negative
'normal close ',negative
'nonjavadoc see ',negative
'decided treat this collection regular object ',negative
'clear the other ones ',negative
'named url the ',negative
'cannot abandon the attempt here the concurrent operations might have released all the buffer comprising our buddy block necessitating merge into higher list that may deadlock with another thread locking its own victims one can only take list locks separately moving down the alternative would release the free list lock before reserving however iterating the list that way difficult wed have keep track things the main path avoid retrying the same headers repeatedly wed rather keep track extra things failure ',negative
'metastore schema version different than hive distribution needs ',negative
'copied from errormsgjava ',negative
'mappings ',negative
'check stats are same need update ',negative
'task data structures have been initialized ',negative
'read via object store ',negative
'list cvalue list rowvalues assertequals cvaluesize ',negative
'invalid expression throw some exception but not incompatible metastore ',negative
'different sign just add the absolute values ',negative
'this set ',negative
'all good combine the baseoriginal only etl strategies ',negative
'partitions not match currently not merge ',negative
'split into digit middle and lowest longwords hand ',negative
'optional string appid ',negative
'perform minor compaction again this time will remove the subdir for aborted transaction ',negative
'need this for jackson work ',negative
'used for readfields ',negative
'create the mapping for this column with configured encoding ',negative
'optimize local fetch does not work with llap due different local directories used containers and llap ',negative
'drop table ',negative
'initiate cancel request cancel the thread execution and interrupt the thread thread interruption not handled jobexecutecallable then thread may continue running completion the cancel call may fail for some scenarios that case retry the cancel call until returns true max retry count reached param future future object which has handle cancel the thread ',negative
'',negative
'test that returns emptyset the filter selects partitions ',negative
'move clock backwards that allocation after allocation ',negative
'the partitions are unknown the call says due the expression evaluator returning null for partition sent partial expression ',negative
'check any operator had fatal error early exit during execution ',negative
'not cache this its child rdd intend cached ',negative
'get the aggregate function matching the name the query ',negative
'register that have visited this operator this rule ',negative
'only the hive catalog should cached ',negative
'store the config system properties ',negative
'call getvarcharmaxlength every deserialize call ',negative
'create the default white list from list safe config params and regex list ',negative
'stats values for col ',negative
'nested complex types trigger kryo issue plan deserialization ',negative
'the offset the first input does not need change ',negative
'partial spec ',negative
'number aliases ',negative
'this avoids extra serialization deserialization these objects ',negative
'poolpath ',negative
'the key portion the entry will the internal column name for the join key expression ',negative
'kill server ',negative
'its leaf add the move task child ',negative
'undone ',negative
'invoked for test methods ',negative
'nonjavadoc see ',negative
'schema ',negative
'encountered partitioning column this will better handled metadataonly optimizer ',negative
'table may not found when materialization cte ',negative
'then lets check the one know about ',negative
'column type not specified use string ',negative
'returns the node currently the top the stack ',negative
'job request executor list job status requests ',negative
'stats values for col ',negative
'test mixed case ',negative
'set hashtable memory usage ',negative
'remove the locks didnt see dont look for them again next time ',negative
'now get all the onetomany things start with partitions ',negative
'are good since subquery top level expression ',negative
'get the following out the way when you start the session these take ',negative
'try read the dropped tbl via cachedstore should throw exception ',negative
'its not some operator pass back ',negative
'has been removed does not have child for instance semijoin can quickly skip this one ',negative
'wont metastoreside ppd for the things have locally ',negative
'sanity check that for map got encodings ',negative
'null means cannot wrap the cause logged inside ',negative
'see method comment ',negative
'invalid merge smaller register merge bigger ',negative
'alpha order please ',negative
'everything comes from cache ',negative
'this mapper class used for merge file work ',negative
'blocks until the async query complete ',negative
'see also the usage for binary for now only want text ',negative
'disabling rewriting removing from cache ',negative
'the column must aggregate column inserted gby dont have account for this column when computing product ndvs ',negative
'choose cumulative ',negative
'export works file level you have copyn the table dir youll have those output ',negative
'change the filter condition into join condition ',negative
'can only happen theres evictor thread interrupted ',negative
'set null information the small table results area ',negative
'multiply make room for sign bit ',negative
'there maybe more than splits the group however they all have unique path assert that ',negative
'check individual elements subrecord ',negative
'process the batch ',negative
'left repeats ',negative
'check the contents the file ',negative
'now hivedecimal ',negative
'start offset each field ',negative
'string object ',negative
'inject behavior where repl load failed when try load table and partition ',negative
'grammar prohibits more than column are guaranteed have only element this lists ',negative
'the node cannot accept task the moment ',negative
'',negative
'most likely this means its temp table ',negative
'set double data vector array entries for null elements the correct value unlike other colscalar operations this one doesnt benefit from carrying over nan values from the input array ',negative
'field ',negative
'theres lock manager essentially means didnt acquire locks the first place thus need release locks ',negative
'for now not limit this one per split ',negative
'detect there are multiple attributes join key ',negative
'not call mqgetrowcountjoin will trigger ',negative
'used kyro ',negative
'there was preexisting work generated for the bigtable mapjoin side need hook the work generated for the associated with the rsmj pattern with the preexisting work otherwise need associate that the mapjoin linked the work associated with the rsmj pattern ',negative
'recursively remove any its parent who only have this child ',negative
'first remove all the grants ',negative
'lets make sure only read the relevant part the writable case reuse ',negative
'load them into setlong ',negative
'todo cache the information from the metastore ',negative
'pass removing invalid candidates misses far exceed max tolerable misses ',negative
'note the null array indexed keyindex which not available internally the mapping from long double etc index key index kept once the separate vectorcolumnsetinfo object ',negative
'mrinput not interest since itll always ready ',negative
'out the request provided that its signed ',negative
'rule searching for dynamic pruning expr theres least expression wrapping ',negative
'expecting not change the size internal structures ',negative
'adds the taskid the fspkey ',negative
'check bucketing both was done the same way ',negative
'bare ',negative
'the right input correlator should produce correlated variables ',negative
'letter byte latin capital with grave bytes ',negative
'throw new null fschema fschema pigschema tableschema pigschema tableschema ',negative
'total characters byte length ',negative
'ensure counters are set when data has actually been read ',negative
'this here normally communicator does this ',negative
'follow the reducesink operator upstream which small table side ',negative
'try the extremes precision and scale ',negative
'parse and initialize the hbase columns mapping ',negative
'filters for pushing ',negative
'needed columns ',negative
'the name the functiontable ',negative
'set register value and compute inverse pow for register value ',negative
'bgenjjtree senum ',negative
'note setting these separately very hairy issue certain combinations since cannot decide what type table this becomes without taking both into account and many cases the conversion might illegal the only thing allow true txprops for backward compat ',negative
'sum small tables size this join exceeds configured limit hence cannot convert ',negative
'partnames ',negative
'the object does not exist want add ',negative
'total characters byte length ',negative
'lookup the specified type and set this nodes type precludes forward and self references for now ',negative
'retain the original join desc the map join ',negative
'currently not support ptf operator ',negative
'get list selected column ids ',negative
'not supported ',negative
'this method assumes that the list has null entries that enforced elsewhere the vectorizer class null passed list entry behavior not defined the future null values are allowed the list sure handle valued logic correctly not col null should considered unknown that would become false the where clause and cause the row question filtered out see the discussion jira hive ',negative
'',negative
'round even ',negative
'parse the first byte vintvlong determine the number bytes ',negative
'sql comment prefix beeline also supports shellstyle prefix ',negative
'write datum out stream ',negative
'this optimize queries the form select countdistinct key from where sorted and bucketized key partial aggregation performed the mapper and the reducer gets row partial result per mapper ',negative
'',negative
'what position the mapjoin the different parent work items will have ',negative
'hopefully this will helpful case npes ',negative
'partialsum ',negative
'start concurrent testing ',negative
'bgenjjtree definitiontype ',negative
'merge nomatchs and match selected ',negative
'not ',negative
'expecting only single instance task running ',negative
'both categories are primitive return the comparison type names ',negative
'null out some row column entries undone ',negative
'should this group converted mapside group because the grouping keys for ',negative
'the objectinspector for array and map expects extra layer ',negative
'cancolumnstatsmerge guarantees that accurate before merge ',negative
'make comparisons work properly the factor gets the decimals sign too ',negative
'nonjavadoc see ',negative
'shortminvalue ',negative
'now have base file ',negative
'key databasetablespdplb hive store lowercase table name metastore and counters character case sensitive ',negative
'make copy currentmetavars there race condition that currentmetavars might changed during the execution the method ',negative
'write second nonnull element ',negative
'stats from reader ',negative
'catalog ',negative
'parentdbname ',negative
'ssl enabled override the given value hadooprpcprotection and set authentication this disables any encryption provided sasl since ssl already provides ',negative
'can immediately reuse session theres nothing wait for just return ',negative
'get column statistics for all output columns ',negative
'test drop view ',negative
'grouping sets need transform them into immutablebitset objects for calcite ',negative
'try temporarily adding the parent ',negative
'will call abort ',negative
'create the directory ',negative
'field ',negative
'parent keys are null empty bail out ',negative
'grantortype ',negative
'add row chain except case unb preceding only firstval needs tracked ',negative
'forward arg forward arg workdir llapdaemonsite llapdaemonsite forward via configjson forward arg used localize jars used localize jars used localize jars forward via configjson llapdaemonsite relevant parameter forward arg forward arg forward via configjson llapdaemonsite ',negative
'type name should already set subclass ',negative
'mysql can use intn ',negative
'nway first small table ',negative
'field ',negative
'only returns subset per call ',negative
'for computing the autogenerated field ids thrift ',negative
'todo later may have map ',negative
'containers are not being tracked for reuse this safe ignore since deallocate task will come ',negative
'field ',negative
'hive move this fallback logic cliconfigs ',negative
'enable cache and use default strategy ',negative
'calcite literal millis convert seconds ',negative
'the order the fields the lazybinary small table value must used ',negative
'data structures coming from qbjointree ',negative
'get appropriate object from the string representation the value ',negative
'did not see skew key this table continue next table are trying avoid extra call filesystemexists ',negative
'single long value hash map based the serialize the long key into binarysortable format into output buffer accepted ',negative
'insert reduceside ',negative
'dimension ',negative
'the schemaevolution class has added the acid metadata columns lets update our readertypes ppd code will work correctly ',negative
'for there rankdenserank function there are unpushedpred the form rnkvalue constant then use the smallest constant val the ranklimit the windowingtablfn there are wdw fns with end boundary past the current row the condition can pushed down limit nonjavadoc see javautilstack javalangobject ',negative
'optional int ',negative
'prepends partition spec input path candidate file name ',negative
'setup tablescan ',negative
'add the current constant struct the right hand side the clause ',negative
'define the expected schema ',negative
'count number true values seen far ',negative
'construct case expression handle the null indicator this also covers the case where left correlated subquery projects fields from outer relation since loj cannot produce nulls the lhs the projection now need make nullable lhs reference using nullability indicator this this indicator null means the subquery does not produce any value result any rhs ref this usbquery needs produce null value ',negative
'there need add colname again otherwise will get duplicate colnames ',negative
'this has called before initializing the instance hmshandler using the hook startup ensures that the hook always has priority over settings xml the thread local conf needs used because this point ',negative
'required input format ',negative
'corresponding struct fields the same time ',negative
'end udfrowsequencejava ',negative
'map priv being granted required privileges ',negative
'this filter has correlated reference create value generator ',negative
'nothing ',negative
'add whether the row filtered not ',negative
'bail there are any changes note that dont care about aba here all the stuff the left has been taken out already noone can touch and all the stuff the right yet seen dont care they changed with this its the same free list the processing sequence will remain the same going right ',negative
'there are more active client sessions stop the server ',negative
'destination memory this the only partition memory proceed without check destination partition being empty indicates write buffer will allocated thus need check memory full check periodically ',negative
'the url does not have database name add the trailing ',negative
'testspecific delay just before the check happens ',negative
'tabletypes ',negative
'nonjavadoc see javalangstring ',negative
'fetch all the hints ',negative
'set alias fetch work ',negative
'dynamic partition replace input path root paths with dynamic partition ',negative
'dont bother validating ',negative
'create list top nodes ',negative
'check the context matches ',negative
'user privileges testdb testtable testtable testtable testtable testdb ',negative
'make sure minihs closes all its connections ',negative
'already copied successfully ignore ',negative
'string allaliases ',negative
'all stats variables are visible for testing ',negative
'return the errors that occur the most frequently ',negative
'target compression block the middle the range slice the range two ',negative
'limit length chars ',negative
'need wait the last iteration ',negative
'map keep track which child generated with work ',negative
'anythingelse foo ',negative
'show tables should faster than that ',negative
'update with new values ',negative
'this task was added preemption list remove ',negative
'varchar not between ',negative
'return single arraylist where the first element the number bins bins and subsequent elements represent bins pairs ',negative
'set the results ',negative
'are not allowed lose digits multiply compatible with oldhivedecimal behavior overflow consider does make sense restrictive just did repeated addition would succeed ',negative
'forwarded the batch this method ',negative
'test serialization ',negative
'handle both file and jarurlentry the case shaded hive libs ',negative
'and the operator dot then its table column reference ',negative
'max disabled can safely return true ',negative
'append new config params whitelist ',negative
'happens when unexpected failure occurs getattribute for example ',negative
'cvalue list rowvalues assertequals cvaluesize listval list cvalueget assertequals listvalsize mapval map listvalget assertequals mapvalsize listvalget listval list cvalueget mapval map listvalget assertequals mapvalsize assertequalsb mapvalgeta assertequalsd mapvalgetc listvalget ',negative
'does not need actual time just nonzero distinct value test against ',negative
'both blockedby are either there not ',negative
'dynamic part vals specified ',negative
'get the app report from yarn ',negative
'the case where agg countcorvar changed countnullindicator note any nonnullable field from the rhs can used the indicator however true field added the projection list from the rhs for simplicity avoid searching for nonnull fields projecta all gby keys rewritten nullable projexpr aggregate groupbyall left input refs countnullindicator other aggs projectb all left input refs plus the rewritten original projected exprs joinreplace corvar input ref from leftinputrel leftinputrel project everything from rightinputrel plus the nullindicator true rightinputrel ',negative
'lastheartbeattime ',negative
'must produce the same result murmurhashhash with seed ',negative
'the first group ',negative
'insert creates copy ',negative
'instantiate the metastore server handler directly instead connecting through the network ',negative
'recursive ',negative
'have finished tree walking correlation detection will first see need abort the operator tree has not been changed ',negative
'the caller probably created the new session with the old config but update the ',negative
'authorize against the table operation that location permissions can checked any ',negative
'path path throws ioexception ',negative
'this needed prevent the hikaridatasource from trying connect the ',negative
'set data location and input format must text ',negative
'pending change boolean ',negative
'convert the work containing sortmerge join into work had regular join note that the operator tree not changed still contains the smb join but the plan changed aliastowork etc contain all the paths was regular join ',negative
'bucket ',negative
'need discard the buffer cannot lock eviction takes care that ',negative
'increment the min txn that heartbeat thread will heartbeat only from the next open transaction the current transaction going committed fail dont need heartbeat for current transaction ',negative
'data left current page load data from new page ',negative
'object equality issame means that the objects are semantically equal ',negative
'currentinputfile will updated only inputfilechanged inputfilechanged not called throughout the operator tree currentinputpath wont used anyways ',negative
'destination partition any the intermediate destination directory the final destination directory ',negative
'the left child not join multijoin operator ',negative
'must read through fields not want ',negative
'ensure partinfos tableinfo initialized ',negative
'create subquery ',negative
'returns true statement represented line not complete and needs additional reading from console used handlemultilinecmd method ',negative
'can only start once fragment has completed the map should clear this point ',negative
'todo use for now till dynamicserde ready ',negative
'same responsibility initializeoi but for the rawinput ',negative
'hive depends filesplits ',negative
'important note for multiand the class will catch cases with more parameters ',negative
'initialization isnt finished until all parents all operators are initialized for broadcast joins that means initializing the dummy parent operators well ',negative
'mapjoin later ',negative
'newstate columnstatsstate complete partial none complete complete partial partial partial partial partial partial none complete partial none ',negative
'case theres delay for the heartbeat but the delay within the reapers tolerance then txt should able commit ',negative
'the number data columns that the current reader will return only applicable for vectorrow deserialization ',negative
'initialize with data type conversion parameters ',negative
'validation required ',negative
'trigger scheduling since new node became available ',negative
'there nothing change ',negative
'invalid values ',negative
'devenagari sign virama bytes ',negative
'compare the mapper get offset method the list mappings ',negative
'store the previous value for the path specification ',negative
'all good such partition exists move ',negative
'posmap unfortunate consequence batchingiterating thru results ',negative
'case viewfs need lookup where the actual file know the filesystem use resolvepath sure shot way knowing which file system the file ',negative
'construct setop rel ',negative
'localtmppath the root all the stats under there will selstatsfiles selstatsfiles etc where sel and sel are the ids ',negative
'apply schema evolution adding some columns ',negative
'more data ',negative
'roleprivileges ',negative
'format clustered statement ',negative
'drop one function see what remains ',negative
'compute seconds first subtract the milliseconds stored the nanos field the timestamp from the result gettime ',negative
'avoid allocating temporary variables for special cases signum scale zero ',negative
'dont expect missing buckets from mere actually there should buckets just pass null bucketing context union suffix should also accounted for ',negative
'optional queryidentifier ',negative
'tablesused ',negative
'there should one argument that array struct ',negative
'the join filters out the nulls its there are ',negative
'convert seconds milliseconds ',negative
'native vectorization not supported ',negative
'loop through all the inputs determine the appropriate return typelength return type all char inputs return char all varchar inputs return varchar all charvarchar inputs return varchar all binary inputs return binary otherwise return string ',negative
'error checking later and detect just dot ',negative
'explicitly close zkdatabase since zookeeperserver does not close them ',negative
'required for insertion into treemap ',negative
'',negative
'there can never more concurrent takers than uncommitted ones ',negative
'also add settings clusterspecificconf make sure these get picked whoever started this ',negative
'any preexisting datanucleus property should passed along ',negative
'struct value simply list values the schema can used map the field name the position the list ',negative
'intwritable can just sum the reduce ',negative
'walk over the input row resolver and copy the output ',negative
'tokresourceuri restype respath ',negative
'hive command operations ends here ',negative
'can pretty sure that entire line can processed single command since always add line separator the end while calling ',negative
'prevent construction outside the get method ',negative
'note the sync marker seen the header ',negative
'data was moved from original location cache directory need move back ',negative
'now add the tables and columns from the current connection ',negative
'not block each other since they are part the same txn ',negative
'there current unread chunk read from that else get the next chunk ',negative
'deserialize split ',negative
'multiple means lookup ',negative
'make sure there subquery top level expression ',negative
'use existing location ',negative
'this where set the sort columns that will using for keyvalueinputmerge ',negative
'cache helper data for deserialization could avoid having ',negative
'whatever have ',negative
'final long roundmultiplyfactor absroundpower ',negative
'reset data container prevent being added again ',negative
'not lot you can here ',negative
'are dealing with bag tuple column need worry about subschema ',negative
'are forcing the usage vectorudfadaptor for test purposes ',negative
'update field collations ',negative
'regex the form column name following characters are not allowed column name ',negative
'for nonvectorized operator case wrap the reader possible ',negative
'string storage type overrides table level default binary storage ',negative
'alas crossed some dst boundary the time day doesnt matter the caller well ',negative
'nulls ',negative
'clear jointree and parse context ',negative
'might generate select operator top the join operator for semijoin ',negative
'serialize path offset length using filesplit ',negative
'big case write the length vint and then the value bytes ',negative
'lets validate that the serde exists ',negative
'find the immediate parent possible for for query like select from where implies depends parent would not check last alias the array for parent can not itself ',negative
'note tablealias must valid nonambiguous table alias because weve checked that toktableorcols process method ',negative
'compute the keys ',negative
'cant use toarray here because java dumb when comes generics arrays ',negative
'initialize using data type names projection the column range typessize ',negative
'user specified row limit set the query ',negative
'whether this vertex dummy which does not really exists but created ',negative
'the set join operators which can converted bucketed map join ',negative
'simulate update partitions depending causeconflict choose one the partitions ',negative
'replace column references checkexprast with corresponding columns input ',negative
'utc has such adjustment ',negative
'the first input correlator always the rel defining ',negative
'tests not setting maxrows return all tests setting maxrows return all ',negative
'nothing bail out ',negative
'nothing there operator tree associated with sourcealias source there not operator tree associated with targetalias target ',negative
'events for the source colums tasks ',negative
'one the params null then expected null ',negative
'can have multiple branches due dpp semijoin opt ',negative
'throw exception the user trying truncate column which doesnt exist ',negative
'use the object already have ',negative
'this new key keep writing the first record ',negative
'since this inside delta dir created hive earlier can only contain bucketx bucketxflushlength ',negative
'add the archive file distributed cache ',negative
'new table ',negative
'intermediate key anything between key and the following ',negative
'user specified fraction always takes precedence ',negative
'clone the column stats and return ',negative
'replacement allowed the existing table older than event ',negative
'now add the projuniquekeyset the child keys that are fully projected ',negative
'the gby key constant ',negative
'limit the number threads that can launched ',negative
'all bigtable input columns key expressions are isrepeating then ',negative
'substitution option define ',negative
'make sure metastore doesnt mess with our bogus stats updates ',negative
'enforce minimum precision factor ',negative
'just examine the lower word ',negative
'position the single native vector map join small table ',negative
'test that database and table dont coalesce ',negative
'not generated cookie continue ',negative
'serialize the value ',negative
'urldecoder misnamed class since actually decodes xwwwformurlencoded mime type rather than actual url encoding which the file path has therefore would decode which incorrect spaces are actually either unencoded encoded replace first that they are kept sacred during the decoding process ',negative
'granularity partition column ',negative
'bucket should small and bucket should large make sure thats the case ',negative
'list leaves that werent under and expressions ',negative
'note due tez the session may actually invalid case some errors currently reopen attempted reuse will take care that cannot tell the session usable until try return this the pool even its unusable reopen supposed handle this ',negative
'udf which sleeps for simulate long running query ',negative
'pass ',negative
'just return ',negative
'update the state removedfromlist that parallel notifyunlock doesnt modify ',negative
'need input object inspector that for the row will extract out the vectorized row batch not for example original inspector for orc table etc ',negative
'since subtraction not commutative can must subtract the order passed ',negative
'set tez execution summary false ',negative
'length file count directory count ',negative
'the column partition column skip the optimization ',negative
'rights signum wins notice the negation because are subtracting right ',negative
'failover workeridentity ',negative
'create tables ',negative
'best effort ',negative
'skip the counting the values are the same for windowing countdistinct case ',negative
'there only one blank utf ',negative
'get the tables for the desired pattern populate the output stream ',negative
'this may change after every setmapjoinkey call ',negative
'return static variable with results set some set values ',negative
'optional int physicaledgecount ',negative
'verify mergeandmovetask not optimized ',negative
'this was added during plan generation ',negative
'group distribute sort cluster ',negative
'race protection ',negative
'ready for cleaning state this case ',negative
'simple trims ',negative
'not split ',negative
'the last key column the dummy grouping set figure out which scratch column was used can overwrite the dummy ',negative
'nonencrypted path equals strength ',negative
'script file ',negative
'ignore any predicates partition columns because have already accounted for these the table row count ',negative
'thread name for reporter thread ',negative
'not overwrite there are duplicate keys ',negative
'partition keys can not set but added one remove for ',negative
'note this may just block wait for session based parallelism ',negative
'nonjavadoc see ',negative
'because append the prefix when serializing the column ',negative
'stats values for col ',negative
'set the client ',negative
'stats the big input ',negative
'noop for default output file ',negative
'generate reducesink operator for the join ',negative
'enables orc ppd create delta should push predicate here ',negative
'string chars long byte and byte char ',negative
'merge stats from cache with metastore cache ',negative
'unique keys for this test ',negative
'protected boolean useminmax ',negative
'',negative
'remove the candidate filter ops ',negative
'dictates which operator allowed ',negative
'buffer already allocated keep using dont reallocate ',negative
'finally put the ranges list for future use shared between rgs ',negative
'isatty system call will return the file descriptor terminal else ',negative
'add shutdown hook flush the history history file and also close all open connections ',negative
'get the actual converted schema ',negative
'test failed ',negative
'fastisshort returns false ',negative
'dynamic value which will determined during query runtime ',negative
'nonjavadoc see ',negative
'this should not vectorize all ',negative
'nothing localize ',negative
'restore the hashmap from disk deserializing currently kryo used for this purpose ',negative
'has nulls repeating ',negative
'ignore the keys which are local source warehouse ',negative
'check grpset require additional job ',negative
'char ',negative
'rows ',negative
'the configured owner does not own the file throw ',negative
'optional int vertexindex ',negative
'called set the appropriate input format for tasks ',negative
'grouping sets are not allowed this restriction can lifted future ',negative
'index fetchoperator which providing smallest one ',negative
'for longs and java overheads semiarbitrary ',negative
'the child and operator extract its children ',negative
'multikey value hash map optimized for vector map join the key uninterpreted bytes ',negative
'modify sourcetable ',negative
'partition droppped after repl dump ',negative
'conversion needed and not variablelength argument just return what passed ',negative
'ctimestamp cdecimal cbinary cdate cvarchar cchar cbinary ',negative
'setup web ',negative
'decimal string formatting ',negative
'case max list members max query string length ',negative
'notify tests and global async ops ',negative
'only allow integer index for now ',negative
'test that values that know are missing are shown absent ',negative
'reversedmemorymb set make memory allocation fraction adjustment needed ',negative
'change query fetchtask use new location specified results cache ',negative
'ast expression not valid column name for table ',negative
'only need run the logic for tables missed ',negative
'initialize configuration ',negative
'nothing check ',negative
'register the cacheaware path that parquet reader would thru ',negative
'the following tests use serialized asts that generated using hive from branch ',negative
'test that exclusive locks coalesce one ',negative
'reset the bean ',negative
'ends character this means they appended time indicator ',negative
'running normal async query with exceptionsthen need close ophandle ',negative
'roottasks the entry point for all generated tasks ',negative
'called map operator propagated recursively single parented descendants ',negative
'someone already allocated this arena just the usual thing ',negative
'value ',negative
'otherwise take the child itself ',negative
'nonjavadoc see javasqlrowid ',negative
'vectorized expression that dont expect will called due shortcircuit evaluation ',negative
'another query referring that property with the conf overlay should fail ',negative
'fallthrough ',negative
'this check necessary because for spark branch the result array from getinputpaths above could empty and therefore numthreads could ',negative
'delete the parent node all the children have been deleted ',negative
'need for grouping and the target tasks this code path should never triggered the moment grouping disabled dagutils uses ',negative
'the list has invalid port update with valid port ',negative
'the handling results ',negative
'get splits ',negative
'int ',negative
'bad value type ',negative
'since hashcode not used just put arbitrary number ',negative
'udfoppositive noop however still create and then remove here make sure only allow ',negative
'check that constraints have catalog name properly set first ',negative
'evaluate else expression only and copy all its results second input parameter but column ',negative
'confvar overridden hivesitexml ',negative
'oldinput has the original group keys the front ',negative
'dont increment the reader count for explain queries ',negative
'position the biggest small table ',negative
'not invert between result add column expression here ',negative
'compute required mapping ',negative
'the above example and are simple trees ',negative
'',negative
'unlock database operation release the lock explicitly the operation itself dont need locked set the writeentity writetype ddlnolock here otherwise will conflict with hives transaction ',negative
'cannot restart place because the user might receive failure and return the session the master thread without the irrelevant flag set fact the query might have succeeded the gap and the session might already returned queue restart thru ',negative
'note the following section metadataonly import handling logic interleaved with regular replimport logic the rule thumb being followed here that mdonly imports are essentially alters they not load data and should not creating any metadata they should replacing instead the only place makes sense for mdonly import create the case table thats been dropped and recreated the case unpartitioned table all other cases should behave like noop pure alter ',negative
'blockingqueue methods ',negative
'char ',negative
'filtermode condition always true always false ',negative
'return random number with length digits string results may negative positive ',negative
'nonnative vector map differences for left outer join and filtered ',negative
'vectorized class available problem try use the vectorudfadaptor when configured note assume has not been set because are executing test that didnt create hiveconf etc usage vectorudfadaptor that case ',negative
'create server that doesnt interpret any kerberos stuff ',negative
'maybe will kill the condition ',negative
'lock ',negative
'find available privileges ',negative
'rootoperators are all the table scan operators sequence traversal ',negative
'the move hasnt been made already ',negative
'debugtest related methods ',negative
'create more staging data with copyn files and ldoverwrite ',negative
'dont wait for the cluster not started this besteffort ',negative
'end multijoinjava ',negative
'test empty database ',negative
'case was done and noone looked ',negative
'order columns are used key columns for constructing the reducesinkoperator since not explicitly add these outputcolumnnames need set includekeycols false while creating the reducesinkdesc ',negative
'this cover the case where hive table may have mapkey value but the data file type arraystructvalue value using index place type name ',negative
'must some unix variant ',negative
'set the offset and length for the two elements ',negative
'when deleterecordid currrecordidinbatch this record the batch has been deleted ',negative
'test parent references from statement ',negative
'remember all threads that were running the time started line processing hook the custom ctrlc handler while processing this line ',negative
'this will populated case dynamic partitioning and list bucketing ',negative
'dummy mapping used for all and table name mappings ',negative
'create syntax tree for function call testudfcol col col ',negative
'determine recursively the ptf lead lag function being used expression ',negative
'have come this far either the previous commands are all successful this command line either case this counts successful run ',negative
'untyped nulls ',negative
'add partition cols necessary see for details ',negative
'combine ',negative
'check roundups before settings values result ',negative
'hive pending refactor push file forward ',negative
'get the rootlogger which you dont have logjtestproperties defined will only log errors ',negative
'initialize one columns target related arrays ',negative
'create the queryid appender for the queryid route ',negative
'map ',negative
'since same thread creates metastore client for streaming connection thread and heartbeat thread explicitly disable metastore client cache ',negative
'should fail because the ',negative
'set during the init phase hiveserver auth mode kerberos ',negative
'since this terminal operator update counters explicitly forward not called ',negative
'max fraction errors allowed throw error only after this many errors ',negative
'tez processor needs configure object registry first ',negative
'capture arguments static ',negative
'return value constant case arg constant ',negative
'beeline mode need hook use connect case the showdbinprompt set the database name needed ',negative
'serialize using another serde and read out that object repr ',negative
'there must least one column vector ',negative
'trim the size needed ',negative
'create all children ',negative
'order and null order ',negative
'maxcreatetime ',negative
'verify result rounded digits ',negative
'puts int little endian order ',negative
'add new rel its the maps ',negative
'query acquires the lock and takes secs compile ',negative
'the operator tree till the sink operator needs processed while fetching the next row fetch from the priority queue possibly containing multiple files the small table given file the big table the remaining tree will processed while processing the join ',negative
'for now limit the data types support for vectorized struct ',negative
'that ',negative
'only create the movework for nonmm table action needed for table ',negative
'',negative
'probably view ',negative
'make sure create table fails ',negative
'initializes current key ',negative
'well pass threadlocals the background thread from the foreground handler thread threadlocal hive object needs set background thread the metastore client hive associated with right user current ugi will get used metastore when metastore embedded mode ',negative
'testparam yellow ',negative
'the bucket task map should have been setup the big table ',negative
'add value chain not null skipnulls false ',negative
'known not have any nulls ',negative
'assertequalso struct cannot this because types null lists are wrong ',negative
'add all unique positions referenced ',negative
'inspect the output type each key expression ',negative
'insert two rows table ',negative
'should here ',negative
'todo handle exclusion list figures out which tables make acid and optionally performs the operation ',negative
'longest run trailing zeroes ',negative
'update included with the submit request callback via notifystarted ',negative
'fields ',negative
'only run for milliseconds ',negative
'first argument charcount which consumed this method below ',negative
'invalidate all cache entries using this table ',negative
'verify record was written correctly parquet ',negative
'check metric value ',negative
'expect have happened now since hivetxntimeoutsec ',negative
'before anyone else accesses would have been allocated and decompressed locally ',negative
'todo this depends tez creating separate threads does now that changes some other way propagatefind out attempt would needed see tez ',negative
'cant use the cached table because has spilled ',negative
'add tables outputs ',negative
'this operator then need call the plan generation the ',negative
'runtimegetmax gives very different number from the actual xmx sizing you can iterate through the from javalangmanagement figure this out but the hardcoded params the llap runsh result usable heap xxnewratio survivor region which technically not the usable space ',negative
'custom pattern set case dynamic partitioning configure custom path ',negative
'resultcast cleanup vectorexprargtype ',negative
'wait for minute and check again ',negative
'metastorehome set use else use hivehome for backwards compatibility ',negative
'relativepath ',negative
'this method generates the map bucket file splits ',negative
'read the newly added partition via cachedstore ',negative
'loop get all task completion events because ',negative
'from the hive logshivelog can also check for the info statement fgrep total tasks location hivelog each line indicates one run loadtask ',negative
'dshr ',negative
'the connection was closed create new one ',negative
'handle table properties ',negative
'this method takes something like string only accepts something like string ',negative
'the separators array whether need escape the data when writing out which char use the escape char which chars need escaped ',negative
'keep track view alias view name and read entity for for query like select from where keeps track full view name and read entity corresponding alias vvv ',negative
'are not always create new client for now ',negative
'this for artificially added tokens ',negative
'production field ',negative
'create output directory not already created ',negative
'might under the hive name ',negative
'hadoop acls not work with localfilesystem set minidfs ',negative
'isnt error the following returns rows the local workers could have died with nothing assigned them ',negative
'for numeric types ',negative
'get value element information ',negative
'stop hiveserver increase header size ',negative
'notice the default value for llapioenabled overridden whether are executing under llap ',negative
'create fetchwork for non partitioned table ',negative
'reduce sink operator the defacto operator for determining keycols emit keys map phase ',negative
'spot check decimal columncolumn multiply ',negative
'all columns cluster and sort are valid columns ',negative
'issuing query for all partitions verify that need update the same columns ',negative
'remember the mapping case scan another branch the ',negative
'start with size and double when needed ',negative
'test downcasting when greater than ',negative
'the function name ',negative
'write bytes bos ',negative
'operand ',negative
'format always madvise never ',negative
'set true only when deregistration happens web ',negative
'may the getting created this load ',negative
'threads while the constructor running ',negative
'parenthesis the end ',negative
'drop table tbl via objectstore ',negative
'blank byte ',negative
'the grouping set has not yet been processed create new grouping key consider the query select count from group with cube where being executed mapreduce jobs the plan for tablescan groupby reducesink groupby filesink groupbyreducesink worked grouping sets were not present this function called for groupby create new rows for grouping sets for each input row rows are created for the example above anull null null null ',negative
'partnames tabparttabpart ',negative
'extract configs for processing the python fragments yarn service ',negative
'work should the smallest unit for memory allocation ',negative
'that know which version wrote the file ',negative
'add hive keywords including lowercased versions ',negative
'have found the colname need search more exprnodes ',negative
'assumed islazy flag set only for repl load flow import always deep copy distcpdoasuser will null default replcopywork ',negative
'set the appropriate key the map and test that are able read back correctly ',negative
'recompose filter possibly pulling out common elements from dnf expressions ',negative
'inline merge join operator selfjoin ',negative
'',negative
'skewed column names ',negative
'this point havent found screw dont know where ',negative
'alias key mapping ',negative
'unsigned max ',negative
'job state job request changes the state are synchronized using setstateandresult this required due two different threads main thread and job execute thread tries change state and organize clean tasks ',negative
'decimal point ',negative
'perform some updatedelete ',negative
'finally put uncompressed data cache ',negative
'object inspector hasnt been cached for this typeparams yet create now ',negative
'print dependent vertexs ',negative
'insiderview will tell this tablescan inside view not ',negative
'preallocated members for storing information equal key series for smalltable matches index into the hashmapresults array for the match allmatchindices logical indices into allmatchs the first row match possible series duplicate keys issinglevalue whether there multiple small table values duplicatecounts the duplicate count for each matched key ',negative
'checksum does not match likely the file changedremoved retry from path ',negative
'safety check for postconditions ',negative
'required required optional optional required required optional required optional required required optional optional optional optional optional ',negative
'this will throw nosuchlockexception even though its the transaction weve closed because that will have deleted the lock ',negative
'doesnt clear underlying hashtable ',negative
'simulate emitting all records ',negative
'initialize the integer values ',negative
'parenttablename ',negative
'check whether are replicating ',negative
'this checked expression use different template for checked expressions ',negative
'combinedsplit ',negative
'there are new segments can just bail out ',negative
'suppress headers and all objects below ',negative
'process join filters ',negative
'the lock for updates ',negative
'this solely for testing checks the test has set the looped value false and remembers that and then sets true the end have check here ',negative
'replace original varsampx with sumx sumx sumx countx case countx when then null else countx end ',negative
'delete from tab txn ',negative
'this the last for which this buffer will used remove the initial refcount ',negative
'one the tables that not memory ',negative
'should not happened ignore remaining ',negative
'dont want attempt grab more memory than have available percentage bit arbitrary ',negative
'could the minuncommittedtxnid lesser than nexttxnidntxnnext ',negative
'then throw exception ',negative
'this not bulletproof but should allow close session most cases ',negative
'create random test string ',negative
'arithmetic operations reset the results ',negative
'set the session for driver ',negative
'create the corresponding hive expression filter partition columns ',negative
'bit packing disabled ',negative
'single quote seen and the index not inside double quoted string and the previous character was not escape then update the flag ',negative
'compatible mapwork ',negative
'top will add ',negative
'use deep hashcode for arrays ',negative
'nothing are not running local mode only submitting the job via child process this case its appropriate that the child jvm use the same memory the parent jvm ',negative
'introducing explicit aliases for tbl ',negative
'',negative
'check all parent statistics are available ',negative
'environment variables short values ',negative
'mergeable next loop iteration ',negative
'mark fragment completing but dont actually complete yet the wait queue should now have capacity accept one more fragment ',negative
'add the support for read variations vectorized text ',negative
'second batch last but one batch will actualbatchsize actualbatchsize same batchsize when exceptions are expected ',negative
'big tables that should streamed ',negative
'get alias from topops ',negative
'end ',negative
'required required required required optional ',negative
'get all the dependencies delete ',negative
'this the constructor use for smb ',negative
'mgby already contains group key need remove distinct column ',negative
'check the join operator encountered candidate for being converted ',negative
'all children are done this operator also done ',negative
'test with null empty randomly ',negative
'produces sparse vrb when includes are used need write the dense vrb orc ideally wed use projection columns but orc writer doesnt use them any case would also need build new for orcwriter config this why orcwriter created after this writer the way ',negative
'used store each values length ',negative
'assumes ranges passed cache read have data ',negative
'will split the block headerix splitways ways and take totake blocks which will leave free blocks target size ',negative
'create new inputformat instance this the first time see ',negative
'new connections goes minihs now ',negative
'convert integer related types because converters are not sufficient ',negative
'the session hook should set the property ',negative
'stringstats ',negative
'end month behavior ',negative
'fullresourceplan ',negative
'waits for other threads join and returns with its ',negative
'refer paper ',negative
'for should deterministic and stateless ',negative
'setting non important configuration should return the same client only ',negative
'load empty database ',negative
'null server url means local mode ',negative
'setup the overflow batch ',negative
'get list dynamic partitions ',negative
'get the ',negative
'only two elements expected partexprparts partition column name and partition value ',negative
'optimize revokegrant list remove the overlapping ',negative
'can have multiple branches due dpp semijoin opt use dfs traverse all the branches until hit ',negative
'collect child projection indexes used ',negative
'batchsize when isrepeating ',negative
'col names parent ',negative
'uri the from path ',negative
'dont want check types already checked ',negative
'optional iodescriptor ',negative
'end ',negative
'service not started yet ',negative
'set host ',negative
'the first argument just set the return the standard writable version this ',negative
'handle hint based semijoin ',negative
'evict all blocks ',negative
'copy and fixup the parent list the original child instead just assuming ',negative
'minimum required ',negative
'not used value ',negative
'this will also take care the queries query parallelism changed ',negative
'the current kinds column vectors ',negative
'one row per value ',negative
'check this project only projects one expression scalar ',negative
'null returned then help message was displayed parsecommandline method ',negative
'same also emit extra records from separate thread ',negative
'create environment variable that uniquely identifies this script ',negative
'print the vertex that has more before the vertex that has fewer ',negative
'creates empty union object ',negative
'for nongeneric udf type info isnt available this poses problem for hive decimal ',negative
'constant map projection known length ',negative
'',negative
'batchindex classname match issinglevalue currentkey currentkey ',negative
'converts partnamespartvals into partnameval partnameval ',negative
'marker track there starting single quote without ending double quote ',negative
'register plain sasl server provider ',negative
'currently known tasks ',negative
'this happens when the reducesinks edge has been removed cycle detection logic nothing here ',negative
'the mapjoins parents may have been replaced dummy operator ',negative
'blindly add this non settable list list integers should sufficient for the test case use the standard list object inspector ',negative
'roundpower negative scale means start rounding integer digits the result will integer result will have least absroundpower trailing digits examples where the show the rounding digits round rrr ceiling rrrr notice that any fractional digits will gone the result ',negative
'fake dead session ',negative
'emptybuckets false ',negative
'have records left the group push one those ',negative
'the location string will the form database nametable name parse and communicate the information hcatinputformat ',negative
'cannot merge ',negative
'this not actual list see intermlist ',negative
'serialize again ',negative
'only publish stats this operators flag was set gather stats ',negative
'prepare for next iteration any ',negative
'dates are stored long convert and compare ',negative
'broken configuration from mapreddefaultxml ',negative
'else fall through and add this condition nonequi condition ',negative
'reset exec context that initialization the map operator happens properly ',negative
'determine distkeylength distincts and then add the first present ',negative
'truncate ascii same maximum length ',negative
'create new value ',negative
'let create partitions without any triggers ',negative
'turns out partition columns get marked virtual columninfo need ',negative
'create the metastore client the clientugi doing this way will give the client access the token that was added earlier ',negative
'remove the directories for aborted transactions only ',negative
'test will local mode ',negative
'all the insane dst variations where actually end anyones guess ',negative
'using reflection and update the mdc ',negative
'random column name reduce the chance conflict ',negative
'determine the partition columns using the first partition descriptor ',negative
'partitions not exist for this table ',negative
'pass all the checks can rewrite ',negative
'table valid ',negative
'not null constraint could enforcedenabled ',negative
'extract the information that need ',negative
'new instance session ',negative
'replacement the existing database state newer than our update ',negative
'trigger kill threads and verify get and expected message ',negative
'validate and vectorize the map operator tree ',negative
'dont wait empty take above that will wait for ',negative
'operator with single child ',negative
'skip header lines ',negative
'there should still now directories the location ',negative
'cbo enabled orderby position will processed genplan ',negative
'should never happen ctor just assignments ',negative
'already consistent can happen wnull lsg ',negative
'for null ',negative
'groupby generates new vectorized row batch ',negative
'made sure the references are for different join inputs ',negative
'helper function create vertex for given reducework ',negative
'counters with vertex name suffix desiredcounter inputfiles counters inputfilesmap inputfilesmap outcome inputfile ',negative
'add file paths the files that will moved the destination the caller needs ',negative
'need preserve enabled flag ',negative
'get service host ',negative
'this simulates the completion update tab txn ',negative
'get the cookie name ',negative
'register information about pushed predicates ',negative
'the pruning needs preserve the order columns the input schema ',negative
'push filters only for this qbjointree child qbjointrees have already been handled ',negative
'the user has specified queue name themselves create new session also new session created the user tries submit queue using their own credentials expect that with the new security model things will run user hive most cases ',negative
'hadoop configuration properties properties with null values are ignored and exist only for the purpose giving symbolic name reference the hive source code properties with nonnull ',negative
'not terrible thing even the data not deleted ',negative
'length sync entries number bytes hash escape hash ',negative
'dont set lineage delete dont have all the columns ',negative
'now take the serialized keys just wrote into our scratch column and look them the list ',negative
'substitute outputformat name based ',negative
'string ',negative
'sized block ',negative
'tests for the listpartition string partitionspecs string sourcedb string sourcetable string destdb string desttablename method ',negative
'that recognizes parenthesis delimiter ',negative
'ttl check ',negative
'attempt find maxappendattempts possible alternatives filename appending and seeing that destination also clashes were still clashing after that give ',negative
'now have delta with inserts only push predicate ',negative
'ignore error just return the valid tables that are found ',negative
'wait queue could have been reordered the mean time because concurrent task submission remove the specific task instead the head task ',negative
'first handle the actual thing found ',negative
'look for matches file system counters ',negative
'check fast ',negative
'verify both groupset and aggrfunction are empty ',negative
'reset temp list index ',negative
'the todigitsonlybytes stores digits the end the scratch buffer ',negative
'give the caller context for future errors ',negative
'value not constant bail out ',negative
'display all nonvectorized leaf objects unless only ',negative
'separate client create the catalog ',negative
'commysqljdbcdriver ',negative
'there are txns which are open for the given validtxnlist snapshot then just return ',negative
'all rows should the inmemory hashmap ',negative
'determine the binary words like what does ',negative
'test that fetching nonexistent partition yields objectnotfound ',negative
'for caching column stats for unpartitioned table ',negative
'test repeating right ',negative
'grouping should pruned which the last key columns ',negative
'use subdir inputpath ',negative
'should expect semantic exception being throw this partition should not present ',negative
'dfs autocloseable ',negative
'leftright skip filtered valid skip filtered valid left alias has any pair for right alias continue ',negative
'note pig sets client system properties actually getting the client system properties starting pig you must pass the properties when updating our pig dependency this will need updated ',negative
'run the script using beeline ',negative
'the ptned table should there both source and target rename was not successful ',negative
'this indicates corr var left operand rex call not this used decorrelatelogical correlate appropriately create rex node expression ',negative
'this the data copy ',negative
'picks topn pairs from input ',negative
'this the old logic which assumes that the filenames are sorted alphanumeric order and mapped appropriate bucket number ',negative
'job status request executor get status job ',negative
'list dynamic partitions ',negative
'add partitions with new schema ',negative
'set the buffer that will receive the serialized data the output buffer will not reset ',negative
'introduce top project operator remove additional columns that have been introduced ',negative
'make the new aggrel ',negative
'could have multiple sources restrict the same column need take the union the values that case ',negative
'such abc ',negative
'prepare plan for submission building dag adding resources creating scratch dirs etc ',negative
'check that the table acid ',negative
'dont load defaults note hivesitexml only available client not ',negative
'its new column ',negative
'calculate the number bytes the split that are local each ',negative
'neginfinity start exclusive ',negative
'initial write for ctas and import ',negative
'udfs ',negative
'max number threads can use check noncombinable paths ',negative
'note that originalfiles are all original files recursively not dirs ',negative
'note can actually higher than real value ',negative
'nonjavadoc order update decimal fast allocation need expose access the internal storage bytes and scale ',negative
'netty netty arrowvector arrowmemory arrowformat flatbuffers hppc ',negative
'another special case because timestamp not implicitly convertible numeric types ',negative
'perform simple checksum here make sure nothing got turned null ',negative
'and must have locations outside the table directory ',negative
'add the grouping set key the group operator this not the first group operator but subsequent group operator which forwarding the grouping keys introduced the grouping sets for consider select key value count from group key value with rollup assuming mapside aggregation and skew the plan would look like tablescan select groupby reducesink groupby select filesink this function called for groupby pass the additional grouping keys introduced ',negative
'hold lock file hdfs session dir indicate the use ',negative
'semijoin optimization branch which should look like parentselgbrsgbrs ',negative
'run reverse dns lookup the url ',negative
'can not kerberos auth will the input split generation the ',negative
'first drop all the dependencies ',negative
'just use view name location ',negative
'only apply this rule unionall true and sortfetch not null and more than ',negative
'make sure that the session returned the pool doesnt live the global ',negative
'handle cancel loop ',negative
'regardless the following exception ',negative
'there may speculative tasks waiting ',negative
'need the work detangle this ',negative
'the task hasnt started and killed report back the that the task has been killed ',negative
'mnot test this ',negative
'possible since either container task can unregistered ',negative
'value for indicates that the mapper processed data that does not meet filter criteria merge should noop ',negative
'spend most wait for executors come ',negative
'tries get lock and ',negative
'currently returned bootdumpbeginreplid dont consolidate the events after bootstrap ',negative
'tezjsonparser ',negative
'want query level fairness and dont want the get queue hold session ',negative
'for partial and final ',negative
'committed open ids ',negative
'and load data into the same table which should now land deltaxx ',negative
'try merge the join with the right child ',negative
'read via objectstore ',negative
'create the object inspector for the input columns and initialize the udtf ',negative
'the hmsc not shared across threads the only way could get closed while are doing healthcheck removallistener closes the synchronization takes care that removallistener wont ',negative
'create table with bad avro uri ',negative
'password ',negative
'transaction for the compaction for now ',negative
'existing table valid but the partition spec different then ignore partition validation and create new partition ',negative
'nonjavadoc see ',negative
'convert equivalent time utc then get day offset ',negative
'input record iterator not used ',negative
'setup the bloom filter once ',negative
'store big keys one table into one dir and same keys other tables into corresponding different dirs one dir per table this map stores mapping from big key dir its corresponding mapjoin task ',negative
'process hiveconf get hiveconf param values and set the system property values ',negative
'combo url set literal set none ',negative
'the interface for single long key hash multiset contains method ',negative
'filter disabled injection disabled exception not expected ',negative
'',negative
'set sasl qop ',negative
'print the results ',negative
'the then expression either identityexpression column ',negative
'test that timestamp arithmetic done utc and then converted back local timezone matching oracle behavior ',negative
'additional data type specific setting ',negative
'leave some other host found delay else ends allocating random host immediately ',negative
'constant operator deterministic and all operands are constant ',negative
'the metastore shouldnt care what txn manager hive running but various tests ',negative
'point separating ioexception yarnexception others ',negative
'bypass the clause and select the first disjunct ',negative
'removed and the size before and after the genroottablescan will different ',negative
'process the last byte necessary ',negative
'dummy parent operators well ',negative
'deleterecord firstrecordinbatch until exhaust all the delete records ',negative
'dont prevent using nonacid resources txn but lock them ',negative
'compare class namemethod name using avoid any object conversion which may cause object creation most cases when the class ',negative
'since tab empty update stmt has pblah thus nothing actually update and generate empty dyn part list ',negative
'replacement the existing table state newer than our update ',negative
'build the status message for the status call ',negative
'test need partitionglobal order shufflesort should only used for global order ',negative
'this should not happen but ignore for safety ',negative
'check partitionvals are legitimate ',negative
'the function has explicit name like funcargs then call constructor that explicitly provides the function name the functext argument ',negative
'nothing special needs done for grouping sets this the final group operator and multiple rows corresponding the grouping sets have been generated upstream however addition job has been created handle grouping sets ',negative
'',negative
'this the last element ',negative
'now copy the data into cache buffers ',negative
'per jdbc spec the connection closed sqlexception should thrown ',negative
'adding query specs used ',negative
'equals and compareto are not compatible with hivedecimals want compareto which returns true iff the numbers are equal the same equals returns true iff equal and the same scale set the decimals not the same ',negative
'are translating calcite operators into hive operators ',negative
'will not try partial rewriting for rebuild incremental rebuild disabled ',negative
'update fetchsize modified server ',negative
'dont create sessions for empty entries ',negative
'estimate that there will bytes per entry ',negative
'run using environment context with cascade ',negative
'get the key positions ',negative
'test for invalid group name ',negative
'estimate size key from column statistics ',negative
'this the time zone for test ',negative
'nonjavadoc see ',negative
'get the top nodes for this task ',negative
'validation the same for map join since the small tables are not vectorized ',negative
'check for completed transactions ',negative
'',negative
'outputs are ready ',negative
'enable escaping ',negative
'accept start dag schedule wait time resource wait time etc ',negative
'mapping reducesink mapjoin operators ',negative
'make sure the correlated reference forms unique key check that the columns referenced these comparisons form ',negative
'add list bucketing location mappings ',negative
'inner bigtable only join specific members ',negative
'there will ddl task created case its create table not exists ',negative
'optional bytes token ',negative
'hostname ',negative
'left border the max ',negative
'close commit ',negative
'check the value bloom filter ',negative
'large ',negative
'test reordering ',negative
'schematext ',negative
'entitytype ',negative
'the output final and complete full aggregation which list doublewritable structs that represent the final histogram pairs bin centers and heights ',negative
'this corresponds mapstring ',negative
'',negative
'initialize the forward operator ',negative
'now cancel the delegation token ',negative
'this offer will accpeted and evicted ',negative
'run without roundoff ',negative
'this subquery and must have alias ',negative
'values array ',negative
'set udf classes for type casting data types rowmode ',negative
'root object array map collection add estimators for fields ',negative
'register only the attempt known case unregister call came before the register call ',negative
'define the serde parameters ',negative
'check the config used very often ',negative
'perform the same uri normalization createdatabasecore ',negative
'create new external table ',negative
'and continue processing the children ',negative
'',negative
'spark ',negative
'remove ',negative
'its easier then because simply division and then scale down ',negative
'these aggregations should updated only once ',negative
'nonjavadoc see ',negative
'delegate back the local serializerowid method ',negative
'bound skip beginning and end the source ',negative
'not public since must have the field count and other information ',negative
'help option requested then display help and exit ',negative
'not native vectorized ',negative
'this was acid ',negative
'way ',negative
'need clone the plan ',negative
'should have null for nonspecified comments ',negative
'not native vectorized ',negative
'insert merge lands part updates land there ',negative
'when have multiple values save the next value records offset here ',negative
'removed ',negative
'output columns the following order the columns representing the output from window fns the input rows columns ',negative
'how get config paths and aminfo ',negative
'for orc there tez job for table stats ',negative
'this happens register calls getmetrics ',negative
'obtain input and all related data structures ',negative
'text file comparison ',negative
'initializeop can overridden initializing data structures for vectorforward ',negative
'master node will serialize writercontext and will make available slaves ',negative
'transactions ',negative
'warnings ',negative
'list last keys for each stripe ',negative
'the function deterministic and the children are constants ',negative
'string select columnname from cipartname null tabcolstats partcolstats where dbname cidbname and tablename citablename cipartname null and partitionname cipartname ',negative
'default double but one the sides already decimal complete the operation that type ',negative
'calcite yearmonth literal value months bigdecimal ',negative
'verify proper null output data value ',negative
'assert classinvariant ',negative
'can tolerate this this the previous behavior ',negative
'older version hadoop should have had this field ',negative
'eliminate plans with more than one tablescanoperator ',negative
'convert hive projections calcite ',negative
'create callables with different queries ',negative
'this constructor appeared and specifies that not want linewrap use any newline separator ',negative
'subdirectories ',negative
'get the serialized value for the column ',negative
'isvectormapjoin ',negative
'make sure the ugi contains the token too for good measure ',negative
'update max counter new value greater than max seen far ',negative
'not allow view defined temp table other materialized view ',negative
'evaluate the expression tree ',negative
'bgenjjtree throws ',negative
'possible that some operators add records after closing the processor make sure ',negative
'query being killed until both the kill and the user return ',negative
'the uri requested ',negative
'check its simple cast expression ',negative
'last item end ',negative
'nulls etc vice versa ',negative
'default using long types ',negative
'top null then there are multiple parents well hence lets use parent statistics get data size also maxsplitsize should ',negative
'need consistent with that ',negative
'localdate must present ',negative
'now run its minor compaction dont collapse events ',negative
'the default number threads will that means thread pool not used and operation executed with the current thread ',negative
'reset the perflogger ',negative
'now make copy ',negative
'check result now ',negative
'the schedule loop will triggered again when the deallocatetask request comes for the preempted task ',negative
'step parse the query set dynamic partitioning nonstrict that queries not need any partition ',negative
'test when second argument has nulls ',negative
'base configuration active configuration ',negative
'create table ',negative
'property names needed keep internal structure serde ',negative
'allow cross join from teecurmatch ',negative
'get failed attempts ',negative
'local resources are session based ',negative
'populate the driver context with the scratch dir info from the repl context that the temp dirs will cleaned later ',negative
'the first entry with accumulated count lower corresponds the lower position ',negative
'its difficult impossible pass global things compilation have static cache ',negative
'queries like select from where foo calcites rule chokes arguably can insert cast boolean such cases but since postgres oracle and sql server fail compile time for such queries its arcane corner case not worth adding that complexity ',negative
'optional buffer use when actually copying data next free position buffer ',negative
'table name has present min child and max child ',negative
'cache use during optimization ',negative
'convert partition partition spec string ',negative
'',negative
'timeout and unspecified error ',negative
'file ',negative
'find our bearings the stream normally iter will already point either where want just before however rgs can overlap due encoding may have ',negative
'this will only set the metastore being accessed from metastore thrift server not from the cli also only the ttransport being used connect ',negative
'event ',negative
'test the idempotent behavior abort txn ',negative
'void boolean byte short int float long double ',negative
'bgenjjtree typebool ',negative
'forward any remaining selected rows ',negative
'nonjavadoc see ',negative
'this logic assumes one dag time was not the case itd keep rewriting ',negative
'required required required required required required required required required required optional optional ',negative
'timestamp timestamp intervaldaytime intervaldaytime ',negative
'keys are always primitive respect the binary ',negative
'event ',negative
'todo add section like the restricted configs for overrides when theres more than one ',negative
'dryrun true immutable true ',negative
'each columns length the value ',negative
'query the hbase table and check the key valid and only are present ',negative
'invalid filesystem schemes ',negative
'insert some data this will again generate only insert deltas and delete deltas delta ',negative
'have some sort expression tree try jdoql filter pushdown ',negative
'regression test for defect reported hive ',negative
'many old tests depend this ',negative
'list nondistinct aggrs ',negative
'dataschema can obtained from ',negative
'gather information about the dpp table scans and store the cache ',negative
'context for reading using the regular partition deserializer get the row object and assigning the row object into the vectorizedrowbatch with vectorassignrow ',negative
'nonjavadoc see byte ',negative
'default run ',negative
'this doesnt throw metaexception when setting high max part count ',negative
'project operator can continue ',negative
'results ',negative
'insert entries txntowriteid for aborted write ids ',negative
'store the user name the open request case nonsasl authentication ',negative
'the partition outside the tabledir ',negative
'can inherited from base class ',negative
'load them into set ',negative
'clip off seconds portion bring nanoseconds into integer portion ',negative
'alter table scenarios ',negative
'even without type params return default for varchar ',negative
'remember the mapping case scan another branch the mapjoin later ',negative
'for dense encoding use bias table lookup for hllnobias algorithm ',negative
'void can converted any type ',negative
'verify that the node blacklisted ',negative
'write the suffix the ',negative
'unequal maps ',negative
'were told use some port but its occupied fail ',negative
'remember the branching ops have visited ',negative
'remove the join operator from the query join context data structures coming from qbjointree ',negative
'special handling for decimal because decimal types need scale and precision parameter this special handling should avoided using returntype uniformly for all cases ',negative
'lookup list bucketing pruner ',negative
'after some other submission has evicted ',negative
'once move hadoop dependency the following paramteer can used ',negative
'buffers offset exist exists and stale doesnt ',negative
'through the set key columns and find their representatives the values ',negative
'need work little harder for our comparison note round down for integer conversion anything below the next minmax will work ',negative
'vectorized implementation hexlong that returns string ',negative
'enable trash can tested ',negative
'nontest mode emit log file which can different from the normal hivelog for example using log some file with different rolling policy ',negative
'doublecompare treats and different ',negative
'temptable only set when load rewritten ',negative
'iterate through all the operators that have candidate fks and choose the that has the minimum selectivity assume that and this have the pkfk relationship this heuristic and can ',negative
'remove the locks waiting state ',negative
'stat ',negative
'workcheckfileformat set true only for load task assumption here dynamic partition context null ',negative
'need check catalog for null parsedbname will never return null for the catalog ',negative
'they are not the same constant for example union all and ',negative
'there should new base directory base original bucket files delta directories deletedelta directories and the ',negative
'not supported for temp tables ',negative
'performance problem objectstore does its own new hiveconf ',negative
'required optional required required optional ',negative
'keep track corresponding col partcols ',negative
'ctas should not create void type ',negative
'compute the number tasks ',negative
'put all the mutation ',negative
'test double ',negative
'initialize one columns source conversion related arrays assumes inittargetentry has already been called ',negative
'passes the test valid ',negative
'privilege matrix user user groupa groupb public testdb testtable testtable testtable testtable testdb ',negative
'register the new dag identifier thats not the one currently registered ',negative
'accepted ',negative
'capture system out and err ',negative
'allow implicit string varchar conversion and vice versa ',negative
'set the run frequency low this test doesnt take long ',negative
'call open read data split mockmocktable ',negative
'executes job request operation thread pool not created then job request executed current thread itself param jobexecutecallable callable object run the job request task ',negative
'keep track view alias read entity corresponding the view for for query like select from where keeps track aliases for vvv this used when added input for the query the parents ',negative
'handle kill query results part just put them place will resolve what ',negative
'worth ',negative
'overflow means this smaller ',negative
'all zeroes should have handled this earlier ',negative
'before any the other core hive classes are loaded ',negative
'date ',negative
'need extend the tenancy saw newer file with the same content ',negative
'count all rows ',negative
'fix temp path for alter table concatenate ',negative
'conf and then the children ',negative
'but hive supports assigning bucket number for each partition this can vary ',negative
'new filter currently support comparison functions and between ',negative
'both are last day the month then time part should ignored ',negative
'sort cost ',negative
'begin abort ',negative
'legacy file see its bucket file ',negative
'call the real method instead the mock ',negative
'replication done now the following verifications ',negative
'todo these would need propagated from via progress metricnumber allocated guaranteed executors use metricnumber speculative executors use ',negative
'convert udaf params exprnodedesc ',negative
'generate absolute path relative home directory ',negative
'string encrypt ',negative
'create unpartitioned table event ',negative
'write addition payload required for orc ',negative
'such such abc ',negative
'mystructlist ',negative
'owner can user role ',negative
'whole batch spilled ',negative
'tblprops will null user didnt use tblprops his create table cmd ',negative
'read that many chars ',negative
'count zeros until first nonzero digit encountered ',negative
'this table has associated index table then attempt build index mutations ',negative
'update needs select all the columns rewrite the entire row also need figure out which columns are going replace ',negative
'holds restored from disk big table rows ',negative
'for the big table only need promote the next group the current group ',negative
'now succeeds ',negative
'convert the fields ',negative
'try using jdbc metadata api get column list user should fail ',negative
'data structure control whether certain reference present every ',negative
'find the value object update the timestamp the keyvalue value matches the criteria ',negative
'replace insert overwrite insert into ast tree will have this shape tokquery tokfrom tokinsert tokdestination this token replaced tokinsertinto toktab toktabname defaultcmvmatview tokselect ',negative
'are waiting for connection for long time something really wrong better raise error than hang forever see ',negative
'join from multiple relations ',negative
'map cvalue map rowvalues assertequals cvaluesize ',negative
'keycolumn output name valuetag ',negative
'get the sign the decimal ',negative
'lazy mode only list files and expect that the eventual copy will pull data default that the import mode insert overwrite writeids snapshot for replicating acidmm tables default means replload bootstrapdump export ',negative
'see the filename matches and can read ',negative
'create dbtdtpart dtpart dtpart test recycle single file dtpart recycle single partition recycle table ',negative
'when union followed multitable insert ',negative
'getallfunctions ',negative
'this operator has been visited already the rule ',negative
'when were inserting the key would have inserted here theres key ',negative
'nonjavadoc see javalangstring ',negative
'dont recheck with next only lists each collisions ',negative
'instances likely incorrect ',negative
'',negative
'try marking the query complete this external submission ',negative
'todo unique join ',negative
'then its dynamic partitioning case and shouldnt check the table itself ',negative
'simple task registration and unregistration ',negative
'getcolumn should return the instance passed ',negative
'aws settings ',negative
'walk through all the source vertices ',negative
'can happen retrying deleting the zlock after exceptions like race condition where parent has already been deleted other process when deleted both cases should not raise error ',negative
'',negative
'for now throw away file ',negative
'override nothing the this test not related with vectorization the parent class creates temporary table this test and alters its properties not override this test that temporary table needs renamed however mentioned this does not serve any purpose this test does not relate vectorization ',negative
'should only get here retrying this ',negative
'any filters are present the join tree push them top the table ',negative
'rfile new ',negative
'json key always string ',negative
'all the objects must different ',negative
'delta ',negative
'generate select node for windowing ',negative
'empty queue but capacity available due waitqueuesize and return the element ',negative
'startdate wed full timestamp full day name ',negative
'the name this column the hive schema ',negative
'highword digits ',negative
'movedthis may change ',negative
'get the index separating the user name from domain name the users name the first param username full user name return index domain match not found ',negative
'uses pattern and specifies ',negative
'build reduce sink details keycols valuecols outcolnames etc for this ptfdesc ',negative
'change local ',negative
'files size for splits ',negative
'different number field names ',negative
'process the second childif exists node get partition specs ',negative
'initialize using one target data type info ',negative
'there are multiple stats for the same scheme from different namenode this method will squash them together ',negative
'cant use cacheloader because searcharguments may built either from kryo strings ',negative
'since existsnot exists are not affected presence null keys not need generate count countc ',negative
'move file would require session details needcopy invokes sessionstateget ',negative
'check fast ',negative
'hold one refcount ',negative
'not ',negative
'disallow changing temp table location ',negative
'nulls are handled each individual base writer setter could handle nulls centrally here but that would result spurious allocs ',negative
'simple storagebased auth have information about columns living different files simple partitionauth and ignore the columns parameter ',negative
'prepare the tmp output directory the output tmp directory should exist before jobclose before renaming after job completion ',negative
'puts gets hits unused unused ',negative
'union type not supported calcite ',negative
'todo see can get rid this used one place distinguish archived parts ',negative
'failed update this task instead retrying for this task find another change isguaranteed and modify maps wed need the epic lock will not ',negative
'modify tablescanoperator inplace knows operate vectorized ',negative
'strip off the delimiter ',negative
'new row also inserted the usual delta file for update event ',negative
'finish the scheduled compaction for ttp ',negative
'current installed configuration ',negative
'config set table not temporary and partition being inserted exists capture the list files added for not yet existing partitions insert overwrite new partition dynamic partition inserts the add partition event will capture the list files added generate insert event only inserting into existing partition ',negative
'this used during translation decide the internalname alias mapping from the input the ptf carried forward when building the output for this ptf this used internal ptfs noop make names its input available the output general this should false and the names used for the output columns must provided the ptf writer the function getoutputnames ',negative
'wait for all writes finish before actually close ',negative
'type target column ',negative
'partitioned table ',negative
'the dispatcher fires the processor corresponding the closest matching rule ',negative
'host ',negative
'floor date special handling since function hive does include timeunit observe that timeunit information implicit the function name thus translation will proceed correctly just ignore the timeunit ',negative
'fktabledb ',negative
'prefer right most alias ',negative
'lrfuthreshold inf this case ',negative
'reached end split ',negative
'there any confict then not generate the new select otherwise add into the calcitecollst and generate the new select ',negative
'ignore nullscanoptimized paths ',negative
'setup our inner big table only join specific members ',negative
'always want recreate dont know were created the ',negative
'blocking operator groupbyoperator and joinoperator can ',negative
'foreigncatalogname ',negative
'iterate over the symbol functions the chain are not the end the iterator row null match the current componentfn returns false then return false otherwise set row the next row from the iterator are the end the iterator skip any optional symbol fns star patterns the end but come non optional symbol return false match all fns the chain return true ',negative
'xmx not specified ',negative
'set longpollingtimeout custom value for different test cases ',negative
'note that reset also resets the data buffer for bytes column vectors ',negative
'were here well proceed down the next while loop iteration ',negative
'create data buffers for value bytes column vectors ',negative
'compare every rows ',negative
'create object inspector ',negative
'lock types ',negative
'find the old database ',negative
'the sorting order the parent more specific they are equal will copy the order from the child and then fill the order the rest columns with the one taken from parent ',negative
'this method mainly intended for debug display purposes ',negative
'define summary metrics for each column ',negative
'this case have scale before division otherwise might lose precision this costly but inevitable ',negative
'but want just reset its null ',negative
'optimize performance only looking the first key series equal keys ',negative
'generic options parser doesnt seem work ',negative
'allocate higher priority should not allocated ',negative
'files size for splits ',negative
'this done preread stage where theres nothing special wrefcounts just release ',negative
'set the comparison the iocontext and the type the udf ',negative
'create the hfile writer ',negative
'try extended deduplication ',negative
'the name the field not optional ',negative
'need preserve authorizer flag ',negative
'builtin functions shouldnt the session registry and temp functions shouldnt the system registry persistent functions can either registry ',negative
'required required required required optional optional optional optional optional optional ',negative
'force avoid reading footers ',negative
'the result already present the cache return ',negative
'setup for different kinds vectorized reading supported read the vectorized input file format which returns vectorizedrowbatch the row read using deserialize each row into the vectorizedrowbatch and read using the regular partition deserializer get the row object and assigning the row object into the vectorizedrowbatch with vectorassignrow ',negative
'meaningful path for nonacidacid ',negative
'records will emited from hive ',negative
'filterg cannot parse quoted date try parse date here too ',negative
'verify whether the sql operation log generated and fetch correctly async mode ',negative
'get access the queryinfo instance ',negative
'set the row values ',negative
'handle mergejoin specially and check for all its children ',negative
'push filter top children for discardable ',negative
'leftfast ',negative
'run initiator clean the row fro the aborted transaction from txns ',negative
'there single column return the number distinct values ',negative
'test serialization and deserialization with different schemas ',negative
'use the rewritten ast ',negative
'for writing out single byte ',negative
'then search from parent ',negative
'the toplevel object inspector nonsettable return false ',negative
'the keyevaluate reuses storage that doesnt work with smb mapjoin because holds references keys merging ',negative
'event ',negative
'give hms time handle close request ',negative
'used for ',negative
'ckey not present parent ',negative
'replacement exists for this code point emit out the replacement and append the ',negative
'try scheduling again ',negative
'optional string mystring ',negative
'set bit field not null ',negative
'retried means retries each with retry with random sleep ',negative
'prune partitions ',negative
'test that existing exclusive with new sharedread coalesces ',negative
'',negative
'mapjoin should not affected join reordering ',negative
'hadoop and earlier way finding the sasl property settings initialize the saslrpcserver ensure qop parameters are read from conf ',negative
'drop table with partitions ',negative
'thus run the field trimmer again push them back down ',negative
'datanucleus propagates some pointless exceptions and rolls back the finally ',negative
'get nonexistent key should terminate ',negative
'double scalarlong column ',negative
'ifelse chain arranged likely order frequency for performance ',negative
'check required privileges subset available privileges ',negative
'',negative
'event ',negative
'print operators ',negative
'weve already dropped testdbname constructor also drop teardownafterclass ',negative
'multiply inverse the division ',negative
'data for bucket expect length bucket file ',negative
'similarly this map which vectorized row batch columns are the big table value columns since may have value expressions that produce new scratch columns need mapping ',negative
'detect incorrect lists ',negative
'stripes satisfies the condition ',negative
'nonjavadoc see ',negative
'',negative
'user could encounter this stored view definition contains old sql construct which has been eliminated later hive version need provide full debugging info help with fixing the view definition ',negative
'restore the old value ',negative
'now that are using left outer join join inner count count with outer table wouldnt able tell count zero for inner table since inner join with correlated values will get rid all values where join cond not true where actual inner table will produce zero result handle this case need check both count zero count null ',negative
'default means leave tez decide ',negative
'restrict the set columns that want read from the accumulo table ',negative
'',negative
'previous insertunion creates data files ',negative
'rowcnt than its either empty table table which stats are not computed assume the worse and dont attempt optimize ',negative
'for calciteplanner store qualified name too ',negative
'list need refactored out done only once ',negative
'try the base config ',negative
'affects some less obscure scenario ',negative
'for primary keys retrieve the column descriptors retrievecd true which means alter table statement create table statement but are ',negative
'hive jars the accumulo classpath which dont want ',negative
'the dpp operatorbranch are equal ',negative
'event ',negative
'add the input with project top ',negative
'make sure handle unary and correctly ',negative
'make sure that the unwanted key not present the map ',negative
'setup our hash table specialization will the first time the process method called after hybrid grace reload ',negative
'create lock and trigger heartbeat with heartbeat the lock wont expire ',negative
'for the fetchtask the limit optimization requires fetch all the rows memory and count how many rows get its not practical the ',negative
'create the mapping corresponding the grouping set ',negative
'out the escaped byte the block above already ',negative
'otherwise only order expressions ',negative
'gosh really both scaling and down unscaledvalue significand scale twoscaledownscale check overflow while preserving precision need real multiplication ',negative
'may parsing delta for insertonly table which may not even orc file cannot have rowids ',negative
'get exception finding partition could describe table key return null continue processing for describe table key ',negative
'test the mapping empty string all columns ',negative
'updated bytes per reducer default ',negative
'two the optimization rules convertjoinmapjoin and are put into stats dependent optimizations and run together tezcompiler theres guarantee which one runs first but either case the prior one may have removed chain which the latter one not aware need remember the leaf nodes that chain can skipped for example convertjoinmapjoin removing the reduce sink may also have removed dynamic partition pruning operator chain however doesnt know this and still tries traverse that removed chain which will cause npe this may also happen when happens first ',negative
'name for materialization rebuild name for materialization rebuild ',negative
'convert expr expr usually input ref except for top ',negative
'create semantic analyzer for the query ',negative
'which was modified the update stmt choose nonconflicting one ',negative
'not release beyond current stream dont know which rgs that buffer for ',negative
'move offset point start next input ',negative
'all are null all must selected ',negative
'the last char escape char read the actual char the serialization format escape and make sure the string nullterminated ',negative
'assumes bucketnnnnn format file name ',negative
'get column name ',negative
'this what the vectorizer class does ',negative
'transform the between clause and clause with not top invert true this more straightforward the evaluateexpression method will deal with ',negative
'convert lazy object and return ',negative
'used determined whether the merge can happen ',negative
'succeed trying set transactional true and satisfies bucketing and inputoutputformat requirement ',negative
'note should gather stats rather than merge job since dont ',negative
'initial state one connection ',negative
'inner classes ',negative
'should through single process call ',negative
'either schema literal schema url serialization class must provided ',negative
'seconds wait for app visible ',negative
'given the current input row the mapping for input col info columns and cols return the relative path corresponding the row ',negative
'sort contains limit operation bail out ',negative
'none the operators changing the positions ',negative
'first operator the reduce task not the reducesinkoperator but the ',negative
'this just simple way generate test data ',negative
'create enough note have have separate keyvalueconverter for each keyvalue because the keyvalueconverters can reuse the internal object its not safe use the same keyvalueconverter convert multiple keyvalues ',negative
'use the trick mentioned less hashing same performance building better bloom filter kirsch etal from abstract only two hash functions are necessary effectively implement bloom filter without any loss the asymptotic false positive probability ',negative
'validate the metastore client call ensure throws exception partition fields contain unicode characters commas ',negative
'first item ',negative
'check for dpp and semijoin dpp ',negative
'any value will become zero even possibility rounding ',negative
'will cause overflow for result position must yield null ',negative
'this error code should really produced hive ',negative
'metastore calls timing information ',negative
'when truncated included used its length must least the number source type infos when longer assume the caller will default with nulls etc ',negative
'are close phase have definitely exhausted the big table input ',negative
'check array null empty value null ',negative
'pick the formatter use display the results either the normal human readable output json object ',negative
'the decimalcolumnvector set method will quickly copy the deserialized decimal writable fields ',negative
'find the file the include path ',negative
'primarily avoid multiple shutdowns ',negative
'temporary tables created during the execution are not the input sources ',negative
'null principal ',negative
'replace original varpopx with sumx sumx sumx countx countx ',negative
'use stringbuilder and then call printerror only once printerror will write both stderr and the error log file situations where both the stderr and the log file output simultaneously output single stream this will look cleaner ',negative
'private boolean isouterjoin ',negative
'dont send messages pending tasks with the flags they should killed elsewhere ',negative
'recursive flush flush all the tree operators ',negative
'test does not corrupt existing values mapred env configs ',negative
'sort the works that get consistent query plan for multi executionsfor test verification ',negative
'process the report ',negative
'for wordshift ',negative
'thursday august ',negative
'neither dir should get created ',negative
'statistics annotation fetches column statistics for all required columns which can ',negative
'just had leading zeroes value ',negative
'find the ancestor which exists check its permissions ',negative
'load properties from sparkdefaultsconf ',negative
'prepare aggs for updating ',negative
'this method updates the input expr changing all the exprnodecolumndesc refer columns given the colexprmap for instance col would become valuecol the execution engine expects filters the join operators expressed that way ',negative
'new ngram ',negative
'the input node not operator bail out ',negative
'should find alias this insert and alias this however wont fix positional order alias case cause wed still have star the top level bail ',negative
'set insert doesnt set off but update does ',negative
'first generate the expression for the partition and sort keys the cluster clause distribute clause has the aliases for partition function ',negative
'because there relationship between cds and sds must set the sds null first before dropping the storage descriptor satisfy foreign key constraints ',negative
'see bucketnumreducersq bucketnumreducersq ',negative
'are ctas know there are partitions ',negative
'the character set for decoding constant can optimize that ',negative
'retry the next port ',negative
'deltas and base and leave them the cleaner clean ',negative
'have already locked the table dont lock the partitions ',negative
'root tran must mapinput ',negative
'for dpp case dpp sink will appear task and the target work dpp sink will appear task task the child task task task will traversed before task because taskgraphwalker will first put children task the front task queue spark work which equal other found and removed task the dpp sink can removed when task traversedmore detailed see hive ',negative
'appended once all the session list are added the url ',negative
'insert two rows acid table ',negative
'left and right aliases are all valid two values will inner joined ',negative
'doesnt vectorize uses neither the vectorzied acid readers ',negative
'quotedquerystring ',negative
'tracks the last nonoob heartbeat number which counters were sent the ',negative
'aggkey statswork used for stats aggregation while statsaggprefix filesinkdesc used for stats publishing they should consistent ',negative
'llap object cache unlike others does not use globals thus get the existing one ',negative
'start with ssl ',negative
'boolean value match for other lengths ',negative
'cannot acid ',negative
'writing partitioned table then pigschema will have more columns than tableschema partition columns are not part tableschema nothing matching fschemaalias found target table schema log ',negative
'see ',negative
'collect bucket andor partition information for object hashing ',negative
'every slot long ',negative
'will try combine multiple clauses into smaller number with compatible keys ',negative
'flag for each byte indicate escape needed ',negative
'check all the bit vectors can merge ',negative
'invalid number ',negative
'initialize buffer read the entire stripe ',negative
'addpartitions err duplicate keyvals mpart ',negative
'dispatch current node ',negative
'queries rejected from being cached due nondeterministic functions temp tables other conditions ',negative
'called during translation invokes createevaluator which must implemented subclass sets the evaluator with references the tabledef partitionclass partitionmemsize and the transformsrawinput boolean ',negative
'handle rename and other changes ',negative
'rule cannot applied there are groupingid because will change the value the position will changed ',negative
'the length the long array that needs passed ',negative
'allow accessing field list element structs directly from list ',negative
'print the key ',negative
'kill query null then session might have been released pool closed already ',negative
'this can easily merged into union ',negative
'have kerberos credentials should obtain the delegation token ',negative
'first check can run llap need use user code scriptudf dont ',negative
'should always get different object and cluster fraction should propagated ',negative
'set ismanaged false this not load data operation for which needed ',negative
'check status compaction job ',negative
'while there are still nodes dispatch ',negative
'cachechunks the list just cachechunks from that point ',negative
'have more control ',negative
'for rest the join type will take min the reduction ',negative
'verify that ptned table rename succeded ',negative
'bucket map join the big tables bucketing version considered ',negative
'stuff ',negative
'dump and load insert after truncate record ',negative
'branches ',negative
'copy the digits the right side the array ',negative
'are replacing the current big table with new one thus ',negative
'set the actual events for the tasks ',negative
'required required required required required required required required required required required required optional optional optional optional optional optional ',negative
'expected invoked once ',negative
'register taskattempt unregister container taskattempt should also unregistered ',negative
'neither cred provider conf have entry return null ',negative
'and skewed columns skewed keys selectopclone ',negative
'ditto ',negative
'free the memory for the column vectors ',negative
'the cache and date ',negative
'take integer part ',negative
'this happens when the code inside the jmx bean setter from the java docs threw exception log and skip outputting the attribute ',negative
'used make sure that waiting getsessions dont block update ',negative
'example for using cluster configuration xmls ',negative
'schemas not match currently not merge ',negative
'fail the query the stats are supposed reliable ',negative
'create list topop nodes and walk ',negative
'simulate delay before finishing the task ',negative
'operator since want individually track the number rows from different inputs ',negative
'get actual number rows from metastore ',negative
'update skew task ',negative
'make sure the compactor has chance run once ',negative
'which are going return values from the input batch vector expressions ',negative
'test repeating null selection ',negative
'now delete node for this keys list ',negative
'hash code logic from original ',negative
'gss credentials for server ',negative
'for hybrid grace hash join during the round processing only keep the left side the row not spilled ',negative
'dump the drop events and check tables are getting dropped target well ',negative
'walk through all found table locations get the most encrypted table ',negative
'this the adjusted index after nested column pruning for instance given the struct type sstructaint bboolean only used the pruned type sstructbboolean here the index field changed from when look the data from parquet index needs adjusted accordingly note currently this only used the read path ',negative
'matching methods found ',negative
'optimize for common case just one row for key container acts iterator ',negative
'since map key for pig has strings ',negative
'all data and partition columns ',negative
'runs instance returns whether not succeeded ',negative
'ctrld ',negative
'this method must only process nondecimal column vectors convert decimal columns regular decimal ',negative
'construct mapping partitionbucket file names and partition bucket number ',negative
'generate tags ',negative
'mapping from directory which filesinkoperator writes into the columns which that ',negative
'create curatorframework instance used the zookeeper client use the create appropriate acls ',negative
'should share cte contexts ',negative
'assumes that the catalog has already been set ',negative
'restart theres internal error ',negative
'ndvexpr maxndv expr args ',negative
'error creation and want delete anyway ',negative
'passwordfile file ',negative
'open accumulo connection ',negative
'greg firstname ',negative
'required optional optional optional ',negative
'after this the keywrappers are properly set and hash code computed ',negative
'sum input cardinalities ',negative
'the user has specified ignore mapjoin hint ',negative
'new tai lue letter high bytes ',negative
'todo using multiple places serde cache pass this ',negative
'empty array ',negative
'its not obvious that this the right logic dont record the callback for example and never notify the client job completion ',negative
'issue warning for missing file and throw exception ',negative
'the absence sorted clause the sorted dynamic partition insert ',negative
'timer null start new one timer has completed during previous invocation start new one timer already started and not completed leaving running without resetting ',negative
'test that clidriver does not strip comments starting with ',negative
'create dest table partitions with custom locations ',negative
'substring index refers the index last char the array ',negative
'',negative
'add mapside ptf operator needed ',negative
'clear integer portion keep fraction ',negative
'xor yields xor yields ',negative
'temp functions are not allowed have qualified names ',negative
'sanity check ',negative
'alter existing partition aaa via objectstore ',negative
'typequalifiers ',negative
'abortedbits should all true everything exceptions are aborted txns ',negative
'merge work else create merge work add above work the merge work ',negative
'find the writeid high water mark based upon txnid high water mark found then need traverse through all write ids less than writeid hwm make exceptions list ',negative
'stuff ',negative
'weve already gone beyond the specified range ',negative
'state that the driver enters after destroy called and the end driver life cycle ',negative
'for example partitions sequencefile and rcfile will have different splits ',negative
'check all the contain stats and all the ndv are bitvectors ',negative
'find target for fetch task conversion optimizer not allows subqueries ',negative
'handle datestring common category and numericstring common category ',negative
'range starts here ',negative
'equalkey match bytes ',negative
'are not throwing exception since might transient issue that blocking loading ',negative
'consumes whole key ',negative
'store table descriptor maptargetwork ',negative
'this dbtable ',negative
'check whether substitution allowed ',negative
'have failed reserve single header not undo the previous ones here the caller has handle this avoid races ',negative
'future thought checkforcompaction will check lot file metadata and may expensive long term should consider having thread pool here and running checkforcompactions parallel ',negative
'hardcoded from private field need check the path under what sets for namespace since the namespace created with world acls ',negative
'project the big table key into the small table result area ',negative
'produces fileglobal row number even with ppd ',negative
'subtract the spills get all match and nonmatch rows ',negative
'stats bookkeeping ',negative
'use the specified database specified ',negative
'increments one hms connection ',negative
'renamea has interesting behavior and are directories doesnt exist does the expected operation and everything that was now exists will make child thus make sure the rename done before creating the meta files which will create basex ',negative
'middle items order ',negative
'sanity check ',negative
'certain queries like select count from table not have any projected columns and still have isreadallcolumns false such cases columnreaders are not needed however colstoinclude not empty should initialize each columnreader ',negative
'note incomplete cbs are always exact match ',negative
'helper function create edge property from edge type ',negative
'files size for splits ',negative
'insert timezone for timestamp type ',negative
'handle different types create table command note each branch must call after finalizing table properties ',negative
'latin small letter gamma bytes ',negative
'this expression that should perform comparison for sorted searches ',negative
'skipping because hint mark this info ',negative
'divide down just before scaledown get round digit ',negative
'invalid time part ',negative
'project projects the original expressions ',negative
'default serde for rcfile ',negative
'javameta javaref javaarraymeta javaref ',negative
'test ',negative
'rewrite ',negative
'the count the elements the above that are set ',negative
'create the vertex ',negative
'init the rng for breaking ties histogram merging fixed seed specified here aid testing but can eliminated use timebased seed which would make the algorithm nondeterministic ',negative
'newdirtrue stats not updated ',negative
'the fields older and newer match ',negative
'method signature changed hadoop cast provider keyprovider ',negative
'decimal ',negative
'transaction for which the list tables valid write ids are populated ',negative
'set hive provider path hiveconf sethiveproviderpath true simulates property set ',negative
'case ',negative
'set false block the next loop this must called before draining the lists otherwise addcompletion after draining the lists but before setting false will not trigger run may cause one unnecessary run add comes before drain drain list add request settrue setfalse needs avoided ',negative
'extract all the inputformatclass names for each chunk the ',negative
'remove semijoin optimization smb join created ',negative
'tinyint ',negative
'use separate metastore client for heartbeat calls ensure heartbeat rpc calls are ',negative
'encoded values can push directly row ',negative
'construct keys exprnode ',negative
'abc abc abc abc all other cases such abcde ',negative
'key only string ',negative
'drop existing partition bbb via objectstore ',negative
'the getter should remove the escape character for ',negative
'byte ',negative
'txn ',negative
'populate the filters structure ',negative
'multibyte trims ',negative
'option bypass job setup and cleanup was introduced hadoop mapreduce ',negative
'case ',negative
'lsb bits ',negative
'return value modulo but always the positive range and with the mask zero the sign bit make the input mod positive the output will definitely positive ',negative
'the database directory ',negative
'that geti returns null rather than arrayoutofbounds ',negative
'need not any instanceof checks for following ',negative
'filtering handled the input batch processing ',negative
'this class provides support collect mapreduce stderrstdoutsyslogs from jobtracker and stored into hdfs location the log directory layout compact lilogsjobid directory for jobid directory for attemptid since there api retrieve mapreduce log from jobtracker the code retrieve from jobtracker and parse the html file the current parser only works with hadoop for hadoop would need different parser ',negative
'take care ',negative
'also null down undone ',negative
'this file system counter valid and create counter ',negative
'were passing client credentials null since want them read from the subject ',negative
'this the case where distinct cols are part keys which case still need add out put col names ',negative
'important get exception name collision ',negative
'validate true default enable the constraint ',negative
'dummy insert into command mark proper last repl after dump ',negative
'need check whether this transaction valid and open ',negative
'case ',negative
'make random array longs ',negative
'use construct ',negative
'return result ',negative
'the destf this case the replaced destf still preserves the original destfs permission ',negative
'the regexes look for the log files ',negative
'that long see abandoned session ',negative
'this will null slaves ',negative
'invalid zone ',negative
'package permission that can construct but others cannot ',negative
'get databases for schema pattern ',negative
'its populated from lock info ',negative
'the retry logic reached after copy error then include the copied file well this needed cannot figure out which file incorrectly copied expecting distcp skip the properly copied file based crc check copy crc mismatch ',negative
'change the table partition for collecting stats ',negative
'case ',negative
'call describe ',negative
'the primary isnt done push back into the readers ',negative
'run cleaner ',negative
'the writable return serialize ',negative
'connect using the delegation token passed via configuration object ',negative
'got exception that not ioexception typically oom indexoutofbounds internalerror this most likely corruption ',negative
'all the branches need bucketleaves ',negative
'flatten keyvalue pairs into row object for use serde ',negative
'todo type conversion ',negative
'query either ctas need create druid meta data query ',negative
'obtain additional information should try incremental rewriting rebuild will not try partial rewriting there were updatedelete operations source tables ',negative
'set the range the integermax because ',negative
'all values should pass test ',negative
'enabledisable bias correction using table lookup ',negative
'first stripes will satisfy the predicate and merged single split last two stripe will ',negative
'finally create ptf ',negative
'lock manager ',negative
'expected number partitions dropped each those calls ',negative
'try find the method ',negative
'create query ',negative
'this demuxoperator directly connects muxoperator that muxoperator must the parent joinoperator this case that muxoperator should initialized multiple parents that muxoperator ',negative
'loop over all the tasks recursively ',negative
'tests with queries which cannot executed with directsql because type mismatch the type the num column string but the parameters used the where clause are numbers after falling back orm the number partitions can fetched the method ',negative
'this means hive type doesnt refer this field that comes from file schema the field not required for hive table can occur due schema evolution where some field deleted ',negative
'code copied over from udfweekofyear implementation ',negative
'must parse get the escape count ',negative
'need make sure names are set for tez connect things right ',negative
'set data column count ',negative
'none the cases above matched and everything selected hence will use the same values for the selected and selectedinuse ',negative
'write ids ',negative
'unfortunately the metastore api revokes all privileges that match principal privilege object type does not filter the grator username this will revoke privileges that are granted other usersthis not sql compliant behavior need changeadd metastore api that has desired behavior ',negative
'methods ',negative
'now make sure that can reuse the reencoder against completely different record save resources ',negative
'all others ',negative
'target exists ',negative
'assign values vector ',negative
'session creation should fail since the schema didnt get created ',negative
'likely malformed query select hashdistinct from ',negative
'reject default partitions couldnt determine whether should include not note that predicate would only contains partition column parts original predicate ',negative
'always should this order see ',negative
'ensure destination not empty only for regular import ',negative
'nonjavadoc see ',negative
'any operator present which prevents the conversion ',negative
'exists ',negative
'logger the lineage info ',negative
'two known tasks left and complete evicted rejected ',negative
'final vertices need have least one output ',negative
'the qualified table aliases etc ',negative
'add implicit type conversion necessary ',negative
'remove the entries dont get confused later and think should ',negative
'todo buckets not same splits ',negative
'save logging message for logj output latter after logj initialize properly ',negative
'rowid ',negative
'format ',negative
'started ',negative
'nonjavadoc see javasqlclob ',negative
'drop table exists tablename ',negative
'for grouping sets add dummy grouping key this dummy key needs added reduce key for consider select key value count from group key value with rollup assuming mapside aggregation and skew the plan would look like tablescan select groupby reducesink groupby select filesink this function called for groupby create additional grouping key ',negative
'note the first and last element the byte are not used ',negative
'now actually write table generate some partitions ',negative
'compare data header with signature ',negative
'joinleft joingetright ',negative
'might from before the new resource plan ',negative
'optional bytes vertexbinary ',negative
'stagetype ',negative
'either nonmm query load into table from external source ',negative
'retrieve job conf into logdir ',negative
'map tables fullname its ast ',negative
'synthetic conditions there ',negative
'types ',negative
'tracetrace ',negative
'swap and thx ',negative
'',negative
'next byte null byte there are more bytes ',negative
'when there explicit foreign key name associated with the constraint and the key composite expect the foreign keys send order the input list otherwise the below code will break this the first column the constraint generate the foreign key name the below code can result race condition where duplicate names can generated theory however this scenario can ignored for practical purposes because the uniqueness the generated constraint name ',negative
'skew key now ',negative
'last row starting the end the split would read ',negative
'disable memory checking ',negative
'check the rest command specified explicitly use hcatalog ',negative
'these must after nonpartition cols ',negative
'connects using the command line arguments there are two possible ways connect here using the cmd line arguments like using properties propertyfile ',negative
'restore everything default setup avoid discrepancy between junit test runs ',negative
'numattr this means join one single key column ',negative
'allocate memory for the histogram bins ',negative
'add empty checksum string for filesystems that dont generate one ',negative
'remove cyclic dependencies for dpp ',negative
'there distinct aggregation update all aggregations ',negative
'have created hbase table delete roll back ',negative
'locked for defrag ',negative
'put each check separate trycatch that particular cycle fails itll try again the next cycle ',negative
'create two input paths that two map tasks get triggered there could other ways trigger two map tasks ',negative
'well count misses iterate ',negative
'istransactional ',negative
'swsrwait lock are examining waiting this case keep looking its possible that something front blocking ',negative
'cant remainder null ',negative
'build the routing table ',negative
'therefore the maximum total size serialized timestamp ',negative
'avoid intial spike when using multiple ',negative
'only need calculate once itll the same for other partitions this job ',negative
'checkif current row belongs the current accumulated partition not process the current partition reset input partition set currentkey the newkey null has changed ',negative
'dont expect any nesting most cases lot present union and are some examples where would have few levels respectively ',negative
'the value before the list record offset make byte segment reference absolute ',negative
'strategy requested through config ',negative
'the generated unique then this could affect file golden files for tests that run explain queries ',negative
'here then there are open txns and must resolved committed aborted either way there are open txns with the there because delete below has which correct for the case when there open txn concurrency even new txn starts starts commits still true that there are currently open txns that overlap with any committed txn with commitid commithighwatermark set next line plain readcommitted enough ',negative
'optimizer ',negative
'sources represent vertex names ',negative
'hadoop group mapping that maps user same group ',negative
'corresponding with bucket number and hence their ois ',negative
'for cbo provided orderings dont attempt reorder joins only convert consecutive joins into nway joins ',negative
'inbuilt assumption that the testdir has only one output file ',negative
'incrementset input counters ',negative
'create placeholder entry with pending state ',negative
'used ptfs ',negative
'with the next argument being for beelineopts ',negative
'max threshold for cnf conversion having elements andlist will converted maybe ',negative
'call ',negative
'',negative
'rule and passes the context along ',negative
'the mapping from newtag the index the corresponding child ',negative
'first actually give duck ',negative
'print name ',negative
'checking for deletedelta only that this functionality can exercised code which cannot produce any deltas with mix updateinsert events ',negative
'generate the result for the windowing ending the current row ',negative
'pick trust store config from the given path ',negative
'for updates first column rowid ',negative
'for and listssingleaddcall ',negative
'add not null constraint check ',negative
'trim off ending any ',negative
'call listlocatedstatus mockmocktbl call check existence side file for mockmocktbl ',negative
'there could more scheme after execution execution might accessing different filesystem dont find matching scheme before execution just use the after execution values directly without computing delta difference ',negative
'http mode ',negative
'currently the algorithm flushes the entries this can changed the future ',negative
'avro only allows maps with string keys ',negative
'support both listobject and object have differently ',negative
'need map from the reducer the corresponding reducework ',negative
'data prematurely ended return start dont move our field position ',negative
'looks like operation ',negative
'after decoding can push value ',negative
'the vectorrow deserialization the one row into the vectorizedrowbatch ',negative
'add dummy instances all slots where llaps are mia can haz insertiterator ',negative
'update aggregations the aggregation for distinct case hash aggregation the client tells whether new entry for sortbased aggregations the last row compared with the current one figure out whether has changed cleanup the lastinvoke logic can pushed the caller and this function can independent that the client should always notify whether different row not param aggs the aggregations evaluated param row the row being processed param rowinspector the inspector for the row param hashaggr whether hash aggregation being performed not param newentryforhashaggr only valid hash aggregation whether new entry not ',negative
'convert the first entries ',negative
'some rows have already been assigned values assign the remaining cannot use copyselected method here ',negative
'schematype ',negative
'setup appropriate ugi for the session ',negative
'delegate the groups converters ',negative
'',negative
'this thismag thisscale right rightmag rightscale this right thismag rightmag thisscale rightscale ',negative
'all other ops using txns row ',negative
'the following data should changed other data should the same ',negative
'numeric primitive type ',negative
'thisisouterjoin isouterjoin primitivetypeinfo primitivetypeinfos new readstringresults byteswritable new byteswritable ',negative
'the number writers seems based number jobs for the src query todo check number filesinks length length ',negative
'',negative
'last group batch take the nonstreaming group aggregation values and write output columns for all rows every batch the group each group batch finished being written they are forwarded the next operator ',negative
'',negative
'implementation listobject and assorted methods ',negative
'falling back ',negative
'otherwise sleep and try again ',negative
'nothing ',negative
'second three was selected ',negative
'not using method because partitioning columns are also treated virtual columns columninfo ',negative
'show throw ',negative
'here the slow path that can return info txns which were not expected state ',negative
'print out the sizes pretty set print out human friendly format otherwise print out were row ',negative
'set col expressions for the dynamic values using this input ',negative
'cached values save round trips database ',negative
'ranges implies that there are possible ranges lookup ',negative
'decimal comparison ',negative
'append additional virtual columns for storing statistics ',negative
'when and has children some conjunction over field that isnt the column mapped the accumulo rowid and when conjunction generates ranges which are empty the children the conjunction are disjoint these two cases need kept separate null andranges implies that ranges couldnt computed while empty list ',negative
'make the following condition all the values match for all the columns ',negative
'only initialize once the tasks that need run periodically ',negative
'testing setting length larger than array length which should cap the length itself ',negative
'newbucketcollist had null value means that least one the input bucket columns did not have representative found the output columns assume the data longer bucketed ',negative
'tablenamepattern string columnnamepattern ',negative
'the original lists not contain collisions only one old ',negative
'prepare data good for any implementation variation ',negative
'remove the partition paths know about ',negative
'this method gets called multiple times hive some invocations the properties will empty need detect when the properties are not empty initialise the class variables see javautilproperties ',negative
'empty strings are marked invalid utf single byte sequence valid utf stream cannot ',negative
'this skew key need handle separate map reduce job ',negative
'add the merge job ',negative
'should not happen here this for replication ',negative
'are going something useful now ',negative
'create one dummy lock can through the loop below though only really need txnid ',negative
'test string column varchar literal comparison ',negative
'used log memory usage periodically ',negative
'get the size cache before ',negative
'rebuild the tree since original empty ',negative
'get the groupby aliases these are aliased the entries the select list ',negative
'assume that since are joining the same key all tables would have either optimized nonoptimized key hence can pass any key any table reference that mjkb could determine whether can use optimized keys ',negative
'small table indices has more information keys than retain use exists ',negative
'set test data ',negative
'compute deltas and write the values varints ',negative
'change future ',negative
'invalid load path ',negative
'used queue requests while the sparkcontext being created ',negative
'task requested host got host host full ',negative
'multikey get key ',negative
'dont this optimization with updates deletes ',negative
'optional required optional ',negative
'sessionstatedriver needs restarted with the tez conf settings ',negative
'set temp file containing results sent hiveclient ',negative
'ignore updates that occured before this cached query was created ',negative
'used determine whether child tasks can run ',negative
'making sure treat dynamic partitioning jobs they were immutable ',negative
'mystring ',negative
'each element the array startpositioni startpositioni ',negative
'remove the reducesinkoperator ',negative
'ival ',negative
'java cruft pair long ',negative
'for alter table exchange partition need select delete input insert output ',negative
'traverse through the from string one code point time ',negative
'this record produced result this time remove from the storage will not need produce result with null values anymore ',negative
'reduce sink contains the columns needed need aggregate from children ',negative
'setup the new map work ',negative
'start generate multiple map join tasks ',negative
'connect parentchild work with brodacast edge ',negative
'',negative
'create and attach the filesink for the merge ',negative
'correlated vars across subqueries within same query needs have different ',negative
'columns which are bucketedsorted ',negative
'are left with single child return the child ',negative
'the array index cant reserved ',negative
'sending kill message the right here dont need wait for the task complete ',negative
'kill always takes priority over move ',negative
'remove cast boolean not null boolean vice versa filter accepts nullable and notnullable conditions but cast might get the way other rewrites ',negative
'above get doesnt set ',negative
'perform spnego login using the hadoop shim api the configuration available ',negative
'uninitialized vertices will report count ',negative
'not need anything the expression probably pushed previously ',negative
'nway join all later small tables for all later small tables follow the same pattern the previously loaded tables ',negative
'replica ssd ',negative
'validate noscan ',negative
'setting the comparison equal the search should use the block ',negative
'grab round digit from lowest word ',negative
'',negative
'adding constant memory for stringcommon the rabit hole deep implement memoryestimate interface also constant overhead ',negative
'enable retries work around bonecp bug ',negative
'this record larger than maxkey need stop ',negative
'eventconsumer invalidate cache entries based metastore notification events alter table add partition etc ',negative
'cause was useless intermediate cause and was replace with its own cause ',negative
'convert lazy object ',negative
'the table missing either due droprename which follows the operation ',negative
'compare strings char comparison semantics may different ifwhen implemented ',negative
'simple thread wait until the server has started and then signal the other threads begin ',negative
'last resort ',negative
'bail out can not infer unit ',negative
'the table partitioned need select the partition columns well ',negative
'get the first sel after ',negative
'mark the write ids state per the txn state ',negative
'value null not found exception would get thrown ',negative
'audience cant exist its own ',negative
'reopen happens even when close fails theres not much here ',negative
'add any required resources ',negative
'dont think this can have any filesinkoperators more future proofing ',negative
'remember the connections between and event ',negative
'nonjavadoc see ',negative
'draw and replace ',negative
'',negative
'need preserve currenttimestamp ',negative
'for kafka streaming tables mandatory set kafka topic ',negative
'setstring can override this null propagation ',negative
'check the default values are set for all unfilled attributes ',negative
'for each struct subfield create equivalent resourcefieldschema ',negative
'for tables directory structure ',negative
'get sum for all columns reduce the number queries ',negative
'have conflict the number reducers will not optimize this plan from here ',negative
'checking for status ',negative
'prepare the constant for use when the function called used during initialization ',negative
'there overlap between columns and partitioning columns ',negative
'ensure pig can read data correctly ',negative
'make acid table and make sure assign rowids correctly ',negative
'convert all the children cnf ',negative
'mapjoin mergejoin make sure that they are the reduce side otherwise bail out ',negative
'there should already instance the session pool manager not ignoring fine while stopping hiveserver ',negative
'this operator cannot converted mapjoin cause output expected sorted join key ',negative
'there are nulls our column ',negative
'the vectorizer class enforces that there only one tablescanoperator dont need the more complicated multiple root operator mapping that mapoperator has ',negative
'this code pretty much completely based hadoops the only reason could not use that hadoop class asis was because needs serverconnection object which relevant hadoop rpc but not here the metastore the ',negative
'after this the noninitial refcounts are the responsibility the consumer ',negative
'unpack the output ',negative
'setting empty list results reading none ',negative
'intvalue ',negative
'compress the owids into compressedowid data structure that records ',negative
'here someone must have added new work object should walked find filesinks ',negative
'let the schema and version auto created ',negative
'all column types ',negative
'',negative
'check the list where expressions already added they arent duplicated ',negative
'test serialization and deserialization with different schemas ',negative
'only preempt the task being preempted below the dag ',negative
'require view ownership for alterdrop view ',negative
'create map join task for the given big table position ',negative
'this case the result will surely more than bit even after division ',negative
'end loop over pending tasks ',negative
'get the output schema ',negative
'have check receive prefix partition keys table scheme like tabledshr archive partition will work and archive partitionhr wont ',negative
'parameters null means the input tablesplit empty ',negative
'wait for seconds before checking any init error init should fast error need make this configurable ',negative
'extend any repeating values and nonulls indicator the input ',negative
'matching result for attemptid input ',negative
'uri ',negative
'test drop database ',negative
'serde the throwable string parse for the root cause ',negative
'nope look see our conf dir has been explicitly set ',negative
'invalid case ',negative
'serialize rest the field the aggbuffer ',negative
'total number input rows needed for hash aggregation only ',negative
'this probably doesnt need sync but nobody calls this doesnt matter ',negative
'clone joincond ',negative
'note assume production llap always runs under yarn ',negative
'the aggregation type avg use the average the existing ones ',negative
'get rid sqcountcheck group key constant hive ',negative
'false positives probability are ready tolerate for the underlying bloom filter ',negative
'replaces the join operator with new commonjoinoperator removes the ',negative
'ignore for now will probably try send the count already have again are assuming here that cant talk will eventually fail ',negative
'get the field out struct ',negative
'skip incompatible file files that are missing stripe statistics are set incompatible ',negative
'current cookie name current cookie value ',negative
'ignore agg calls which are not distinct have the wrong set arguments were rewriting aggs whose args are sal will rewrite countdistinct sal and sumdistinct sal but ignore countdistinct gender sumsal ',negative
'skip the big tables ',negative
'create partitions ',negative
'insert overwrite ',negative
'map zexpr for ',negative
'allocate next null byte ',negative
'common inner bigonly join result processing ',negative
'read sync bytes ',negative
'batch full using too much space ',negative
'test will local mode ',negative
'check for overflow ',negative
'hash table memory usage allowed used case nonstaged mapjoin ',negative
'set the foreign key constraints properly the tabcolstats data ',negative
'current state final state notify spark job ids before notifying about the state transition ',negative
'',negative
'set the backup task from curr task ',negative
'get all the ops ',negative
'truncate slice byte array maximum number characters and return the new byte length ',negative
'convert join gby semijoin ',negative
'because llap arrow output depends the code path this required for row outputs need write size batch signal eos the consumer ',negative
'close closing open scope should ',negative
'blocks particular size well try split yet larger blocks until run out ',negative
'parse out the context and havent already done and while were also parse out the precision factor the user has supplied one ',negative
'fill host with tasks leave host empty try running task host should preempt await preemption request try running another task host should preempt await preemption request ',negative
'for development can add and and state nvaestatenumber ',negative
'not power two add one more ',negative
'the set object containing the list this optimized for lookup the data type the column ',negative
'the number entries added the hash table since the last time checked the average variable size ',negative
'this makes the jars available sqoop client ',negative
'checks the caller must ensure the offsets are correct ',negative
'the class name the generic udf being used the filter ',negative
'one child ',negative
'cleaner would remove the obsolete files ',negative
'commenttest columns name col type string format storedas rcfile ',negative
'bgenjjtree header ',negative
'given that not delete empty slot means match ',negative
'stats are already present and forcerecompute isnt set nothing ',negative
'throws hcatexception see ',negative
'either argument string convert double decimal because number string form should always convertible into either those ',negative
'',negative
'level create col col count for each branch ',negative
'check for max input size ',negative
'insert overwrite path ',negative
'hive remove parallelism check the beeline test flakyness gets fixed int numthreads ',negative
'enable extended nesting levels ',negative
'bigint ',negative
'unknown ',negative
'there sortmerge join followed regular join the smbjoinoperator may not get initialized all consider the following query smb join ',negative
'this means the name empty ',negative
'ensure child size ',negative
'this the constructor use for the bucket map join case ',negative
'text string ',negative
'need use the current cluster for the scan operator views ',negative
'tagtoinput for reduce work ',negative
'merge the files the destination tablepartitions creating maponly merge job underlying data rcfile orcfile rcfileblockmerge task orcfilestripemerge task would created ',negative
'transform first insert branch into update ',negative
'nothing soft references will clean themselves ',negative
'set the output record fiddling with the pointers that can ',negative
'and finally were ready create and start the session ',negative
'check the partitions dont exist the sourcetable ',negative
'verify fields were altered during the altertable operation ',negative
'after super analyze readentity defaultacidtblpart writeentity defaultacidtblpart tableinsert after udsa read defaultacidtblpart write partitionupdate partitionupdate todo why acquire per partition locks you have many partitions thats hugely inefficient ',negative
'serialize keyrow into key bytes ',negative
'read the fields ',negative
'nonjavadoc see ',negative
'need enforce nullability applying additional cast operator over the transformed expression ',negative
'cursor for the inlist array cursor for element list per innot inclause cursor for inclause lists per query ',negative
'maps recursively compare the key and value types ',negative
'decimaltxt ',negative
'something preventing metastore from starting ',negative
'test select namedstruct from ',negative
'create partition key schema ',negative
'list map join ',negative
'for while ',negative
'authorize drops there was drop privilege requirement ',negative
'set this multiple times ',negative
'call the corresponding handler evaluate this row and forward the result ',negative
'nonjavadoc see javalangstring int ',negative
'current code version datas version datas version exceptions expected ',negative
'the big table ',negative
'map keep track what reduce sinks have hooked ',negative
'all child expressions deterministic function are constants evaluate such udf immediately ',negative
'must deterministic order map see hive ',negative
'some part the requested range not cached the cached offset past the requested ',negative
'decide whether rewrite subquery ',negative
'get own kerberos credentials for accepting connection ',negative
'true when the random access readfield method deserializeread are being used ',negative
'next this for tables and partitions ',negative
'create view has limit operator this can happen fetch parent operator ',negative
'throws the new column types are not compatible with the current column types ',negative
'this case have find out which columns can merged ',negative
'mock inputs ',negative
'update the partitions for table cache ',negative
'partial null then there was overflow and myaggsum will marked not set ',negative
'session already has violation ',negative
'nonjavadoc see ',negative
'might have been deleted already ',negative
'this only mode just initialize the dfs root directory ',negative
'replicate all the events happened far ',negative
'default case ',negative
'couldnt find the parent insert replace with allcolref ',negative
'add path set ',negative
'test that underflow produces null ',negative
'execution mode not set null returned ',negative
'timestamp columnscalar where scalar really cast constant timestamp ',negative
'begin write commit ',negative
'encountered numeric value extract out the entire number ',negative
'recreate cluster ',negative
'only predicate supported right now this has extended support expression tree when multiple conditions are required hive ',negative
'the mapping from newtag its corresponding oldtag oldtag the tag assigned reducesinkoperators before correlation optimizer optimizes the operator tree newtag the tag assigned reducesinkoperators after correlation optimizer optimizes the operator tree example have operator tree shown below join gby join gby join and join are executed the same reducer optimized correlation optimizer will have oldtag newtag need know the mapping from the newtag oldtag and revert the newtag oldtag make operators the operator tree ',negative
'allow multiple foreign keys snowflake schema csfkssize parentssize means have single and all ',negative
'the absence column statistics the estimated number rowsdata size that will ',negative
'populate pathtopartitioninfo and pathtoaliases paths ',negative
'disruptor logjapi logjcore logjslfj logjapi needed for ndc ',negative
'ensure theres more invocations ',negative
'some version upgrades often dont change schema they are equivalent version that has corresponding schema equivalent ',negative
'privilege ',negative
'the set routine enforces precision and scale ',negative
'test that setting read all resets column ids ',negative
'add the expr reducekeys not present ',negative
'must inside together with queries ',negative
'still exist ',negative
'set admin role user belongs there ',negative
'forwarding ',negative
'the response will have one entry per table and hence get only one openwriteids ',negative
'none the expressions are constant nothing ',negative
'create scratch dir without lock files ',negative
'falling off and the executor service being ready schedule new task ',negative
'removing headerlogix from getsecondintlogix head logix ',negative
'change body implemented methods use file settings file templates ',negative
'already contains stats stats not updated when forcerecompute isnt set ',negative
'just check nametype for equality dont compare comment ',negative
'toktablepartition ',negative
'just loop through all values not need store anything though this just for test purposes ',negative
'stats from writer ',negative
'configuration for async thread pool sessionmanager ',negative
'successfully converted agg calls into projects ',negative
'and not column because that how data processed ',negative
'filesystemcachemap ',negative
'the same size sort file name followed startposition ',negative
'older committed state equivalent newer state then there should committed ids between oldhwm and newhwm and newinvalidids should have exactly newhwm oldhwm ',negative
'nonhash aggregation ',negative
'then return immediately ',negative
'should not happen since the input were verified before passed ',negative
'',negative
'created using hint skip ',negative
'not all clear how flatten these last two out useful way and one uses ',negative
'allocate the various arrays ',negative
'update max register value ',negative
'the user has already been notified completion sessioninitcontext ',negative
'drop view ignore error ',negative
'need also push projections calling setoutputschema hcatinputformat have get the requiredfields information from the udfcontext translate schema and then pass the reason this here because setlocation called pig runtime and time are not sure when hcatinputformat needs know about pruned projections doing here will ensure communicate hcatinputformat about pruned projections getsplits and createrecordreader time ',negative
'add metric with nonnull value ',negative
'get all the failure execution hooks and execute them ',negative
'note this real query that depends one the metastore tables ',negative
'statistics not implemented currently ',negative
'binary mode ',negative
'first entry ',negative
'this point have delta files for insert for update should push predicate into one but not the following select were push into the update delta wed filter out before doing merge and thus produce the value for row the right result rows ',negative
'remaining column expressions would candidate for value ',negative
'sort columns ',negative
'only one the tasks should ever added restsks ',negative
'completion notifier vars ',negative
'now set the response headers ',negative
'make sure nonnullvalued confvar properties override the hadoop configuration ',negative
'used kryo ',negative
'mark this entry being use caller will need release later ',negative
'requires conditional evaluation for good performance ',negative
'case might get called repeatedly ',negative
'validate the second parameter which the number histogram bins ',negative
'test that write blocks two writes ',negative
'note this assumes that the pattern where the same session object reset with different tez client not used was used lot the past but appears gone from most session pool paths and this patch removes the last one reopen ',negative
'assert class invariant ',negative
'disable dictionary encoding for the writer ',negative
'verify dirs ',negative
'why are still using writables ',negative
'some query attempted lock thus lockwaiting state but giving due timeout for example ',negative
'current getinputsummary returns for each file found current getinputsummary returns for each file found ',negative
'for testing only ',negative
'some the conversion methods throw this exception numeric parsing errors ',negative
'currently only optimized the query the content the from clause ',negative
'after compactioncleanup all entries from txntowriteid should cleaned all txns are committed ',negative
'nonjavadoc see javalangobject int ',negative
'parse string query hint ',negative
'false and true the input vector simple dictionary used with two entries references false and references true the dictionary ',negative
'subtract out the bits just added ',negative
'the expected number distinct values when choosing values with replacement from integers have several uniformly distributed attributes with distinct values they behave one uniformly ',negative
'are committing this vertex vectorized ',negative
'runas ',negative
'setup scratch batch that will used play back big table rows that were spilled ',negative
'have correlated column build data type from outer ',negative
'loginforeading offset prevheadoffset lrptroffset ',negative
'killed something ',negative
'need deserialize and serialize query intervals are written the json druid query with user timezone this default hive time semantics ',negative
'spot check only null repeating behavior are checked elsewhere for the same template ',negative
'have register this front right now otherwise its possible for the task start ',negative
'numhashfunctions byte numbits bytes ',negative
'hdfs warehouse ',negative
'only get here could map all join keys source table columns ',negative
'process singlecolumn long inner join vectorized row batch ',negative
'construct using ',negative
'required for deserialization ',negative
'since allow write operations cache while prewarm happening dont add databases that were deleted while were preparing list for prewarm skip overwriting exisiting object which present because was added after prewarm started ',negative
'remove ending and starting ',negative
'change the query use partvals instead the name which ',negative
'the dateformat not provided the user invalid use the default format yyyymmdd ',negative
'ignore the interrupt status while returning the session but set back the thread case anything else needs deal with ',negative
'once have decided the map join the tree would transform from join mapjoin big table small table for tez ',negative
'check that delta dir has version file with expected value ',negative
'defer ',negative
'pretend its vectorized the nonvector wrapped enabled ',negative
'thanks hbase storage handler ',negative
'type ',negative
'for tables other than the big table need fetch more data until reach new group done ',negative
'start the heartbeat after delay which exceeds the hivetxntimeout ',negative
'bgenjjtree typedefinition ',negative
'this hook used for verifying the table access key information that generated and maintained the queryplan object the tableaccessanalyer all the hook does print out the tablekeys per operator recorded the tableaccessinfo the queryplan ',negative
'launch hadoop command file windows ',negative
'updates deletes inserts ',negative
'startrow inclusive while stoprow exclusive this util method returns very next bytearray which will occur after the current one padding current one with trailing byte ',negative
'the hash map for this specialized class ',negative
'insert overwrite table temp select from temp such case select will only have one instance and would have two locating bucketcol such cases will generate error bail out ',negative
'whether this deduplicated ',negative
'try infer possible sort columns the query the sequence must prsselfsparent ',negative
'also need the expr for the partitioned table ',negative
'with the high message size limit this connection should work ',negative
'the sel operator the semijoin branch there should only one column the operator ',negative
'set the operator plan after generating splits that changes configs ',negative
'set can add them the list input cols check ',negative
'this will happen only when loading tables and reach the limit number tasks can create hence know here that the table should exist and there should lastpartitionname ',negative
'this does not work hence opening and closing file for every event writerhflush ',negative
'order overlapping keys should exactly the same ',negative
'this entry point are going assume that these are logical table columns perhaps should thru the code and clean this more explicit for now will start with this single assumption and maintain clear semantics from here ',negative
'are dropping from unmanaged unset the flag and vice versa ',negative
'register information about created predicates ',negative
'partitionspecs ',negative
'set backup task ',negative
'need verify that when reading datum with updated reader schema that the datum then returns the reader schema its own since depend this behavior order avoid reencoding the datum ',negative
'set the first one active others are backups ',negative
'exceptions including interruptexception and other keeperexception ',negative
'required optional ',negative
'normal case the last parameter normal parameter conversionhelper can called without method parameter length checkings for terminatepartial and merge calls ',negative
'mocked session starts with default queue ',negative
'bucketjoin possible need correct bucketing ',negative
'trigger the creation llap registry client use clients may using different ',negative
'meta store check command equivalent add partition command input objects are passed currently but keeping admin priv requirement inputs just case some input object like file ',negative
'create default socket factory based standard jsse trust material ',negative
'database not the one currently using ',negative
'verify create tablefunction calls only add partitions ',negative
'max disabled can safely return false ',negative
'implemented navigable set protected single lock and using conditions manage blocking ',negative
'ignored ',negative
'create file sink operator for this file name ',negative
'will required ',negative
'add partition metastore for dynamic partition make metastore call for every new partition value that encounter even partition already exists exists check require metastore call anyways ',negative
'directly serialize fieldbyfield the lazybinary format this alternative way serialize than what provided lazybinaryserde ',negative
'should not remove the dynamic partition pruner generated synthetic predicates ',negative
'created tables under ',negative
'create new columninfo replacing structcolumn with structcolumn ',negative
'the order the join condition expressions dont matter merge can happen every target condition present some position the node condition list there node condition which not equal any target condition ',negative
'always show array ',negative
'need localize the additional jars and files ',negative
'alternate unused ',negative
'type ',negative
'int ',negative
'address the colexp collist etc for the sel ',negative
'assign all remaining rows ',negative
'add new request executed ',negative
'the ptned table should miss target the table was marked virtually dropped ',negative
'this may happen for schemaless tables where columns are dynamically supplied serdes ',negative
'cache disabled dont use ',negative
'stageattributes ',negative
'metastore stuff sure update hiveconfmetavars when you add something here ',negative
'simulates the set command ',negative
'files size for splits ',negative
'these methods are required serialization ',negative
'use both args ease development delete this one may ',negative
'password ',negative
'check entries beyond first ',negative
'successfully perform compaction tablepartition that have successful records ',negative
'not change the initial bytes which contain ',negative
'merge the two partials ',negative
'threshold switch from sparse dense encoding ',negative
'thread local conf from hms ',negative
'trivially retryable ',negative
'casts exact types including long double etc are needed some special cases ',negative
'this test assumes the hivecontrib jar has been built part the hive build also dependent the udfexampleadd class within that jar ',negative
'this what allows the crossdomain reads the contents only apply idempotent get ops all others need crumbs ',negative
'note the default value for null fields vectorization for int types nan for ',negative
'the pruning needs preserve the order columns the input schema ',negative
'make path ensure slash the end ',negative
'left key only needs adjusted there are system ',negative
'allocated caller ',negative
'add shutdown hook for catching sigterm sigint ',negative
'object overhead bytes for bitcount bytes for bitlength bytes for firstnonzerobytenum bytes for firstnonzerointnum bytes for lowestsetbit bytes for size magnitude since max precision only for hivedecimal bytes padding since java memory allocations are byte aligned ',negative
'get the column names and their corresponding types ',negative
'various utility method ',negative
'walk through udaf and add them ',negative
'different tasks ',negative
'optional int guaranteedtaskcount ',negative
'verify that returns events after specified event ',negative
'this proves data written acid layout was made acid ',negative
'handle map can read list struct data liststructkey value mapkey ',negative
'failed set job status completed which mean the main thread would have exited and not waiting for the result call cleanup execute any cleanup ',negative
'map since user group checks and config files are terms short name ',negative
'multikey hash map based the ',negative
'create the route objects based the nodes ',negative
'backtrack partition columns crs prs ',negative
'stored directories ',negative
'construct column name list for reference filter push down ',negative
'split since only have bucket file base delta not flushed committed yet empty ',negative
'test get stats column for which stats doesnt exist ',negative
'use the original bytes case decoding should fail ',negative
'gmt zone gmt pst ',negative
'bootstrap repl and then export table ',negative
'exact numbers power can the same ',negative
'join ',negative
'this must final map reduce task the task containing the file sink operator that writes the final output ',negative
'',negative
'note sessions torestart are always use they cannot expire parallel ',negative
'count number null values seen far ',negative
'defaultvalue ',negative
'vars that are not used the join key ',negative
'explicit avro serialization not supported yet revert default ',negative
'todo split count not the same buckets ',negative
'write the base ',negative
'the first child directory then rest would directory too according hcatalog dir structure recurse that case ',negative
'there path element other than report but not fail ',negative
'since maxdepth not yet reached are missing partition columns currentpath ',negative
'update has writer but which creates buckets where the new rows land ',negative
'user from ',negative
'equalkey match big length ',negative
'',negative
'this function determines whether sparkpruningsink with mapjoin this will called check whether the tree should split for dpp for mapjoin wont also called determine whether dpp should enabled for anything other than mapjoin ',negative
'testsputnew undercol ',negative
'happen since layer either knows how produce rowid not but safe ',negative
'have unset the env workarounds they dont confuse each other between tests ',negative
'setup some helper config variables ',negative
'create payload ',negative
'operationstarted ',negative
'write the number elements sparse map required for reconstruction ',negative
'for deserialization ',negative
'verify that there data the resultset ',negative
'perform casting using hive rules ',negative
'found all the operators that are supposed process ',negative
'response written response headers mapoutput ',negative
'hand reset the big table columns ',negative
'day granularity ',negative
'process global init file hiverc ',negative
'drop all tables ',negative
'creates the static cache ',negative
'cost transferring map outputs gby operator ',negative
'mysql returns the string not wellformed numeric value return bytevalueof but decided return null instead which more conservative ',negative
'make sure negative numbers comes before positive numbers ',negative
'figure out what have read ',negative
'partial aggregation not done for distincts the mapper however the data bucketedsorted the distinct key partial aggregation can performed the mapper ',negative
'compare must compare with scaling updown ',negative
'the write count does not matter the map will fail its first ',negative
'version guava the classpath depending the deploy mode ',negative
'end relbuilderjava ',negative
'',negative
'sum input and output are decimal any mode partial partial final complete ',negative
'add our base the list directories search for files ',negative
'the output partial aggregation struct containing ',negative
'session being killed need coordinate between that and the user these two cases dont need distinguished for now ',negative
'will update current number open txns ',negative
'shouldnt cast strings other types because that can break original data cases leading zeros zeros trailing after decimal point ',negative
'source table spec for tablescanoperator same for filesinkoperator same for filesinkoperator aggregation key prefix are stats completely reliable ',negative
'first row determines isgroupresultnull and double firstvalue stream fill result repeated ',negative
'check result now ',negative
'open new client session ',negative
'now weve got table check that works ',negative
'previous base directory should stay until cleaner kicks ',negative
'allow set and dfs commands used during testing ',negative
'when weve buffered the max allowed spill the oldest one make space ',negative
'let function decide can handle this special case ',negative
'remove backlink ',negative
'for complex object serialize json format ',negative
'coldouble ',negative
'only expect here because well get whichever the partitions published its stats last ',negative
'long columnscalar ',negative
'check result ',negative
'convert object types used the authorization plugin interface ',negative
'handle overflow precision issue ',negative
'collect the dynamic pruning conditions ',negative
'requested logger not found add the new logger with the requested level ',negative
'todo the way add hash fns does exhibit some irregularities seems like the iter has better distribution many cases even better that the original hash that trips the above criteria even the rest flat ',negative
'todo nested ',negative
'not specified ',negative
'skip the child unique key part not projected ',negative
'this point one will take the write lock and update can the last check ',negative
'initialize record writer with connection and write info ',negative
'boolean keysareequal currentkeys null newkeys null newkeys false ',negative
'memory for compaction map job minor compaction more than delta dirs major compaction more than ',negative
'lsb bits used locate offset within the block ',negative
'same reason above this the case when have the main work item after the merge work has been created for the small table side ',negative
'add all input columns ',negative
'',negative
'preserved table ',negative
'event truncate last repl repldumpidxy ',negative
'satisfying precondition means column statistics available ',negative
'nonjavadoc see ',negative
'see comment dumping rows via sql for why this doesnt work pig hive tget lget ',negative
'query qualify for the optimization ',negative
'open files ',negative
'add hive operator level statistics recordsin recordsout ',negative
'check that all the jars are added the classpath ',negative
'schema should same ',negative
'this indicates there second vint containing the additional bits the seconds field ',negative
'union keys ',negative
'translate the grouping set bit field into boolean arrays ',negative
'get valid window function spec ',negative
'for the big table only need promote the next group the current group ',negative
'lock the buffer validate and add results ',negative
'this not join condition ',negative
'not want modify the writable provided the object since not copy ',negative
'this where cut the tree described above also remember that ',negative
'partitions updated ',negative
'querys session has compile lock timeout sec should ',negative
'constraintname ',negative
'insert overwrite one partition with multiple files ',negative
'lockidinternal ',negative
'allow tcp keep alive socket option for for hiveserver maximum timeout for the socket ',negative
'undone convert byte character ',negative
'now have the roottasks set for insert select ',negative
'try with parameterized varchar types ',negative
'taken care higher level ',negative
'are seeing this mapjoin for the first time initialize the plan are seeing this mapjoin for the second later time then atleast one the branches for this mapjoin have been encounered join the plan with the plan created ',negative
'but the writeentity complete ddl operations instead ddl sets the writetype use determine its lockmode and first check the writetype was set ',negative
'there should calls create partitions with batch sizes ',negative
'single node may fail ',negative
'not authorized this implementation operation allowed ',negative
'from javautilcalendar ',negative
'check config properties expected with embedded metastore client ',negative
'this could replaced bucketing bucketed fetcher for next ',negative
'todo extract interface when needed ',negative
'nonjavadoc see javaioinputstream long ',negative
'find the common class for type conversion ',negative
'rename unpartitioned table ',negative
'initialize ',negative
'steptracker should now which indicates has started ',negative
'make the partition target not empty ',negative
'drop cascade functions dropped cascade ',negative
'these will become ',negative
'destination file present and checksum source mismatch then retry copy ',negative
'were cloning the operator plan but were retaining the original work that means that root operators have replaced with the cloned ops the replacement map ',negative
'column doesnt appear partition column for the table ',negative
'this not mistake catname the where clause twice ',negative
'found udf metastore now add the function registry ',negative
'rewrite case into nvl ',negative
'get the first record ',negative
'return the compressioncodec used for this file ',negative
'user might have only specified partial list partition keys which case add other partition keys partspec ',negative
'the entire storage xffs means xffs means ',negative
'add new table via cachedstore ',negative
'copy with buffer not close ',negative
'test february leap year viewed due days diff from ',negative
'restore original values ',negative
'release this shared timer resource ',negative
'this mapjoin but not suited for sort merge bucket map join check outer joins ',negative
'remove old parents ',negative
'all partitions are altered atomically all prehooks are fired together followed all post hooks ',negative
'',negative
'create table must fail ',negative
'was never called want start with first txn ',negative
'create the column stats table ',negative
'nothing here silently return ',negative
'the copytobuffer will reposition and reread the input buffer ',negative
'case test just close the log files not remove them ',negative
'our dbname equal but tablename blank were interested this dblevel event ',negative
'doas not enabled pass the principalkeypad sparksubmit order support the possible delegation token renewal spark ',negative
'max allow tez pick ',negative
'setup our batch with the same column schema the output columns plus any scratch columns since the overflow batch will get forwarded children operators ',negative
'mystringenummap ',negative
'this handle synpos where pos headerend ',negative
'cut prefix from hives map key ',negative
'tarjans algo ',negative
'reset just the value columns and value buffer ',negative
'retrieve settings hiveconf that arent also set the jobconf ',negative
'setup ',negative
'must spark branch ',negative
'share the same write buffers with our value store ',negative
'set inner schema for dtype ',negative
'either initialcapacity too large overflows ',negative
'replace the current task with the new generated conditional task ',negative
'security property names ',negative
'case max list members max query string length and exact members single clause ',negative
'nonempty java opts with bad xmx specification ',negative
'count ',negative
'partition columns occur data want remove them find out positions partition columns schema provided user also need update the output schema with these deletions ',negative
'merge work only needs input and output ',negative
'replace the task with the new task copy the children and parents the old ',negative
'are running this constructor has the same behavior but the default was changed add wrapping and newlines ',negative
'this point the task has been added into the queue may have caused eviction for some other task ',negative
'were visiting terminal weve created ourselves just skip and keep going ',negative
'there could interval where desired counter value not populated the time make this check ',negative
'test mode print the logs the output ',negative
'after that heuristic used decide ',negative
'max tolerable variance for matches ',negative
'used for sending information for scheduling priority ',negative
'nonjavadoc see ',negative
'already encoded thriftable object thriftformatter ',negative
'javacc not edit this line ',negative
'actionexpression ',negative
'sorting the keys from the properties helps create deterministic url which tested for various configuration ',negative
'the list being drained cannot increase the delta anymore ',negative
'serializationlib ',negative
'mapside work ',negative
'the alias may not present case subquery ',negative
'this for runtime minmax pushdown dont need not between ',negative
'sigh ',negative
'appropriate locations ',negative
'see the comment the other issamekey ',negative
'second group ',negative
'verify that when stats are already present and forcerecompute specified they are recomputed ',negative
'close the inner output stream before closing the outer output stream for chunked output this means dont write endofdata indicator ',negative
'include type name for precisionscale ',negative
'this inspector initialized still need ',negative
'this sql standard average zero items should null ',negative
'key index can nullnull there only single stripe just start fresh ',negative
'set vertexmanagerplugin whether its cross product destination vertex ',negative
'this not transactional ',negative
'negative number ',negative
'cannot slice compressed files ',negative
'notifies when memory usage after ',negative
'accumulate the counts ',negative
'set primary key name null before sending listener ',negative
'unlike may call this method multiple times for each ',negative
'our input repeating inputcolnumber ',negative
'column authorization checked through table scan operators ',negative
'nothing needs done ',negative
'run cleaner should remove the delta dirs and old base dir ',negative
'the local batch has been consumed entirely reset ',negative
'try eat trailing blank padding ',negative
'set common hash for this job that when create any temporary directory later guaranteed unique ',negative
'optional int signaturekeyid ',negative
'construct the astnode for the column that will join with the outerquery expression for select from where select from this will build toktableorcol identifiersq identifierb where the alias generated for the subquery ',negative
'make sure isnt ',negative
'int hadoopmem ',negative
'delete data ignore unknowndb cascade ',negative
'these two are used indicate that are running tests ',negative
'here each batch has written data and committed bucket since table only has bucket each deltas has bucket and bucketflushlength furthermore each bucket has now received more datalogically its buffered but not yet committed ',negative
'last batch successful remove from partsnotinms ',negative
'undone copied from ',negative
'validate and vectorize the reduce operator tree ',negative
'any log specific settings via hiveconf will ignored ',negative
'buffer now the heap buffer the list use buffer again ',negative
'must old client talking dont know its conservative ',negative
'forward reset key and value columns ',negative
'existing shard present the database use the current version ',negative
'this method reached when error occurs while sending msg the session must bad ',negative
'partitions updated ',negative
'need release memory cache eviction ',negative
'move the files back original data location ',negative
'throw hivesqlexception when async calls ',negative
'need patch the dest back original into new query this makes assumptions about the structure the ast ',negative
'round with default decimal places ',negative
'map keep track which root generated which work ',negative
'may need merge with list temp tables ',negative
'the user didnt specify serde use the default ',negative
'may some retry logic here ',negative
'loginfoegetkey egetvalue ',negative
'skip this validatecolumnname always returns true ',negative
'the settings conf overlay should not part session config ',negative
'select true fields child none child and none ',negative
'nothing default ',negative
'checks the status the rpc call throws exception case error ',negative
'set checkpoint task dependant add partition tasks same dump retried for bootstrap skip current partition update ',negative
'return false for null ',negative
'error should not timeout ',negative
'from metastore for security ',negative
'hive doesnt support primary keys using local schema with empty resultset ',negative
'compaction doesnt work under transaction and hence pass null for validtxnlist ',negative
'warning ',negative
'doubleval ',negative
'note that pathtoalias will behave the original plan was join plan ',negative
'nonjavadoc see javautillist ',negative
'reduction but lets still test the original predicate see was already constant which case dont need any runtime decision about filtering ',negative
'must deterministic order maps see hive ',negative
'vectorize this parents children plug them into vectorparents children list add those children vector children nextparentlist ',negative
'put parameters aggregations reducevalues ',negative
'the overflow batch ',negative
'',negative
'timestamp patterns should default normal timestamp format ',negative
'required required required required required required ',negative
'extra bytes the end ',negative
'note assume that batchsize will consistent with vectors passed this rather brittle same other readers ',negative
'default ',negative
'allocate free allocate ',negative
'list the new files destination path which gets copied from source ',negative
'argument handling ',negative
'all the elements are representing null then return true ',negative
'',negative
'reset the log buffer verify new dump without any api call does not contain func ',negative
'all are filtered ',negative
'put session into the pool ',negative
'first authorize the call ',negative
'child isnt flattened because parent repeating null ',negative
'rowid ',negative
'column statistics index contains only the number rows ',negative
'from ',negative
'everyone has permission write but with sticky set that delete restricted this required since the path same for all users and everyone writes into ',negative
'delimiters for sql statements are any nonletterornumber characters except underscore and characters that are specified the database valid name identifiers ',negative
'favor broad plans over deep plans ',negative
'need keep the original list operators the map join know ',negative
'map from primitivetypeinfo ',negative
'irrelevant from eventids this can tracked the itself instead polluting the task also since have all the mrinput events here theyll all sent together ',negative
'input has not been rewritten not rewrite this rel ',negative
'rights signum wins ',negative
'read all the world ',negative
'this means correated value generator wasnt generated ',negative
'are done since there are keys check for ',negative
'the start the split points into the middle the cached slice cannot use the cached block its encoded and columnar cannot map the file ',negative
'this keep track subquery correlated and contains aggregate this computed calciteplanner while planning and later required subuery remove rule hence this passed using hiveplannercontext ',negative
'finish for execute ',negative
'convert agg args calcite ',negative
'entire response written out safe enable timeout handling ',negative
'see comment ',negative
'lookup the delegation token first the connection url then configuration ',negative
'ensures all params are indented ',negative
'',negative
'return the number columns recorded this files header ',negative
'substitution only supported nonbeeline mode ',negative
'the output columns does not contains the input table ',negative
'need disable join emit interval since for outer joins with post conditions need have the full view the right matching rows know whether need produce row with null values not ',negative
'hosts per host requests the same priority first host next host last with host third should allocate host host will wait ',negative
'mocks publishing logic ',negative
'multikey outer get key ',negative
'sqlstate errorcode should set ',negative
'first need find the minuncommittedtxnid which currently seen any open transactions there are txns which are currently open aborted the system then current value ',negative
'acid doesnt maintain this just makes logic more explicit ',negative
'dont use fieldschemaequals since also compares comments which unnecessary for this method ',negative
'create the required command line options ',negative
'parse out the individual parts ',negative
'are aborting all txns the current batch need heartbeat ',negative
'remove the existing partition columns from the field schema ',negative
'test major compaction ',negative
'partitionview does not have not need update its column stats ',negative
'hll algorithm shows stronger bias for values range compensate for this short range bias linear counting used for values before this short range the original paper also says similar bias seen for long range values due hash collisions range for the default case not have worry about this long range bias the paper used bit hashing and use bit hashing default values are too high observe long range bias hash collisions ',negative
'let the dummy the parent mapjoin ',negative
'have extracted the count from the hash multiset result dont keep ',negative
'',negative
'all columns select for example ',negative
'assuming the used memory equally divided among all executors ',negative
'turn bytes back into identifier for cache lookup ',negative
'high word multiplier multiply digits decimal words digit commad ',negative
'create expressions for project operators before and after the union ',negative
'make sure semijoin not enabled then not remove the dynamic partition pruning predicates ',negative
'find databases which name contains tofind hidden ',negative
'retain output column number from parent ',negative
'result grantor principal ',negative
'create database with view ',negative
'ensure partition present ',negative
'check already have initiated are working compaction for this partition table skip are just waiting cleaning can still check may time compact again even though havent cleaned todo this not robust you can easily run alter table start compaction between ',negative
'set rreturntype ',negative
'bgenjjtree typebyte ',negative
'interval year month comparisons ',negative
'semantic error not possible must bug convert internal error ',negative
'value too large should also null ',negative
'create valid table ',negative
'for use from construct from userinput ',negative
'record info metrics ',negative
'',negative
'the last line didnt match the patterns either the stack trace definitely over ',negative
'for partial and final objectinspectors for partial aggregations list ',negative
'buffers test are fakes not linked cache notify cache policy explicitly ',negative
'did not change the location there need move the table directories ',negative
'check exprnodecolumndesc wrapped expr ',negative
'specific order ',negative
'get table details from tabnametotabobject cache ',negative
'this should fail because txn aborted due timeout ',negative
'this will abort the txn ',negative
'for each task set the key descriptor for the reducer ',negative
'dynamic partitions dynamic partitions are generated dppartspecs may not initialized ',negative
'alternate between returning deleted and not this easier than actually tracking operations test that this getting properly called checking that only half the records show base files after major compactions ',negative
'above decimal ',negative
'test that existing sharedwrite partition with new exclusive coalesces ',negative
'initialize constant arguments ',negative
'todo determine the progress ',negative
'multiply ',negative
'num rows whose output evaluated ',negative
'dot operator remember the field name the rhs the left semijoin ',negative
'use udf query ',negative
'can now create multijoin operator ',negative
'this nonlocal warehouse then adding resources from the local filesystem may mean that other clients will not able access the resources disallow resources from local filesystem this case ',negative
'insert overwrite not supported for acid tables ',negative
'perform alters for incremental replication ',negative
'this now lifo operation ',negative
'get the current batch size ',negative
'class static variables ',negative
'the batchindex for the rows that are for the thenelse rows respectively ',negative
'columns ',negative
'these are only used for tests ',negative
'now have archive with partitions ',negative
'get the partition specs ',negative
'validate the function name ',negative
'prepare buffers ',negative
'firstname owen foobar substrlastname and firstname between david and greg ',negative
'the lock then there are locks this heartbeat ',negative
'xmx specified bytes ',negative
'the input the select does not matter over the expressions ',negative
'initialize the object inspectors ',negative
'production async functiontype name fieldlist throws commaorsemicolon ',negative
'wrap the client with threadsafe proxy serialize the rpc calls ',negative
'nonjavadoc see javalangstring ',negative
'nonjavadoc see ',negative
'colexprmapsize size cols from sel branch ',negative
'whether root tasks after materialized cte linkage have been resolved ',negative
'inside our own that can also store requested quantile values between calls ',negative
'loop for middles ',negative
'through each event and dump out each event eventlevel dump dir inside dumproot ',negative
'end ',negative
'debugstacktrace egetstacktrace ',negative
'simple container registration and unregistration without any task attempt being involved ',negative
'test with null args ',negative
'the column number for this one column join specialization ',negative
'check any left pair exists for right objects ',negative
'newinput ',negative
'nonjavadoc see ',negative
'hivecommand nonsql statement such setting property adding resource ',negative
'nonjavadoc see ',negative
'for all the source tables that have lateral view attach the ',negative
'default partition and bucket columns are sorted ascending order ',negative
'when creating the reader below there are read ops per bucket file liststatus and open ',negative
'create new projectunionproject sequences ',negative
'find operators which are the children specified filterop and there are these ',negative
'and one partition ',negative
'now allow the users specify any pools ',negative
'update the object ',negative
'rounds ',negative
'now vary isrepeating nulls possible left right ',negative
'anything else including boolean and string null ',negative
'the task has been terminated and the duck accounted for based local state whatever were doing irrelevant the metrics have also been updated ',negative
'this test case currently fails since add partitions dont inherit anything from tables ',negative
'nonjavadoc see int ',negative
'get the lowvalue ',negative
'should cancel hcat token was acquired hcat and not was supplied oozie the latter ',negative
'longs produces intervaldaytime ',negative
'end additional steps ',negative
'the fix delayed that the parent operators arent modified until the entire operator tree has been vectorized ',negative
'represents the schema exposed queryblock ',negative
'block ',negative
'its move task get the path the files were moved from this what any preceding map reduce task inferred information about and moving does not invalidate those assumptions this can happen when conditional merge added before the final movetask but the ',negative
'but for native tables need prefix match for subdirectories unlike nonnative tables prefix mixups dont seem potential problem here since are always dealing with the path something deeper than the table location ',negative
'data type conversion needed data type conversion check assume alter table prevented conversions that cannot handle vectordeserialize lazysimple capable converting its own lazybinary partition schema assumed match file contents conversion necessary from partition field values vector columns rowdeserialize partition schema assumed match file contents conversion necessary from partition field values vector columns ',negative
'use nonsettable struct object inspector ',negative
'can handle only case trunc date type ',negative
'raw splits ',negative
'aggregate expression from the rest nodes ',negative
'are also not supposed call setdone since are only part the operation ',negative
'pktabledb ',negative
'this should never happen with linked list queue ',negative
'need create the table manually rather than creating task since has exist compile the insert into ',negative
'repeated string tablesread ',negative
'jdoexception wrapped metaexception wrapped invocationexception ',negative
'get tmp file uri ',negative
'only one instance available cannot failover ',negative
'trigger movetrigger new moveexpression new etl trigger killtrigger new moveexpression new string query select tundercol tvalue from tablename join tablename tundercoltundercol order tundercol tvalue liststring setcmds new arraylist setcmdsaddset setcmdsaddset setcmdsaddset liststring errcaptureexpect new arraylist manager events summary get move kill return movebigread bigwritekill violation queue trigger movetrigger violated violation etl queue trigger killtrigger violated hdfsbytesread hdfsbyteswritten get pool cluster move pool etl cluster kill pool null cluster return pool null cluster setcmds killtrigger violated errcaptureexpect ',negative
'recent hadoop versions use deleteonexit clean tmp files ',negative
'the remainder ',negative
'generate all expressions from lateral view ',negative
'status ',negative
'should either select select distinct ',negative
'effectively final but has volatile since its accessed different ',negative
'concatenation dbname tablename and partition keyvalues ',negative
'production struct thisname fieldlist ',negative
'ensure task preempted based time match its allocated containerid ',negative
'using init instead this because the operation that needs run before delegating the other ctor and this messes chaining ctors ',negative
'udafs are present new columns needs added ',negative
'signature restriction nsoe and nsoe being flat exception prevents from setting the cause the nsoe the metaexception should not lose the info got here but its very likely that the metaexception irrelevant and actually nsoe message should log and throw nsoe with the msg ',negative
'use the ugi object that got added ',negative
'numnulls ',negative
'lets try store original column name this column got folded this useful for optimizations like groupbyoptimizer ',negative
'set the new hivesitexml ',negative
'insert mapside ',negative
'drain any calls which may have come during the last execution the loop ',negative
'handling subquery expressions where clause contains subquery expressions then true else false extract subquery expressionsn from where clause this nested subquery nthere are more than subquery expressions then yes throw unsupported error else rewrite search condition nremove subquery predicate build qbsubquery extract correlated predicates nfrom where clause add correlated items nselect list and group construct join predicate nfrom correlation predicates generate plan forn modified subquery build the join conditionn for parent query subquery join build the qbjointree from the join condition update parent query filtern with any post join conditions endif endif support for sub queries having clause and large this works the same way subqueries the where clause the one addum the handling aggregation expressions from the outer query appearing correlation clauses such correlating predicates are allowed minouterquertx subqueryy this requires special handling when converting joins see qbsubqueryrewrite method method for detailed comments ',negative
'operator with children ',negative
'crlf substitution return original line ',negative
'task processed ',negative
'copied from acidutils dont have put the code using this into ',negative
'tracks existing requests which are cycled through ',negative
'partition has been spilled disk its size will wont picked ',negative
'evaluate the function result for each row the partition ',negative
'texception ',negative
'create lfd even for ctas its noop move but still seems used for stats ',negative
'remove ending ',negative
'create the reduce sink operator ',negative
'column index stream type buffers ',negative
'column interpreted the row key ',negative
'sortby asc ',negative
'call listlocatedstatus mockmocktbl call check existence side file for mockmocktbl call open mockmocktbl call check existence side file for mockmocktbl ',negative
'optional int appattemptnumber ',negative
'make clone existing hive conf ',negative
'supportskeytypes ',negative
'necessary clean the transaction the ',negative
'use the unparsetranslator resolve unqualified table names ',negative
'shouldnt actually called the test will fail ',negative
'ssl support ',negative
'nonjavadoc see ',negative
'this the initial state for lock ',negative
'see the class comment hif handles for all input formats try handle again particular for the nonrecursive originalsonly getsplits call will just get confused this bypass was not necessary when tables didnt support originals now that they use this path for anything table related although everything except the originals could still handled acidutils like regular nontxn table ',negative
'ctas make the movetasks destination directory the tables destination ',negative
'check the root expression for final candidates ',negative
'now decompress copy the data into cache buffers ',negative
'step merge mapjointask into the mapside its child ',negative
'nonjavadoc see ',negative
'initializing stats publisher ',negative
'not come from the local branch ',negative
'file for local ',negative
'verify the next write ',negative
'repeating with other nullable ',negative
'the output the commonjoinoperator the input columninfo ',negative
'our limit max precision integer digits leading zeros below the dot has zeroes below the dot ',negative
'rerun constant propagation fold any new constants introduced the operator optimizers ',negative
'request task task already started previously set time ',negative
'run the cleaner thread when the cache maxfull full ',negative
'trim the trailing ',negative
'localityrequested randomly pick node containing free slots ',negative
'and udf ',negative
'default return the urlparam passed asis ',negative
'orc files can converted full acid transactional tables ',negative
'tests for droppartitionstring dbname string tblname liststring partvals boolean deletedata method ',negative
'latin small letter with hook bytes ',negative
'remove the table folder ',negative
'point old releases ',negative
'add root tasks runnable ',negative
'',negative
'remains copy from current read buffer less than wbsize def ',negative
'location ',negative
'make sure hostport pair valid the status the location does not matter ',negative
'nonjavadoc see ',negative
'here just reuse the threadcount configuration for since this results better performance the number missing partitions discovered are later added metastore using threadpool size have different sized pool here the smaller sized pool the two becomes bottleneck ',negative
'you can blame this owen ',negative
'the table could not cached due memory limit stop prewarm ',negative
'create the batch will use return data for this ',negative
'foo ',negative
'the master thread and various workers ',negative
'long colulmn text column ',negative
'has not keyword ',negative
'avoid tmp directories closer ',negative
'and got duck ',negative
'create the work for moving the table ',negative
'efficiently add computed values the last batch group key ',negative
'how many jobs have been started ',negative
'dont proper overlap checking because would cost cycles and think will never happen perform the most basic check here ',negative
'default assume filter tag are good ',negative
'reset database location ',negative
'the partition columns are set once for the partition and are marked repeating ',negative
'align the thenelse types ',negative
'task and its child task has been converted from join mapjoin see the two tasks can merged ',negative
'test doubledouble version ',negative
'these values are equal when txn entry made should never equal after heartbeat which ',negative
'but that will specific hdfs through storagehandler mechanism ',negative
'methods setreset getnextnotification modifier ',negative
'construct value table desc ',negative
'any exact match ',negative
'repl policy should created based the table name context ',negative
'this depends ',negative
'add schemarr relnodeschema map ',negative
'dont check explicit pool match for apps both are specified the jdbc string ',negative
'always return something from getaclforpath this should not happen ',negative
'boolean value match for char field ',negative
'insert reduce side with reduce side input ',negative
'dynamic partition throw the exception ',negative
'add the move task for those partitions that not need merging ',negative
'extract partition desc from sorted map ascending order part dir ',negative
'the timestamp column ',negative
'split split ',negative
'class variables ',negative
'container ',negative
'use only reducer case cartesian product ',negative
'add jars find them map contents jar name that can avoid ',negative
'generate ddl task and make dependent task the leaf ',negative
'limit milliseconds only ',negative
'mapjoinoperator ',negative
'make sure doesnt already exist ',negative
'convert nonskewed table skewed table ',negative
'the last field singlevaluetrue ',negative
'remove table entry from sessionstate ',negative
'generate groupbyoperator ',negative
'test that throws hcatexception the partitionkey incorrect ',negative
'complex pattern ',negative
'construct agginfo ',negative
'use partition schema properties set the partition descriptor properties set true ',negative
'get the semijoin rhs table name and field name ',negative
'all column names referenced including virtual columns used ',negative
'see fasthivedecimalimpl for more details these fields ',negative
'make sure nothing escapes this run method and kills the metastore large ',negative
'the update fails because the task has terminated the node ',negative
'some spark plans cause hash and other modes get this ignore ',negative
'inputsplitinfo false ',negative
'sending out messages before taimpl knows how handle them ',negative
'will need handle alternate workdir well this case derive from branch ',negative
'since have open transaction only values above are expected ',negative
'just the primitive types ',negative
'cannot simplify bail out ',negative
'evaluator results are first ',negative
'get the column table aliases from the expression start from the tokfunction ',negative
'gather expressions ast aggregations ',negative
'directly deserialize with the caller reading fieldbyfield the lazybinary serialization format the caller responsible for calling the read method for the right type each field after calling readnextfield reading some fields require results object receive value information separate results object created the caller initialization per different field even for the same type some type values are reference either bytes the deserialization buffer other type specific buffers those references are only valid until the next time set called ',negative
'the columns row schema not contained column expression map then those are the aggregate columns that are added gby operator will estimate the column statistics for those newly added columns ',negative
'check that the correlated variables referenced these ',negative
'this ensures the incremental dump doesnt get all events for replication ',negative
'not relevant creating new partition ',negative
'for outer join remember our input rows before expression filtering before hash table matching can generate results for all rows matching and non matching ',negative
'dummyops reference all the hashtabledummy operators the plan these have separately initialized when setup task their function mainly root ops give the mapjoin the correct ',negative
'and necessary load the jars this thread ',negative
'new tai lue letter high bytes ',negative
'determine which input rel oldordinal references and adjust oldordinal relative that input rel ',negative
'not selected ',negative
'expected exception due lexer error ',negative
'jump the field want and read ',negative
'used windowing ',negative
'look for hivesitexml the classpath and log its location found ',negative
'want start sufficiently high enough the iterator stack ',negative
'array partitions holding the triplets total number small table rows memory the max memory limit that can allocated the actual memory used row size the small table whether theres any spilled partition the partition into which spill the big table row ',negative
'remove filtermap for outer alias filter not exist that ',negative
'check that subquery top level conjuncts remove from the where clause ast ',negative
'check the parent coming from table scan what the version ',negative
'for each joining table set dir for big key and small keys properly ',negative
'figure out who should run the file operations ',negative
'calcite literal millis need convert seconds ',negative
'create new mapredlocalwork ',negative
'indicates malformed version ',negative
'support nested column pruning need track the path from the top the nested ',negative
'hashbased aggregations ',negative
'restrictionh subqueries only supported the sql where clause ',negative
'the table different dfs than the partition replace the partitions dfs with the tables dfs ',negative
'does not work the underlying inputsplit has package visibility ',negative
'common class between charvarchar string ',negative
'dont want not exit because issue with logging ',negative
'updates deletes from insert and new part ',negative
'',negative
'http transport mode set the thread local proxy username thrifthttpservlet ',negative
'hive native ',negative
'infer sort columns from operator tree ',negative
'long ',negative
'complete txn ',negative
'leading spaces ',negative
'one vint without nanos ',negative
'routines for stubbing into writables ',negative
'execdriver has plan path cannot derive vrb stuff for the wrapper ',negative
'copy the properties from storagehandler jobproperties ',negative
'should never executed ',negative
'replace insert overwrite insert ',negative
'ivalue ',negative
'for example the case select from join would direct dependency ',negative
'check compatibility with subsequent files ',negative
'return false otherwise ',negative
'killed something but still got rejected wait bit give chance our previous victim actually die ',negative
'setup the based the input tabledefs columns the window functions ',negative
'reached here did not find replication spec the node its immediate children defaults are pretend replication not happening and the statement above running asis ',negative
'change new table and append stats for the new table ',negative
'kryo will set this hope ',negative
'the newposition the same the previousposition weve reached the end the binary search the new position least big the size the split any ',negative
'origpathstrhdfshost for example ',negative
'select udtf ',negative
'delete the parent temp directory all custom dynamic partitions ',negative
'this best effort optimization bail out error conditions and try generate and execute slower plan ',negative
'column types all partitioned columns used for generating partition specification ',negative
'the fifth could combined again ',negative
'save state for future iterations ',negative
'the buffer can only removed after the removed flag has been set are able lock here noone can set the removed flag and thus remove that would also mean that the header not free and noone will touch the header either ',negative
'nonjavadoc see javasqlblob ',negative
'',negative
'dont log the exception people just get confused ',negative
'its not internal name this what want ',negative
'will adjust start and end that could record the metrics save the originals ',negative
'prevent excessive logging case deadlocks slowness ',negative
'allow implicit string date conversion ',negative
'close the underlying connection pool avoid leaks ',negative
'since left integer always some products here are not included ',negative
'write byte say whether skip pruning not ',negative
'that uniont null converted just within map ',negative
'tracks hivequeryid queryidentifier this can only set when config parsed tezprocessor all the other existing code passes queryid equal everywhere switch the runtime and move parsing the payload the the actual hive queryid could ',negative
'make right child left ',negative
'tests for partition tablename string dbname liststring partvals method ',negative
'schemagroup ',negative
'locality information before those without locality information ',negative
'test basic right trim and truncate vector ',negative
'check map side aggregation possible not based column stats ',negative
'vectorization enabled ',negative
'collect operator observe the output the script ',negative
'udtf ',negative
'weve already set this one need clone for the next work ',negative
'zookeeper property name pick the correct jaas conf section ',negative
'body ',negative
'newoutput the index the cor var the referenced position list plus the offset referenced position list ',negative
'convert integer value representing timestamp nanoseconds one that represents timestamp seconds with fraction since the epoch ',negative
'position file where the first row this block starts ',negative
'there should adjustments for leap seconds ',negative
'fifo policy doesnt care ',negative
'note this behavior may have change ever implement vectorized merge join ',negative
'add key key slot slot pairindex pairindex empty slot ',negative
'keep the columns only the columns that are part the final output ',negative
'verify mergeonlytask not optimized ',negative
'bail out nothing ',negative
'not external table then create one ',negative
'construct ',negative
'first change the filter condition into join condition ',negative
'need make metastore call ',negative
'dont drop notificationlog sequencetable and its used other table which are not txn related generate primary key these tables are dropped and other tables are not dropped then will create key duplicate error while inserting other table ',negative
'division with overflowzerodivide check error produces null output ',negative
'create filesinkoperator for the file name tasktmpdir ',negative
'vecaggrdesc aggregationbuffer void agg vectorizedrowbatch unit void int aggregateindex vectorizedrowbatch vrg void agg long boolean matchesstring name columnvectortype inputcolvectortype columnvectortype outputcolvectortype mode mode batch int batchindex int columnnum aggregationbuffer agg ',negative
'this called either with error that was queued error that was set into the atomic reference this class the latter besteffort and used opportunistically skip processing long queue when the error happens ',negative
'check for aborted txns ',negative
'make sure they are the same before and after compaction ',negative
'note and power multiply ',negative
'setting success false make sure that the listener fails rollback happens ',negative
'todo factor security manager into pipeline todo factor out encodedecode permit binary shuffle todo factor out decode index permit alt models ',negative
'check for nested message found set the schema else return ',negative
'analyze table partition compute statistics the plan consists simple mapredtask followed statstask the task just simple tablescanoperator ',negative
'refering sqloperationjava there chance that hivesqlexception throws and the async background operation submits thread pool successfully the same time cleanup ophandle directly when got hivesqlexception ',negative
'todo define groups regex and use ',negative
'propagate ',negative
'usually controlled bucketing ',negative
'private constructor ',negative
'the interface adds the single long key hash multiset contains method ',negative
'write the plan out ',negative
'test for char type ',negative
'request and validate the request cookies ',negative
'test that changing column data type fails ',negative
'references external fields for async splitinfo generation ',negative
'get select expression list ',negative
'case when this expression ',negative
'outputrel the generated augmented select with extra unselected ',negative
'initialize event processor and dispatcher ',negative
'get the constant value associated with the current element the struct ',negative
'successful ',negative
'for hashmap ',negative
'some the columns stats are missing this implies partition schema has changed will merge columns present both overwrite stats for columns absent metastore and leave alone columns stats missing from stats task this last case may leave stats stale state this will addressed later ',negative
'clone all the operators between union and filescan and push them above the union remove the union the tree below union gets delinked after that ',negative
'the wait queue should able fit least waitqueue currentfreeexecutor slots ',negative
'point making acid table these other props are not set since will just throw exceptions when someone tries use the table ',negative
'mgby follow mgby here start countkey ',negative
'not registered for this node register and send state successful ',negative
'give more ',negative
'create partition ',negative
'collect the newly added partitions will report the dynamically added partitions txnhandler ',negative
'based the statement generate the selectoperator ',negative
'found parent mapjoin operator its size should already reflect any other mapjoins connected ',negative
'total rows emit during the whole iteration excluding the rows emitted the separate thread ',negative
'deletedata ',negative
'this test assumes the hivecontrib jar has been built part the hive build ',negative
'collect table access keys information for operators that can benefit from bucketing ',negative
'verify ',negative
'the predicate matches then return true otherwise visit the next set nodes that havent been seen ',negative
'require the use recursive input dirs for union processing ',negative
'should have severed the ties ',negative
'multiply divide digit commad scale down fraction digits negative exponent number zeros after dot down shift ',negative
'log and ignore ',negative
'verify that the number events since began least more ',negative
'the master thread ',negative
'note the current implementation does not allow importing external location this intentional since want the destination tables managed tables this assumption should change some point the future will need some its checks changed allow for replacing external tables ',negative
'dont worry about setting raw data size diff have idea how calculate that without finding the row are updating deleting which would mess ',negative
'change the newly created mapred plan was join operator ',negative
'check physical path ',negative
'partition columns virtual columns ',negative
'select all with the first expression and expect the other children not invoked ',negative
'read the record with the same record reader ',negative
'the group expression anything other than list columns ',negative
'another thread adds entry before the check this one ',negative
'this required for writing null key for file based tables ',negative
'',negative
'all checks short names ',negative
'update sum length with the new length ',negative
'this replication spec then replacemode semantics might apply were already asking for table replacement then can skip this check however otherwise replication scope and weve not been explicitly asked replace should check the object were looking exists and trigger replacemode semantics ',negative
'note the reason use string name the hive hbase handler here because not want introduce compiledependency the hivehbasehandler module from within hivehcatalog this parameter was added due the requirement hive ',negative
'original path and not available well ',negative
'the original mapredtask and this new generated mapredlocaltask ',negative
'check the ndvrows from the sel the destination tablescan the semijoin opt going ',negative
'positive numbers flip just the first bit ',negative
'build error message ',negative
'input ',negative
'schedule tasks give out two ducks two higher pri tasks get them give out more the last task gets and one duck unused give out more goes unused then revoke similarly steps with the opposite effect ',negative
'nonjavadoc see ',negative
'drop table will clean the table entry from the compaction queue and hence worker have effect ',negative
'and finally hook any events that need sent the tez ',negative
'',negative
'merge ',negative
'tracks appmasters which heartbeats are being sent this should not used for any other messages like taskkilled etc ',negative
'the jars libjars will localized cwd the launcher task then libjars will ',negative
'locks newrequestlist locks completednodes ',negative
'construct using ',negative
'this insert update delete acid table then mark that the ',negative
'since there only have locks for current query any ',negative
'set the mapjoin hint needs disabled ',negative
'ignore not required this will never ',negative
'user provided configs ',negative
'this test ',negative
'ignore closing quote ',negative
'create the equality condition ',negative
'add spark job metrics metrics collected spark itself jvmgctime ',negative
'boring scenario two concurrent revokes same above ',negative
'deletedelta and deletedelta which are created result compacting ',negative
'todo now assume the key object supports hashcode and equals functions ',negative
'for information only ',negative
'verify the accumulated logs ',negative
'get completed attempts ',negative
'update credential provider location the password the credential provider already set the sparkconf ',negative
'count the number tasks and kill application goes beyond the limit ',negative
'create the context for walking operators ',negative
'virtualcolumncount ',negative
'dont break old callers that are trying reuse storages ',negative
'create rows file copy ',negative
'myenum ',negative
'there are nulls then the iteration the same all cases ',negative
'add embedded rawstore can cleanup later avoid memory leak ',negative
'hive introduces update event which will capture changes allocation after get ',negative
'any ',negative
'add new table via objectstore ',negative
'try next available server zookeeper retry all the servers again retry enabled ',negative
'double ',negative
'drop foreign key ',negative
'set partition sorted partition bucket sorted ',negative
'verbose logs should contain everything including execution and performance ',negative
'dont accidentally cache the value shouldnt ',negative
'check most significant part first ',negative
'update value ',negative
'number elements average elements average elements times the variance elements times the variance elements times the covariance ',negative
'internal input format used ',negative
'test select powrootcolb rootcol from table testroot ',negative
'the code point from from string already has replacement deleted dont need anything just move the next code point ',negative
'return true ',negative
'boolean signal whether tagging will used join ',negative
'right repeats and null ',negative
'vrb was created from vrbctx already have preallocated column vectors return old cvs any caller assume these things all have the same schema ',negative
'handle the logical operators ',negative
'begin commit ',negative
'use get the original parents because ',negative
'there are some elements that were cached parallel take care them ',negative
'tablename ',negative
'which case stats need not updated ',negative
'not support task level progress nothing here ',negative
'hold output needed hold boolean output ',negative
'are here either unpartitioned table partitioned table with predicates ',negative
'the arg part another list ',negative
'test decimal column decimal scalar addition this used cover all the cases used the source code template ',negative
'required required required required required required optional optional ',negative
'storagedescriptor has empty list fields serde will report them ',negative
'update service registry configs caveat this has nothing with the actual settings read the needed use hiveconf dynamically switch between instances ',negative
'file dump should write session state consoles error stream ',negative
'need obtain intersection all the privileges ',negative
'min txn incremented linearly within transaction batch ',negative
'major version number should match for backward compatibility ',negative
'need check that datasource was not specified user ',negative
'add added files ',negative
'get calcite agg ',negative
'footersummary ',negative
'',negative
'always serialize the string type using the escaped algorithm for lazystring ',negative
'passthru constructors ',negative
'should not treated like needs data ',negative
'skip one ',negative
'the most preemptable task still too important for kill put back ',negative
'itself missing then throw error ',negative
'duplicate avoid modification ',negative
'from ',negative
'singlecolumn long check for repeating ',negative
'use global when key for reduce ',negative
'uses all decimal longs ',negative
'handle mapjoin specially and check for all its children ',negative
'setentryvalid already increments the reader count set usedcacheentry gets released ',negative
'partitioned table ',negative
'not present ',negative
'partition already exists and arent overwriting then respect its current location info rather than picking from the parent tabledesc ',negative
'',negative
'must class ',negative
'process partition pruning sinks ',negative
'get should fail now since ttl and weve snoozed for seconds ',negative
'move the original parent directory the intermediate original directory ',negative
'this initializes currentfileread ',negative
'read single value ',negative
'handle repeating ',negative
'test mod create test log file which will contain only logs which are supposed written the qtest output ',negative
'new vertex ',negative
'while threads are blocked should still able get and return session ',negative
'implies limit ',negative
'choosing the function since the one duplicated the dummy database ',negative
'used spark mode decide whether global order needed ',negative
'conf for nonllap ',negative
'location ',negative
'convert constant back rexnode ',negative
'acls for znodes nonkerberized cluster the world ',negative
'get all parents ',negative
'valid filesystem schemes ',negative
'now diff the lists ',negative
'this union type ',negative
'project projects the original expressions ',negative
'nonstrict mode and there predicates all get everything ',negative
'creates job request object and sets execution environment creates thread pool execute job requests param requesttype job request type param config name used extract number concurrent requests serviced param config name used extract maximum time task can execute request ',negative
'throw new not sort order and unique ',negative
'dont visit multiple times ',negative
'remove old table objects hash ',negative
'dont attempt scheduling for additional priorities ',negative
'verify entries added compactionqueue for each tablepartition ',negative
'specifically necessary for dpp because might have created lots and true and true conditions ',negative
'local session path ',negative
'the sort and bucket cols have match both sides for this ',negative
'with this root operator ',negative
'the table bucketed and bucketing enforced the following the number buckets smaller than the number maximum reducers create those many reducers not create multifilesink instead filesink the multifilesink will spray the data into multiple buckets that way can support very large number buckets without needing very large number reducers ',negative
'keysaddkey would need list stmtrs pairs for each key ',negative
'when the first buffer loaded resultsetnext should called times ',negative
'',negative
'groups the clause names into lists that any two clauses the same list has the same group and distinct keys and clause appears more than one list returns list the ',negative
'should having tree which looks like this join are the join operator now ',negative
'the time either success failed are called the task itself knows that has terminated and will ignore subsequent kill requests they out ',negative
'close the session which should free the txnhandlerlocks held the session done the finally block make sure free the locks otherwise the cleanup teardown will get stuck waiting the lock held here acidtbl ',negative
'spark property for now dont support changing spark app name the fly ',negative
'last batch successful remove from partsnotinfs ',negative
'update runtime ',negative
'nonjavadoc see int int ',negative
'one the branches definitely bucketleaf ',negative
'open till limit but not exceed ',negative
'find the biggest small table also calculate total data size all small tables ',negative
'initialize using and column projection list ',negative
'avoids creating tez client sessions internally takes much longer currently ',negative
'handle the rejection outside the lock ',negative
'what need way get buckets not splits ',negative
'where clause and pick the second disjunct from the operation ',negative
'object overhead bytes for intcompact bytes for precision bytes for scale size biginteger ',negative
'should initialize the value for createvalue ',negative
'more entries than oldinvalidids ',negative
'separator ',negative
'there will not any tez job above this task ',negative
'forward arg forward arg forward arg ',negative
'enable assertion ',negative
'big table candidates ',negative
'invoke the inputformat entrypoint ',negative
'not using expressions ',negative
'and transformation creates nodes andor thus triggered ',negative
'from the original sparkwork ',negative
'this not subquery predicate then join the null check subquery see for details why and how this constructed ',negative
'vector reduce key partition columns are repeated test element ',negative
'construct using ',negative
'theres memory available fail ',negative
'note calling last flush length below more for futureproofing when have streaming deletes but currently dont support streaming deletes and this can ',negative
'use the columnnames initialize the reusable row object and the columnbuffers reason this being done buffer full should reinitialize the ',negative
'indexstore trying tell something ',negative
'msd and should same objects not sure how make then same right now ',negative
'exprfield constant iff expr constant ',negative
'todo hive make use splitsizeestimator the actual task computation needs looked well ',negative
'failover didnt succeed log error and exit ',negative
'execute the setup queries ',negative
'try the output primitive object ',negative
'verify the buffer was reset real output doesnt happen because was mocked ',negative
'nonjavadoc see int int ',negative
'double between ',negative
'for keeping track the number elements read just for debugging ',negative
'special handling needed times for date timestamp string char and varchar they can named specifically argument types longcolumnvector intfamily date intervalfamily doublecolumnvector floatfamily decimalcolumnvector decimal bytescolumnvector string char varchar timestamp intervaldaytime ',negative
'created when the task executed dont care about the correct state here ',negative
'',negative
'addsplitsforgroup collects separate calls setinputpaths into one where possible the reason for this that this faster some inputformats orc will start threadpool the work and calling multiple times unnecessarily will create lot unnecessary thread pools ',negative
'check the table should skipped ',negative
'this leaf add exporttask follow ',negative
'use the same filesystem input file backuppath not explicitly specified ',negative
'initialization fails with retry resource plan change ',negative
'todo key and values are object which can eagerly deserialized lazily deserialized accurately estimate the entry size every possible objects key value should implement memoryestimate interface which very intrusive assuming default entry size here ',negative
'tez use different way transmitting the hash table basically use reducesinkoperators and set the transfer broadcast instead partitioned consequence use different serde than the mapjoin case ',negative
'begin pattern ',negative
'unique key the filterinputrel ',negative
'lookup the field corresponding the given field and return ',negative
'the dependencies should include depth and depth inferred ',negative
'locked someone move forceevict evicted this cachespecific removed from allocator structures the final state the memory was released memory manager new allocation before the first use cannot forceevict ',negative
'only used acid writer ',negative
'the current version jetty server doesnt have the status hence passing this constant ',negative
'milliseconds ',negative
'not allocated yet ',negative
'the following data members are only required support the deprecated constructor and builder ',negative
'for partial specifications need partitions follow the scheme ',negative
'create filter sqcountcheckcount instead project because relfieldtrimmer ',negative
'commented out because the name becomes innerfield default call course pig but the metadata itd anonymous this would autogenerated which fine ',negative
'case outer joins need push records through even one the sides done sending records for the case full outer join the right side needs send data for the join even after the left side has completed sending all the records its side this can done once initialize time and close these tags will still forward records until they have more send also subsequent joins need fetch their data well since any join following the outer join could produce results with one the outer sides depending the join condition could optimize for the case inner joins the future here ',negative
'byte ',negative
'required required optional required ',negative
'full table name format dbnametablename ',negative
'adding same property key twice should throw unique key constraint violation exception ',negative
'assume almost always performance win fill all isnull can safely reset nonulls ',negative
'subquery filter ',negative
'pool didnt exist wouldnt have returned ',negative
'and table the table not sorted ',negative
'ignore client only queries ',negative
'optimization the first child file have reached the leaf directory move the parent directory itself instead moving each file under the directory see hcatalog note for future append implementation this optimization another reason dynamic partitioning currently incompatible with append mutable tables ',negative
'for caching partition objects ',negative
'change serde lazysimpleserde columnsetserde ',negative
'add uncovered acid delta splits ',negative
'use the basic string bytes read get access then use our optimal truncatetrim method that does not use java string objects ',negative
'should set child class ',negative
'restore original state ',negative
'could not resolve all the function children fail ',negative
'everything try normal shutdown ',negative
'discard possibly type related sorting order and replace with alphabetical ',negative
'all calls fail ',negative
'validate the plan ',negative
'nullable remarks columndef sqldatatype sqldatetimesub charoctetlength ordinalposition isnullable scopecatalog scopeschema scopetable sourcedatatype isautoincrement ',negative
'handle the isnull array first tight loops ',negative
'verify all scopes are closed ',negative
'ignore dummy inputs ',negative
'test both are not configured ',negative
'add new column with cascade option ',negative
'create should not return resultset ',negative
'dfs stuff ',negative
'querytimeout means timeout ',negative
'generate full partition specification ',negative
'store some hash bits ref for every expansion need add one bit hash have enough bits well that dont well rehash loginfoexpanding the hashtable capacity capacity ',negative
'build rel for src subquery join ',negative
'expect that theres only one field schema ',negative
'lag the whole partition not the iterator range ',negative
'replace the buffer our big range list well current results ',negative
'categorize the partitions returned and confirm that all partitions are accounted for ',negative
'the manner which the values this column are deserialized fromto accumulo ',negative
'',negative
'allow decimals and will return truncated integer that case therefore wont throw exception here checking the fractional part happens below ',negative
'opening allowed after closing ',negative
'the encoding method simple replace all the special characters with the corresponding number ascii note that unicode not supported table names and have explicit checks for ',negative
'',negative
'set columns list for temp table ',negative
'try pulling directly from url ',negative
'diffaftersleep total sleeptime ',negative
'scale down left and compare ',negative
'after super analyze read defaultacidtblpart write defaultacidtblpart tableinsert after udsa read defaultacidtblpart write partitionupdate todo this causes read lock the whole table clearly overkill ',negative
'table partitioned user did not specify partition ',negative
'multiply scale normalize not use negative scale our representation example has negative scale since scale number digits below the dot normalized scale ',negative
'all field names are the form key value ',negative
'only evaluate veve cast constant recursive casting ',negative
'permissions for metric directory ',negative
'marshalunmarshal znode data ',negative
'replication destination will not external ',negative
'multikey hash set optimized for vector map join the key stored the provided bytes uninterpreted ',negative
'',negative
'find the number reducers such that divisor totalfiles ',negative
'node has free capacity but disabled node has capcaity delay reenable tiemout ',negative
'assume that for table theres only the base directory are good ',negative
'this not called consecutivechunk stuff parquet this were used might make sense make faster ',negative
'note that create the cluster name from user conf hence user can target cluster but then create the signer using hiveconf hence control the config and stuff ',negative
'validate ',negative
'omit nulls ',negative
'create encoded data reader ',negative
'check whether the part exists not ',negative
'interim row count can not less due containment assumption join cardinality computation ',negative
'make mockinstance here setting the instance name the same this mock instance ',negative
'inner join specific ',negative
'creates more files that partition ',negative
'match for entire batch ',negative
'note originally named this isempty but that name conflicted with another interface ',negative
'add all dependencies edges the graph ',negative
'todo expose nonprimitive structured object while maintaining jdbc compliance ',negative
'environment variables name ',negative
'calculate the expected timeout based the elapsed time between waiting start time and polling start time ',negative
'add column info corresponding virtual columns ',negative
'test that exclusive lock blocks read and write ',negative
'null tests ',negative
'create initialize and test ',negative
'this test will make sure that every entry has test here ',negative
'remove the previous renewable jars ',negative
'the tpcds scale set this way the optimizer will generate plans for set ',negative
'hadoop this timeout spec behaves strnage manner means with retry however does this but does thrice essentially retries the number times the entire config ',negative
'test inputformat with column prune ',negative
'stub outputformat actions ',negative
'for now only alter name owner parameters cols bucketcols are allowed ',negative
'fetch the counters ',negative
'dont know the acceptable size for java array well use boundary ',negative
'check the file format the file matches that the partition ',negative
'ensure find the single row which matches our timestamp where field has value ',negative
'least partition does not contain row count then mark basic stats state partial ',negative
'read should get rows ',negative
'this code that creates the result for the granularity functions has been brought from druid ',negative
'last row last batch determines isgroupresultnull and double lastvalue ',negative
'partition level statistics requested add predicate and group needed rewritten query ',negative
'insert overwrite table from source table ',negative
'',negative
'should send message undo what just did ',negative
'not clean the writers the callback should ',negative
'only inline followed array supported cbo ',negative
'only store longs our longcolumnvector ',negative
'captures how input should partitioned this captured list astnodes that are the expressions the distributecluster clause specifying the partitioning applied for ptf invocation ',negative
'possible that nullscan can fire skip this rule ',negative
'localize llap client jars ',negative
'the next byte should the marker ',negative
'',negative
'nulls not repeating ',negative
'partitions dropped ',negative
'all tables are cached this not possible future can support this randomly ',negative
'help ',negative
'todo use hbase fixed ',negative
'reconnect was successful ',negative
'cut the operator tree not retain connections from the parent downstream ',negative
'the second one should combined into the first ',negative
'fill high long from lower long ',negative
'param tuple return null throws ioexception see ',negative
'start here with least one byte ',negative
'throw special exception since its usually wellknown misconfiguration ',negative
'char text value already stripped trailing space ',negative
'constants for bit variant ',negative
'nonjavadoc see int ',negative
'set parameters the ngram estimator object ',negative
'compare long value with ',negative
'temp tables exempted from checks ',negative
'the optimization has been stopped for the reasons like being not qualified lack the stats data not continue this process for example for query select maxvalue from src union all select maxvalue from src has been union remove optimized the ast tree will become tsselgbyrsgbyfs tsselgbyrsgbyfs branch for src not optimized because src does not have column stats ',negative
'key value are already read ',negative
'clear most members ',negative
'all the parent sparktasks that this new task depend they dont already exists ',negative
'hhelp ',negative
'',negative
'delete output file exit ',negative
'copy the current object contents into the output only copy selected entries indicated selectedinuse and the sel array ',negative
'use different separator values ',negative
'but preserve table name sql ',negative
'already verified that the join can converted bucket map join ',negative
'clear rounding portion lower longword and add right scale roundmultiplyfactor ',negative
'disabled service discovery enabled return the already populated params mode params already populated with active server host info ',negative
'nonfinalcandidates predicates should empty ',negative
'default version ',negative
'version schema for this version hive ',negative
'create hiveconf once since this expensive ',negative
'already called doas need doas here ',negative
'there should call create partitions with batch sizes ',negative
'have just ensured the item not the list have definite state now ',negative
'test class write series values the designated output stream ',negative
'move the data files this newly created partition temp location ',negative
'note were not creating copy the list for saving memory ',negative
'there are original format files ',negative
'returns whether not two lists contain the same elements independent order ',negative
'hook default ',negative
'sort pushed bail out ',negative
'todo return ',negative
'check that the defaults did not remain ',negative
'nonjavadoc see ',negative
'any operator the stack does not support autoconversion this join should not converted ',negative
'this min number reducer for deduped avoid query executed too small number reducers for example queries groupbyorderby can executed ',negative
'',negative
'room for optimization since cannot create empty project operator ',negative
'need support field names like key value between mapreduce boundary ',negative
'run distcp source filedir too big ',negative
'table already transactional migration needed ',negative
'string char varchar ',negative
'configured limit for reducers ',negative
'',negative
'have space the cache run cleaner thread ',negative
'groupingc groupingc groupingc ',negative
'write stmt ',negative
'check the input operator with struct children ',negative
'collect all dpp sinks ',negative
'use common binary decimal conversion method share with ',negative
'aggregate functions ',negative
'transform have created new filter operator ',negative
'reached here then were successful finding alternate internal column mapping and were about proceed ',negative
'nonjavadoc see ',negative
'count ',negative
'used external cache used local cache ',negative
'return the row only its not corrupted ',negative
'todo embedded metastore changes the table object when clientcreatetable called table storage descriptor location should null ',negative
'violation queue ',negative
'add the support the ',negative
'bloom filter any input and output bytes just modes partial complete ',negative
'ref ',negative
'allows mocking testing ',negative
'set all child tasks ',negative
'alias cte contains the table name not the translation because ',negative
'find all the agg expressions use linkedhashset ensure determinism ',negative
'spaces the end the line ',negative
'this node will likely activated after the task timeout expires ',negative
'concern the way this mapping goes the order needs preserved for and ptngetvalues ',negative
'releasing the locks ',negative
'must reset the isnull could set from prev batch use ',negative
'aggregation columns hive ',negative
'these are sessionstate objects that are copied over work allow for parallel execution based the current use case the methods are selectively synchronized which might need taken care when using other methods ',negative
'set the size the struct ',negative
'delete sample jars ',negative
'code sections initialize fastsetfrom take integer fractional portion binary decimal conversion decimal binary conversionr emulate serializationutils deserialization used orc emulate serializationutils serialization used orc emulate biginteger deserialization used lazybinary and others emulate biginteger serialization used lazybinary and others decimal integer conversion decimal noninteger conversion decimal comparison decimal rounding decimal scale updown decimal precision trailing zeroes decimal addition subtraction decimal multiply decimal division remainder decimal string formatting decimal validation decimal debugging ',negative
'specified generate alias using func name ',negative
'group mapping groupa user user ',negative
'',negative
'call with new input inspector ',negative
'',negative
'create map ',negative
'verify that there are two calls because two instances the authorization provider ',negative
'testing using good enough because use create objectinspectors ',negative
'filter condition null transform false ',negative
'removing job credential entry cannot set the tasks ',negative
'for partial and complete ',negative
'return the new list ',negative
'allow the user set the orc properties without getting error ',negative
'high word gets integer rounding ',negative
'some changes optional ',negative
'use the hive table name ignoring the default database ',negative
'returns the bucket number which the record belongs ',negative
'the interface for single byte array key hash map lookup method ',negative
'update catalogs ',negative
'test that exclusive blocks exclusive and read ',negative
'add tokwindow child udaf ',negative
'get scheme from filesystem ',negative
'assert cvbcolslength must constant per split ',negative
'this should use ',negative
'process the records the input iterator until new output records are available for serving downstream operator input records are exhausted ',negative
'return the new expression containing only partition columns ',negative
'helper method create yarn local resource ',negative
'field present both validate type has not changed ',negative
'get one top level directly from the stack ',negative
'iterate through the children nodes the clauses starting from index which corresponds the right hand side the list ',negative
'table being modified external need make sure existing table doesnt have enabled constraint since constraints are disallowed with such tables ',negative
'requested host died unknown host requested fallback random selection ',negative
'old logic ',negative
'initialize and evaluate ',negative
'required required required optional optional optional ',negative
'nullsafe issame for lists exprnodedesc ',negative
'choose keep the invalid stats and only change the setting ',negative
'otherwise convert rawtype will fall into the following ',negative
'datetimestamp higher precedence than stringgroup ',negative
'partitionid ',negative
'make the offset nonzero keep things interesting ',negative
'was committed all others open ',negative
'negative number ',negative
'check for empty partitions ',negative
'update only the basic statistics the absence column statistics ',negative
'prepare output buffer accept results ',negative
'the conf using the connection hook ',negative
'restriction subquery cannot use the same table alias one used the outer query ',negative
'the lock may have been released ignore and continue ',negative
'little strange that forget the dummy row read ',negative
'ask default first ',negative
'now start concurrent txn ',negative
'first look the classpath ',negative
'metastore schema only allows maximum for constraint name column ',negative
'process records until done ',negative
'weve prewarmed this database continue with the next one ',negative
'this method will scale down and round fit necessary ',negative
'will use decimal all else fails ',negative
'maps from work the dpps contains ',negative
'create the join operator with its descriptor ',negative
'execute child jvm ',negative
'invoked during from ast tree processing encountering ptf invocation tree form tokptblfunction name partitioningspec arguments setup ptfinvocationspec for this top level ptf invocation ',negative
'add the attemptdir the watch set scan and add the list found files ',negative
'now copy missing chunks and parts chunks into cache buffers ',negative
'the partitions were not added due memory limit return false ',negative
'the input sorted and are executing search based the arguments this filter ',negative
'joined with multiple small tables different keys ',negative
'add tez counters for task execution and llap ',negative
'are revoking from updating task ',negative
'project everything from the lhs and then those from the original ',negative
'test without nulls ',negative
'empty string delim ',negative
'getfunction ',negative
'reserve blocks this arena that would empty the sections requisite size ',negative
'looks like doesnt exist lock that two threads dont create once ',negative
'materialized view based rewriting disable for ctas and creation queries trying avoid any problem ',negative
'expect well only see notacquired here ',negative
'root interface for vector map join hash table which could hash map hash multiset hash set ',negative
'empty hschema construct ',negative
'through all small tables and get the mapping from bucket file name ',negative
'room above for rounding ',negative
'get explain plan for the query ',negative
'modify table schema the source ',negative
'the valuelist will save all data for listcolumnvector temporary ',negative
'use construct ',negative
'supposed get rows maxrows isnt set ',negative
'report the row its the first time ',negative
'select one child none child and none ',negative
'for nonlist single value the offset for the variable length long vlong holding the value length followed the key length ',negative
'slice before the start the split ',negative
'case overflow return ',negative
'both schema information are provided they should the same ',negative
'narrow down the possible choices based type affinity ',negative
'break loop equal comparator ',negative
'sgetlength and will never resize the buffer down ',negative
'todo this method ever called more than one jar getting the dir and the ',negative
'joinoperator assumes the key backed list consistent the value array also converted ',negative
'test with just high water mark ',negative
'already registered send updates this node for the specific source nothing for now unless tracking tasks later point ',negative
'without the round this conversion fails ',negative
'partial aggregation result returned terminatepartial partial result struct containing long field named count ',negative
'since now have scalar subqueries can get subquery expression having dont want include aggregate from within subquery ',negative
'allow analyze the whole table and dynamic partitions ',negative
'this negative because want the positive the default when nothing specified ',negative
'should also possible calculate this based tsgettime only ',negative
'summary for test result ',negative
'base time ',negative
'union ',negative
'repeat the expression the same batch the result must unchanged ',negative
'args child func ',negative
'need constant one side ',negative
'return either the arguments null ',negative
'because hive doesnt support null type appropriately typed boolean ',negative
'called generate the taks tree from the parse contextoperator tree ',negative
'verify resulting dirs ',negative
'create walker which walks the tree dfs manner while maintaining the operator stack ',negative
'without any caching ',negative
'instead fall back default behavior for determining input records ',negative
'very small heap elements ',negative
'only tasks that cannot finish immediately are preemptable other words all inputs ',negative
'the serde ',negative
'this pretty lame qtestutilqtestutil uses hivesiteurl load specific hivesitexml from dataconfsubdir this makes follow the same logic otherwise hiveconf and metastoreconf may load different hivesitexml for example hiveconf uses and metastoreconf dataconfhivesitexml ',negative
'make current task depends this new generated localmapjointask ',negative
'case the statement create materialized view ',negative
'interim row count can not less due containment assumption join cardinality computation interimnumrows represent number matches for join keys two sides represent number nonmatches ',negative
'for count ',negative
'result principal ',negative
'case all files locations not exist ',negative
'may the table getting created this load ',negative
'seconds for first retry assuming object was closed and open will fix ',negative
'read can unlock initial refcounts for the buffers that end before ',negative
'test for varchar type ',negative
'single value ',negative
'serializes decimal the maximum bit precision decimal digits note major assumption the fast decimal has already been bounds checked and least has precision not bounds check here for better performance ',negative
'for persistent function ',negative
'last chance look the old hive config value still avoiding defaults ',negative
'rename the event directories such way that the length varies will encounter createtable truncate followed insert for the insert set the event longer such that old comparator picks insert before truncate event ids createtable truncate insert changed createtable truncate insert but truncate have then having insert wont sufficient test the scenario set any event comes after createtable starts with event ids createtable truncate insert changed ',negative
'set the destination for the select query inside the ctas ',negative
'cannot use base file its range contains open write param writeid from basexxxx ',negative
'includes use the standard batch ',negative
'get vertex status can use but will expensive get status from for every refresh the lets infer the state from task counts ',negative
'select distinct windowing gby handled ',negative
'secondly extract information about the part the tree that can merged well some structural information memory consumption that needs ',negative
'this will also handle copyn files any ',negative
'just negate get the size ',negative
'both commit rollback clear all locks for this ',negative
'construct using ',negative
'only for charvarchar return types ',negative
'open txn ',negative
'principaltype ',negative
'handle the close ',negative
'else skip this one ',negative
'invalidate ',negative
'perform delete ',negative
'retrieve log from task tracker ',negative
'wait for the events processed ',negative
'not the first put blank separator ',negative
'get the result ',negative
'disable auto parallelism for bucket map joins ',negative
'trigger transformation ',negative
'first request for host ',negative
'round using the halfeven method used hive ',negative
'unsupported null ',negative
'should generate finf ',negative
'only need update the work with the hashtable sink operator with the same mapjoin desc can tell that comparing the bucket file name mapping map instance they should exactly the same one due the way how the bucket mapjoin context constructed ',negative
'tasks should have been started yet checked initial state ',negative
'test set random adds high precision ',negative
'writing both acid and nonacid resources the same txn txnid ',negative
'request came from old version the client this matches old behavior ',negative
'partition value not there then dynamic partition key ',negative
'utility visit all nodes ast tree ',negative
'getfunctionsstring catalog string schemapattern string functionnamepattern getschemas gettablesstring catalog string schemapattern string tablenamepattern string types gettabletypes gettypeinfo ',negative
'conf for llap ',negative
'get detailed read position information help diagnose exceptions ',negative
'query for minimum values all the queries and they can only increase any concurrent ',negative
'dummy impl ',negative
'may not idempotent but safe retry ',negative
'trailing spaces are not significant ',negative
'results cache directory should cleaned process termination ',negative
'sorting the list the descending order that deletes happen backtofront ',negative
'create new columnstatistics desc represent partition level column stats ',negative
'validate there the new insertions for column ',negative
'were combining sses and the time has expired ',negative
'compute value and hashcode wed either store forward them ',negative
'add udtf aliases ',negative
'find out all equivalent works the set ',negative
'parentschemaname ',negative
'handle case with nulls dont function the value null save time because calling the function can expensive ',negative
'translate projection indexes join schema adding offset ',negative
'evaluate the result given partition ',negative
'denominator zero convert the batch nulls ',negative
'disabled hive ',negative
'add back the queue for the next heartbeat and schedule the actual heartbeat ',negative
'check materialization defined its own invalidation time window ',negative
'this ensures that show locks prints the locks the same order they are examined ',negative
'test basic right trim bytes slice ',negative
'reach beginning the row group this required for ispresent stream ',negative
'also check hashcode ',negative
'default allow only addprops and dropprops alteroptype null case stats update ',negative
'already initialized ',negative
'semijoin created using hint marked useful skip ',negative
'doing string comps here value objects hive pig are different equals doesnt work ',negative
'cannot called during mapreduce tasks cache necessary values during query compilation and rely plan serialization bring this info the object during the mapreduce tasks ',negative
'read the stopping point for the first flush and make sure only see ',negative
'add the list work decompress ',negative
'the next item will new root ',negative
'optional string classname ',negative
'this contains basexxx deltaxxxyyy ',negative
'use the original fsop path here case while the new fsop merges files inside the directory the original movetask still commits based the parent note that this path can only triggered for merge thats part insert for now tables not support ',negative
'nonpartitioned ',negative
'environmentcontext ',negative
'initialize based table properties and they are not available see can find the job configuration have look these two places instead just the conf because streaming ingest uses table properties while normal hive sql will place this ',negative
'complete fractional digits shear off zero result ',negative
'ndv the join can not exceed the cardinality cross join ',negative
'this test done ',negative
'select none child one child and none ',negative
'multikey specific variables ',negative
'change the parent the original smbjoin operator point the map ',negative
'need check the other input branches for union following the first branch may need cast the data types for specific columns ',negative
'specifying explicitly will override the values from the url make sure dont override the values present the url with empty values ',negative
'add part spec range spec child tokwindow ',negative
'this test checks that have minor compacted delta for the txn range then will make any delete delta that range obsolete ',negative
'note rarely called unless buffers are very large evict lot lfu case ',negative
'check that the path between crs and prs there are only select operators ',negative
'generate special repeated case ',negative
'does this make sense ',negative
'but the registry was fully initialized thus need add ',negative
'for typeinfofactory use only ',negative
'get the key column names and check the keys are all constants ',negative
'only explain uses ',negative
'the form partition ',negative
'get columns for sel from lvj ',negative
'test date string ',negative
'insert overwrite command ',negative
'nonpartition expressions are converted nulls ',negative
'verify rename after bootstrap successful ',negative
'open record reader read next split ',negative
'side files are only created streaming ingest this compaction may have insert delta here with side files there because the original writer died ',negative
'the user asked for formatted output dump the json output the output stream ',negative
'adds delta and deletedelta ',negative
'test nul character ',negative
'the cross product the big table equal key rows values against the small table matching key which has value rows into overflow batch ',negative
'the encoding namecodes dont contain pound signs ',negative
'for cases where different rel nodes are referring same correlation var case not avoid generating another correlation var and record the rel using the same correlation ',negative
'means this interior transaction should already have transaction created that active ',negative
'the specified path directory iterate through all files ',negative
'nonjavadoc see ',negative
'although technically its unbounded its unlikely will ever see ndv ',negative
'need necessarily override this method since default impl assumes hdfs based location string ',negative
'objectstore methods overridden with injected behavior ',negative
'tablescan will also followed select operator find the expressions for the ',negative
'resolve futures used for testing ',negative
'call the regular method since does error checking ',negative
'the column number and type information for this one column string reduce key ',negative
'the registry dynamic refreshes ',negative
'the number times has returned nonnull errors ',negative
'only process partition which skewed and list bucketed ',negative
'builds partition spec can build suitable where clause ',negative
'iterator the reducer operator tree ',negative
'initialize satisfy compiler finals ',negative
'subset keycols the size should too this condition maybe too strict may extend the future ',negative
'all inserts are committed and hence would expect txntowriteid entries for acidtbl and entries for acidtblpart each insert would have allocated writeid ',negative
'temporary functions dont have any database namespace associated with ',negative
'check equivalent versions should compatible ',negative
'the starttime may not set the sparktask finished too fast because sparkjobmonitor will sleep for second then check the state right after sleep the spark job may already completed this case set starttime the same submittime ',negative
'exception handling routines ',negative
'',negative
'more required ',negative
'comments are separated see method getschema metastoreutils where this string columnscomments generated ',negative
'',negative
'initially children inputs set later with setinput methods ',negative
'now try evict with locked buffer still the list ',negative
'identical strings should equal ',negative
'the simd optimized form ',negative
'find databases which name contains tofind ',negative
'unicode case ',negative
'find given owid rowid pair deleted not perform two binary searches most the first binary search the compressed owids match found only then the next binary search the larger rowid vector between the given toindex fromindex ',negative
'stolen from hives metricstestutils probably should break out into its own class ',negative
'doesnt have notion small and saves the full value int overflow expectednull but was ',negative
'construct using ',negative
'run initialization statements the connection ',negative
'avro should always use the table properties for initialization see hive ',negative
'there more than one ',negative
'mystructset ',negative
'bytes per long the refs assume data will empty this just sanity check ',negative
'the bucket value should same for all the records ',negative
'none pattern ',negative
'want have only one auth bridge the past this was handled shimloader but since were longer using that well here ',negative
'check that the data removed ',negative
'partition ',negative
'write any remaining bytes the out stream ',negative
'test ',negative
'isexternalquery the call from within hte daemon permission check required ',negative
'the output partial aggregation struct containing long count two double averages two double variances and double covariance ',negative
'will cache have the entire part ',negative
'parse until union separator currentlevel ',negative
'the query should completed now ',negative
'input parameters ',negative
'this our way documenting that are mutating the contents this writables internal timestamp ',negative
'nonjavadoc see int this processor has pushpull model first call this method push but the rest pulled until run out records ',negative
'interfaces for functions without uservisible name ',negative
'when last txn finished abortcommit the currenttxnindex pointing that txn need start from next one any also batch was created but was never called want start with first txn ',negative
'statistics ',negative
'sumc ',negative
'need check guaranteed here was false would already the queue ',negative
'',negative
'for orc and parquet all the following statements are the same analyze table partition compute statistics analyze table partition compute statistics noscan ',negative
'create limit desc with limit value ',negative
'become available ',negative
'call the real precreatetable method ',negative
'the general column statistics ',negative
'test that schema was loaded correctly ',negative
'row ',negative
'this should make the search linear sync the beginning the block being searched set the comparison null and the flag reset the range should unset ',negative
'all columns the expression must partitioned columns ',negative
'iterate over all expression either after select select transform ',negative
'the values are equal the queue limit fixed ',negative
'only store the latest error there are multiple ',negative
'check admin option has been specified ',negative
'stack methods ',negative
'test that existing sharedwrite with new sharedwrite coalesces ',negative
'verify that udf whitelist can executed ',negative
'error with driver ',negative
'case lateral views followed join the same tree can traversed more than one ',negative
'operator allpath ',negative
'store postexec hooks calls can look them later ',negative
'only meant for use the querytracker ',negative
'have add this bit exception handling here because functionapply does not allow throw the actual exception that might checked exception wind needing throw runtimeexception with the previously thrown exception its cause however since returns throwable instead exception have account for the possibility that the underlying code might have thrown throwable that wrapped instead which case continuing throw the runtimeexception the best thing can ',negative
'liststring from tableacidtbl order didnt match autocommittrue stringifyvaluesrows ',negative
'add the rest the metadata keys ',negative
'path shared ',negative
'for ',negative
'originally the mvtask and the child move task the mrandmvtask contain the same movework object the blobstore optimizations are and the inputoutput paths are merged the move only movework the mvtask and the child move task the mrandmvtask will contain different movework objects which causes problems not just this case but also general the child move task the mrandmvtask should ',negative
'analyze create table command ',negative
'now copy over the data when isnullindex false ',negative
'tests location returned when the first file found later lookup order ',negative
'convert the rest and put into the last entry ',negative
'retrieved from object cache ',negative
'add virtual columns for analyze table ',negative
'the not vectorized information has been stored the mapwork vertex ',negative
'count null input which for count and output long just modes partial complete ',negative
'then the buffer was the list remove ',negative
'cast single column ',negative
'remove semijoin optimization creates cycle with mapside joins ',negative
'need first join and flush out data left the previous file ',negative
'local mode ',negative
'repl load not partition level always table level passing null for partition specs also repl load doesnt support external table and hence location set well ',negative
'the token file location and mapreduce job tag should right after the tool argument ',negative
'fail trying set transactional false not allowed ',negative
'global used when setting errors etc ',negative
'long between ',negative
'not closing this the moment shutdown since this could shared instance ',negative
'findroots returns all root operators ops that result operator ',negative
'not really much can here ',negative
'always use localhost for hostname some tests like ssl validation ones are tied localhost being present the certificate name ',negative
'files size for splits ',negative
'schedule outside the schedulelock which should only used wait the condition ',negative
'since store references hivedecimalwritable instances must use the update method instead plain assignment ',negative
'todo invalid valid invalid ',negative
'there end node that not the limitwherefalse ',negative
'need fetch the table before dropped that can passed postexecution hook ',negative
'this can happen all values stream are nulls last row group values are all null ',negative
'floatdouble string types have default value for null ',negative
'dependency class used for explain ',negative
'note that this uses short user name without consideration for kerberos realm this seems the common approach for hdfs permissions but may better consider the realm although not the host not the full name ',negative
'first need check valid convert mergeinsert into succeed modify the plan and afterwards the ast should acid table ',negative
'hcatpy will become the first argument pass command python ',negative
'modify the middle for view rewrite ',negative
'check the status all the columns all the partitions exists ',negative
'deserialization code ',negative
'from the duplicate publish ',negative
'',negative
'cannot contain nondeterministic function ',negative
'inherit java system variables ',negative
'note could just what already above from disk data except for the validation that not strictly necessary and knowntornstart which optimization ',negative
'null null not allowed this check ',negative
'drop all the tables ',negative
'not set environment context ',negative
'have set ndv ',negative
'column exprmap ',negative
'granttime ',negative
'single long key hash set optimized for vector map join ',negative
'literal decimal ',negative
'for current schema evolution ',negative
'method hivemetastoreclient ',negative
'compare timestamp integer seconds double seconds with fractional nanoseonds ',negative
'ignore will generated sel ',negative
'',negative
'generate sortcols and order ',negative
'create dummy task move needed ',negative
'this correlator was generated previous invocation this rule further work ',negative
'volatile because heartbeat may different thread ',negative
'lock entire heap heap still full should not able evict insert ',negative
'the nullable side the ',negative
'check the database location the default location based the old warehouse root then change the database location the default based the current warehouse root ',negative
'time zone file was written from metadata ',negative
'changes the value variable the corresponding change will made this mapping ',negative
'constructor used hiverexexecutorimpl ',negative
'this only lives for the duration the service init ',negative
'there could races here heartbeat delivered the old value just after have received successful confirmation from the api are about overwrite the latter could solve this adding version smth like that ignoring discrepancies unless have previously received update error for this task however the only effect ',negative
'dont clear the attempt the stuff will cleared ',negative
'singlecolumn long specific variables ',negative
'projections from child ',negative
'first group ',negative
'get local time tozone ',negative
'retrievecd false not need deep retrieval the table column descriptor ',negative
'execute cli driver work ',negative
'construct column statistics object from the result ',negative
'there are more than children any level dont anything ',negative
'now general lookup ',negative
'nothing when the optimization off ',negative
'lastaccesstime ',negative
'the underlying sslsocket object bound hostport with the given sotimeout and sslcontext created with the given params ',negative
'given jar add stored key and all its transitive dependencies value used for deleting transitive dependencies ',negative
'test from server client too ',negative
'this can set for old behavior nulls printed empty strings ',negative
'sessionhandle ',negative
'make nonblocking ',negative
'window frame that has only the start boundary then interpreted between start boundary and current row window specification with order specification and window frame interpreted range between unbounded preceding and current row window specification with order and window frame interpreted rows between unbounded preceding and unbounded following ',negative
'separate split ',negative
'add and scratchdir specify nondefault scratch dir ',negative
'handle tablescanoperator here can safely ignore table alias and the current comparator implementation does not ',negative
'check the input line multiline command which needs read further ',negative
'extract the collation for this operator and the collations ',negative
'set output format parameters these are not supported but only ',negative
'see custom compositekey class was provided ',negative
'doing characters comparison directly instead regular expression matching for simple patterns like abc ',negative
'anything but ',negative
'swap column vectors but keep selected vector unchanged ',negative
'these members are used outofband params for the innerloop supperprocessop callbacks ',negative
'there cannot exist any distinct aggregate ',negative
'dont actually create the key ',negative
'can the join operator converted sortmerge join operator ',negative
'can retrieve later ',negative
'ignore files eliminated ppd length ',negative
'above should have thrown there such catalog ',negative
'idempotent case for destdb ',negative
'there may race for this slot requery after delay with some probability ',negative
'serialize the row byteswritable ',negative
'dont propagate errors from close since this will lose the original error above ',negative
'there are not enough failed compactions yet should return false ',negative
'figure out can lock too ',negative
'optional bytes userpayload ',negative
'calculate key once lookup once ',negative
'invoke the method ',negative
'jdbc driver error ',negative
'check whether this list map ',negative
'augment conf with the settings from the started llap configuration ',negative
'map original column index among selected columns ',negative
'now generate operator ',negative
'update outrwsch ',negative
'make new generated task depends all the parent tasks current task ',negative
'convert the table acid todo remove transprop after hive ',negative
'random ',negative
'patterns that are included performance logging level performance mode show execution and performance logger messages ',negative
'now determine the small table results ',negative
'loop over all the operators recursively ',negative
'todo use fileid right from the list after hdfs get dfs client and ',negative
'generate the reducesinkoperator for the group query block the new reducesinkoperator will child inputoperatorinfo will put all group keys and the distinct field any the mapreduce sort key and all other fields the mapreduce value param numpartitionfields the number fields for mapreduce partitioning this usually the number fields the group keys return the new reducesinkoperator throws semanticexception ',negative
'data should not visible ',negative
'drop two files they are moved ',negative
'current map join null means has been handled currentmapjoin process ',negative
'cartesian product row count easy infer ',negative
'max numbitvectors bytes enough ',negative
'this will null master ',negative
'since using non strict mode get shared lock ',negative
'set synthetic flag that would push filter below this one ',negative
'check argument string array strings ',negative
'remove trailing comma ',negative
'workaround for ignore like jdo does for now ',negative
'sizes least tables nway join known and their sum smaller than ',negative
'get the return objectinspector ',negative
'since this conversion from nonacid acid nextwriteid should not have entry for this table also has unique index case should not violated ',negative
'recheck make sure someone didnt create while waited ',negative
'assert that the table created has hcat instrumentation and that were still able read ',negative
'check table transactional ',negative
'have manually reset the jobconf make sure gets picked ',negative
'property defined hivesitexml only ',negative
'for now this should true ',negative
'currently multiinsrt doesnt allow same tablepartition output branch ',negative
'typeclassname ',negative
'timestamp represented long internally need any thing here ',negative
'',negative
'are now sending message update again return both callbacks ',negative
'mark this task final map reduce task ignoring the optional merge task ',negative
'intermediate outputs joinsgroupbys ',negative
'used all flavors ',negative
'construct column name list and types for reference filter push down ',negative
'for partitionless table initialize partvalue empty string can have partitionless table even have partition keys when there only only partition selected and the partition key not part the projectioninclude list ',negative
'magic value usage invalid with nanotime once years may log extra ',negative
'close writer ',negative
'call file stat split mockmocktable ',negative
'exception happens after docopyonce then need call getfilestoretry with copy error false retry ',negative
'these many values reach beginning the row group ',negative
'write the blob ',negative
'simple tree with single parent ',negative
'this session should never default session unless something has messed ',negative
'when done handleupdate may break the iterator the order these checks important ',negative
'load the hash table ',negative
'get rid spills before start modifying the batch ',negative
'violating which can cause data loss ',negative
'elements key value ',negative
'compare the two map objects for equality ',negative
'database itself null then can not filter out anything ',negative
'parameter value still false connection the alter still goes through ',negative
'can null since the task may have completed meanwhile ',negative
'the simd optimized form ',negative
'test setter for configuration object ',negative
'unless least one was not found ',negative
'remove dpp based expected size the output data ',negative
'extract stage plans ',negative
'verify that session wasnt closed transport close ',negative
'move data from temp directory the actual table directory metastore operation required ',negative
'constants for bit variant ',negative
'need check the original schema see this actually fixed ',negative
'get all the driver run hooks and preexecute them ',negative
'tab tab ',negative
'check aggoutputproj projects only one expression ',negative
'runcreate materialized view dbname matview select from dbname unptned driver verifysetupselect from dbname matview unptndata driver ',negative
'this should never happen least for now throw ',negative
'filterinputrel ',negative
'subclass must indicate whether will transform the raw input before fed through the partitioning mechanics ',negative
'tracks instances known both yarn service and llap ',negative
'lets dont fail future timeout since have timeout for prewarm ',negative
'the join ',negative
'files size for splits ',negative
'oninit will called hmshandler initialization and set this true ',negative
'this method will return only after the cache has updated once ',negative
'test basic truncate bytes slice ',negative
'task typically task gets rerun times fails ',negative
'',negative
'not null constraint name default constraint name ',negative
'need extrapolation ',negative
'also match for this converted maponly job ',negative
'test with open transactions ',negative
'event ',negative
'for all other kinds operators assume the output big the ',negative
'set min ndv value both columns involved join ',negative
'indicating that the previous cookie has expired ',negative
'hive disable for explain analyze ',negative
'have classlevel annotation that says whether the udfs vectorization expressions ',negative
'read database table partition via cachedstore ',negative
'support for decimal input must convert ',negative
'use the basic the extended version the optimizer ',negative
'setup our batch with the same column schema the big table batch that can used build join output results ',negative
'make sure initialize necessary ',negative
'cases out could pass the path and type directly metastore ',negative
'create new conf file using contents from current one ',negative
'mapping from tablename table object metastore ',negative
'change the table name back ',negative
'job callable task for job list operation overrides behavior execute list jobs need override behavior cleanup there nothing done list jobs operation timed out interrupted ',negative
'write the orc file the mock file system ',negative
'reset ',negative
'unlock the previous lock ',negative
'test ',negative
'create warehouse with that user impersonation has issues ',negative
'',negative
'touch the next file ',negative
'step create the insert query ',negative
'now calculate which rows were filtered out they are logically matches ',negative
'pattern ',negative
'write key buffer compute hashcode and compare its new key will ',negative
'',negative
'this function called the parent should only include constant ',negative
'method get the valid write ids list for the given table ',negative
'worker threads stuff ',negative
'create test table ',negative
'noautocompact need check both cases ',negative
'will touch all blocks random order ',negative
'updates key with sequence number ',negative
'this thread should throw exception ',negative
'',negative
'successfully insert some data into acid tables that have records ',negative
'deltaxxxxyyyy format ',negative
'real column name which the operation being performed ',negative
'ignore ',negative
'partition spec not specified but column schema can have partitions specified ',negative
'expression for the table ',negative
'ignored the mbean itself was not found which should never happen because just accessed perhaps something unregistered inbetween but this happens just dont output the attribute ',negative
'for local src file copy hdfs ',negative
'groupby into the reduce keys ',negative
'encodedcolumnbatch already decompressed dont really need pass codec but need know the original data compressed not this used skip positions row index properly the file originally compressed then position compressed offset row index should skipped get uncompressed offset else position should not skipped ',negative
'add all except the right side the bad positions ',negative
'the else clause ',negative
'perform compaction join result after compaction should still the same ',negative
'multikey long check for repeating ',negative
'write the escaped byte ',negative
'build exprnode corresponding colums ',negative
'the new conjuncts are already present the plan bail out ',negative
'the value doesnt matter ',negative
'regardless whether was removed successfully after failing remove restart since just restart this from under the user mark handle properly when ',negative
'mark one the transactions exception test that invalid transactions are being handled properly exclude transaction ',negative
'commit the txn under hwm ',negative
'spot check decimal column modulo decimal column ',negative
'special char ',negative
'cannot obtain better estimate without providing somehow which case using statistics would completely unnecessary ',negative
'the target data textinputformat ',negative
'definitely not byte ',negative
'fits two longwords ',negative
'hint disable runtime filtering ',negative
'need evaluate result for every pruned partition ',negative
'not transactional nothing more ',negative
'through the argclasses and for any string void date time start looking for doubles ',negative
'out range due time ',negative
'number bits store the number zero runs ',negative
'get the latest timestamp all the cells the row timestamp from hbase ',negative
'whether this operator outer join ',negative
'see working uts ',negative
'all dml should fail with dummytxnmanager acid table ',negative
'the table alias should exist ',negative
'second insert round with new inserts into previously existing partition yesterday ',negative
'override external stuff these could also injected extra classes ',negative
'set small time unit cookie max age that the server sends ',negative
'need pass virtual columns reader ',negative
'this intentionally duplicated because hive ',negative
'the future could allow users specify quote character that doesnt need escaping but for now ',negative
'did not see skew key this table continue next table ',negative
'assumption top portion tree could only limitobproject ',negative
'for now expose nonprimitive string ',negative
'reallocate only any filters pruned ',negative
'get all the stats for colnames partnames ',negative
'close ',negative
'eventid ',negative
'nonjavadoc see ',negative
'read enough data for just the first message decoded ',negative
'maxdecimal with round longer than digits ',negative
'validate the first parameter which the expression compute over this should ',negative
'verify that non whitelist params cant set ',negative
'hadoop doesnt support credential merging this will fail ',negative
'initialize the rowid array when have some delete events ',negative
'todo rsjoin ',negative
'find the index the least significant bit that ',negative
'testing with nulls ',negative
'srsw lock are examining shared write ',negative
'use the remapped arguments for the nondistinct aggregate calls ',negative
'caching instances only case the yarn registry each host based list will get its own copy ',negative
'overflow batchs ',negative
'write json the temp file ',negative
'left border the min ',negative
'have more than one group key batch will buffer their contents dont buffer the key columns since they are constant for the group key buffer the nonkey input columns and buffer any streaming columns that will already have their output values ',negative
'there are some txns the list which does not have write allocated and hence ahead and get the next write for the given table and update with new next write ',negative
'get the databases for the desired pattern populate the output stream ',negative
'flag ',negative
'were dealing with array strings ',negative
'can tablereference subquery another ptf invocation for tableref set the source the alias returned processtable for subquery set the source the alias returned processsubquery for ptf invocation recursively call processptfchain ',negative
'implement needed ',negative
'walk through existing map truncate path that test wont mask then can verify location right ',negative
'should propagate the error message properly ',negative
'where the inverse multiplication result find the quotient integer decimal portion please see comments for ',negative
'revert back local ',negative
'resolve for the method based argument types ',negative
'create database specifically not replicated across per design since user drops database and recreates another with the same one want distinguish between the two will replicate the drop across but after that the goal that new created new replication definition should created the replication implementer above this thus extend noopreplicationtask and the only additional thing validate event type ',negative
'order key columns partition columns bucket number column ',negative
'job request got interrupted job kill should have started return client with with queueexception ',negative
'verify that udf default whitelist can executed ',negative
'simply create ',negative
'restore state repeating and non nulls indicators ',negative
'for now decrement the count avoid accounting errors ',negative
'the starting position grouping set need known ',negative
'',negative
'additional conf settings specified the command line ',negative
'need translate the exprnodefielddesc too identifiers struct ',negative
'let the vectorassignrow class the conversion ',negative
'the call succeeded presumably the api there ',negative
'',negative
'insert some data this will generate only insert deltas and delete deltas delta ',negative
'because need revert the tag row its old tag and cannot pass new tag this method which used get the old tag from the mapping newtagtooldtag bypass this method muxoperator and directly call process children process method ',negative
'should not getting invoked should invoked instead ',negative
'want have project after join since sqcountchecks count expression wouldnt needed further ',negative
'called lazymap ',negative
'print the per vertex summary ',negative
'nonjavadoc see ',negative
'has nothing cached ',negative
'implementation and assorted methods ',negative
'check left valid ',negative
'stateful implies nondeterministic regardless whatever the deterministic annotation declares ',negative
'convert the elements ',negative
'delete and all tables ',negative
'thread cancelling the query ',negative
'beginning with distcpoptions should honoured ',negative
'create the object inspector for the input columns and initialize the ',negative
'remember the jobconf cloned for each mapwork wont clone for again ',negative
'values accumulators can only read the sparkcontext side this field used when creating snapshot sent the rsc client ',negative
'dont create context for the column ',negative
'table and view second read entity ',negative
'serialize time ',negative
'set the server side see ',negative
'initially zero ',negative
'for every field ',negative
'for constructing hcatpartitions afresh argument ',negative
'but whether the table itself partitioned not know ',negative
'try again with null value ',negative
'implicit use batchindex ',negative
'check partitioning column order and types ',negative
'log exception but ignore inability start ',negative
'requires schema change ',negative
'small table information ',negative
'binary mode for embedded mode the jdbc uri the form and does not contain hostport string result port parsed per the java uri conventions ',negative
'carefully handle nulls ',negative
'not null constraint should reference single column ',negative
'were dealing with array arrays strings ',negative
'check the forward and backward compatibility ',negative
'not demuxoperator should have single child ',negative
'function name ',negative
'irrelevant see comment above irrelevant see comment above ',negative
'for primitive type add directly ',negative
'create new mapping ',negative
'try make something reasonable pass the base class ',negative
'prepare updated partition columns for small tables get the positions bucketed columns ',negative
'loginforeturning final parent ptnrootlocation ',negative
'singlecolumn string specific save key ',negative
'test dryrun schema initialization ',negative
'set them back ',negative
'dont save maxwidth automatically set based the terminal configuration ',negative
'will not deleted the user will run archive again clear this ',negative
'find out the segment with latest version and maximum partition number ',negative
'set num threads that singlethreaded checkmetastore called ',negative
'bucket bucket bucket ',negative
'always call init because the hook name the configuration could have changed ',negative
'truncatetable event unpartitioned table ',negative
'for the filtered out rows that didnt logically get looked the hash table need generate match results for those too ',negative
'have not added this column desc before bail out ',negative
'hiveconf well ',negative
'the work needs know about the dummy operators they have separately initialized ',negative
'find all aggregate calls without distinct ',negative
'this not real bloom filter but cheap version the memory access bloom filters several cases well have mapjoin spills because the value columns are few hundred columns text each while there are very few keys total few thousand this cheap exit option prevent spilling the bigtable such scenario ',negative
'need add select since order schema may have more columns than result schema ',negative
'getname ',negative
'todo remove hive ',negative
'make sure this isnt one the partitioning columns thats not supported ',negative
'output get the evaluate method ',negative
'second connection should not able see the table ',negative
'result ',negative
'get all locks for particular object ',negative
'cleanup thread ',negative
'the client has wait and retry ',negative
'shared all session functions ',negative
'after each test ',negative
'accurate long value cannot obtained ',negative
'this acid format always read recursively regardless what the jobconf says ',negative
'table empty can only lock the table ',negative
'this basically means stop has been called ',negative
'string comparisons ',negative
'recurse over all the source tables ',negative
'read just the first column ',negative
'first row the process should only started necessary may conflict with some ',negative
'convert long string the string output into the argument byte array beginning character the length returned ',negative
'execute another query ',negative
'check the output fixacidkeyindex should indicate nothing required fixing ',negative
'the output partial aggregation struct containing long count two double averages and double covariance ',negative
'operator stack the dispatcher generates the plan from the operator tree ',negative
'replace default keystore with keystore for wwwexamplecom ',negative
'get the abortedwriteids which are already sorted ascending order ',negative
'validation methods ',negative
'not need this format accessor using objectnode this candidate for removal well ',negative
'set auth privileges ',negative
'update partition schema have fields ',negative
'the varchar type info need set prior initialization and must preserved when the plan serialized other processes ',negative
'hive vars ',negative
'since were reusing the compiled plan need update its start time for current run ',negative
'imetastoreclient needed access token store dbtokenstore used will got via hivegetconfgetmsc thread where the called avoid the cyclic reference pass the hive class dbtokenstore where used get threadlocal hive object with synchronized metastoreclient using java reflection note there will two lifelong opened mscs one stored thread local hive object the other daemon thread spawned remove expired tokens ',negative
'create some delta directories ',negative
'the implementation hcatfieldschema bit messy since with the addition parametrized types char need represent something richer than enum but for backwards compatibility and effort required full refactoring this class has both type and typeinfosimilarly for ',negative
'success but with nothing return can return empty list ',negative
'',negative
'remove and sel introduced enforce bucketingsorting config ',negative
'addconstraint event ',negative
'our preliminary mapping wont work out well handle that below ',negative
'assume that putlock throws the middle its treat buffers not being locked and blindly deallocate them since they are not going used therefore dont remove them from the cleanup list will after sending consumer this relies sequence calls cachefiledata and sendecb ',negative
'capacity left node the next task should allocated node after times out ',negative
'guava stores the hashcodes little endian order ',negative
'resolve column expression input expression using expression mapping current operator ',negative
'optimization copied from bigdecimal ',negative
'root operator union can happen reducers ',negative
'udfyear ',negative
'remove trailing empty splits ',negative
'right trim and truncate slice byte array maximum number characters and return the new byte length ',negative
'create partitioned table ',negative
'build the new predicate and return ',negative
'the dummy option should not have made either only options ',negative
'try transform possible candidates ',negative
'for executors ',negative
'the error message changed for then need modification getnextnotification ',negative
'decimal noninteger conversion ',negative
'query hints ',negative
'populating the empty string bytes putting static since should immutable and can shared ',negative
'next file the path ',negative
'byte bit patterns the form xxxxxx are continuation bytes all other bit patterns are the first byte character ',negative
'unique rows ',negative
'instantiate driver compile the query passed this udf running part existing query which may already using the sessionstate txnmanager this new driver also tries use the same txnmanager then this may mess the existing state the txnmanager initialize the new driver with new txnmanager that does not use the ',negative
'shared plan utils for tez ',negative
'sometimes rowschema empty fetch stats columns exprmap ',negative
'batchindex ',negative
'the writehwm under txnhwm ',negative
'think this wrong the alter table statement should come the table topic not the ',negative
'write partitioninfo into output ',negative
'return the mockinstances connector ',negative
'make sure works with nothing expire ',negative
'reach here succeed ',negative
'this tests the case where older data has ambiguous structure but the correct interpretation can determined from the repeated name ',negative
'for current query ',negative
'the rhs table columns should not output from the join ',negative
'copy across file system encryption zones ',negative
'write many records because sometimes the recordwriter for the format test behaves different with one record than bunch records ',negative
'groupby query ',negative
'add log links and other diagnostics from yarn service ',negative
'the following two are used for join processing ',negative
'also reading beyond our byte range produces null ',negative
'for acid nonbucketed case the filenames have the format consistent with insertupdatedelete ops like copy etc the extension only maintained for files which are compressed ',negative
'this nested sql script then flatten ',negative
'generate groupbyoperator ',negative
'indicates request has completed node ',negative
'with local spark context all user sessions share the same spark context ',negative
'',negative
'create clone the operator ',negative
'input data ',negative
'copy the src the destination and create local resource ',negative
'the jobtracker setting its initial value ',negative
'getrows will call estimaterowcount ',negative
'list configurations currently the list consists hadoop version and execution mode only ',negative
'the properties does not define any transactional properties return default type ',negative
'well wait for for node creation ',negative
'least single item project required ',negative
'get number partitions doing count partid ',negative
'test that existing exclusive table with new sharedread coalesces ',negative
'need replace the dummy operators the work with the cloned ones ',negative
'assumes the reader count has been incremented automatically the results cache either lookup creating the cache entry ',negative
'assumes partitioned table ',negative
'just set really small lower bound ',negative
'admin check ',negative
'done grouping partitions within tabledir ',negative
'initialize the merge operators first ',negative
'this session bad dont allow reuse just convert normal get ',negative
'nulls repeating ',negative
'special handling for druid rules here otherwise planner will add druid rules with logical builder ',negative
'some the partitions miss stats ',negative
'know rowset has only one element ',negative
'note there are many different onsuccessonfailure callbacks floating around that this will probably called twice for the done state this given the sync ',negative
'import static import static ',negative
'gmt gmt americanewyork est ',negative
'symlink file contains first file from first dir and second file from second dir ',negative
'tblname ',negative
'decrement only element was removed ',negative
'child map join ',negative
'this many bytes are necessary store the reversed nanoseconds ',negative
'not order preserving ',negative
'gen tree from resolved parse tree ',negative
'pattern ',negative
'for auto reduce parallelism max reducers requested ',negative
'get locations again and make sure theyre the same ',negative
'statistics stored metastore ',negative
'optional sourcestateproto state ',negative
'can invalidate the entry now but calling removeentry requires write lock and may already have read lock taken now add entriestoremove delete later ',negative
'limit factor too big ',negative
'useexactbytes ',negative
'this going slow hold ',negative
'optional int vertexparallelism ',negative
'column name the second group from current match ',negative
'return how the list columns passed match return nomatch either the list empty null there mismatch for and return nomatch return completematch both the lists are nonempty and are same return prefixcolmatch list strict subset list and return prefixcolmatch list strict subset list for and return completematch prefixcolmatch and prefixcolmatch respectively ',negative
'there could some spilled partitions which needs cleaned ',negative
'require delete privilege this insertoverwrite ',negative
'testparam ',negative
'merge join into multijoin operators possible ',negative
'create walker which walks the tree dfs manner while maintaining the operator stack the dispatcher generates the plan from the operator tree ',negative
'the partition does not have partition level privilege table level ',negative
'copy all the properties ',negative
'the cache entry has just been invalidated need for the scheduled invalidation ',negative
'also set this the thread contextclassloader new threads will inherit this class loader and propagate into newly created configurations those ',negative
'read configuration for the target path first from jobconf then from table properties ',negative
'empty list non partitioned ',negative
'mark the mapredwork and filesinkoperator for gathering stats ',negative
'allow numeric string ',negative
'valiade ',negative
'bother about generating schema only schema retriever class wasnt provided ',negative
'quarter granularity ',negative
'read should get rows immutable mutable ',negative
'prepare the bloom filter ',negative
'all rows qualify ',negative
'the sign the string for required because prepares arguments expecting sign will fail prepare the arguments correctly without the sign present ',negative
'now lets load this file into new hive table ',negative
'end readonlysublist ',negative
'initialize udf which will output the return type for the udf ',negative
'since this special cased when rewritten subqueryremoverule ',negative
'query the hive query string select from src associated with this set tasks logs ',negative
'ignore nondirectory files ',negative
'nonjavadoc see javalangstring javalangstring ',negative
'fkname ',negative
'populating the empty string bytes putting static since should immutable and can shared ',negative
'this member has information for data type conversion not defined there conversion ',negative
'the destination file exists for some reason delete ',negative
'for the file size check ',negative
'the reducer contains groupby which needs restored ',negative
'replace prs with crs and remove operator sequence from prs crs ',negative
'set periodic progress reporting case the udtf doesnt output rows ',negative
'swap the fields with the passed orcstruct ',negative
'noop for session killed ',negative
'first infer the type object ',negative
'nonconstant nonprimitive constants ',negative
'dont want set autocommit truefalse get mixed with set hivefoobar ',negative
'add jar current thread class loader dynamically and add jar paths jobconf spark may need load classes from this jar other threads ',negative
'pound statement ifelseendif ',negative
'construct one location map not exists ',negative
'findwriteslot slot slot tripleindex tripleindex empty ',negative
'some other operation progress using the same lock subsequent fragmentcomplete expected come ',negative
'but this tricky implement and well leave future work for now ',negative
'key key key ',negative
'slow way get the number decimal digits ',negative
'external table ',negative
'nothing two messages have canceled each other before could react ',negative
'previous row was for large bytes value use smallbuffer possible ',negative
'for use that the minimum equal key can advanced ',negative
'had the put succeeded for our new buffer would have refcount from put and from notifyreused call above old buffer now has the from put new buffer not cache releasebuffer will decref the buffer and also deallocate ',negative
'form key object array ',negative
'execute the udf ',negative
'after column not null but did not find ',negative
'assumes the lists are sorted ',negative
'change the current thread name include parent thread executed thread pool useful extract logs specific job request and helpful debug job issues ',negative
'subject list exceptions writeidlist not show above example ',negative
'dont bail failure try detail below ',negative
'tez needs its own scratch dir per session todo delink from sessionstate tezsession can linked different hive sessions via the pool ',negative
'common name constants for event messages ',negative
'used hashbased groupby mode hash partials ',negative
'even the cleanup throws some exception will continue ',negative
'the sql should completed now ',negative
'those top layer reducesinkoperators ',negative
'curr value becomes old and viceversa ',negative
'test repeating left ',negative
'none the partitions will dumped the partitions list was empty ',negative
'does vectorization use stripped char values ',negative
'this messagetype only has one optional field whose name mapcol original type map ',negative
'create enough elementconverters note have have separate elementconverter for each element because the elementconverters can reuse the internal object its not safe use the same elementconverter convert multiple elements ',negative
'remoteexception with will thrown the file currently held writer ',negative
'set new and verify get ',negative
'convert ast expr exprnode ',negative
'any name does not matter ',negative
'reduceside join use mrstyle shuffle ',negative
'insert overwrite create some invalid deltas and import into nonmm table ',negative
'check can process not the index distinct ',negative
'preserve partitioning and ordering ',negative
'row group position within stripe ',negative
'nothing ',negative
'read state ',negative
'sort the list requested ',negative
'passed but super slow ',negative
'required required required required ',negative
'clear gworkmap ',negative
'convert the table acid ',negative
'most the method got skipped but still need handle the duck ',negative
'setup serde ',negative
'type major since theres base yet ',negative
'populate other data structures ',negative
'call increasebufferspace will ensure that buffer points byte with sufficient space for the specified size ',negative
'create the dummy aggregation ',negative
'used give unique name each subquery currently there can most subqueries query the where clause and the having clause ',negative
'load bytescolumnvector copying large data enough force the buffer expand ',negative
'these are suffixes attached intermediate directory names used the ',negative
'shouldnt getting called hive but somehow does should just set all the configurations for input and output ',negative
'add keys reduce keys ',negative
'checktgt calls ugirelogin only after checking close tgt expiry hadoop relogin actually done only every minutes hadoop ',negative
'caches disabled nodes for quicker lookups and ensures request node which was skipped does not out order ',negative
'rowidoffset could all files before current one are empty ',negative
'since enforcing precision and scale can cause hivedecimal become null must read enforce here and either return null buffer the result ',negative
'send only the state has changed ',negative
'perf times ',negative
'mserdeinfo serdeinfo should same well ',negative
'now positions contains all the distinct positions need first sort them group set and then get their position later ',negative
'the operator tree till the sink operator has already been processed while fetching the next row fetch from the priority queue possibly containing multiple files the small table given file the big table now process the remaining tree look comments dummystoreoperator for additional explanation ',negative
'the event will not sent ats there are too many outstanding work submissions ',negative
'',negative
'the avro deserializer would deserialize our object and return back list object that hive can operate here should getting the same object back ',negative
'susbset files for the partition are sufficient for the optimization ',negative
'note critical this here that logj reinitialized before any the other core hive classes are loaded ',negative
'nodemap registration ',negative
'create all the files this required because empty files need created for empty buckets ',negative
'end hiverelmdcostjava ',negative
'the only difference numeric types pick the method with the smallest overall numeric type ',negative
'empty batch will appear the end the stream ',negative
'this true then there data the batch have hit the end input ',negative
'now could get previous and next day figure our how many hours were inserted removed and from which the days etc but this point our gun pointing straight our foot ',negative
'valid range rangerows between preceding and preceding for preceding case ',negative
'add empty line ',negative
'all good ',negative
'binary ',negative
'interestingly decimal means decimal ',negative
'look for databases without pattern ',negative
'the sortmerger heap data structure that stores pair deleterecordkey deletereadervalue each node and ordered deleterecordkey the deletereadervalue the actual wrapper class that has the reference the underlying delta file that being read and its corresponding deleterecordkey the smallest record for that file each iteration this loop extractpoll the minimum deleterecordkey pair once have processed that deleterecordkey advance the pointer for the corresponding deletereadervalue the underlying file itself has more records then remove that pair from the heap else add the updated pair back the heap ',negative
'multikey specific imports ',negative
'strictly not required just for consistency ',negative
'new tai lue letter low kva bytes ',negative
'nonjavadoc see javautilmap ',negative
'during the tests run with prealloc the logs windows systems prealloc was seen take seconds resulting test failure client timeout first session set env and directly order handle static initgc issues ',negative
'partialcount ',negative
'make decimal batch with three columns including two for inputs and one for the result ',negative
'this point weve found the fork the pipeline that has the pruning child plan ',negative
'create the default properties object ',negative
'cant set constructor due circular dependency ',negative
'get the distribute aliases these are aliased the entries the select list ',negative
'process location onebyone ',negative
'the planner seems pull this one out ',negative
'simple pattern hmsnnnnnnnnn ',negative
'signature and generate field expressions for those ',negative
'skip the transaction under evaluation already committed ',negative
'copy logintimeout from driver manager thrift timeout needs millis ',negative
'all evaluation should processed here for valid aliasfiltertags for mapjoin filter tag precalculated mapredlocaltask and stored with value ',negative
'the arg self ',negative
'both sides ',negative
'parse from creation metadata ',negative
'normal insert ',negative
'since key expression can calculation and the key will into scratch column need the mapping and type information ',negative
'make sure that not change anything there anything wrong ',negative
'set the correct last repl return the user ',negative
'expand and write result ',negative
'now insert the new buffer its place and restore heap property ',negative
'this case have scale before division otherwise might lose precision ',negative
'now try reuse with other sessions remaining should still work ',negative
'verifysetup set true all the test setup will perform additional verifications well which useful verify that our setup occurred correctly when developing and debugging tests these verifications however not test any new functionality for replication and thus are not relevant for testing replication itself for steady state want this false ',negative
'test that are cleaning aborted transactions with components left txncomponents put one aborted transaction with entry txncomponents make sure dont accidently clean too ',negative
'copy entire string value ',negative
'puts long little endian order ',negative
'this the only public constructor filesplit ',negative
'save prev val the key threadlocal ',negative
'persist the column statistics object the metastore note this function shared for both table and partition column stats ',negative
'create new the first time fire the rule ',negative
'subsequent instances when its taken off the queue ',negative
'first entry count ',negative
'age ',negative
'the sorting property not obeyed ',negative
'the code point exists deletion set need emit out anything for this code point ',negative
'create tables verify query ',negative
'was able execute something before the last blacklist reset the exponent ',negative
'nonjavadoc see ',negative
'convert date value days ',negative
'',negative
'test that the whole things works when theres nothing the queue this just survival test ',negative
'need propagate this the responder ',negative
'there distinctfuncexp add all parameters the reducekeys ',negative
'validate all integer type values are stored correctly ',negative
'output record readers ',negative
'regen plan from optimized ast ',negative
'for each aggregation ',negative
'array full ',negative
'have just removed the session from the same pool dont check concurrency here ',negative
'field node get amyfield from ',negative
'add new synthetic columns for projections not provided select ',negative
'aggregate does not change input ordering corvars will ',negative
'null ',negative
'restore broken links between operators and remove the branch from the original tree ',negative
'compile internal will automatically reset the perf logger ',negative
'dump and load only second insert records ',negative
'here may checking level lock against table level lock alternatively could have used intention locks for example request for lock table would cause lock that contains the table similarly partition level ',negative
'and not ',negative
'todo handle quoted tablenames ',negative
'zero and above numbers indicate big table key needed for small table result area ',negative
'todo ',negative
'traversing origin find exprnodedesc sources and replaces with exprnodedesc targets having same index ',negative
'equivalent aliases for the column ',negative
'see can load all the delete events from all the delete deltas memory ',negative
'from key and value tabledesc ',negative
'float min and max ',negative
'case cross join disable hybrid grace hash join ',negative
'columnstatisticsobj with info about its table partition table partitioned ',negative
'write the terminating null byte ',negative
'means index doesnt exist ',negative
'cast ',negative
'overflowbatchsize overflow ',negative
'extract the hex digits num into value from right left ',negative
'explicitly remove the setting lastreplid from the object parameters loadtask going run multiple times and explicit logic place which prevents updates tables when level last repl set and create alterdatabasetask the end processing database ',negative
'rewrite into query tokquery tokfrom join tokinsert tokdestination tokdir toktmpfile tokselect ',negative
'need fix create the two replacement project ',negative
'all tests are identical the other seek tests ',negative
'batches will sized ',negative
'reuse the reencoder evolved schema create and store new encoder the map for reuse ',negative
'',negative
'try with chunked streams ',negative
'keep asis ',negative
'todo this boolean flag set only stats annotation this point ',negative
'table locks for this ',negative
'the operators specified depth and removed from the tree ',negative
'key out range for whole hash table ',negative
'now different from ',negative
'have mindful order during filtering are not returning all partitions ',negative
'add hive function names for functions that arent infix operators add open ',negative
'however these expressions should not considered valid expressions for separation ',negative
'fall through acquire ',negative
'always reschedule the next callable irrespective task count case new tasks come later ',negative
'since user names need valid unix user names per ieee std they cannot contain comma can safely split above string comma ',negative
'for all practical purposes code point fancy name for character java char data type can store characters that require bits less however the unicode specification has changed allow for characters whose representation requires more than bits therefore need represent each character called code point from hereon int more details ',negative
'this map defines the progression casts numeric types ',negative
'look for tables without pattern ',negative
'columns are output from the join from the different reduce sinks the order their ',negative
'leading space significant ',negative
'this appears leave the remove transaction inconsistent state but the heartbeat now cancelled and will eventually time out ',negative
'column name not contained needed column list then partition column not need evaluate partition columns filter expression since will taken care partitio pruner ',negative
'only single subquery expr supported ',negative
'char and varchar types can specified with maximum length ',negative
'close the existing ctx etc before compiling new query but does not destroy driver ',negative
'ideally should just call here but that wont work since needs job object instead jobcontext which are handed here ',negative
'trimfalse ',negative
'have checked all the parents for the index position ',negative
'note get query here rather than the caller where would more correct because know which exact query intend kill this valid because are not expecting query change never reuse the session for which ',negative
'calculate the std result when count public vectorization code can use etc ',negative
'make sure aborted txns dont redflag basexxxx hive ',negative
'transformation outer query left join inner query correlated predicate ',negative
'make ssl connection ',negative
'previous batch was the last group batches remember the next the first batch new group batches ',negative
'nothing because and and and not supports null value evaluation note the future all udfs that treats null value unknown both parameters and return values should derive from common base class udfnullasunknown instead listing the classes here would test whether class derived from that base class all childs are null set unknown true ',negative
'unixtimestampargs tounixtimestampargs ',negative
'druid only support appending more partitions linear and numbered shardspecs ',negative
'first check the two table scan operators can actually merged ',negative
'create empty output object which will populated when convert invoked ',negative
'iterative through the children dfs manner see there more than table alias ',negative
'obtain list col stats use default they are not available ',negative
'position doesnt make sense for async reader chunk order arbitrary ',negative
'preallocated member for remembering the big tables selected array the beginning the process method before applying any filter for outer join need remember which rows did not match since they will appear the outer join result with nulls for the ',negative
'validate that can add partition without escaping escaping was originally intended avoid creating invalid hdfs paths however escape the hdfs path that deem invalid but hdfs actually supports possible create hdfs paths with unprintable characters like ascii metastore will create another directory instead the one are trying repair here ',negative
'should the same the moveworks sourcedir ',negative
'there need for the user specify mapjoin for ',negative
'compaction doesnt work under transaction and hence pass for current txn ',negative
'release initial refcounts ',negative
'the registers ',negative
'check the specified partitions ',negative
'statsdesc ',negative
'use when merging variance and partialcount and mergecount note mergecount and mergesum not include partialcount and partialsum yet ',negative
'evaluate then expression only and copy all its results ',negative
'use loadtask ',negative
'theres fraction part return immediately avoid the cost divide ',negative
'either the slice comes entirely after the end split following gap cached data the split ends the middle the slice its the same the startix logic wrt the partial match either dont want cannot use this theres need distinguish these two cases for now ',negative
'random ',negative
'semanticanalyzer ',negative
'this regex bit lax order compensate for lack any escaping done amazon for example useragent string can have double quotes ',negative
'replace the filter expression reference output the join ',negative
'java primitive type ',negative
'skipping columns since partition level field schemas are the same table levels skipping partition keys since the same table level partition keys ',negative
'gettable invoked after fetching the table names ',negative
'add child project rel needed generate output input sel rel ',negative
'create the new rowschema for the projected column ',negative
'create dummy partitions ',negative
'child the name the column ',negative
'full decimal maximum digits lower longs digits here ',negative
'use this copy method when the source batch safe and will remain around until the target batch finished any bytes column vector values will referenced the target column instead copying ',negative
'finally submit the job ',negative
'read the first characters from the url ',negative
'and initiates the sasl handshake ',negative
'convert decimal into the scratch buffer without allocating byte each time for better performance ',negative
'nonjavadoc see ',negative
'two way left outer right outer join take selectivity only for ',negative
'column projection ',negative
'mysql returns the string not wellformed numeric value but decided return null instead which more conservative ',negative
'repl status ',negative
'gby operator the operator ',negative
'hash map overhead ',negative
'scratch dir initially ',negative
'run the worker explicitly order get the reference the compactor job ',negative
'now start with everything and test losing stuff ',negative
'default assume can user directsql thats kind the point ',negative
'only teztask sets this and then removes when done dont expect see ',negative
'incorrect precision expected xxxxx yyy but was xxxxx yyy ',negative
'verify when third argument repeating ',negative
'metrics system will get this via reflection ',negative
'like type ',negative
'dont clear the hash table reuse possible will take care ',negative
'same but repeating value null ',negative
'should get back the latest reader schema ',negative
'drop table without saving trash setting the purge option ',negative
'success ',negative
'the cannot inlined need the hasnext evaluated post the current retrieved ',negative
'test for null partition value map ',negative
'clone make sure new prop doesnt leak ',negative
'put the query user not llap user into the message and token ',negative
'got error attempting ssclose then its not likely that sserr valid were back systemerr also dont change the return code simply log warning and return whatever return code expected already ',negative
'following sequence ',negative
'note the only sane case where this can happen the nonpool one should get rid nonpool case perf doesnt matter might well open get time and then call update like the else can happen the user sets the tez flag after the session was established ',negative
'tried all back original code for error message ',negative
'parents arent llap neither should the child ',negative
'will update current number open txns back ',negative
'omitting zone time part allowed ',negative
'conflicting operations proceed with the rest commit sequence ',negative
'returns false there selectexpr that not constant aggr ',negative
'the subquery ',negative
'for backward compatibility ',negative
'over all the keys and get the size the fields fixed length keep ',negative
'nonjavadoc this provides lazyshort like class which can initialized from data stored binary format see int int ',negative
'because should impossible get incompatible outputs ',negative
'collect columns copy from the big table batch the overflow batch ',negative
'escaping happened need copy bytebybyte set the length first ',negative
'will used estimate num nulls ',negative
'test for null input strings ',negative
'self describing need send column info per partition since its not used anyway ',negative
'dont emit usertimestamp info test mode that the test golden output file fixed ',negative
'list sparkworkdependency ',negative
'could not renewed return that information ',negative
'map that keeps track the last operator task the following work ',negative
'test that not changing the database and the function name but only other parameters like ',negative
'reset table params ',negative
'deep copy expr node desc ',negative
'use not private because the copyonwrite irrelevant for deleted file ',negative
'',negative
'virtual trailing zeroes ',negative
'nothing cleanup ',negative
'then scale with ',negative
'through the reduce keys and find the matching columns the reduce values ',negative
'nesting not allowed ',negative
'add key key slot slot pairindex pairindex found key ',negative
'for debug tracing information about the map reduce task operator operator class etc ',negative
'special handling ',negative
'create walker which walks the tree dfs manner while maintaining ',negative
'cartesian product not supported strict mode ',negative
'the expected tags from the parent operators see processop before ',negative
'mimicking behaviour createtabledesc tabledesc creation returning null table description for output ',negative
'nonjavadoc see int javalangstring ',negative
'call lhs and rhs and and join the results with optype string ',negative
'process multikey outer join vectorized row batch ',negative
'there should only one mrinput ',negative
'then check distinct key ',negative
'initialize deleteeventwriter not yet done lazy initialization ',negative
'dont log exception here ',negative
'the root might have changed because tree modifications compute the new root for this tree and set the aststr ',negative
'apply rest the configuration only hiveserver ',negative
'cant divide null ',negative
'set java key provider for encrypted hdfs cluster ',negative
'try get default value only this default constraint ',negative
'the header look for use xxsrfheader this null methods not filter default getoptionsheadtrace null ',negative
'vectormapoperator ',negative
'print header vertices status total completed running pending failed killed ',negative
'the eventual goal monitor the progress all the tasks not only the map reduce task the execute method the tasks will return immediately and return task specific handle monitor the progress that task right now the behavior kind broken execdrivers execute method calls progress instead should invoked driver ',negative
'accumulo ranges ',negative
'convert the join operator bucket mapjoin join operator ',negative
'common comparison class for charvarchar string ',negative
'skip the same value avgdistinct true ',negative
'can safely convert the join map join ',negative
'will retrieve stats from the metastore only for columns that are not cached ',negative
'could make some assumptions given how the reader currently does the work consecutive chunks etc blocks and columns stored offset order the lists but wont just save all the chunk boundaries and lengths for now ',negative
'assert that there one partition present and had hcat instrumentation inserted when was created ',negative
'source operator get the number entries ',negative
'use construct ',negative
'step replace the corresponding part childmrworks mapwork ',negative
'transaction batch size case ',negative
'repeat the same check for droptable ',negative
'table exists ',negative
'should not get here ',negative
'conditions ',negative
'setref used below and this safe because the reference data owned this column vector this column vector gets reused the whole thing reused together there danger dangling reference ',negative
'form result from lower middle and middle words ',negative
'the new base dir now has two bucket files since the delta dir has two bucket files ',negative
'proceed only wed actually succeeded anyway otherwise ',negative
'testparam ',negative
'excludedprovidedby framework excludedconfigured ',negative
'hcat output format related errors ',negative
'indicate last batch current group ',negative
'helper function retrieve the basename local resource ',negative
'since currentreadblock may assigned currentwriteblock need store ',negative
'this will throw expected exception since client communicating with the wrong http service endpoint ',negative
'throw hiveexception the tablepartition archived ',negative
'find out database name and table name target table ',negative
'need some value that indicates null ',negative
'done have bytes continue reading this buffer ',negative
'update old data with values for the new schema columns ',negative
'get the sort order ',negative
'are just relay send unpause encoded data producer ',negative
'the tablescanoperators needed columns are just the data columns ',negative
'for nonacid tables paths all data files are getoriginalfiles list ',negative
'verify found them all ',negative
'close output stream open ',negative
'tokalterviewas ',negative
'',negative
'the api that finds the jar being used this class disk ',negative
'number columns pertaining keys vectorized row batch ',negative
'because every value will null ',negative
'inputoutput settings ',negative
'cte ',negative
'that will not too far from the correct digit later ',negative
'infovalue ',negative
'doublecheck the header under lock ',negative
'now add the corvars from the input starting from position oldgroupkeycount ',negative
'use junits assume skip running this fixture against any storage formats whose serde the disabled serdes list ',negative
'hive has max limit for strings ',negative
'for snapshot isolation dont care about txns greater than current txn and stop here also need not include current txn exceptions list ',negative
'have emulate distinct otherwise tables with the same name may returned ',negative
'true insert overwrite ',negative
'able report progress ',negative
'dont use assertfail are catching assertion errors ',negative
'the object that determines equal key series ',negative
'collect column access information ',negative
'decide whether this already hashmap keys hashmap are deepcopied version and need use ',negative
'task lock but acquires lock the scheduler ',negative
'noop ',negative
'this internal error something odd happened with reflection log and dont output the bean ',negative
'routines for copying between ',negative
'there are fewer than leadamt values leadwindow start reading from the first position otherwise the window starts from nextposinwindow ',negative
'this point have seen the exponent letter and have decimal information isnegative precision integerdigitcount and fast fast fast after determine the exponent will appropriate scaling and fill fastresult ',negative
'map ',negative
'create the destination does not exist ',negative
'special handling for timezone ',negative
'lazybinary seems work better with row object array instead java object ',negative
'dealing with views ',negative
'maybe valid too expensive check without parse ',negative
'calculate relative offset ',negative
'rows are combination the ondisk hashmap and the sidefile ',negative
'the vertex that this operator belongs ',negative
'funcname ',negative
'since dont have nonnative passthru version vectorptfoperator not have enableconditionsmet like have for etc ',negative
'stateful ',negative
'check query results cache the case that row column maskingfiltering was required not support caching ',negative
'handle the case like sumlagf over aggregation function includes laglead call ',negative
'',negative
'build not null conditions ',negative
'obtain col stats for partitioned table ',negative
'check access columns from columnaccessinfo ',negative
'such abc ',negative
'the key wasnt present the mapping and the function didnt return default value ignore and use our default ',negative
'binaryval ',negative
'wrapper extends qlmetadatapartition for easy construction syntax ',negative
'subquery either where lhs subquery form where exists subquery form first case lhs should not bypassed ',negative
'this just for debug ',negative
'local path doesnt depend drone variables ',negative
'insert overwrite acid table from source table ',negative
'could not have removed the pool for this session would have canceled the init ',negative
'query ',negative
'this table cannot big table ',negative
'map splits map splits ',negative
'copy jar dfs ',negative
'visiblefortesting ',negative
'test that jdbc does not allow shell commands starting with ',negative
'the join keys matches the skewed keys use the table skewed keys ',negative
'the first byte the vint the vint itself indicating that there second vint but the nanoseconds field actually ',negative
'this test the parameter value denotes the method which needs throw error ',negative
'first read the header due orc estimates zcr etc this can complex ',negative
'you change this function remove the ignore from test these changes mysql and mssql use the state code for rollback postgres uses and oracle seems return different sqlstates and messages each time ive tried capture the different error messages there appear fewer different error messages than sql states derby and newer mysql driver use the new ',negative
'now reinitialize batch simulate batchobject reuse ',negative
'remove all detached objects from the cache since the transaction being rolled back they are longer relevant and this prevents them from reattaching future transactions ',negative
'were pretty screwed cant load the default conf vars ',negative
'current hashmap use ',negative
'call here because this point the has been set ',negative
'start tests that check values from pig that are out range for target column ',negative
'form the expression node corresponding column ',negative
'',negative
'truncate reopening fileoutputstream ',negative
'first find the path searched ',negative
'update the database cache ',negative
'present the child hence add child project rel ',negative
'clusterby ',negative
'add signature ',negative
'singlecolumn string get key ',negative
'parse the string determine column level storage type for primitive types for variable length string format storage for fixed width binary storage bytes for table storage type which defaults utf string string data always stored the default escaped storage format the data types byte short int long float and double have binary byte oriented storage option ',negative
'append mode ',negative
'nonjavadoc see javalangstring javalangstring javalangstring javautillist ',negative
'right now they come from jpoxproperties ',negative
'template classnameprefix returntype funcname ',negative
'use only reducer for order ',negative
'this point dont have anything special this case just run through the regular paces creating new task ',negative
'connection above ',negative
'continue the next code point ',negative
'open the client transport ',negative
'euro sign bytes ',negative
'operator check uses struct ',negative
'restrictionm disallow nested subquery expressions ',negative
'can safely convert the join map join ',negative
'extract the partitions keys segments granularity and partition key any ',negative
'not use the new cache buffers for the actual read given the way read api therefore dont need handle cache collisions just decref all the buffers ',negative
'this happens case map join operations the tree looks like this are here perhaps mapjoin are the pointed above and may have already visited the following the have already generated work for the tsrs need hook the current work this generated work ',negative
'suppress useless evaluation ',negative
'kerberos ',negative
'there predicate partitioning column need all partitions this case ',negative
'driver not initialized ',negative
'',negative
'hiveserver using hiveconf this combine paths ',negative
'since hivedecimal now uses fasthivedecimal which stores decimal digits per long lets test edge conditions here ',negative
'walk through the projection list and replace the column names with the expressions from the original update under the tokselect see above the structure looks like tokselect tokselexpr expr tokselexpr expr ',negative
'clone the search ast apply all rewrites the clone ',negative
'how many times well sleep before giving ',negative
'bunch these are hivemetastoreclient but not imetastoreclient have marked these deprecated and not updated them for the catalogs really want support them should add them imetastoreclient ',negative
'select from src lateral view udtf mytable join src not supported instead the lateral view must subquery select from select from src lateral view udtf mytable join src ',negative
'load jars under the ',negative
'proper children the union ',negative
'change curr ops row resolvers tab aliases subq alias ',negative
'nullindicator after the transformation ',negative
'only seal those partitions that havent been spilled and cleared because once hashmap cleared will become unusable ',negative
'generate the hiveconfargs after potentially adding the jars ',negative
'the execution engine set the mapreduce env with the credential store password ',negative
'replicationspeckey scopekey ',negative
'table bigtables then its output big ',negative
'try fold key and key not null key where can note key and key not null cannot folded ',negative
'server thread pool ',negative
'process else statement ',negative
'bgenjjtree enumdeflist ',negative
'validate reserved values ',negative
'handle case with nulls dont function the value null because the data may undefined for null value ',negative
'this table not yet loaded cache the prewarm thread working this tables database lets move this table the top stack that gets loaded the cache faster and available for subsequent requests ',negative
'copy intervening noncrlf characters but not including current index ',negative
'stringsplit returns single empty result for splitting the empty ',negative
'alternate unused ',negative
'try get prime number table size have less dependence good hash function ',negative
'represents ptf invocation captures function name and alias the partitioning details about its input its arguments the astnodes representing the arguments are captured here reference its input ',negative
'remove from cache materialized view ',negative
'found remove and its child and connect its parent ',negative
'batches will sized ',negative
'run cleaner shouldnt impact anything ',negative
'for case conversion convert both values common type and then compare ',negative
'each partition maintains large properties ',negative
'since warehouse path nonqualified the table should located second filesystem ',negative
'sanity check for overlap with regions already being expanded ',negative
'construct temp table name ',negative
'subsequent hashes are used generate bits within block words ',negative
'because there only one for analyze statement can get ',negative
'only for live instances ',negative
'the tasks are not ready yet the task eligible for preemptable ',negative
'hive variables ',negative
'this can happen for numbers less than for rawprecision scale this case well set the type have the same precision the scale ',negative
'the percentage maximum allocated memory that triggers job tracker this could overridden thru the jobconf ',negative
'find the privileges that are looking for ',negative
'lazy binary value serializer ',negative
'test that existing sharedread with new exclusive coalesces ',negative
'since previously opened txn was killed ',negative
'may need update the conditional tasks list this happens when common map join task exists the task list and has already been processed such case the current task the map join task and need replace with its parent the small table task ',negative
'other fields are skipped for this case ',negative
'dont fail execution due counters just dont print summary info ',negative
'only check hostport pair valid wheter the file exist not does not matter ',negative
'add partition event ',negative
'combo literal set url set none ',negative
'return empty string ',negative
'bail exception out the loop ',negative
'called stop the query running clean query results and release resources ',negative
'find tables which name contains tofind the default database ',negative
'key aggregate partition values column name and the value the col stat object ',negative
'try this map ',negative
'string enclosed single quotes ',negative
'requirements for smb sorted their keys both sides and bucketed get key columns ',negative
'choose array size have two hash tables hold entries the sum the two should have bit more than twice much space the minimum required ',negative
'this before checking failedupdate because that might break the iterator ',negative
'the output needed for the qfile results ',negative
'the zookeeper connection use ',negative
'this version the loop eliminates condition check and branch and measurably faster ',negative
'disable new tasks from being submitted ',negative
'create the temporary file its corresponding filesinkoperaotr and ',negative
'remove failures for tasks that succeeded ',negative
'get the set all partition columns custom path ',negative
'strip the column name the targetid ',negative
'divide down just before round point get round digit ',negative
'setautocommit called and the autocommit mode not changed the call noop ',negative
'logbase col special case and will implemented separately from this template ',negative
'catch the exceptions every other metastore could stopped well log least there slight possibility find out about this ',negative
'the table already present ',negative
'remove additional elements the list reused ',negative
'',negative
'allocate the bean the beginning ',negative
'set the wrong type parameters for prepared sql ',negative
'druid storage timestamp column name ',negative
'batchindex big table ',negative
'use path relative datadir directory not specified ',negative
'check input pruning enough ',negative
'columns being updated update expressions setrcols last param null because use actual expressions ',negative
'conversion the target data type requires helper target writable few cases ',negative
'partition null either these then they are claiming lock the whole table and need check otherwise ',negative
'the columnencoding column name and type are all irrelevant this point just need the cfcq ',negative
'the offset was never added offset filesize ',negative
'filter tags for objects ',negative
'now the other enum possibility ',negative
'the list servers the can locate hive username for use when creating the user not for connecting hive password for use when creating the user not for connecting hive database for use when creating the user not for connecting ',negative
'need connect this cloned parent work with the corresponding child work ',negative
'construct using ',negative
'big alias partitioned table its partition spec bucket number ',negative
'most accurate domain cardinality would source column ndv available ',negative
'will enabled the customvertex ',negative
'from precision scale ',negative
'ctas path insert into filedirectory ',negative
'the free list level the blocks from which need merge ',negative
'tasksidemetadata set rowgroupoffsets null ',negative
'this update statement thus any isolation level will take write locks will block ',negative
'order convert from integer float correctly need apply the float cast not the double cast hive ',negative
'logger debug message from oproc after logj initialize properly ',negative
'the planner puts constant field for the dummy grouping set will overwrite ',negative
'use common decimal binary conversion method share with fastbigintegerbytes ',negative
'compression buffer size should only set compression enabled ',negative
'start instance hiveserver which uses minimr ',negative
'invalidate the entry rely query cleanup remove from lookup ',negative
'list ',negative
'collect the needed columns from all the aliases and create ored filter ',negative
'check that property that begins the same also hidden ',negative
'might not able assign all rows because input nulls start tracking any unassigned rows ',negative
'for complex types like struct map etc not support need writer that does nothing assume the vectorizer class has not validated the query actually try and use the complex types they show inputobjinspector and need ',negative
'original scan only ',negative
'timestamp column type druid timestamp with local timezone represents specific instant time thus have this value and need extract the granularity split the data when are storing druid however druid stores the data utc thus need apply the following logic the data extract the granularity correctly read the timestamp with local timezone value extract utc epoch millis from timestamp with local timezone cast the long timestamp apply the granularity function the timestamp value that way utc and pst same instant will end the same druid segment ',negative
'generate new cookie and add the response ',negative
'guard because returns null children available bug ',negative
'reconstruct join tree ',negative
'filter timestamp against timestamp long seconds and double seconds with fractional nanoseconds filter timestampcol timestampcolumn filter timestampcol longdoublecolumn filter longdoublecol timestampcolumn filter timestampcol timestampscalar filter timestampcol longdoublescalar filter longdoublecol timestampscalar filter timestampscalar timestampcolumn filter timestampscalar longdoublecolumn filter longdoublescalar timestampcolumn ',negative
'set the required field ',negative
'enable trash can tested ',negative
'union all insert for nonmm tables subquery creates another subdirectory the end for each union queries ',negative
'might have generated dynamic partition operator chain since were removing the reduce sink need remove that too ',negative
'add the layout the queryid appender ',negative
'note like vectorizer this assumes partition columns after data columns ',negative
'put sample data the columns ',negative
'set the correct position ',negative
'add the partition expressions the order there order and validate order spec ',negative
'use the example from hive where the integer digits the result exceed the enforced precisionscale ',negative
'handle cancellation the promise ',negative
'enc colix allencgeti ',negative
'track you walk the tree there operator along the way that changes the rows from the table through joins aggregations only allowed operators are selects and filters ',negative
'before after ',negative
'convert the join operator sortmerge join operator ',negative
'per the javadocs condition not depend the condition alone start gate since spurious wake ups are possible ',negative
'set the index table information ',negative
'conversion needed ',negative
'',negative
'hash function should map the long value hence hash value has nonnegative ',negative
'explicitly disable bit packing ',negative
'such database ',negative
'based userspecified parameters check the hash table needs ',negative
'for complex type helper object that describes elements keyvalue pairs fields ',negative
'logs ',negative
'dont need the check for utnull here because well give the real type deserialization and the object inspector will never see the actual union ',negative
'ignore the tag passed which should not what want ',negative
'prspgbycrscgby ',negative
'process singlecolumn string outer join vectorized row batch ',negative
'step fill stuff local work ',negative
'besteffort check see the comment the method ',negative
'interrupt all threads and verify get and expected message also raise kill operations and ensure that retries keep the time out occupied for sec ',negative
'conditionaltask ',negative
'try with null void ',negative
'this one uses the arcsin method involves more pisum sum nnnnnn sum note that split that each term not overflown ',negative
'mix binarynonbinary args ',negative
'its resulted from rsdedup optimization which removes following under some condition ',negative
'doing major compaction its possible where full compliment bucket files not required tez that basex doesnt have file for bucket ',negative
'generic lookup ',negative
'class factory ',negative
'for reasons dont understand and too lazy debug the moment the ',negative
'partitioned specified for partitioned table lets fetch all ',negative
'remainder dividend ',negative
'there need continue processing branch ',negative
'functions ',negative
'read the column value ',negative
'have splitupdate turned for this table then the delta events have already been split into two directories deltaxy and deletedeltaxy when you have splitupdate turned the insert events deltaxy directory and all the delete events deletexy update event will generate two events delete event for the old record that put into deletedeltaxy followed insert event for the updated record put into the usual deltaxy therefore everything inside deltaxy insert event and all the files deltaxy can treated like base files hence each these are added baseororiginalfiles list ',negative
'add same jar multiple times and check that dependencies are added only once ',negative
'request ',negative
'partitions specified partitions inside tablespec ',negative
'generated earlier get possible nulls ',negative
'sets and might the last one call make sure setting false ',negative
'loginfohar file harfile ',negative
'and fetch the sql operation log with fetchnext orientation ',negative
'get the highvalue ',negative
'should not get any rows ',negative
'scale down ',negative
'and for complex types also leave the children types place ',negative
'the subquery identifier from ',negative
'test uri with dbname ',negative
'sleep for and cancel again ',negative
'cost cost writing intermediary results local cost reading from local for transferring gby ',negative
'source local then source files wont deleted and have delete them here ',negative
'through the map and print out the stuff ',negative
'null first default for ascending order ',negative
'adapted from only check privileges for loadadddfscompile and admin privileges ',negative
'always use foreach action submit rdd graph would only trigger one job ',negative
'sourcedb ',negative
'builderliteraltrue variablesset ',negative
'serde ',negative
'set union operator child each leftop and rightop ',negative
'internal fields ',negative
'static partition without list bucketing ',negative
'cast long get rid periodic decimal ',negative
'remove from the list ',negative
'unexpected metric type ',negative
'get non null row count from root column get max vector batches ',negative
'comparisons come from the correlatorrel ',negative
'from tez eventually changes over the llap protocol and protocolbuffers ',negative
'cvalue map rowvalues assertequals cvaluesize assertequalsx assertequalsy ',negative
'test executed times worst case original retries ',negative
'char starts index and with length covering the rest the array ',negative
'hivehome not defined file not found hivehomeconf then load default ivysettingsxml from class loader ',negative
'there should calls create partitions with batch sizes ',negative
'check whether log file created test running ',negative
'handle skewed value skewed value add directory path unless value false ',negative
'number headers smallest blocks per target block next free list from which will splitting ',negative
'create default database inside the catalog ',negative
'second row ',negative
'perfloggeroptimizer baseplan hepplanbaseplan true mdprovider executorprovider perfloggeroptimizer calcite prejoin ordering transformation push down semi joins ',negative
'add filter just scan the keys that pick everything ',negative
'other counter sources currently used llap ',negative
'now know where put row ',negative
'mapping from constraint name list unique constraints ',negative
'for managed tables make sure the file formats match ',negative
'semijoin attempted then replace the condition with minmax filter and bloom filter else ',negative
'break polling times out ',negative
'create gsscontext for authentication with the service ',negative
'notify clear pending events any ',negative
'not part ',negative
'assumes line would never null when this method called ',negative
'decimal ',negative
'output type information ',negative
'test dropping fields first middle last ',negative
'fail transactional property set invalid value ',negative
'input metrics ',negative
'create the table ',negative
'called latemapjoin processor for example ',negative
'bail out ',negative
'filtercorrelaterule rule mistakenly pushes filter consiting correlated vars top logicalcorrelate within left input for scalar corr queries which causes exception during decorrelation this has been disabled for now ',negative
'our aggregation buffer has nothing just copy over other deserializing the arraylist pairs into array coord objects ',negative
'nothing this property not specified empty ',negative
'process each level parallel ',negative
'determine there match between big table row and the corresponding hashtable three states can returned match match found nomatch match found from the specified partition spill the specified partition has been spilled disk and not available the evaluation for this big table row will postponed ',negative
'nothing for null object ',negative
'optimize physical tree translate target execution engine ',negative
'not sequential ',negative
'position first row ',negative
'read friendly string ',negative
'mapping from operator the columns which its output sorted ',negative
'implementing ',negative
'future decide how ask input file format what vectorization features supports ',negative
'will bloomfilter bytewritable ',negative
'run given query and validate expected result ',negative
'for bytes type can mapped decimal ',negative
'the transactional listener response will set already the event there not need pass the response the nontransactional listener ',negative
'keysi for the ith join operator key list ',negative
'default behavior when neither hivejobcredstore location set nor this case hadoop credential provider configured job config should use that else should remain unset ',negative
'now compact compaction produces single range for both delta and delete delta that both delta and deletedeltas would compacted into delta and deletedelta ',negative
'test reset ',negative
'noone could have moved have the heap lock ',negative
'populate the complete query with provided prefix and suffix ',negative
'nonjavadoc see javaioreader long ',negative
'skip escape ',negative
'more places get the schema from give may have reencode later ',negative
'join condition must equality predicate both sides must reference column needed flip the columns ',negative
'these are the output columns for the small table and the outer small table keys ',negative
'shortcut for hdfs ',negative
'now remove all the unions throw away any branch thats not reachable from the current set roots the reason that those branches will handled ',negative
'resolve all the kill query requests flight nothing below can affect them ',negative
'tablescan with same alias ',negative
'pab ',negative
'see can use reencoding read the format thru elevator ',negative
'pass along hashcode avoid recalculation ',negative
'base javaobject primitives javafieldref entry javaobject javafieldref ',negative
'mock out the predicate handler because its just easier ',negative
'count position ',negative
'retrieve the tables from the metastore batches alleviate memory constraints ',negative
'remember the condition variables for explain regardless ',negative
'this point weve verified the types are correct ',negative
'start small table random generation from beginning ',negative
'create dummy partitions ',negative
'add empty stats object for each column ',negative
'use remove instead get that not parsed again ',negative
'testlazybinaryfast source rows serde serdefewer primitivetypeinfos useincludecolumns false dowritefewercolumns true ',negative
'nop theres caching ',negative
'configuration ',negative
'change the join operator reflect this info ',negative
'local dirs ',negative
'this will only available when are doing table load only replication not otherwise ',negative
'multikey hash map based the ',negative
'contains ',negative
'should never get here ',negative
'for future use ',negative
'insert transaction entries into minhistorylevel ',negative
'find the partition will working with there one ',negative
'stage started but not complete ',negative
'the table scan for big table then skip ',negative
'print all results for standalone select statement ',negative
'just digits ',negative
'the number columns output the udtf ',negative
'updates the references that are present every operand till now ',negative
'oozie does not change the service field the token hence default token generation will have value new text hiveclient will look for use with service ',negative
'make sure were locking the whole table since this dynamic partitioning ',negative
'must set isnulli false make sure gets initialized case set nonulls true ',negative
'after catching oom java says undefined behavior dont even try clean can get stuck shutdown ',negative
'create table related objects ',negative
'max characters when auto generating the column name with func name ',negative
'dbname ',negative
'now set some tree properties related multiinsert ',negative
'since integer always some products here are not included ',negative
'assertassertequals ',negative
'requires calculate stats new and old have different fast stats ',negative
'create new vectorization context create new projection but keep same output column manager must inherited track the scratch the columns ',negative
'call further down rely upon opabort ',negative
'value ',negative
'given work descriptor and the taskname for the work this responsible check each mapjoinop for cross products the analyze call returns the warnings list for the taskname the stagename for tez the vertex name ',negative
'store this the udf context can get later ',negative
'make sure all the partitions have the catalog set well ',negative
'return output null because additional work needed ',negative
'complex types map list struct union ',negative
'make expression for default value ',negative
'this copy genericudfnvl which builtin well make generic custom udf for test purposes ',negative
'dont compare locations because the location can still empty the preevent listener before created ',negative
'addpartition ',negative
'clone configuration before modifying pertask basis ',negative
'isolated from the other transaction related rpc calls ',negative
'run worker delete aborted transactions delta directory ',negative
'acquiredat ',negative
'cset doesnt reset millis ',negative
'output sorted ',negative
'nonjavadoc see ',negative
'expect correlated variables hivefilter only for now also check for case where operator has inputs tablescan ',negative
'this table has keys ',negative
'try read from the cache first ',negative
'support for schema evolution ',negative
'constructing the row objectinspector the row consists some set primitive columns each column will java object primitive type ',negative
'char test ',negative
'generate groupbyoperator ',negative
'possible that all the async methods returned the same thread because the session with registry data and stuff was available the pool this happens well take the session out here and cancel the init skip ',negative
'this offer will accepted and evicted ',negative
'this test method here initial call parsedriver and prevent any tests with timeouts the first ',negative
'insert into appends old version ',negative
'druid query ',negative
'substitution option hivevar ',negative
'separate required columns non partition and partition cols ',negative
'update cross size ',negative
'send out the actual submitworkrequest ',negative
'the sort order contains whether the sorting happening ascending descending ',negative
'evaluate else expression only and copy all its results ',negative
'filter may have sensitive information not send debug ',negative
'now compact one important thing note this test that minor compaction always produces deltaxy and counterpart deletedeltaxy even when there are deletedelta events such choice has been made simplify processing ',negative
'physical files are resides local file system the similar location ',negative
'may happen that know wont use some cache buffers anymore the alternative that will use the same buffers for other streams separate calls ',negative
'the key not found mapcolumnvector set the output null columnvector ',negative
'txnids ',negative
'was deleted during the transaction ',negative
'key nodes candidate list ',negative
'the extra parameters will added server side check that the required ones are present ',negative
'remove semijoin there any the semijoin branch can potentially create task level cycle with the hashjoin except when dynamically partitioned hash ',negative
'search mapping for any strings and return their output columns ',negative
'should true for sure because already checked before ',negative
'put now available buffered batch end ',negative
'expressions are not supported currently without alias ',negative
'default utc utc ',negative
'get any new notification events that have been since the last time checked and pass them the event handlers ',negative
'iterate over each clause ',negative
'check non null ',negative
'return subquery ',negative
'class store necessary information for attempt log ',negative
'remember map joins encounter them ',negative
'verify that the table created successfully ',negative
'set lambda the heap size becomes lru ',negative
'all the fastsetfrom methods require the caller pass fastresult parameter has been reset for better performance ',negative
'sequence number used name vertices map reduce ',negative
'overlay hivesitexml exists ',negative
'create remote metastore ',negative
'may add noandstop future where combine impossible and other should not base ',negative
'specialized class for native vectorized reduce sink that reducing uniform hash multiple key columns single nonlong nonstring column ',negative
'relationship ',negative
'nulls the join keys ',negative
'add cstatstask dependent all the nonstatsleaftasks ',negative
'now this should block until unlocks ',negative
'put the mapping task aliases ',negative
'this only called for replication that handles tables need for mmctx ',negative
'case repeating has nulls ',negative
'check file system permission ',negative
'create snapshot ',negative
'should convert ',negative
'add all the public member classes that implement evaluator ',negative
'need filter those that have been pushed already stored the join and those that were already the subtree rooted child ',negative
'renewer ',negative
'running queued ',negative
'important sorting here retain order its used match with values runtime ',negative
'fetch across schemas ',negative
'ensure filters are not set from previous pushfilters ',negative
'the message from remote exception includes the entire stack the error thrown from hive based the remote exception needs only the first line ',negative
'cluster than the default one but least for the default case wed have covered ',negative
'mapping from column name check expr ',negative
'our original foo should the wrapper ',negative
'checkcorrect codec ',negative
'reset for filling ',negative
'hiveserver configs that this instance will publish zookeeper that the clients can read these and configure themselves properly ',negative
'basic test ',negative
'for each partition spec get the partition ',negative
'next locate the aggregation buffer set for each key ',negative
'txn started implicitly previous statement ',negative
'process user groups for which doas authorized ',negative
'the task will either killed already the process completing which will trigger the next scheduling run result available slots being higher than ',negative
'normalize label row ',negative
'vieworiginaltext ',negative
'add limit order bys without limit can disabled for safety reasons ',negative
'nonjavadoc see javautilcalendar ',negative
'install the jaas configuration for the runtime ',negative
'create and set provider ',negative
'close ',negative
'node became available enable the node and try scheduling ',negative
'get all target paths first because the number total target paths used determine number splits each target path ',negative
'vint ',negative
'handle the special cases here perhaps could have more general structure even configurable set like storage handlers but for now only have one ',negative
'attempt make the path case does not exist before check ',negative
'the values from timestampgetnanos ',negative
'function correctly ',negative
'outerjoinpos otherposfilterlen otherposfilterlen ',negative
'hive depends filesplits wrap hbasesplit ',negative
'for and condition cascadingly update stats ',negative
'walk over all the sources which are guaranteed reduce sink operators the join outputs concatenation all the inputs ',negative
'unknown unknown ',negative
'cookie based authentication allowed generate ticket only when necessary the necessary condition either when there are server side cookies the cookiestore which can send back when the server returns error code ',negative
'not delete for tables either want the file succeed must delete explicitly before proceeding the merge fails ',negative
'unpartitioned table filters ',negative
'create the delta directory dont worry already exists that likely means another task got first then move each the buckets would more efficient try move the delta with its buckets but that harder make race condition proof ',negative
'adding this child the union later ',negative
'the schema after like this all keys sumc sumvcolc the column size the same unioncolumnsize for except distinct add filter then add the project for except all add project change all keys then add the udtf ',negative
'use the session the one supplied constructor ',negative
'perform the data read asynchronously ',negative
'the default unless serde overrides ',negative
'connect after the lifetime there should not any failures ',negative
'actual batch size that will used ',negative
'close one connection verify still one left ',negative
'store into configuration ',negative
'change the selected vector ',negative
'records will emitted from hive ',negative
'otherwise expect the user already logged ',negative
'there are any open txns then the minimum minopentxnid from minhistorylevel table ',negative
'send task off another jvm ',negative
'generate test jar files ',negative
'input job properties ',negative
'and remember link between event and table scan ',negative
'checked partitions matching specification are marked archived the metadata they are and their levels are the same would set later means previous run failed and have the recovery ',negative
'validate that setop feasible according hive using type ',negative
'check same filter exists already ',negative
'stats key prefix ',negative
'need set type name should always decimal ',negative
'the basic idea for cbo support udtf treat udtf special project ast return path just need generate selexpr just need remember the expressions and the alias return path need generate sel and then udtf following old semantic analyzer ',negative
'doharcheckfsharfile ',negative
'the implementation balks when this method invoked multiple times ',negative
'taskstatus ',negative
'function create subcache ',negative
'once the conversion done can set the partitioner bucket cols the small table ',negative
'can just use setkeyprovider ',negative
'index means this key ',negative
'transaction manager the driver has been initialized with can null this set then this transaction manager will used during query rather than using the current sessions transaction manager this might needed situation where driver nested within already running driverquery the nested driver requires separate transaction manager not conflict with the outer driverquery which using the session ',negative
'cancel other tasks ',negative
'reserve space for potential future list ',negative
'schedulingpolicy ',negative
'choose max ',negative
'this better generic struct with constant values the children ',negative
'vectorized doesnt adjust usage for the keys while processing the batch ',negative
'required for mdc based routing appender that child threads can inherit the mdc context ',negative
'outer join are going add the inspector from the inner side but the key value will come from the outer side need create converter from inputoi outputoi ',negative
'just safe about numrows ',negative
'hive has concept avros fixed type fixed arrays bytes ',negative
'worstcase hash aggregation disabled ',negative
'because inverse ',negative
'partitions ',negative
'commenthello there propertiesab ',negative
'should only have one aggregate ',negative
'acidmm tables then need find the valid state wrt given validwriteidlist ',negative
'default partitions not defined ',negative
'include specified but this module not the set ',negative
'the operator not rexcall type fail fall through add this condition the list nonequijoin conditions ',negative
'use the source ordering flavor for the mapping ',negative
'insert current common join task conditional task ',negative
'equals ',negative
'returns value null ',negative
'try factoring out common filter elements separating deterministic nondeterministic udf this needs run before ppd that ppd can add onclauses for old style join syntax select from join where rxrx and ',negative
'test that existing sharedread partition with new exclusive coalesces ',negative
'failed compacts left and other since only have failed ones here ',negative
'output the exit code ',negative
'create transactional table ',negative
'try allocate memory havent allocated all the way maxsize yet very rare ',negative
'multibyte characters with blank ranges ',negative
'only expect transactional components ',negative
'blockedbyextid ',negative
'set the memory treshold that get before need flush ',negative
'namemethod name constant java string constant text stringwritable ',negative
'list operation for which log ',negative
'all precision has been lost result ',negative
'embedded metastore mode ',negative
'use when calculating intermediate variance and count note count has been incremented sum included value ',negative
'get the file status upfront for all partitions beneficial cases blob storage systems ',negative
'need add twice ',negative
'now make sure its array doubles floats dont allow integer types here ',negative
'object receive results reading decoded variable length int long ',negative
'are assuming the updateerror bad and just try kill ',negative
'creating stats table not exists ',negative
'generate the temporary file must the same file system the current destination ',negative
'unique set for operation when run from base encoded value ',negative
'write the key out ',negative
'get different dates ',negative
'return the desired vectorexpression found otherwise return null cause ',negative
'stay with multikey ',negative
'perform major compaction nothing should change both deltas and base dirs should have the same name ',negative
'need scale down thisscale rightscale newscale ',negative
'verify throws exception ',negative
'object constructed from output wdw fns before put the wdw processing partition set ',negative
'return new hash map result implementation specific object the object can used access the values when there match access spill information when the partition with the key currently spilled ',negative
'constructing the row objectinspector the row consists some string columns each column will java ',negative
'get jobids from job status dir ',negative
'output header ',negative
'sql usage inside larger transaction droptable may not desirable because some databases postgres abort the entire transaction when any query fails ',negative
'scalar subquery ',negative
'have already locked the table ddlsemanticanalyzer dont again here ',negative
'the subquery alias ',negative
'bloom filter false positive probability ',negative
'the table exists and found valid create table event then need drop the table first and then create this case possible the event sequence droptablet createtablet need drop here handle the case where the previous incremental load created the table but ',negative
'grant option revoke remove the whole role ',negative
'operator specific logic goes here ',negative
'todo change type the one the table schema ',negative
'create new join ',negative
'operation with insertupdate ',negative
'noop sba does not attempt authorize auth api call allow ',negative
'the same thing setchildren when there nothing read the setchildren method initializes the object inspector needed the operators based path and partition information which dont have this case ',negative
'are making what are trying more explicit theres union alias ',negative
'cancel other futures ',negative
'cannot call class testclidriver since thats the name the generated code for the scriptbased testing ',negative
'sync total record length key portion length ',negative
'hard know exactly for decimals ',negative
'allow string double conversion ',negative
'check for rounding ',negative
'dont need lookup ordercolumnidbyname because know must ',negative
'this will used rexnodeconverter create cor var ',negative
'check our config value first explicitly avoiding getting the default value for now dont want our default override hive set value ',negative
'append the deserialized standard object row using the current batch size ',negative
'ideally these properties should part llapdameonconf rather than hiveconf ',negative
'create new schema that subset original ',negative
'typespecific handling done here ',negative
'for example original max dist min rss schema key max min ',negative
'tracks various maps for dagcompletions this setup here since statechange messages ',negative
'bugbug somewhat fragile below substring expression ',negative
'the dbtype hive this setting the information schema hive will set the default jdbc url and driver overriden command line options passed url and driver ',negative
'unlikely but log the actual values case one the two was emptynull ',negative
'the buffer was pointing smallbuffer then nextfree keeps track the current state the free index for smallbuffer now need save this value smallbuffernextfree ',negative
'insert sparkhashtablesink and dummy operators ',negative
'create identity projection ',negative
'push first record group ',negative
'called explicitly through dynamic return false default ',negative
'this simulates the completion txnididtxnupdate ',negative
'param basedir not null its either tablepartition root folder basexxxx its basexxxx its dirstosearch else the actual original files all leaves recursively are the dirstosearch list ',negative
'udaf present and column expression map empty then must full aggregation query like count which case number ',negative
'detecting failed executions exceptions thrown the operator tree ',negative
'create table with multiple partitions ',negative
'string tests ',negative
'there should now directories the location ',negative
'build the path from bottom pick list bucketing subdirectories ',negative
'obtain metastore clients ',negative
'the methods older and newer match ',negative
'nulls come first otherwise nulls come last ',negative
'just access key and value ensure they are correct ',negative
'check there are enough entries the tree constitute hint ',negative
'fix for sfnet bug ',negative
'length green ',negative
'use the minmax instead the byte range ',negative
'first qualify ',negative
'dont print full exception trace debug not ',negative
'and check hdfs before and after ',negative
'decimal classes cannot converted printf convert them doubles ',negative
'the offsets are the same assume our initial jump did not cross any dst boundaries ',negative
'create lot locks ',negative
'collection methods ',negative
'this simulates the completion txnididtxnupdate ',negative
'expected ',negative
'for nonmm tables the final destination partition directory created during move task via rename for tables the final destination partition directory created the tasks themselves ',negative
'move task will create final path ',negative
'use construct ',negative
'when there are exceptions this has called always make sure incompatible files are moved properly the destination path ',negative
'remains here the legacy the original higherlevel interface getinstance ',negative
'already processed skip ',negative
'groupnames ',negative
'check that the change stuck ',negative
'unsupported ',negative
'test using loadfilework ',negative
'',negative
'update ',negative
'default type all string ',negative
'plan ',negative
'some other task ',negative
'make location hints ',negative
'now grant all privs admin ',negative
'note this does not work for embedded channels ',negative
'perform kerberos login using the hadoop shim api the configuration available ',negative
'aggregate itself should not reference cor vars ',negative
'regardless other criteria ducks are always more important than nonducks ',negative
'link queryid txnid ',negative
'get the serde parameters ',negative
'find all root tss and add all data sizes not adding other stats rows col stats since only data size used here ',negative
'load the test files into tables ',negative
'union expr for distinct keys ',negative
'avoid calculating modulo ',negative
'set the operator plan before setting splits the ',negative
'add this filter for deletion does not have nonfinal candidates ',negative
'determine the index expr child schema note calcite can not take compound exprs without being ',negative
'columns not expressions yes proceed ',negative
'test for only partnames being empty ',negative
'the matched field leaf which means all leaves are required not need deeper ',negative
'and return all the dummy parent ',negative
'are just converting common merge join operator the shuffle join mapreduce case ',negative
'dynamicpartitionctx indicate that needs dynamically partitioned ',negative
'some ddl task that directly executes teztask does not setup context and hence triggercontext setting queryid messed some ddl tasks have executionid instead proper queryid ',negative
'date value boolean doesnt make any sense ',negative
'are not using the key and value contexts nor support mapjoinkey ',negative
'should not have more than load file for ctas ',negative
'set input format information necessary ',negative
'set the configuration parameters ',negative
'bbhashcode ',negative
'get the list task ',negative
'try qualifying with current name for permanent functions ',negative
'wait before sending another heartbeat otherwise consider oob heartbeat ',negative
'may need convert common type compare ',negative
'first column empty ',negative
'partitioned table ',negative
'the queue does not have capacity does not throw rejection instead will return the task with the lowest priority which could the task which currently being processed ',negative
'will true there are null entries ',negative
'caller must make sure product inputs not too big ',negative
'copied over from ',negative
'enforce hive defaults ',negative
'consider query like select mapjoinsubq from select akey avalue from tbl subq join select akey avalue from tbl subq subqkey subqkey aliastoopinfo contains the selectoperator for subq and subq need traverse the tree using tableaccessanalyzer get the base table the object being mapjoined base table then aliastoopinfo contains the tablescanoperator and tableaccessanalyzer noop ',negative
'nothing that can really about ',negative
'since the oldname table not under its database see hive the renamed oldname table will keep its location after hive changed check the existence the newname table and its name instead verifying its location ',negative
'iterate through each token and create appropriate object here ',negative
'assumes the query has already been compiled ',negative
'case ctas statement ',negative
'',negative
'total completed running pending failed killed ',negative
'allow lookup query string ',negative
'right trim and truncate byte array maximum number characters and return byte array with only the trimmed and truncated bytes ',negative
'minimum rows per stripe ',negative
'container prewarming tell the how many containers need ',negative
'ignore the exception this may caused external jars ',negative
'bucket count for test tables set for easier debugging ',negative
'this function serves the wrapper ',negative
'verify that can create table with ifof some custom nonexistent format ',negative
'when dowritefewercolumns try read more fields than exist buffer ',negative
'ids were all aborted and the metadata cleaned would lose the record the aborted ids this case are not able determine the new writeidlist has equivalent commit state compared the previous writeidlists ',negative
'add the checkpoint key the database binding current dump directory retry using same dump shall skip database object update ',negative
'recurse into memoized decorator ',negative
'this deletes the side file ',negative
'insert them all before the get requests from this iteration ',negative
'the first query has full batches and the second query only has batch which only contains member ',negative
'scratch column information ',negative
'there any unknown partition create mapreduce job for the filter prune correctly ',negative
'mux operator with parent ',negative
'dont override confvars with null values ',negative
'least one mrtezspark job ',negative
'number objects the block before spilled ',negative
'filter the partitions show based supplied spec ',negative
'replace original avgx with sumx countx ',negative
'alter table tbl via objectstore ',negative
'the set dynamic partitions ',negative
'middle word ',negative
'compare required privileges and available privileges for each hive object ',negative
'compose the seconds field from two parts the lowest bits come from the first four ',negative
'are recordwriterclose make sense that the context would taskinputoutput ',negative
'existing thrift data ',negative
'default the bounds checking for maximum number dynamic partitions disabled ',negative
'suffix should timestamp ',negative
'find out the vertex for the big table ',negative
'this the only place where isquery set true defaults false ',negative
'assumes serialized dags within and reset structures after each dag completes ',negative
'calcite stores timestamp with local timezone utc internally thus when bring back need add the utc suffix ',negative
'create the functions and reload them from the metastore ',negative
'insert into values gets written into insert from select dummytable this table dummy and has stats ',negative
'set rhsexp picks the next char from the token stream ',negative
'nonvectorized regular acid reader ',negative
'make filter pushdown information available getsplits ',negative
'return true this any kind float ',negative
'compute locally and assign ',negative
'',negative
'each row ',negative
'minimum seconds ',negative
'check that change the hidden list should fail ',negative
'the same for rolling the key recreate the fsm with only the key ',negative
'have traits and table info present the traits know the exact number buckets else choose the largest number estimated ',negative
'',negative
'filter mode the column must boolean ',negative
'for now make sure that serde exists ',negative
'null ',negative
'reattach all registered listeners ',negative
'serialize table definition deserialize using the target hcatclient instance ',negative
'this method tries convert join smb this done based traits the sorted columns are the same the join columns then can convert the join smb otherwise retain the bucket map join still more efficient than regular join ',negative
'given tokselect this checks there subquery top level expression else throws error ',negative
'the update failed and could retried ',negative
'propagate constants ',negative
'default treat the table single column col ',negative
'data types ',negative
'set the credential provider passwords found there job specific password the credential provider location set directly the execute method localsparkclient ',negative
'generic udfs ',negative
'throw new ',negative
'this may need change the implementation changes ',negative
'case theres delay for the heartbeat and the delay long enough trigger the reaper then the txn will time out and aborted here just dont send the heartbeat all infinite delay ',negative
'does not support timestamp ',negative
'base javaobject primitives javafieldref javaarray entry javaobject javafieldref primitives ',negative
'total running ',negative
'the aggregation buffers use for each key present the batch ',negative
'array structures containing the ngram and its estimated frequency ',negative
'read authorization does not work with defaultlegacy authorization mode chicken and egg problem granting select privilege database the grant statement would invoke getdatabase which needs select privilege ',negative
'for each take the target mapwork and see dependent sparktask ',negative
'this hook verifies that the location every partition the inputs and outputs does not start with the location the table very simple check make sure the location not subdirectory ',negative
'propagate reporter and output collector all operators ',negative
'verify the auth should fail ',negative
'convert the complex lazybinary objects standard java objects downstream operators like filesinkoperator can serialize complex objects the form they expect java objects ',negative
'converts date timestamptz ',negative
'the binarysortable serialization the current key ',negative
'each the errorheuristics repeat for all the lines the log ',negative
'the operator type ',negative
'nonjavadoc see javaioinputstream ',negative
'when the repeated match due filtering need restore the selected information ',negative
'modify insert branch condition particular need modify the ',negative
'operation may have been cancelled another thread ',negative
'fetch operator not vectorized and such turn vectorization flag off that ',negative
'convert integer value seconds since the epoch timestamp value for use long column vector which represented nanoseconds since the epoch ',negative
'updaterule ',negative
'track still have the entire part ',negative
'cookie based authentication when using http transport ',negative
'the table does not have any partitions ',negative
'setup output stream redirect output ',negative
'insert some data this will generate only insert deltas and delete deltas delta ',negative
'pick unknown case and let and operator handle the rest ',negative
'dont use this one ',negative
'files size for splits ',negative
'create structs ',negative
'not inner join not push the ',negative
'make sure small table bytescolumnvectors have room for string values the big table and ',negative
'use the positions only pick the partitioncols which are required the small table side ',negative
'test whether that held ',negative
'for other registered patterns find exact matches ',negative
'create string appender capture log output ',negative
'skip processing has done first before continuing ',negative
'sorting columns the parent are more specific than those the child but sorting order the child more specific than that the parent ',negative
'instance tsocket this also not set when kerberos used ',negative
'having ',negative
'optional string uniquenodeid ',negative
'could here either because its unpartitioned table because there are pruning predicates partitioned table ',negative
'todo enable caching for queries with maskingfiltering ',negative
'publish new segments metadata storage ',negative
'other exceptions which defaults ',negative
'not making configurable for perf reasons avoid checks ',negative
'prepare children ',negative
'this creates and publish new segment ',negative
'somebody took away our unwanted ducks ',negative
'num reduce sinks hardcoded because has parents ',negative
'write keyvalue pairs one one ',negative
'when indicator null ',negative
'project ',negative
'the output buffer used serialize value into ',negative
'note that and have table with common name ',negative
'collect information vectorptfdesc that doesnt need the use this information for validation later when creating the vector operator create additional object vectorptfinfo ',negative
'not used ',negative
'test table with portion ',negative
'want signal error the function doesnt exist and were ',negative
'mix functions for ',negative
'now validate for the table ',negative
'ptf need selectop ',negative
'have their permissions mimic the table permissions ',negative
'local mode implies that scheme should file can change this going forward ',negative
'reach here means needs table authorization check and the table authorization may already happened because other ',negative
'modified ',negative
'met are not going try merge ',negative
'clean anything from the txns table that has components left txncomponents ',negative
'the mapjoin operator will encountered many times times for nway join since reducesink operator not allowed before mapjoin the task for the mapjoin will always root task the task corresponding the mapjoin converted root task when the operator encountered for the first time when the operator encountered subsequently the current task merged with the root task for the mapjoin note that possible that the mapjoin task may performed bucketized mapside join sortmerge join the map join operator enhanced contain the bucketing info when encountered ',negative
'for partitioned table always track writes partition level never table and for non partitioned always table level thus the same table should never have entries with partition key and ',negative
'longs ',negative
'there only one destination query try push where predicates join conditions ',negative
'this used get hold reference during the current creation tasks and initialized with tasks such that will non consequential any operations done with task tracker compositions ',negative
'bgenjjtree const ',negative
'package and compress all the hashtable files archive file ',negative
'need iterate more when threshold reached beneficial especially for object stores ',negative
'also clone the colexprmap default need deep copy ',negative
'this operator has been removed remove from the list existing operators ',negative
'add via objectstore ',negative
'test that fetching nonexistent tablename yields objectnotfound ',negative
'nonjavadoc see ',negative
'spill tables are ',negative
'otherwise have failed the callback has taken care the failure ',negative
'returns the node the top the stack and remove from the stack ',negative
'ignore object fail not admin succeed admin ',negative
'map that keeps track the last operator task the work ',negative
'set columns read conf ',negative
'preserve the selected reference and size values generated ',negative
'use the target directory not specified ',negative
'obtain token directly invoking the metastore operationwithout going through the thrift interface obtaining token makes the secret manager aware the user and that gave the token the user also set the authentication method explicitly kerberos since the metastore checks whether the authentication method kerberos not for getdelegationtoken and the testcases dont use kerberos this needs done ',negative
'fieldindex becomes simple note that pos starts from while fieldindex starts from ',negative
'for small table parents that have already been processed need add the tag the work the reduce work that contains this map join this was not being done for normal mapjoins where the small table typically ',negative
'note check for existence deletedeltafile required because may not have ',negative
'make sure well use different plan path from the original one ',negative
'testtable ',negative
'case partition have move each file ',negative
'attempt acquire write resources waiting they are not available ',negative
'you store the materialized view ',negative
'primitive types ',negative
'decided not reposition and reread the buffer copy with will still correctly positioned for the next field ',negative
'mix functions for ',negative
'description ',negative
'left child ',negative
'indicates temporary error not corruption rethrow this exception ',negative
'scale ',negative
'with ptfs there maybe more note for ptfchains ',negative
'add write hooks needed ',negative
'tolerate repeated use big table column ',negative
'check configs are hidden ',negative
'werent provided any actual qualifier name set these ',negative
'ends getting rid project since not used further the tree ',negative
'call hiveclosecurrent that closes the hms connection causes hms connection leaks otherwise ',negative
'run rules aid translation from calcite tree hive tree ',negative
'the form partition not stripping quotes here need use while framing partition clause insert query ',negative
'early exit getting file lengths can expensive object stores ',negative
'newinstance should always the same type object this ',negative
'now notify the executorservice that the task has moved finishable state ',negative
'add some columns ',negative
'hadoop this what controls the timeout ',negative
'implements ',negative
'the first argument const then just set the flag and continue ',negative
'vrb mode process the vrbs with cache data the new cache data coming later ',negative
'replicate the remaining insert overwrite operation the table ',negative
'creating path expensive cache the corresponding path object normalizedpaths ',negative
'remove the condition replacing with true ',negative
'catch the exception log and rethrow ',negative
'need enforce precisionscale here ',negative
'nonjavadoc see javaioinputstream int ',negative
'will estimate collection object only its field ',negative
'project any correlated variables the input wants pass along ',negative
'',negative
'empty map case ',negative
'the owner can change also owner might appear user grants well keep owner privileges separate from usergrants ',negative
'whether the variable was true when the vectorizer class evaluated vectorizing this node when vectorized input file format looks this flag can determine whether should operate vectorized not some modes the node can vectorized but use row serialization ',negative
'logwarnno partition found genereated dynamic partitioning loadpath with dynspecdynpathspec ',negative
'called for each partition big table and populates mapping for each file the partition ',negative
'per jdbc spec section the driver implementation understands the url will return connection object otherwise returns null ',negative
'get substring ',negative
'mapreduce job ',negative
'report failure the main thread ',negative
'initialize reporters ',negative
'the only conf allowed have the metastore pwd keyname the hidden list configuration value ',negative
'care only about openaborted txns below currenttxn and hence the size should determined for the exceptions list the currenttxn will missing opentxns list only rare case like txn aborted and compactor actually cleans the aborted txns for such cases get negative value for sizetohwm with found position for currenttxn and ',negative
'need stay out the way any sequences used the underlying database otherwise the next time the client tries add catalog well get error there should never billions catalogs well shift our sequence number ',negative
'noop testing events only ',negative
'save the vector description for the explain ',negative
'handle table populate aliases appropriately leftaliases should contain the first table rightaliases should contain all other tables and basesrc should contain all tables ',negative
'compute collations ',negative
'this lossy invert the function above which produces hashcode which collides with the current winner the register lose all higher bits but get all bits useful for lesser pbit options ',negative
'stop tracking the fragment and rethrow the error ',negative
'check different filesystems ',negative
'constructors are marked private use create methods ',negative
'manufacture statsaggregator ',negative
'only need write out close the deletedelta there have been any ',negative
'deserialize the fields into the overflow batch using the buffered batch column map ',negative
'check the table directory ',negative
'some the data set the server side reset those ',negative
'session identifier ',negative
'update mappings oldinput newinput newproject oldinput newinput transformed oldinput newproject ',negative
'replace existing table ',negative
'first time this seen log ',negative
'toklateralview tokselect tokselexpr tokfunction identifierinline valuesclause identifier tablealias ',negative
'linking these two operator declares that they are representing the same thing currently important because statistincs are actually gather for newop but the lookup done using oldop ',negative
'additional data structures needed for the join optimization ',negative
'template classname valuetype operatorsymbol descriptionname descriptionvalue ',negative
'successfully scheduled ',negative
'groupby query results records ',negative
'set appropriate ownerperms the dir only need recurse ',negative
'nothing set ',negative
'didnt seem useful create another constants class just for these though ',negative
'delete table data ',negative
'due the way use the allocationfree cast from hivedecimalwriter decimal not have the luxury bytebuffer ',negative
'the output objectinspector ',negative
'only print out one task because thats good enough for debugging ',negative
'create row per table name ',negative
'use boundarytype boundaryamt sort key order behavior case preceding unb any any error preceding unsigned int null desc end partitionsize asc end preceding unsigned int not null desc scan backward until row such that rsk rsk bndamt end ridx preceding unsigned int not null asc scan backward until row such that rsk rsk bndamt end ridx current row null any scan forward until row such that rsk not null end ridx current row not null any scan forward until row such that rsk rsk end ridx following unb any any end partitionsize following unsigned int null desc end partitionsize asc scan forward until row such that rsk not null end ridx following unsigned int not null desc scan forward until row such rsk rsk bndamt end ridx asc scan forward until row such rsk rsk bndamt end ridx ',negative
'check that the agg the following type ',negative
'this null only read the isnull byte for level because the toplevel struct can never null ',negative
'variance check ',negative
'iterate thru all the filecaches this besteffort these superlonglived iterators affect the map some bad way ',negative
'write file ',negative
'add the privileges not supported the list privileges supported implementation defined ',negative
'the code inside the attribute getter threw exception log and fall back the class name ',negative
'initialize complete map reduce configuration ',negative
'current hive parquet timestamp implementation stores timestamps utc but other components not this case skip timestamp conversion this file written version hive before hive file metadata will not contain the writer timezone convert the timestamp the system reader time zone file written current hive implementation convert timestamps the writer time zone order emulate time zone agnostic behavior ',negative
'the table sorted partition column not valid for sorting ',negative
'after committing the initial txns and updating current number open txns back ',negative
'get udaf info using udaf evaluator ',negative
'default dont convert unix ',negative
'timeout for the iteration case asynchronous execute ',negative
'',negative
'noncbo path retries execute subqueries and throws completely different exceptionerror eclipse the original error message avoid executing subqueries noncbo ',negative
'create archived version the partition directory ending thats the same level the partition does not already exist does exist assume the dir good ',negative
'noticed that also suffer from the same issue hive only want call field inited when its nonnull check twice make sure get null both times ',negative
'this can only happen case failure read some data but didnt decompress deallocate the buffer directly not decref ',negative
'flush the print stream doesnt include output from the last command ',negative
'test february nonleap year viewd due days diff from ',negative
'timeouts are bad mmmkay ',negative
'tokenstrform ',negative
'convert the set into list ',negative
'set child environment setting hadoopclientopts will replaced with hadoopopts updated too since hadoopclientopts appended hadoopopts most cases this way the local task jvm can ',negative
'the channel listener instantiates the rpc instance when the connection established ',negative
'assuming that this closes the underlying streams ',negative
'restrictions correlated expression outer query must not contain unqualified column references disabled its obvious allow unqualified refs ',negative
'column family mapped mapstringstring ',negative
'nothing can here just proceed normally from now ',negative
'verify that hiveserver config not loaded ',negative
'open transactions ',negative
'not reduce the input size bail out ',negative
'prevents task from being processed multiple times ',negative
'rootpath ',negative
'testing with multibyte string ',negative
'bgenjjtree constlist ',negative
'the current nonnull key position ',negative
'implicit cast needed ',negative
'conf validator already checks this will never trigger usually ',negative
'ensure the session open and has the necessary local resources ',negative
'test conversion longstring ',negative
'might visiting twice because reutilization intermediary results that the case not need anything because either have already connected this operator will connect subsequent pass ',negative
'skip tokquery ',negative
'register both not fire the rule them again ',negative
'dag might have been killed lets try get vertex state from before dying ',negative
'bigtablefound means weve encountered table thats bigger than the ',negative
'has tag need set later ',negative
'mask this digit ',negative
'first add all children this work into queue processed later ',negative
'compare with tenscale example tenscale will zero after scaling ',negative
'this decoded compression buffer add ',negative
'set total number rows from all memory partitions ',negative
'verbose mode print update per recordprintinterval records ',negative
'map work starts with table scan operators ',negative
'this point everything the list going have refcount one unless failed between the allocation and the incref for single item should ',negative
'underscoreint ',negative
'exhausted all delete records return ',negative
'special treatment for filter operator that ignores the dpp predicates ',negative
'create row related objects ',negative
'reuse existing text member varchar writable ',negative
'for native vectorized map join require the key serde binarysortableserde note the may not really get nativelyvectorized later but changing serde wont hurt correctness ',negative
'update the partition columns small table ensure correct routing hash tables ',negative
'escape the escape ',negative
'preallocated member for storing index into the hashmultisetresults for each spilled row ',negative
'determine mapping between project input and output fields hive sort always based rexinputref only need check project can contain all the positions that sort needs ',negative
'adjacencylist ',negative
'close client session ',negative
'these parameters controls the maximum number concurrent job submitstatuslist operations templeton service more number concurrent requests comes then they will rejected with busyexception ',negative
'output has nonulls set false set the isnull false carefully ',negative
'recursively call the join the other rhs tables ',negative
'the actual size will assigned setchildreninfo after reading complete ',negative
'not primitive check struct and can infer common class ',negative
'check any the txns the list committed yes throw exception ',negative
'tried scheduling everything that could scheduled this loop ',negative
'the job for compaction ',negative
'destf ',negative
'test for duplicate publish this will either fail job creation time and throw exception will fail runtime and fail the job ',negative
'there should only directory left basexxxxxxx ',negative
'the plan for this reducer does not exist initialize the plan ',negative
'test replicated drop should drop this time since replstateid evid ',negative
'high word ',negative
'minutes ',negative
'insert select operator here used the columnpruner reduce ',negative
'create split for the previous unfinished stripe ',negative
'could acquire table level sharedwrite intead ',negative
'first one will fail count ',negative
'create all nulls key ',negative
'ordering ',negative
'',negative
'high and middle word must zero check for overflow digits lower word ',negative
'external llap clients would need set llapzkregistryuser the llap daemon user hive rather than relying ',negative
'the vertex cannot configured until all dataevents are seen ',negative
'use construct ',negative
'swap debug options hadoopclientopts those that the child jvm should have ',negative
'which can affect the working all downstream transformations ',negative
'estimate the same way for compressed and uncompressed for now ',negative
'optional int ',negative
'any child work for this work already added the targetwork earlier should connect this work with ',negative
'reader creation updates hdfs counters dont here ',negative
'second granularity ',negative
'try nonchunked stream there should issues assuming flushed the streams before closing ',negative
'compose query that select transactions containing update ',negative
'delegate the new api ',negative
'col ',negative
'use spark rdd async action submit job its the only way get jobid now ',negative
'invoke the right unpack method depending data type the column ',negative
'couldnt jdoql filter pushdown get names via normal means ',negative
'core pool size ',negative
'end ',negative
'prefix used auto generated column aliases this should started with ',negative
'nothing compact update expr with compacted children ',negative
'case when user has not specified any ingestion state the current command there kafka supervisor running then keep last known state start otherwise stop ',negative
'this local file ',negative
'not applicable ',negative
'this mapping collects all the configuration variables which have been set the user explicitly either via set the cli the hiveconf option system property mapping from the variable name its value note that user repeatedly ',negative
'avoid traversing the tree later save memory this could array byte arrays ',negative
'fetch the row inserted before schema altered and verify ',negative
'call the metastore get the status all known compactions completed get purged eventually ',negative
'txnid means its select iud which does not write acid table insert overwrite table partitionp select from and autocommittrue ',negative
'allowcomplex ',negative
'temporarily ',negative
'see hcatalog ',negative
'casts ',negative
'also print out the generic lineage information there any ',negative
'this tells the pending update any that whatever doing irrelevant and also makes sure dont take the duck back twice this called twice ',negative
'save previous longword ',negative
'initialize some variables which used initialized commonjoinoperator ',negative
'the default fraction ',negative
'return the passed string value ',negative
'cast int double ',negative
'remove ',negative
'prepare output set the projections ',negative
'dont deal with columns rhs set expression since the whole expr part the rewritten sql statement and thus handled semanticanalzyer nor have figure which cols rhs are from source and which from target ',negative
'type intervaldaytime ',negative
'this function for internal use only ',negative
'cache mkeygroup ',negative
'the current filters use follows evfilter eventutilsandfilter tblnameorpattern eventto test each those three filters and then test andfilter itself ',negative
'define how pass options the child process launching client local mode the driver options need passed directly the command line otherwise ',negative
'cleanup pathtopartitioninfo ',negative
'adjust noconditional task size threshold for llap ',negative
'direct and not memory mapped ',negative
'remove distinctcolindices set reducer reset keys ',negative
'trigger post compilation hook note that the compilation fails here then beforeafter execution hook will never executed ',negative
'original files delta directory deletedelta directory and base directory ',negative
'create new operator which share the table desc ',negative
'finally create the vertex ',negative
'not use datetime tests avoid result changes ',negative
'keep track all the contexts that are created this query can clear them when finish execution ',negative
'contract eof differs between datainput and inputstream ',negative
'get the partition columns from the end derivedschema ',negative
'smaller prefix mdhash and later will stored such staging stats table when stats gets aggregated statstask only the keys that starts with prefix will fetched now that hashed smaller prefix will not retrieved from staging table and hence not aggregated avoid this issue will remove the taskid from the key which redundant anyway ',negative
'the file held writer will throw ',negative
'cache uses allocator allocate and deallocate create allocator and then caches ',negative
'this test with hdfs acls will only work filesystemaccess available the version hadoop used build hive ',negative
'block until all semaphore resources are released outstanding async writes ',negative
'try singular ',negative
'parenttblname ',negative
'archiving unarchiving process ',negative
'the epoch ',negative
'the api authorizer use the session state getauthorizer return null here disable authorization use api the the additional authorization checks happening hcatalog are designed work with storage based authorization client side should not try doing additional checks authorizer use the recommended configuration use storage based authorization metastore server however user define custom authorization will honored ',negative
'dont allow swapping between virtual and materialized view replace ',negative
'server will create new threads max necessary after idle period will destroy threads keep the number threads the ',negative
'table tstage ',negative
'dont use the hadoopjobexechooks for local tasks ',negative
'try preempting task that higher priority task can take its place ',negative
'evaluate the column boolean converting necessary ',negative
'expect the start and count divisible step ',negative
'repeating case for first boolean flag argument ',negative
'both cases move the file under destf ',negative
'stream offset relation the stripe figure out which columns have present stream ',negative
'set the fetch formatter noop for the listsinkoperator since well write out formatted thrift objects sequencefile ',negative
'isextended ',negative
'declare this method final for performance reasons ',negative
'multikey specific members ',negative
'copy the data the buffer ',negative
'use protected for the fields the fasthivedecimalimpl class can access them other classes including hivedecimal should not access these fields directly ',negative
'todo pass this exception ',negative
'use the rowid directly ',negative
'finally start the server ',negative
'dont record encodings for unneeded columns ',negative
'skip overwriting exisiting table object which present because was added after prewarm started ',negative
'sanity checks ',negative
'ensure there operation related object leak ',negative
'not actually getter ',negative
'this correlation muxoperators ',negative
'verify zerodivide result for position ',negative
'parse until field separator currentlevel ',negative
'spark local mode need search added files root directory ',negative
'the following loop should create stripes the orc file ',negative
'read split ',negative
'put the keyvalue into the map ',negative
'not using position alias and number ',negative
'binary sortable key serializer ',negative
'must deterministic order map for consistent qtest output across java versions ',negative
'for unit tests ',negative
'pass lineagestate when driver instantiates another driver run ',negative
'still nothing raise exception ',negative
'this operation ',negative
'needed intercept readclassandobject ',negative
'production thisname basetype maptype settype listtype ',negative
'partname ',negative
'currently getprimarykeys always returns empty resultset for hive ',negative
'generate groupbyoperator for mapside partial aggregation ',negative
'the general case this set restricts automatic type conversion just these functions ',negative
'the default hook ',negative
'currently avgdistinct not supported partitionevaluator ',negative
'otherwise the registry has not been initialized skip for the time being ',negative
'lazy object inspectors for stringcharvarchar will all cached the same map ',negative
'see ',negative
'getfunctions ',negative
'skip the step connect the metastore ',negative
'grab round digit from middle word ',negative
'this full outer join this can never mapjoin any type return false ',negative
'include failedupdate only after looking all the tasks the same priority ',negative
'get evaluator for string concatenation expression ',negative
'issue would happen there was tiny delay the network dont care ',negative
'nonjavadoc see ',negative
'for partitions flag controlling whether the current table specs are used ',negative
'generate the temporary file ',negative
'all must selected otherwise size would zero repeating property will not change ',negative
'reset keyinitedmapsize flag since may set true the case previous empty entry ',negative
'construct using ',negative
'remove the dummy store operator from the tree ',negative
'check the function really removed ',negative
'nonjavadoc see ',negative
'addbasefilet ',negative
'have set the bucketing columns differently for update and deletes ',negative
'here are some negative cases below ',negative
'use linkedhashset give predictable display order ',negative
'note could skip creating the table and just add table type stuff directly the ',negative
'this the same the setchildren method below but for empty tables takes care the following create the right object inspector set the childrenoptooi with the object inspector ensure that the initialization happens correctly ',negative
'varchar should take string length into account varchar varchar varchar ',negative
'confirm the batch sizes were expected ',negative
'txns ',negative
'discard the blocks ',negative
'partition columns occur data want remove them ',negative
'note pass null factory because allocate objects here could also pass percall factory that would set filekey set after put ',negative
'same getrecordreader ',negative
'check the cache first ',negative
'the deserializer responsible for actually reading each record from the stream ',negative
'case select the data size does not change ',negative
'init parse context ',negative
'sort the objects first you are guaranteed that partition being locked the table has already been locked ',negative
'revoke with grant option only remove the grant option but keep the role ',negative
'create output row objectinspector ',negative
'generate possibly get from cached result parent sparktran ',negative
'succeeded state ',negative
'this function returns the grouping sets along with the grouping expressions even rollups and cubes are present the query they are converted ',negative
'worthwhile only more than split and filesplit ',negative
'exclude insert queries ',negative
'add tinyint values ',negative
'key val ',negative
'verify that the actual action also went through ',negative
'after one exception everything expected run ',negative
'skip the next child since already took care ',negative
'excepted ',negative
'lets add lot constant rows test the rle ',negative
'set the cookie max age very low value that ',negative
'change the key need ',negative
'nothing there not index definition for this table ',negative
'first breaking the filter conditions into equality comparisons between rightjoinkeysfrom the original filterinputrel and correlatedjoinkeys correlatedjoinkeys can expressions while rightjoinkeys need input ',negative
'get most the fields for the ids provided ',negative
'this should called rarely enough for now its just lock every time ',negative
'the object count longwritable sum resulttype reused during evaluating ',negative
'all parents should reduce sinks pick the one just walked choose the number reducers the joinunion case they will all sortorder case where matters there will only one parent ',negative
'clear out any parents reducer the root ',negative
'final string ',negative
'wait for stream threads finish ',negative
'would possible support this but this such pointless command ',negative
'ssilent ',negative
'release all the locks acquired for this object this becomes important for multitable inserts when one branch may take much more time than the others better release the lock for this particular insert the other option wait for all the branches finish set true which will mean that the first multiinsert results will available when all the branches multitable ',negative
'replace earlier element must have lower offset ',negative
'validate unset non existed table properties ',negative
'first exact match and then prefix matching the latter due input dir could dirdspart where part not part partition ',negative
'set the relevant information the configuration for the accumuloinputformat ',negative
'todo vcpu settings possibly when drfa works right ',negative
'allocate triples cannot above highest integer power ',negative
'use numdistinctvalues possible ',negative
'replace the map join operator localmapjoin operator the operator tree ',negative
'path format ',negative
'partition columns are repeated test element ',negative
'swap and thx ',negative
'test that existing sharedwrite table with new sharedwrite coalesces ',negative
'alter partitioned table rename partition ',negative
'row sufficient know have kill the query ',negative
'average value size will sum all sizes aggregation buffers ',negative
'call addtranslation just get the assertions for overlap ',negative
'set local work ',negative
'normalize was used then should have the same sign this currently working with that assumption ',negative
'validate the items are only constants ',negative
'indicates malformed version ',negative
'for query the type insert overwrite table select from subq union all subqu subq and subq write directories parentchild and parentchild respectively and union removed the movetask that follows subq and subq tasks moves the directory parent ',negative
'convert lower case case are getting from serde ',negative
'callback method used subclasses set the outputoi the evaluator ',negative
'the real implementation for the instanceset instanceset has its own copy the cache yet completely depends the parent every other aspect and thus unneeded ',negative
'from preacid insert ',negative
'there should different txn ids associated with each lock ',negative
'master thread methods ',negative
'list columns comma separated ',negative
'followed each element ',negative
'definitely byte most bytes fall here ',negative
'supports all types ',negative
'create standard settable union object inspector ',negative
'see also code clidriverjava ',negative
'this chicanery get around the fact that the table needs final order ',negative
'nothing special needs done for grouping sets this the final group operator and multiple rows corresponding the grouping sets have been generated upstream however addition job has been created handle grouping sets additional rows corresponding grouping sets need created here ',negative
'ignore other types for purposes authorization ',negative
'closed from orc writer still need the data not discard anything ',negative
'input type date epochdays ',negative
'cpu cost sorting cost for each relation ',negative
'char ',negative
'todo the name from the creation metadata for any the tables has changed ',negative
'this wont usually called otherwise ',negative
'the file whence this split ',negative
'pop list map ',negative
'call listlocatedstatus mockmocktable call check side file for mockmocktbl call open mockmocktbl call check side file for mockmocktbl ',negative
'find the right ',negative
'operations will lost once owning session closed ',negative
'then try serdeproperties ',negative
'now only used for bucket mapjoin there exactly one event the list ',negative
'any the fields struct are representing null then return true ',negative
'class ',negative
'asts are slightly different ',negative
'whether the method takes variablelength arguments whether the method takes array like object string etc the last argument ',negative
'',negative
'for performance dont check that that the fieldref isnt recid everytime just assume that the caller used and thus doesnt have that fieldref ',negative
'recheck ',negative
'currently none ',negative
'note that set basic stats false will wipe out column stats too ',negative
'nonjavadoc see javaioinputstream int ',negative
'subtraction with overflow check overflow produces null output ',negative
'ugi for the hivehost kerberos principal ',negative
'archiving was done this upper level every matched partition would archived not being archived means archiving was done neither this nor upper level ',negative
'enabled for acid case and the file format orc ',negative
'create lazystruct with serialized string with expected separators ',negative
'outputstream null means need process for explain formatted ',negative
'prefix used specify module specific properties mainly avoid conflicts with older unittests properties ',negative
'the aggregation buffer already contains partial histogram therefore need merge histograms using algorithm from the benhaim and tomtov paper ',negative
'incase acid the file orc the extension not relevant and should not inherited ',negative
'new partition for example ',negative
'colvals ',negative
'true ',negative
'bucketed mapjoin cannot performed ',negative
'constants for sparse encoding ',negative
'another thread might have already created these tables ',negative
'optional int version ',negative
'theres bug ctor where seconds are not converted ',negative
'clear timing this threads hive object before proceeding ',negative
'assign tables without nested column pruning info the default conf ',negative
'requires calculate stats new partition doesnt have ',negative
'test repeating case for null value ',negative
'singlecolumn long specific save key ',negative
'this table working with acid semantics turn off merging ',negative
'get right table alias ',negative
'initialize the list event handlers ',negative
'since key expression can calculation and the key will into scratch column ',negative
'nothing vectorized expression that passes all rows through ',negative
'load the current incremental dump and ensure does nothing and lastreplid remains same ',negative
'important important important the keys used store info into the job configuration any new keys are added the hcatstorer needs updated the hcatstorer updates the job configuration the backend insert these keys avoid having call setoutput from the backend which would cause metastore call ',negative
'there equivalent version return that else return this version ',negative
'now set one column nullable ',negative
'test the unsecure base case when neither hadoop nor jobspecific credential provider set ',negative
'regular create table ddl ',negative
'removing semijoin optimization when may not beneficial ',negative
'more then one part exist ',negative
'skip cardinality violation clause ',negative
'and not using this privilege mapping but might make sense move here ',negative
'nanos converted millis ',negative
'sleep fails should exit now before things get worse ',negative
'parser enforces that table alias added but check again ',negative
'make sure the basic query properties are initialized ',negative
'bround without digits ',negative
'the hivejardir can determined once per client ',negative
'clear the mapredwork output file from outputs for ctas ddlwork the tail the chain will have the output ',negative
'allocate the buffers prepare cache keys ',negative
'compress fastbitset bytes ',negative
'pbb ',negative
'cannot rebuild not materialized view ',negative
'get notifications from metastore ',negative
'clean the cache ',negative
'need reinitialize the currentusername currentroles fields ',negative
'cluster ',negative
'create table can work against ',negative
'ideally this should never happen and this should assert ',negative
'but default partition ',negative
'should not done for semijoin since will change the semantics invert join inputs this done because otherwise the semanticanalyzer methods merge joins will not kick ',negative
'negative scale decimals ',negative
'required bytes ',negative
'get the vint that represents the map size ',negative
'sarg present get relevant stripe metadata from cache readers ',negative
'byteswritable valuebyteswritable byteswritable valuewritable collect keywritable valuewritable ',negative
'here means last txn the batch resolved but the close hasnt been called yet there nothing heartbeat ',negative
'tolerance check float equality ',negative
'compute stats before compaction ',negative
'swap the join ',negative
'position before the last written value ',negative
'followed key and nonkey input columns some may missing ',negative
'nonbean fields needed during compilation ',negative
'convert the remainders into list that are anded together ',negative
'size means nopooling case passthru ',negative
'note critical this here that logj reinitialized ',negative
'note that inputs and outputs can changed when the query gets executed ',negative
'output string such replacement exists emit out the original input code point ',negative
'ignore constant ',negative
'start inclusive infinity ',negative
'avoid concurrent modification ',negative
'create virtual row type for project only rename fields ',negative
'the removal before change the element avoid invalid queue ordering ',negative
'second copy red different object ',negative
'nothing actually hashes bucket updatedelete deltas dont have ',negative
'this not likely the right thing for compaction original files when there are copyn files ',negative
'table not partitioned ',negative
'know never went that far when were inserting ',negative
'this case are actually scaling dont have complicated things because doing scalingup after multiplication doesnt affect overflow doesnt happen happens anyways ',negative
'try find this input rel the position cor var ',negative
'create input byteswritable this would have capacity greater than length ',negative
'check have right delete delta files after minor compaction ',negative
'remove the node has expired ',negative
'bucketing version skip ',negative
'maximum length seen far ',negative
'convert list above set was created using setsunion for reasons explained there ',negative
'foreignkeys ',negative
'provide the path the field the error message ',negative
'construct using ',negative
'this admittedly bit simple seems allow old stats attributes kept the new values not overwrite them ',negative
'set nonhdfs tables external unless transactional should have been checked before this ',negative
'set the number buckets here ensure creation empty buckets ',negative
'see ',negative
'corrupt last entry ',negative
'todo check whether rowgroupoffsets can null then need apply the predicate push down filter ',negative
'volatile because heartbeat may different thread updates this are piggybacking ',negative
'position the row schema the filesink operator ',negative
'can just stop all the sessions ',negative
'length mixedup ',negative
'save original values ',negative
'changes the type the input references adjust nullability ',negative
'specialized class for doing vectorized map join that outer join singlecolumn long using hash map ',negative
'for multiinsert query currently only optimize the from clause hence introduce multiinsert token top however first need reset existing token insert using would equivalent but use avoid setting the property multiple times ',negative
'verify orc sarg still works ',negative
'opening the meta table ensures that cluster running ',negative
'simple case union ',negative
'single string key hash multiset optimized for vector map join the key will deserialized and just the bytes will stored ',negative
'determine there only one tablescanoperator currently map vectorization not try vectorize multiple input trees ',negative
'gets new templeton controller objects ',negative
'set not null constraint name null before sending listener ',negative
'given session the name can fixed across all invocations ',negative
'lets use the remaining space column progress bar ',negative
'mapper can span partitions make sure splits does not contain multiple oplist combination this done using the map pathfilter ',negative
'nonjavadoc this provides lazyhivedecimal like class which can initialized from data stored binary format see int int ',negative
'from here evaluate the auto mode ',negative
'update source info the state succeeded ',negative
'these bottom layer reducesinkoperators ',negative
'delink union ',negative
'rounding ',negative
'need pass all the columns used the where clauses reduce values ',negative
'opening the meta table ensures that cluster running ',negative
'now take local offset midnight utc say are that means surprise ',negative
'reject all paths force continue when more paths should throw exception ',negative
'kerberos ',negative
'that are not our wordlist table and column names ',negative
'have too many results return null for full scan ',negative
'add not operator the beginning this for the cloned operator because ',negative
'start index query that equivalent using query internal offset ',negative
'this happens when the code inside the jmx bean setter from the java docs threw exception log and fall back the class name ',negative
'imported serdeformat null then set just ',negative
'bounded queue could specified here but that will lead blocking unbounded and will release soft referenced kryo instances under ',negative
'build keys grouping set starting position ',negative
'there are sample cols and bucket cols then throw error ',negative
'raise error could not find the column ',negative
'offset because the first frames are just calls get down here ',negative
'each group walk from most recent and count occurences each state type once you have counted enough for each state satisfy retention policy delete all other instances this status ',negative
'casesensitive string found ',negative
'created using field name ',negative
'let caller set negative sign necessary ',negative
'execute select statement and verify the result set should two rows ',negative
'delete functions created the tests enough remove functions from the default database other databases are dropped ',negative
'expected result entry the recordidentifier data entry file before compact ',negative
'verify that the beginning entry the only one that matches ',negative
'assumes one instance this singlethreaded compilation for each query ',negative
'system registry should not used check persistent functions see ispermanentfunc ',negative
'estimated number bytes needed ',negative
'inactive nodes restart every call ',negative
'framework ',negative
'binary arithmetic operator ',negative
'pick the length the first ptn expect all ptns listed have the same number keyvals ',negative
'create new vertex ',negative
'delta base ',negative
'nonskewed value add list for later handle default directory ',negative
'update for next iteration ',negative
'copy java object because that saves object creation time note that average the number copies logn thats not very important ',negative
'iterate through the batch and for each owid rowid the batch check deleted not ',negative
'track the variable length keys ',negative
'map from partid stattypevalue ',negative
'map the newly allocated write ids against the list txns which doesnt have preallocated ',negative
'roundlongcol returns long and noop will not implemented here ',negative
'process multikey leftsemi join vectorized row batch ',negative
'resourceplan ',negative
'operators for which the optimization will successful ',negative
'for dummy partitions only partition name needed ',negative
'serde helper methods ',negative
'set some parameters for prepared sql not all them ',negative
'groupby operator reducer may not processed parallel skip optimizing ',negative
'parse analyze optimize and compile ',negative
'again with correct output type ',negative
'specify the default logj properties file ',negative
'loop over the given inlist elements ',negative
'recurse over the subqueries fill the subquery part the plan ',negative
'are only interested exprnodecolumndesc ',negative
'create znode under the rootnamespace parent for this instance the server znode name ',negative
'extrapolation needed for some columns this case least column status for partition missing ',negative
'there insert the subquery ',negative
'env interface mock out dealing with environment variables this allows interface with environment vars through beelineopts while allowing tests mock out env setting needed ',negative
'update the time hasnt been specified ',negative
'single item clause transform from equals except complex types ',negative
'remember used this one the query ',negative
'for each dynamic partition check needs merged ',negative
'this conversion frequently used this should optimized converting decimal from the input bytes directly without making new string ',negative
'this should only ever called testing scenarios there should not any other users the cache its entries this may mess cleanup ',negative
'the given value type lazyobject then only try and convert that form ',negative
'build hivetox column mapping ',negative
'timer that reports every minutes the jobtracker this ensures that even the operator returning rows for greater than that duration progress report sent the tracker that the tracker does not think that the job dead ',negative
'creating the context can create bunch files make sure clear out ',negative
'get and process the current datum ',negative
'keep toread list for future use dont extract ',negative
'cost cost transferring small tables join node ',negative
'primitives except binary ',negative
'same test above but with jars sharing dependencies ',negative
'this where the spilling may happen again ',negative
'note might want smarter threadids are low and there more arenas than threads ',negative
'node ',negative
'this operator ',negative
'col ',negative
'add children self the front the queue that order ',negative
'set seconds have wait for least ',negative
'note that cannot allow users provide app since providing somebody elses appid would give one llap token and splits for that app could verify somehow yarn token nothing can udf could get from client already running yarn such the clients running yarn will have two app ids aware ',negative
'negative max cache size means unbounded ',negative
'switch iterate foreign keys ',negative
'there are uncompacted original files they will included compaction ',negative
'tests for listpartition partitions boolean ifnotexists boolean needresults method ',negative
'test serialization and deserialization with different schemas ',negative
'are moving the partition across filesystem boundaries inherit from the table properties otherwise same filesystem use the original partition location see hive and hive for background ',negative
'mutable threadsafe map store instances ',negative
'add column info corresponding partition columns ',negative
'default type table ',negative
'case now with entries try the above settings ',negative
'blurb list ',negative
'process deregister ',negative
'case column stats hash aggregation grouping sets ',negative
'prevents throwing exceptions our raw store calls since were not using rawstoreproxy ',negative
'string type hive represented varchar with precision integermaxvalue turn the max varchar precision should however the value not used for validation but rather only internally the optimizer know the max precision supported the system thus varchar precision should fall between ',negative
'construct using ',negative
'column name within regex pattern with its corresponding value provided ',negative
'auth has been initialized ',negative
'nonjavadoc see javautilcalendar ',negative
'such job ',negative
'submit the job ',negative
'try replace bucket map join with sorted merge map join ',negative
'other formats can converted insertonly transactional tables ',negative
'not update metrics didnt update removal ',negative
'hadoop fixed version alpha ',negative
'moving tablespartitions depend the dependencytask ',negative
'single byte array value hash map based the since does not interpret the key binarysortable optimize this case and just reference the byte array key directly for the lookup instead serializing the byte array into binarysortable rely just doing byte array equality comparisons ',negative
'parse the json map ',negative
'make copy the statements result schema since may ',negative
'nop ',negative
'the number children slots used ',negative
'remove from the runningtasklist ',negative
'present ',negative
'embedded metastore used per config far ',negative
'heartbeats the txn table this commits not enter with any state ',negative
'since the user running the test wont belong nonexistent group foobargroup the call will fail ',negative
'nothing here ',negative
'this not join condition will get handled predicate pushdown ',negative
'',negative
'type date longcolumnvector storing epoch days minus type date produces type intervaldaytime storing nanosecond interval longs ',negative
'add procedures they can invoked functions ',negative
'singlecolumn long specific lookup key ',negative
'here are disconnecting root with its parents however need save few information since future may reach the parent operators via different path and may need connect parent works with the work associated ',negative
'this used for tests where theres always just one batch work and the checks after the batch the check will only come the end queueing ',negative
'report forward ',negative
'found semijoin branch there can more than one semijoin branch coming from the parent ',negative
'assume that norgs value only set from sarg filter and for all columns intermediate changes for individual columns will unset values the array skip this case for column read could probably specialcase just like encodedreaderimpl but for now its not that important ',negative
'this will null slave nodes ',negative
'this point tasks running both priority ',negative
'create our vector map join optimized hash table variation above the map join table container ',negative
'hdfs session path ',negative
'qualifiers ',negative
'this isnt really used for anything ',negative
'find proxy user any from query param ',negative
'must struct ',negative
'',negative
'schedule low pri first when high pri scheduled takes away the duck from the low pri task when the high pri finishes low pri gets the duck back ',negative
'compute join keys and store reducekeys ',negative
'add new item the spark work ',negative
'increase the row count ',negative
'repl export has repllastid and replscopeall import repl dump table has repllastid will likely ',negative
'update the log level for the specified logger ',negative
'expression udf not permanent udf ',negative
'dont want put any limits this task this essential before start processing new database events ',negative
'local file system path for spilled hashmap status hashmap true disk false memory when theres enough memory cannot create hashmap used create empty same above same above how many rows saved the ondisk hashmap disk ',negative
'write the null byte every eight elements this the last element and serialize the ',negative
'traverse the byte buffer containing the input string one code point time ',negative
'undone col col scalar col col scalar classname long ',negative
'convert the keyvalue pairs ',negative
'try again with include vector ',negative
'runs stat against the servers ',negative
'one row existence ',negative
'the one created here will not added ',negative
'least the header should fit ',negative
'have mintxn maxtxn and ismajor jobconf could figure out exactly what the dir name that want rename leave for another day ',negative
'unfortunately cannot directly read protected field nonthis object ',negative
'the block being moved the move will release memory ',negative
'put partial aggregation results reducevalues ',negative
'stored here only async operation context ',negative
'use calcite cost model for view rewriting ',negative
'cannot optimize any others ',negative
'booleanstats ',negative
'this was the only predicate set filter expression const ',negative
'preallocated member for storing any nonspills nonmatches merged row indexes during ',negative
'this hash function returns the same result doublehashcode while returns different result ',negative
'max file num and size used single copy after that distcp used ',negative
'nonnull column alias ',negative
'zero result ',negative
'synthetic predicates from semijoin opt should not affect stats ',negative
'this where counters are logged ',negative
'see the comment ',negative
'opnodes type always either kwexists kwin never notexists notin figure this out need check its grand parents parent ',negative
'clear rounding portion high longword and add right scale roundmultiplyfactor middle and lower longwords result ',negative
'the function distinct partial aggregation has not been done the client side distpartagg set the client letting know that partial aggregation has not been done for select countbc countdistinct group for countbc partial aggregation has been performed then directly look for countbc otherwise look for for distincts partial aggregation never performed the client side always look for the parameters ',negative
'extract the sequence number this ephemeralsequential znode ',negative
'the extra non existing values will ignored ',negative
'additional data structures needed for the join optimization through hive ',negative
'could just tolowercase here and let qualify but lets proper ',negative
'not ctas dont need further and just return ',negative
'remember which reducesinks weve already connected ',negative
'override public partitionid hashpartition return ',negative
'create dummy database and couple dummy tables ',negative
'call open mockmocktbl ',negative
'tracks only running portion the query ',negative
'object inspectors for input rows ',negative
'and finally lets check output sizes ',negative
'print vertexname ',negative
'test alter table with schema change ',negative
'case when user ctrlc twice kill hive cli jvm want release locks ',negative
'only the column stats available update the data size from the column stats ',negative
'read totalmonths from datainput ',negative
'spit marked isoriginal but its not immediate child partition nor base delta this should never happen ',negative
'blockcompressed should always false ',negative
'order its just row ',negative
'lenvalue ',negative
'add children taskstovisit ',negative
'create new ',negative
'apply the rules the spec fill any missing pieces every window specification also validate that the effective specification valid the rules applied are for wdw specs that refer window defns inherit missing components window spec with parition spec partitioned constantnumber for missing wdw frames for frames with only start boundary completely specify them the rules link validate the effective window frames with the rules link validatewindowframe there order then add the partition expressions the order ',negative
'store the chunk indices split file that way several callers are reading the same file they can separately store and remove the relevant parts the index ',negative
'invert words ',negative
'sort the list get sorted deterministic output for ease testing ',negative
'suppose its the first major compaction only have deltas ',negative
'long masks and values ',negative
'join which are part join keys ',negative
'current write ids are not valid ',negative
'empty rows for each table ',negative
'value for kale ',negative
'need partitions for firing events and for result needs mpartitions drop great maybe could bypass fetching mpartitions issuing direct sql deletes ',negative
'change lock manager otherwise unittest doesnt through ',negative
'not control character nothing ',negative
'functiontype ',negative
'emit the rest word ',negative
'run cleaner delete rows for the aborted transaction ',negative
'add new rel its the maps ',negative
'table alias small input file name big target file names small ',negative
'this will happen case joins the current plan can thrown away after being merged with the original plan ',negative
'long columncolumn ',negative
'write back previous fields null byte ',negative
'serialize the union ',negative
'for internal getacidstate purposes and means the delta dir had statement ',negative
'load into compressed buf first ',negative
'stores all the downloaded resources key and the jars which depend these resources values form list used for deleting transitive dependencies ',negative
'verify that hive caching the object inspectors for ',negative
'explicitly using only the start offset split and not the length splits generated block boundaries and stripe boundaries can vary slightly try hashing both the same node there the drawback potentially hashing the same data multiple nodes though when large split sent node and second invocation uses smaller chunks the previous large split and send them different nodes ',negative
'blank byte new tai lue letter low bytes ',negative
'validation ',negative
'create http client from the configs ',negative
'try opportunistically for the common case the samesized allocated buddy ',negative
'token expiration ',negative
'that explain doesnt leak tmp tables ',negative
'the exception not nodeexists rethrow ',negative
'determine the query qualifies reduce input size for limit the query only qualifies when there are only one top operator and there transformer udtf and block sampling used ',negative
'finishes inside ',negative
'then write chunk bytes ',negative
'dropfunction ',negative
'number buckets ',negative
'were hijacking the big table evaluators replace them with our own custom ones ',negative
'not allowed ',negative
'were supporting dynamic service discovery well add the service uri for this hiveserver instance zookeeper znode ',negative
'fill colname ',negative
'take what all input formats support and eliminate any them not enabled the hive variable ',negative
'always want read all the delete delta files ',negative
'init input object inspectors ',negative
'how many blocks this size comprise arena ',negative
'reset buffer ',negative
'the loaded hash table empty for some conditions can skip processing the big table rows ',negative
'serialize work ',negative
'fill the temp list before merging sparse map ',negative
'null byte start ',negative
'the serde part from testlazysimpleserde ',negative
'this point hinted semijoin case has been handled already check big table big enough that runtime filtering ',negative
'not always allow but always return true because later subq remove rule will generate diff plan for this case ',negative
'rowid also found ',negative
'this assumes splitting ',negative
'translate window spec ',negative
'nonjavadoc see ',negative
'get length word ',negative
'true same value repeats for whole column vector vector holds the repeating value ',negative
'default tokenstore memorytokenstore ',negative
'fetch table alias ',negative
'retrieve not null constraints ',negative
'set output collector for any reduce sink operators the pipeline ',negative
'preserve the old behavior failing when cannot write even deletedata and even the table external that might not make any sense ',negative
'consider query like select count from select key count from select mapjoina akey key avalue val bvalue val from tbl join tbl akey bkey subq group key subq the table alias should subqsubqa which needs fetched from topops ',negative
'but snapshot not valid recompile the query ',negative
'using init instead this because the new operation that needs run before delegating the other ctor and this messes chaining ctors ',negative
'add fsd that the loadtask compilation could fix its path avoid the move ',negative
'map from tablename task columnstatstask which includes basicstatstask ',negative
'reset correct http path ',negative
'for the following method ',negative
'this can change based new splits ',negative
'dfs ',negative
'proceed ast contains partition not exists ',negative
'vertices elapsed time contains footersummary startime ',negative
'the below loop may perform bad when the destination file already exists and has too many copy files well desired approach was call listfiles and get complete list files from the destination and check whether the file exists not that list however millions files could live the destination directory and concurrent situations this can cause oom problems ill leave the below loop for now until better approach found ',negative
'replimports are replaceinto unless the event insertinto ',negative
'hive date representable pig datetime ',negative
'table and will exist and partition india will exist rest failed operation failed ',negative
'tests ',negative
'use threads resolve directories into splits ',negative
'process groupby pattern ',negative
'must called under the epic lock ',negative
'call open read data split mockmocktable ',negative
'are doing update delete the number columns the table will not match the number columns the file sink for update there will one too many because the rowid and the case the delete there will just the rowid which dont need worry about from lineage perspective ',negative
'print the results ',negative
'dont allow turning auto parallel once has been explicitly turned off that avoid scenarios where auto parallelism could break assumptions about number reducers hash function ',negative
'the bottom layer reducesinkoperators these reducesinkoperators are used record the boundary this subtree which can evaluated single ',negative
'this shouldnt ever happen ',negative
'unionfield ',negative
'reevaluate expression current row trigger the lazy object caches reset the current row ',negative
'reuse record reader ',negative
'order clause ',negative
'instance fields ',negative
'return true false based whether bucketed mapjoin can converted successfully sortmerge map join operator the following checks are performed the mapjoin under consideration bucketed mapjoin all the tables are sorted same order such that join columns equal prefix the sort columns ',negative
'trailing blank field ',negative
'nonjavadoc see javautilcalendar ',negative
'convenient constructor for initial batch creation takes ',negative
'test that zerodivide produces null for all output values ',negative
'check that write still valid ',negative
'evaluate the aggregators ',negative
'matches only joinoperators which are reducers rather than map joins smb map joins etc ',negative
'temp partition input path does not match exist temp path ',negative
'munging innerschemas ',negative
'approximate ',negative
'reset for the new partition ',negative
'filtered ',negative
'fill high long from some middle long ',negative
'store the orc configuration from the first file all other files should ',negative
'unionfield ',negative
'join not left right outer bail out any sort column not part the input where the ',negative
'there are functions doesnt matter much whether aggregate the inputs before the join because there will not any functions experiencing cartesian product effect but finding out whether the input already unique requires call arecolumnsunique that currently until calcite make metadata more robust fixed places heavy load the metadata system choose imagine the the input already unique which untrue but harmless ',negative
'export valid directories with modified name they dont look like basesdeltas could also dump the delta contents all together and rename the files names collide ',negative
'overridden and used mode ',negative
'case the empty grouping set preset but output has done the summary row still needs emitted ',negative
'aborted break out the loop and cancel all subsequent futures ',negative
'maxcollen ',negative
'the nonmm path only finds new partitions looking the temp path produce the same effect will find all the partitions affected this txn note ignore the statement here because its currently irrelevant for movetask where this used always want load everything also the only case where have multiple statements anyway union ',negative
'only retrieve the materialization corresponding the rebuild turn pass true for the parameter cannot allow the materialization contents stale for rebuild want use ',negative
'the value for the constant does not matter replaced the grouping set value for the actual implementation ',negative
'the columns used the join appear the output the aggregate ',negative
'test that locking database prevents locking tables the database ',negative
'must ensure the exactness the doubles fractional portion the fraction part will converted and significantly reduce the savings from binary serialization ',negative
'outputs empty which means this create table happens the current ',negative
'clean the staging table ',negative
'this needs major compaction ',negative
'allocate writeid txn under hwm this will get greater than txn hwm ',negative
'support for authorization partitions needs added ',negative
'start the creation znodes ',negative
'note cannot use copyselected below since whole column operation ',negative
'get the output objectinspector from the return type ',negative
'and put writeentity for postexec hook ',negative
'derby commandline parser ',negative
'destpath directory exists rename call will move the srcpath into destpath without failing check before renaming ',negative
'used for avoid extra byte copy ',negative
'updated only when thread has failed ',negative
'all columns have primitive ',negative
'unionfield ',negative
'tblvalidwriteids ',negative
'truncate table ',negative
'interleaved writes both batches ',negative
'governs remotefetchinput behaviour set true well assume that the input has files file present which lists the actual input files copy and well pull each those read set false itll behave traditional copytask ',negative
'extract the group positions that are part the collations and sort them they respect ',negative
'try transform predicates filter into simpler clauses first ',negative
'the equality implemented fully the greaterthanlessthan values not implement transitive relation ',negative
'outputformat ',negative
'this join has already been processed ',negative
'end ',negative
'nonblocking execute ',negative
'nothing for other modes ',negative
'parse key ',negative
'the subtree gather the references ',negative
'now for each entry the queue see all the associated locks are clear ',negative
'fullyspecified partition ',negative
'twice more skip dedup ',negative
'for now theres nothing special return addedvals just return the footer ',negative
'now create the new project ',negative
'for now require select with grant ',negative
'now test that dont timeout locks should not ',negative
'implementations may choose override this ',negative
'have set for each partition too ',negative
'output file system information ',negative
'',negative
'with nulls ',negative
'forward the current keys needed for sortbased aggregation ',negative
'local time zone store separately because calendar would clone ',negative
'write out the first bits worth data ',negative
'should this escaped string ',negative
'reset member variables dont get halfconstructed state ',negative
'note that the state the failed service still inited and not started even though the last service not started completely still call stop all services including failed service make sure cleanup happens ',negative
'iterator cursor the currblock size current read block append cursor the lastblock for the row object inspector for the row ',negative
'lock part txn heartbeat info txn record ',negative
'make sure the null flag agrees ',negative
'alert already running low memory starting with low memory will lead frequent auto flush ',negative
'add this condition the list nonequijoin conditions ',negative
'backtracking from ',negative
'list tezworkdependency ',negative
'create the project before ',negative
'now have written all information about the next value work the new value ',negative
'tez task were currently processing ',negative
'set version ',negative
'allow stateful functions the select list but nowhere else ',negative
'accurate byte value cannot obtained ',negative
'didnt find the token cant proceed log the tokens for debugging ',negative
'the key missing shouldnt able verify ',negative
'note the materialized view does not contain table that contained the query not need check whether that specific table outdated not rewriting produced those cases because that additional table joined with the existing tables with appendcolumns only join pkfk not null ',negative
'true only one value null ',negative
'initialize schema ',negative
'exponent ',negative
'type affinity does not help when multiple methods have the same type affinity ',negative
'retention ',negative
'convert valuelist array for the ',negative
'map needs two separators key and keyvalue pair ',negative
'save compiletime perflogging for webui executiontime perf logs are done either another threads perflogger reset perflogger ',negative
'this point the number reducers precisely defined the plan ',negative
'allowed operations intervalyearmonth intervalyearmonth intervalyearmonth date intervalyearmonth date operands not reversible timestamp intervalyearmonth timestamp operands not reversible intervaldaytime intervaldaytime intervaldaytime date intervalyearmonth timestamp operands not reversible timestamp intervalyearmonth timestamp operands not reversible timestamp timestamp intervaldaytime date date intervaldaytime timestamp date intervaldaytime operands reversible ',negative
'for performance reasons not want chase the values the end determine the count use hasrows and issinglerow instead ',negative
'',negative
'special handling for serde reader text llap file format version null then are processing text llap which case get vectors instead streams vectors contain instance decimalcolumnvector should use decimalstreamreader which acts wrapper around vectors ',negative
'simulate missing table scenario renaming couple tables ',negative
'generate table access stats required ',negative
'will always excuse the first error can decide single ',negative
'app never seen previous dag has been unregistered ',negative
'not affected the not about transactional ',negative
'need the directory hdfs which shall put all these files ',negative
'set third argument ',negative
'preallocated member for storing the physical batch index rows that need spilled ',negative
'verify all the aborted write ids are replicated the replicated ',negative
'compare stats obj ensure what get what wrote ',negative
'add token only already doesnt exist ',negative
'allow estimated stats for the columns then shall set the boolean true since otherwise will throw exception because columns with stimated stats are actually added the list columns that not contain stats ',negative
'dont make copy dont have noinspection unchecked ',negative
'cannot convert map join weve already chosen big table size and theres another one thats bigger ',negative
'decimal place ',negative
'optional bool result ',negative
'wrapper class for reading and writing metadata about dump responsible for dumpmetadata files ',negative
'not throw exception table does not exist ',negative
'cannot delimited split for some commands like dfs cat that prints the contents file which may have different delimiter will split only when the resultschema has more than column ',negative
'loads all the delete events from all the delete deltas into memory prevent outofmemory errors this check rough heuristic that prevents creation object this class the total number delete events exceed this value default has been set million delete events per bucket ',negative
'the the job this tracking node represents ',negative
'keys are the column names basically this maps the position the column ',negative
'nonjavadoc see javaioinputstream long ',negative
'get tokens for all other known fss since hive tables may result different ones ',negative
'for some reason this just locks the table alter table add this partition then end locking both table and partition with shareread plan has readentities same for other locks below ',negative
'level ',negative
'input gby different than the source columns find the same column input ',negative
'conjuctive elements ',negative
'key contains scheme such pfile and want only the path portion fix hive ',negative
'empty array ',negative
'key correct ',negative
'lastaccesstime ',negative
'support for statistics ',negative
'with the fast hash table implementation currently not support hybrid grace hash join ',negative
'llap server depends hive execution the reverse cannot true create the singleton once daemon startup the said singleton serves the interface ',negative
'finally monitor will print progress until the job done ',negative
'table columns mapping ',negative
'always generate list with least value ',negative
'lock was outdated and was removed then maybe another transaction picked ',negative
'unscaledvalue negativefalse fractionaldigits unscaledvalue negativetrue fractionaldigits unscaledvalue negativefalse fractionaldigits exponent unscaledvalue negativefalse fractionaldigits ',negative
'druidoutputformat will write segments intermediate directory ',negative
'get the first valid row the batch still available ',negative
'create hepplanner ',negative
'todo this method ever called more than one jar getting the dir and the ',negative
'this based the existing valid write list that was built for select query therefore assume all the aborted txns etc were already accounted for all adjust the high watermark only include contiguous txns ',negative
'add writeentity for each matching partition ',negative
'may happen that theres not enough memory instantiate hashmap for the partition that case dont create the hashmap but pretend the hashmap directly spilled ',negative
'update the property before offering ',negative
'this plan projecta all gby keys rewritten nullable projexpr aggregate groupbyall left input refs aggrewritten expression agg projectb rewriten original projected exprs join loj cond true leftinputrel rightinputrel ',negative
'tests that doing tablelevel repl load updates table repllastid but not dblevel repllastid ',negative
'required optional optional optional optional ',negative
'long and double are handled using descriptors string needs specially handled ',negative
'preserve the original configuration ',negative
'try this list ',negative
'passed the unparsed name here getpartitionsps expects parse ',negative
'create walker which walks the tree dfs manner while maintaining the operator stack the dispatcher ',negative
'ctas with acid target table ',negative
'copy null ',negative
'the current plan can thrown away after being merged with the union plan ',negative
'update jobconf using mrinput info like filename comes via this ',negative
'filtering the server and have fall back client path ',negative
'the logger name not found root logger returned dont want change root logger level since user either requested new logger specified invalid input which will add the logger that user requested ',negative
'end conversion ',negative
'this normal insert delta which only has insert events and hence all the files ',negative
'looking for map reduce ',negative
'someone else replacedremoved paralleladded stale value try again max confusion ',negative
'get options from arguments ',negative
'less than ',negative
'finally create the outer struct contain the key value structs ',negative
'build rel for clause ',negative
'for writes transaction batch not closed yet ',negative
'from ',negative
'suffix reduce len ',negative
'floattype treated doubletype ',negative
'there may more data after the gap ',negative
'following fields for displaying queries webui ',negative
'update them all ',negative
'the current task root task ',negative
'walk down expression see which arguments are actually used ',negative
'set writeentity for replication ',negative
'have processed this the previous run after has already queued the message ',negative
'how much have minimum size completely memory blowout factor datasize memory size ',negative
'hadoop property names set templeton logic ',negative
'this for reconciling hbasestoragehandler for use hcatalog ',negative
'returns fileid for smbjoin which consists part result file name ',negative
'foo bar blah form ',negative
'case last row was large bytes value ',negative
'roots ',negative
'char starts from index and total length should bytes max ',negative
'some inputs will probably never actually happen ',negative
'will keycolx valuecolx ',negative
'create permanent function ',negative
'request interceptor for any request preprocessing logic ',negative
'similarly need mapping since value expression can calculation and the value will into scratch column ',negative
'has the permissions the table dir ',negative
'for udfs that expect primitive types like int instead integer intwritable this will catch the the exception that happens they are passed null value then the default null handling logic will apply and the result will null ',negative
'entries the vgby are flushed ',negative
'next bits are used locate offset within longword ',negative
'could also join with acid tables only get tables with outdated stats ',negative
'return garbage value metrics havent been initialized that callers dont have keep checking the resulting value null ',negative
'remove the comments ',negative
'check groupby empty and there other cols aggr this should only happen when newparent constant ',negative
'replicate only one insert into operation the table ',negative
'need check the druid metadata ',negative
'timer that tops rptimer after long timeout ',negative
'remove entire priority level its been emptied ',negative
'dont lock files directories also skip locking temp tables ',negative
'initialize singlecolumn string members for this specialized class ',negative
'create copy the function descriptor ',negative
'warn the user bytes per reducer much larger than memory per task ',negative
'nothing here this not invoked the logj framework should likely not the logj interface ',negative
'hive only supports primitive map keys ',negative
'parent stats are not populated yet ',negative
'allocate and initialize new conf since test can ',negative
'match there filter sqcountcheck right input join which left input another join ',negative
'splits are equal number files worst case ',negative
'unit test convenience method for putting the key into the hash table using the actual type ',negative
'called reservememory know that theres memory waiting for somewhere however have class rare race conditions related the order lockingchecking different allocation areas simple case say have arenas available arena look arena someone deallocs from arena and allocs the same from arena look arena and find memory for single arena threads reserve each and single block available when the thread locks the freelist the one might have already examined the and lists finding nothing blocks placed into smaller lists after its split done will not found given that freelist locks dont overlap may even run completely between the time takes out the block and the time returns the remaining two solutions this are some form crossthread helping threads putting demand into some sort queues that deallocate and split will examine having and actor allocator thread threads per arena the one probably much simpler and will allow get rid lot sync code ',negative
'files size for splits ',negative
'accurate int value cannot obtained ',negative
'save some positional state ',negative
'they gave value but not time unit assume the default time unit ',negative
'create the converters ',negative
'scale down factor account for approximate values ',negative
'plugin interface for storage handler which supports input estimation ',negative
'allocate pairs cannot above highest integer power ',negative
'relogin with kerberos this makes sure all daemons have the same login user ',negative
'with nulls ',negative
'assuming grouping enabled always ',negative
'per split strategy basis and has same for all the files that strategy ',negative
'read many records because sometimes the recordreader for the format test behaves different with one record than bunch records ',negative
'todo test dropping nonempty catalog ',negative
'draw and return order further run should return last returned ',negative
'equal maps ',negative
'need reset true case previous aggregateproject has set false ',negative
'evaluate the result given partition and the row number process ',negative
'error ',negative
'driver class ',negative
'',negative
'work ',negative
'code below shameless borrowed from hadoop streaming ',negative
'each iteration cleans the file cache single unit unlike the orc cache ',negative
'delimiter check dot delimited qualified names ',negative
'keeping mintxnid atomic shared with heartbeat thread ',negative
'setting the default batch size makes the memory check rows work the same the row row writer was the default the smallest stripe size would rows which changes the output some the tests ',negative
'',negative
'the hash table slots for long key hash table each slot longs and the array sized the slot pair nonzero reference word the first value bytes and the long value ',negative
'corresponds semanalyzer ',negative
'have check here since invalid decref will overflow ',negative
'batches will sized ',negative
'found stale value cannot incref try replace with new value ',negative
'since renewal kerberos authenticated token may not cached ',negative
'check this grant statement will end creating cycle ',negative
'have keep least branch before support empty values hive ',negative
'bgenjjtree enumdef ',negative
'update statistics based column statistics conditions keeps adding the stats independently this may result number rows getting more than the input rows which case stats need not updated ',negative
'check that the table valid under strict managed tables mode ',negative
'the applicationlevel name ',negative
'dedup file list ',negative
'table was renamed ',negative
'only need aborted since dont consider anything above minopenwriteid ',negative
'this point should add any relevant jars that would needed for the udf ',negative
'the index ',negative
'list terminal operation states measure only completed counts for operations these states ',negative
'increment the counters only when there are violations ',negative
'varchar char length ',negative
'set dont repeat this initialization ',negative
'should able execute without failure the session whose transport has been closed ',negative
'operation fails invalid input ',negative
'handle sqlline command beeline which starts with and does not end with ',negative
'right now only one parent ',negative
'check here for each dir were copying out see already exists error out also treat dynwrites writes immutable tables dryrun true immutable true ',negative
'filter operator ',negative
'allow for empty string etc ',negative
'get metastorethrift privilege object using metastore api ',negative
'transitive ',negative
'the output partial aggregation struct containing long count and doubles sum and variance ',negative
'then drop the database ',negative
'the failure occurred before even made entry compactionqueue ',negative
'add alias aliastoopinfo and optoalias ',negative
'the old new output position mapping will the same that ',negative
'date integer internally ',negative
'queue notify generate the next batch rows ',negative
'task execution time out configured for submit operation then job may need killed execution time out these parameters controls the maximum number retries and retry wait time seconds for executing the time out task ',negative
'the next line works ',negative
'call open read data split mockmocktable call call call ',negative
'check appropriate codec available ',negative
'value simpleentry rowcount ',negative
'the random values must between and distributed uniformly the average value large set should about verify close this value ',negative
'will only interrupt checking the lowestlevel operator for multiple joins ',negative
'test decimal scalar divided column this tests the primary logic for template ',negative
'verify ',negative
'populate the operator ',negative
'metadata should get created ',negative
'the operation metastore fails dont anything change management but fail the metastore transaction having copy the jar change management not going ',negative
'function setup locks ',negative
'first argument hiveversion compatible argument dbversion greater than equal check the compatible case ',negative
'not this the identity the rule will nothing ',negative
'get synchronized wrapper the meta store remote ',negative
'try return stuff that was killed from under should noop ',negative
'compile another query ',negative
'operator with enough children ',negative
'the work that cannot done via async calls ',negative
'the column has been obtained from cache ',negative
'lets write more bytes the files test that estimator actually working returning the file size not from the filesystem ',negative
'last block affect all bits all the case statements fall through ',negative
'test gettables with table name pattern ',negative
'now add enough failed compactions ensure will attempt delete enough for this but also want enough tickle the code ',negative
'lets wait the async ops before continuing ',negative
'sequence file read ',negative
'column names ',negative
'only recompute stats after major compact they existed before ',negative
'need reload ',negative
'for acid table insert overwrite shouldnt replace the table content keep the old ',negative
'toss timestamp and date ',negative
'call droppartition each the tables partitions follow the procedure for cleanly dropping partitions ',negative
'impl notificationfetcher ',negative
'extract columns missing current keyvalue ',negative
'create tables one partitioned and other not also have both types full acid and tables ',negative
'first compare the length and then compare the directory name ',negative
'note that will called before this thus some type promotionconversion may occur short integer should refactor this that its hapenning one place per moduleproduct that are integrating with all pig conversion should done here etc ',negative
'since guaranteed produce most one row ',negative
'without vectorization ',negative
'signing not required for tez ',negative
'hive ast right child join cannot another join thus need introduce project top but only need the additional project the left child another join too not astconverter will swap the join inputs leaving the join operator the left also parent hivesemijoin since astconverter wont swap inputs then this will help triggering multijoin recognition methods that are embedded semanticanalyzer ',negative
'ensures that the list doesnt have dups and keeps track directories have created ',negative
'alter partition ',negative
'find how much compressed data was added for this column ',negative
'test the vectorized udf adaptor verify that custom legacy and generic udfs can run vectorized mode ',negative
'',negative
'row was processed ',negative
'nothing here ',negative
'first just allocate just the projection columns will using ',negative
'tabletype specified was null need figure out what type was ',negative
'hive has max limit for binary ',negative
'evaluate union object ',negative
'this turns splitupdate udi ',negative
'split each row duplicate which will cause update into rows and augment with col which has insert update ',negative
'get row resolvers column map for original left and right input ',negative
'construct using ',negative
'cas race look again ',negative
'nonjavadoc see ',negative
'nothing far and shouldnt called ',negative
'create bloom filter with same number bits but different hash functions ',negative
'one input path would mean only one map task ',negative
'copy ',negative
'mark any scratch small table scratch columns that would normally receive copy the key null too ',negative
'single mapreduce job launched ',negative
'deserializes bit decimals the maximum bit precision decimal digits note major assumption the input decimal has already been bounds checked and least has precision not bounds check here for better performance you can bounds check beforehand with mathabsdecimallong ',negative
'month granularity ',negative
'rowid ',negative
'',negative
'modifier letter triangular colon bytes ',negative
'global contains excludes individual modules can only contain additional excludes ',negative
'clean the database ',negative
'values colexpmap and rowschema ',negative
'become part the record otherwise will just write over later ',negative
'dont compact and opened ',negative
'for rsselrs case reducer operator reducer task cannot null task compiler ',negative
'once the feature stable ',negative
'synchronized lock ',negative
'some other things that could added here model cost cost computingsending partial bloomfilter results bloomfiltersize mappers for reduceside join add the cost the semijoin table scandependent tablescans ',negative
'register for notifications inside the lock should avoid races with happens different submission thread avoid register running for this task ',negative
'',negative
'want avoid expiring locks for txn expiring the txn itself ',negative
'add another value ',negative
'register the shard sub type used the mapper ',negative
'determine which stripes read based the split ',negative
'create operation log root directory operation logging enabled ',negative
'longer the wait time for attempt wrt its start time higher the priority gets ',negative
'first breaking the filter conditions into equality comparisons between rightjoinkeysfrom the original filterinputrel and correlatedjoinkeys correlatedjoinkeys can only rexfieldaccess while rightjoinkeys can ',negative
'save the original job tracker ',negative
'zero need shiftscale ',negative
'create test input file with specified number rows ',negative
'returning void because ignore this production ',negative
'currently using the format ',negative
'range check needed ',negative
'test nonvectorized acid combine ',negative
'determine bit width for bitpacking and encode header ',negative
'boolean value match for char field ',negative
'bump its internal version ',negative
'this removed using poll because there can case where there partitions iterator empty but because both the producer and consumer are started simultaneously the while loop will execute because producer not terminated but wont produce anything queue will empty and then should only wait for specific time before continuing the next loop cycle will fail ',negative
'clear out isnull array ',negative
'adds the missing schemeauthority for the new table location ',negative
'first child subquery second child alias set the node interest and the subquery ',negative
'return key from any the readers ',negative
'the incoming split may not file split when are regrouping tezgroupedsplits the case smb join this case can early exit not doing the calculation for bucketsizemap each bucket will assume can fill availableslots waves preset for smb join ',negative
'getposition different columns should never give the same value ',negative
'string group comparison ',negative
'fetch task query ',negative
'note that deletedelta should not read when minor compacted deletedelta present ',negative
'traverse data and masks array together check for set bits ',negative
'set sessionusername dfs querytab ',negative
'copy the files from different source file systems one destination directory ',negative
'otherwise didnt understand mark maybe ',negative
'exclusive locks occur before shared locks ',negative
'now insert from union here create data files sub dirs ',negative
'found map objectinspector grab the objectinspector for the value and initialize aptly ',negative
'need unique ids refer each minmax key value the ',negative
'tokfrom subtree ',negative
'preserve only partitioning ',negative
'drop database everything all meta tables should disappear ',negative
'run common join task ',negative
'replace the node place ',negative
'matches times one time the original node one time the new node created the rule ',negative
'nonmm case ',negative
'groupby reorders the keys emitted hence the keycols would change ',negative
'shutdown hook clean resources process end ',negative
'simple one long key map join benchmarks build with mvn clean install dskiptests pdistitests main hive directory from itestshivejmh directory run java jar targetbenchmarksjar inner innerbigonly leftsemi outer rowmodehashmap rowmodeoptimized vectorpassthrough nativevectorfast ',negative
'filter timestamp against timestamp interval day time against interval day time ',negative
'disable sargs for deleteeventreaders sargs have meaning ',negative
'for rule mapjoinmapjoin have child mapjoin the the current mapjoin local work will put the current mapjoin the rejected list ',negative
'hivedecimal suppresses trailing zeroes ',negative
'different kinds vectorized reading supported read the vectorized input file format which returns vectorizedrowbatch the row read using deserialize each row into the vectorizedrowbatch and read using the regular partition deserializer get the row object and assigning the row object into the vectorizedrowbatch with vectorassignrow ',negative
'set servers idle timeout very low value ',negative
'used check recursive cte invocations similar viewsexpanded ',negative
'serialize bytes ',negative
'unquoted space ',negative
'that know the type table are creating acidmm match what was exported ',negative
'dont add partition data already exists ',negative
'fix the input column numbers and output column numbers ',negative
'setobject the yet unknown type javautildate ',negative
'truncate the excess chars fit the character length also make sure take supplementary chars into account ',negative
'row limit does not match currently not merge ',negative
'pass ',negative
'build plan ',negative
'the total size local tables after merge localworks larger than the limit set ',negative
'there should original bucket files the location and ',negative
'suppress empty column map ',negative
'the data shuffle ',negative
'all other cases throw exception its whitelist allowed operations ',negative
'create the hadoop archive ',negative
'contains aggregate and not full acid table not rewrite need merge support ',negative
'window spec with parition spec partitioned constantnumber ',negative
'minor optimization avoiding creating new objects ',negative
'alias not fully qualified ',negative
'passing null matches everything ',negative
'check whether this input operator produces output has residual not skip this output will add select top the join ',negative
'now propagate the constant from the parent the child ',negative
'target paths last component also the column family name ',negative
'various restrictions ',negative
'handle stop this process from the outside needed ',negative
'have estimator for this type assume low overhead and hope for the best ',negative
'use boundarytype boundary amt sort key order behavior case preceding unb any any start preceding unsigned int null asc start desc scan backwards row such that rsk not null start ridx preceding unsigned int not null desc scan backwards until row such that rsk rsk amt start ridx preceding unsigned int not null asc scan backward until row such that rsk rsk bndamt start ridx current row null any scan backwards until row such that rsk not null start ridx current row not null any scan backwards until row such rsk rsk start ridx following unb any any error following unsigned int null desc start partitionsize asc scan forward until such that rsk not null start ridx following unsigned int not null desc scan forward until row such that rsk rsk amt start ridx asc scan forward until row such that rsk rsk amt ',negative
'task requested host got host ',negative
'authorize the operation ',negative
'that operator writes into the bucketsort columns for that data ',negative
'projection that casts proj expr nullable type ',negative
'ascending ',negative
'partition keys ',negative
'add new node the cache ',negative
'relying the rpc threads keep the service alive ',negative
'nonjavadoc see ',negative
'execute malformed query ',negative
'partstats ',negative
'todo even listener for check new true this ',negative
'',negative
'assume splits will never start the middle the stripe ',negative
'reset the buffer were going use ',negative
'test that existing exclusive table with new sharedwrite coalesces ',negative
'minimize allocations ',negative
'change column ',negative
'increments one hms connection hiveget ',negative
'call open read data split mockmocktable ',negative
'long scalarcolumn ',negative
'filesystemcache ',negative
'timestamps are not supported both dates were changed ',negative
'',negative
'insert overwrite table ',negative
'get sitexml loaded ',negative
'containerendtaskend invocation ',negative
'link the rpc and the promise that events from one are propagated the other needed ',negative
'note this will determine the order columns the result for now the columns for each table will together the order the tables well the columns within each table deterministic but undefined stores them the order addition ',negative
'reorder tags need ',negative
'keep draining the queue the same session ',negative
'room for optimization since cannot convert empty project operator ',negative
'the first thread detect the error cleanup old connection reconnect ',negative
'only bonecp should return true ',negative
'dont add the partition table created during the execution the input source ',negative
'cancel the watchkey since the output dir has been found ',negative
'collect keyvalues for this row ',negative
'update largest relation ',negative
'',negative
'true only one date null ',negative
'setup values registry ',negative
'ignore and break ',negative
'mapper can span partitions combine into few one split subject the pathfilters set using combinecreatepool ',negative
'hint disable mapjoin ',negative
'just compare the magnitudes signums set ',negative
'dont propagate the error termination was done part closing the client ',negative
'default runs slightly over day long ',negative
'include the original blank value longminvalue the negatives make sure get ',negative
'previous record the write buffers see writebuffers javadoc ',negative
'pool exhausted return new object ',negative
'pattern for keyvaluekeyvalue ',negative
'check the output fixacidkeyindex should indicate the index was invalid ',negative
'now its time rewrite the aggregate ',negative
'left repeats and null ',negative
'use that one mapper processes exactly one file ',negative
'iterate through the ophandles and close their operations ',negative
'create segment file the destination location with linearshardspec ',negative
'return metadataonly ',negative
'unpartitioned table ',negative
'subquery project ',negative
'required required required ',negative
'fetch the first group for all small table aliases ',negative
'test that partition key not allowed data ',negative
'grab the tag and the field ',negative
'the hadoop cluster secure kerberos login for the service from the keytab ',negative
'matched instance not leader ',negative
'try reconnect child job one found ',negative
'groupby query results ',negative
'lost statistics optraits through cloning try get them back ',negative
'this async method always launch threads even for single task ',negative
'for now just decimal inputs and decimal boolean output ',negative
'executorservice for sending heartbeat metastore periodically ',negative
'given list partstats this function will give you aggr stats ',negative
'this means the column was not included the projection from the underlying read ',negative
'generate the token for query user applies all splits ',negative
'for each source write get the appropriate lock type its overwrite need get exclusive lock its insert overwrite than need shared its update delete then ',negative
'there are filter operators the pipeline ',negative
'check filter input contains correlation ',negative
'analyzecreateview uses thisast but dophase doesnt only reset here ',negative
'make sure session init gets stuck init ',negative
'double wait time until min ',negative
'want use the because otherwise will return for two objects that have different object inspectors and the roir will help convert both values common type that they can compared reasonably ',negative
'update the nextlevel with newly discovered subdirectories from the above ',negative
'create the list needed ',negative
'dont expect cache requests from the middle ',negative
'theres some special handling for dummyops required mapjoins wont properly initialized their dummy parents arent initialized since cloned the plan ',negative
'zero ',negative
'message needed ',negative
'',negative
'enable blobstore optimizations for the rest tests ',negative
'null hiveconf passed jdbc driver side code since driver side supposed independent conf object create new hiveconf object here this case ',negative
'the version doesnt exist then create ',negative
'the counters are missing there point trying print progress ',negative
'need clean data directory ensure that there interference from old runs cleaning happening here allow debugging case tests fail dont have clean logs since append mode ',negative
'create walker which walks the tree dfs manner while maintaining the operator stack the dispatcher generates the plan from the operator tree ',negative
'replicate insert event and verify ',negative
'decimal conversion instead ',negative
'both are nonempty only copy now ',negative
'the jdoexception may wrapped further metaexception ',negative
'trigger rewriting remove union branch with ',negative
'all txns below minuncommittedtxnid are either committed emptyaborted are allowed ',negative
'should now have new lock acidtblpart ',negative
'nonacid ',negative
'check whether the shuffle version compatible ',negative
'construct using ',negative
'',negative
'already setup the create method ',negative
'the skewedvalues contains and the user looking for positions the result would get the skewed key values that are part the join key param skewedvalueslist list all the skewed values param positionskewedkeys the requested positions return sublist skewed values with the positions present ',negative
'queries without source table currently are not supported cbo ',negative
'start third batch but dont close this delta will ignored compaction since ',negative
'the cookie based authentication already enabled parse the ',negative
'nope look see can find conf file finding our jar going one directory and looking for conf directory ',negative
'put the mapping from table scan operator partpruner map ',negative
'diff against table target ',negative
'location not shown test mode ',negative
'only database object updated ',negative
'csvreader will throw exception any separator quote escape the same but the csv format specifies that the escape character and quote char are the same very weird ',negative
'dont have many file formats that implement inputformatchecker wont holding ',negative
'move data from temp directory the actual table directory ',negative
'isallparts ',negative
'required required required required required required required required optional ',negative
'remote metastore mode ',negative
'',negative
'this class should replaced with class once fixed this code should removed copy ',negative
'column statistics from different sources are put together and ',negative
'event ',negative
'should fail because the transactionbatch timed out ',negative
'whether the native vectorized map join operator has performed its common setup ',negative
'update stmt has pblah thus nothing actually update and generate empty dyn part list ',negative
'prewarm cachedstore ',negative
'checkh for and not the subquery must implicitly explicitly only contain one select item ',negative
'add uncovered acid delta splits ',negative
'make sure assign correct ids ',negative
'these namestypes are the data columns plus partition columns ',negative
'sleep before send checklock again but with back off ',negative
'try valid alter table partition key comment ',negative
'add small table result columns ',negative
'validate the update new column even old rows ',negative
'datastr not null and datastr not pattern ',negative
'expand the nested script the metadbtype set this setting the information schema hive that specifically means that the sql commands need adjusted for the underlying rdbms correct quotation strings etc ',negative
'populate source ',negative
'all the joins fit into half the memory lets safe and scale them out ',negative
'parse resultexpr str and setup ',negative
'creat default dir ',negative
'this class the payload for custom vertex serializes and deserializes numbuckets the number buckets the big table vertextype this the type vertex and differentiates between bucket map join and smb joins numinputs the number inputs that are directly connected the vertex mrinputmultimrinput case bucket map join always inputname this the name the input used case smb joins empty case bucketmapjoin ',negative
'repl imports are replaceimports and thus are idempotent note that this assumes that this importcommand running export dump created using export for replication the scope importcommand were eventually expand importing dumps created regular exports then this needs updating ',negative
'test rpcserveraddress not configured but server host configured ',negative
'make the list transactional tables list which are getting read written current txn ',negative
'note given that return pool sessions the pool the finally block below and that ',negative
'end input confirm got end stream indicator from server well done status from fragment execution ',negative
'are testing for both type modes always not passing that parameter for now ',negative
'serialize the result struct ',negative
'retain this digit ',negative
'tracks tasks which are running useful for selecting task preempt based when started ',negative
'maxcapacity should calculated based percentage memorythreshold which divide row size using long size ',negative
'delete clause ',negative
'unique key the leftinputrel ',negative
'validation substitute class name for thisclass only public fields and methods are versioned methods compare nonstatic return type name parameter types exceptions thrown fields compare nonstatic type name value when static ',negative
'decimal means decimal ',negative
'note the normalize call with rounding hivedecimal will currently reduce the precision and scale the value throwing away trailing zeroes this may may not desirable for the literals however this used the default behavior for explicit decimal literals keep this behavior for now ',negative
'this field not null ',negative
'regular singlepartition write into partitioned table ',negative
'safety check ',negative
'save the conf variable values that they can restored later ',negative
'initialization here adapted from method ',negative
'validate there added null for column ',negative
'get output committer ',negative
'call open mockmocktbl ',negative
'data read ',negative
'third row ',negative
'fatal errors happen should kill the job immediately rather than ',negative
'test basic right trim vector ',negative
'add unique element list per occurrence order skewed value occurrence order skewed value doesnt matter ',negative
'store char vector row batch with padding stripped ',negative
'rpc already handles retries will just try kill the session here this will cause the current query fail could instead keep retrying ',negative
'serialize numdistinctvalue estimator ',negative
'run hive metastore server ',negative
'mock operationmanager for session ',negative
'case data sorted time and extra hashing dimension see thus use segment partition addition time dimension data with the same and time interval will end the same segment ',negative
'check that writeid has been excluded check that the data sorted order ',negative
'should never happen since are reading bucketx written acid write ',negative
'not any blocking ops this thread ',negative
'however can constant too that case need track the column that originated from the input operator can propagate the aliases ',negative
'this batch only used vectorrow deserializer readers ',negative
'special handling for sql delete from table where ',negative
'show database level privileges ',negative
'add hivesitexml add this first that gets overridden the new metastore ',negative
'hiveserver specific configs ',negative
'create new outgoing vectorization context because column name map will change ',negative
'chars try keep cols aligned ',negative
'set current user session conf ',negative
'file pattern that set propertiesfile ',negative
'prspgbycrs ',negative
'ensure pig can write correctly smallinttinyint columns this means values within the ',negative
'alter all partitions ',negative
'create the row object ',negative
'final result ',negative
'deserialize the result ',negative
'temp tables that not through semanticanalyzer may not have location set here for example export acid tables generates query plan that creates temp table ',negative
'find which columns need update for this partition any ',negative
'and preserve rows only from left side ',negative
'create root scratchdir with write all that user impersonation has issues ',negative
'unionwork null means the first time need create union work object and add this work subsequent work should reference the union and not the actual work ',negative
'aggregation result null ',negative
'from txncomponents ',negative
'process the first node extract tablename ',negative
'now have table with data files multiple different levels ',negative
'checkexpression ',negative
'need add for the default supported local jar driver ',negative
'set the thread local address ',negative
'reimplemented use primitivecategory rather than typeinfo because typeinfos from the same qualified type varchar decimal should still seen equivalent ',negative
'init output ',negative
'func may null when gby closing see mvn test dqfileexplainuserq original behavior create fmsketch ',negative
'need make sure that all the field associated with the union are settable ',negative
'cant use the current table the big table but its too big for the map side ',negative
'cycle consists atleast one dynamic partition pruningdpp optimization and atleast one minmax optimization dpp better optimization unless ends scanning the bigger table for keys instead the smaller table ',negative
'nonjavadoc see javalangobject int ',negative
'nonjavadoc see ',negative
'partitioning columns the child are assigned assign these the partitioning columns the parent ',negative
'loginfofound list record assumes are here after key compare ',negative
'new true this ',negative
'the product the toppermutation and bottompermutation yields the identity then can swap the join and remove the project ',negative
'synchronized locking itself ',negative
'when make new connection should get from minihs this time ',negative
'tablename and pattern ',negative
'after set originaldata null incref the buffer and the cleanup would decref note that this assumes the failure during incref means incref didnt occur ',negative
'data for hll bias correction ',negative
'directory ',negative
'basen cannot contain updatedelete original files are all insert and need compact only there are updatedelete events ',negative
'create aggregate top with the new aggregate list ',negative
'should generate infinf ',negative
'move onto the next null byte ',negative
'marked being read defaults true that the most common case ',negative
'extend any repeating values and nonulls indicator the inputs ',negative
'return genconvertcoldest tab tabledesc input arraysaslist convert the case update and delete the bucketing column always the first column and isnt the table info rather than asking the table for well construct ourself and send back this based the work done genconvertcol below ',negative
'these are global since orc reuses objects between stripes ',negative
'steps create the archive temporary folder move the archive dir intermediate dir that the same dir the original partition dir call the new dir intermediatearchive rename the original partition dir intermediate dir call the renamed dir rename intermediatearchive the original partition dir change the metadata delete the original partition files ',negative
'nonjavadoc see ',negative
'remove ',negative
'parse the configuration parameters ',negative
'number reducers set default ',negative
'each bucket ',negative
'not match ignore the line return row with all nulls ',negative
'effect the input null because outofrange precisionscale ',negative
'has been committed all others open ',negative
'whether contains sort merge join operator ',negative
'always choose the function with least implicit conversions ',negative
'join operators which may converted commonjoinresolver ',negative
'confoverlay ',negative
'order facilitate partition pruning the where clauses together and put them the ',negative
'rewrite the ast replace tabref with maskingfiltering ',negative
'',negative
'insert mapside ',negative
'end the pools array ',negative
'send failover request again minihs and get failure ',negative
'got the expr for one full partition spec determine the prefix length ',negative
'save old values ',negative
'see ',negative
'need staging directories long single partition needed addition ',negative
'paths bucket files small table for current bucket file big table initializes fetchoperator for each file paths reuses fetchoperator possible currently number paths always the same bucket numbers are all the same over all partitions table ',negative
'initialize export path ',negative
'validate that the multijoin valid star join before returning ',negative
'unequal strings ',negative
'verifyrunselect from repldbname matview ptndata drivermirror ',negative
'caches objects before constructing forward cache ',negative
'add the rest the memory consumption ',negative
'verify the fetched log incrementally ',negative
'what are trying get the equivalent new dateymdgettime the local where ymd whatever represents how works this ',negative
'can create calcite isdistinctfrom operator for this but since our join reordering algo cant handle this anyway there advantage thisso bail out for now ',negative
'test when two jars with shared dependencies are added the classloader contains union the dependencies ',negative
'intersperse getat and next calls ',negative
'make sure the referenced schema exists ',negative
'will only throw jsonexception when statsputbasicstats true has duplicate key which not possible ',negative
'only mechanical data retrieval should remain here ',negative
'only column family ',negative
'',negative
'this unsupported operator ',negative
'create walker which walks the tree bfs manner while maintaining the ',negative
'the target column list has the format targetwork ',negative
'note use col the key provided again col ',negative
'class read and reread the values ',negative
'nonjavadoc see utillist ',negative
'need make sure that the underlying fields are settable well hence the recursive call for each field note that equalscheck false while invoking getconvertedoi because need bypass the initial check ',negative
'reuse super renewal logic ',negative
'the applicationlevel name component name component description name for each metric record ',negative
'serialized sizes after serialization and deserialization should equal ',negative
'variables hold state from before flattening can easily restored ',negative
'required required required optional optional optional optional optional ',negative
'first check the local cache ',negative
'nothing ',negative
'wed have thrown exception ',negative
'need sessionhooktest hivesite ',negative
'duplicate the value merging should remove duplicates ',negative
'sleep for expiry time and then fetch again ',negative
'upgrade schema from latest ',negative
'apply comparison rules ',negative
'for close the local work ',negative
'output also big the output size medium ',negative
'for left outer joins left alias sorted but right alias might not ',negative
'errors ',negative
'grow ',negative
'try roll the key none found ',negative
'dont construct illegal cache key ',negative
'try fold key and key not null key ',negative
'initializes them ',negative
'finishes the vectorization context after all the initial ',negative
'all the locks are created under this parent ',negative
'such abc ',negative
'preserve precision ',negative
'create single child representing the scratch column where will ',negative
'test strict locking mode backward compatible locking mode for nonacid resources with nonstrict mode insert got sharedread lock instead exclusive with acid semantics ',negative
'since are running the mapred task the same jvm should update the job conf ',negative
'the big table can divided buckets small tables ',negative
'update max max greater than the largest value seen far ',negative
'disable ansi sql arithmetic changes ',negative
'setting these parameters here just case that the code got changed future these are not missing ',negative
'boolean purposely excluded ',negative
'now run another compaction make sure empty dirs dont cause issues ',negative
'check potential trigger nullscan ',negative
'compress key and write key out ',negative
'gathering stats ',negative
'set readallcolumns false ',negative
'update file sink descriptor ',negative
'usually called after close commit rollback query and end the driver life cycle ',negative
'display error message for tasks with the highest failure count ',negative
'not set bit the null byte when are writing null ',negative
'reasons roots are data sources leaves are data sinks know ',negative
'for each path getsplits ',negative
'fop exists this not the top level filter and fop not ',negative
'create the required temporary file the hdfs location the destination ',negative
'set stats config for filesinkoperators which are cloned from the filesink ',negative
'all its parents operators are state close and called close children note close being called and its state being close difference since close could called but state not close one its parent not state close ',negative
'start delete from tab txn ',negative
'this invalid decimal value getting hivedecimal from will return null ',negative
'rows looked one repeated key are match but filtered out rows need generated nonmatches too ',negative
'connect via kerberos and get delegation token ',negative
'lest for now load data woverwrite not allowed txn hive ',negative
'adding postgres jdbc driver exists ',negative
'note that partitioning fields dont need change since either partitioned randomly all grouping keys distinct keys ',negative
'remote metastore situation ',negative
'authorization error not really expected filter call the impl should have just filtered out everything checkprivileges call would have already been made authorize this action ',negative
'the split doesnt exclusively serve one alias ',negative
'outer join involved ',negative
'only left input repeating and has nulls ',negative
'search for match the rhs table ',negative
'files size for splits ',negative
'this should block behind the lock ',negative
'sum lengths all values seen far ',negative
'the join operation the child not the same keys ',negative
'per length means null list ',negative
'add allnull record ',negative
'handle leadingtrailing whitespace ',negative
'format the stored statement ',negative
'todo more writers are added separate out ',negative
'condition for merging not met see genmrfilesink ',negative
'validate response ',negative
'sessionstate null this unlikely happen just case ',negative
'keys from fetchsampler are collected here ',negative
'note cache slices one one since need lock them before sending consumer could lock here then cache them together then unlock here and return ',negative
'the state only changes from truefalse once set false may not change back true ',negative
'uncaught exception handler that will set for all threads before execution ',negative
'expect base delta dir this list ',negative
'default children inputs ',negative
'exchange partition not allowed with transactional tables only source transactional table then target will see deleted rows too snapshot isolation applicable for nonacid tables only target transactional table then data would visible all ongoing transactions affecting the snapshot isolation both source and targets are transactional tables then target partition may have deltabase ',negative
'the dispatcher fires the processor corresponding the closest matching rule and passes the context along ',negative
'optional group containing multiple elements ',negative
'are done with the buffers unlike data blocks are also the consumer release ',negative
'small table ',negative
'two parts kerberos principal ',negative
'cte actually subquery ',negative
'execute select and verify that aborted operation not counted for table ',negative
'this should eventually hang the delay code from the background thread ',negative
'reuse the same hashmap reduce new object allocation this means counts can empty when there input data ',negative
'while ',negative
'quite reliable ',negative
'metaexception here really means classnotfound see the utility method any these happen that means can never succeed ',negative
'',negative
'initialize fetch work such that operator tree will constructed ',negative
'get the values repetition and definitionlevel ',negative
'instead maintaining complex state for the fetch the next group know for sure that the end all the values for given key will definitely reach the next key group ',negative
'write base ',negative
'spread bits adjacent longs default spreading hash bits within blocksize longs will make bloom filter cache friendly ',negative
'set bootstrap dump location used but for tablepartition missing ',negative
'collect column stats which need rewritten and remove old stats ',negative
'setup local dirs ',negative
'end entry reached ',negative
'make sure know saw error that dont recognize ',negative
'multikey specific save key and lookup ',negative
'for webui kept alive after queryplan freed ',negative
'note that enablebitvector does not apply here because columnstatisticsobj itself will tell whether bitvector null not and aggr logic can automatically apply ',negative
'pig script was successful ',negative
'repeat the procedure for the new select ',negative
'writeid ',negative
'reducer ',negative
'try some time zone boundaries ',negative
'removes any union operator and clones the plan ',negative
'theres key return ',negative
'init file contains incorrect row ',negative
'reset and add counters this can happen during start query session being moved another pool with its own set triggers ',negative
'folded the regex usually into ',negative
'build new table ',negative
'make sure that the user doesnt happen the super group ',negative
'the cache buffer comprises the tail the requested range and possibly overshoots the same above applies may throw cache buffer larger than the requested range and theres another range after this that starts the middle this cache buffer currently cache exact offsets the latter should never happen ',negative
'set data empty explicitly ',negative
'are trying check acls the workers directory which noone except should able write higherlevel directories shouldnt matter dont read them ',negative
'show create table more sensitive information includes table properties etc ',negative
'nontransient field used runtime kill task exceeded memory limits when running llap ',negative
'patterns isrepeating columns for boolean tristate null for others null somevalue nonulls sometimes false and there are nulls random selectedinuse too ',negative
'use the table default storage specification ',negative
'bgenjjtree typei ',negative
'make sure the arguments make sense ',negative
'valid schemes ',negative
'this backward compatible for nonacid resources acid semantics ',negative
'have ensured that the keys are columns ',negative
'use construct ',negative
'delete delta file with delete events ',negative
'this one the columns were setting record its position can come back later and patch add one the index because the select has the rowid the first column ',negative
'add stuff here implemented ',negative
'special case for root parent ',negative
'make sure that each session has its own udfclassloader for details see link udfclassloader ',negative
'currently this method only sets database functionname ownername ownertype classname ',negative
'first try selecting methods based the type affinity the arguments passed the candidate method arguments ',negative
'validate that join condition legal function refering both sides join only equi join todo join filter handling only supported for runtime supported for well ',negative
'now save stats for partition wont modify ',negative
'check that the tables used not resolve temp tables ',negative
'the collect method override for ',negative
'create the merge file work ',negative
'put all virtual columns rowresolver ',negative
'pkcolumnname ',negative
'fix needed due dependency for hbasemapreduce module ',negative
'return the mapping for table descriptor the expected table ',negative
'this mapping which keys will copied from the big table input and key expressions ',negative
'obtain filter for shared operator ',negative
'verify that driver works fine with latest schema ',negative
'shouldnt happen ',negative
'exception thrown looks good ',negative
'expected error should throw ',negative
'code borrowed from ',negative
'loginfopartition specgetkey ',negative
'write directly into our bytescolumnvector value buffer ',negative
'case find rows which have been deleted ',negative
'add sign byte since high bit ',negative
'when using only hbase then could change this ',negative
'this safe because positive ',negative
'key stored text format get bytes representation constant also text format ',negative
'avoid ',negative
'once are done processing the line restore the old handler ',negative
'acquired all the locks commit and return acquired ',negative
'make sure getting table the wrong catalog does not work ',negative
'dont expect conflicts from bad estimates ',negative
'need acquire lock twice the same object ensured that exclusive locks occur before shared locks the same object ',negative
'try with row file ',negative
'batch size and decaying factor ',negative
'generate the dummy driver using txt file ',negative
'there should still one request the locks still held ',negative
'for all the existing partitions check the value can type casted nonnull object ',negative
'before notify though lock the list lock cannot remove from the list ',negative
'for type casts ',negative
'need not traversed again ',negative
'this will insert and between the and its parent ',negative
'were faking out hive work through type system impedence mismatch pull out the backing array and convert list ',negative
'get the reflection methods from ',negative
'tablepropkey that was passed lead valid uri resolution update parts match the oldnnloc else add badrecords ',negative
'one session will running the other will queued ',negative
'check out the statistics ',negative
'get the big table row container ',negative
'dont acquire locks for any these have already asked for them ddlsemanticanalyzer ',negative
'verify that attempt was made schedule the task but the decision was skip scheduling ',negative
'mapreduce case need always clear mapreduce doesnt have object registry ',negative
'vertexs children vertex ',negative
'set the buffer that will receive the serialized data the output buffer will reset ',negative
'retry any other exception ',negative
'catalogs cannot parsed part the query seems bug ',negative
'dagclient such should have bearing jobclose ',negative
'get colstats for the original table column for selcol possible this would have ',negative
'clear all threadlocal cached mapworkreducework after plan generation this may executed pool thread ',negative
'want signal error the function doesnt exist and were configured not ignore this ',negative
'add the columns join filters ',negative
'used for create ',negative
'analyze table partition compute statistics the plan consists simple sparktask followed statstask the spark task just simple tablescanoperator ',negative
'compute the pseudorandom position from the above then derive the actual header ',negative
'partitions locations which might need deleted ',negative
'the stack contains either filter filter filter with the head the stack being the rightmost symbol just pop out the two elements from the top and the second one them not table scan then the operator the top ',negative
'the default such that there throttling ',negative
'report suspicious gaps writebuffers ',negative
'otherwise replace parent sibling ',negative
'the objectinspector for the current column ',negative
'check other parts ',negative
'get total size and individual aliass size ',negative
'run partition pruner get partitions ',negative
'get all cols ',negative
'fstrashintervalkey hadoop ',negative
'cant multiply null ',negative
'make the list transactional tables list which are getting written current txn ',negative
'check the lastrecordoutput ',negative
'open default connections which will used throughout the tests ',negative
'tblgetpath null for views ',negative
'table specified check all tables and all partitions ',negative
'match not supposed there ',negative
'this constant null ',negative
'the old instance has not been unregistered and the new instances has not registered yet ',negative
'the table the pending prewarm list move the top ',negative
'crossproduct keys really ',negative
'update the filesinkoperator include partition columns ',negative
'values will override any values set the underlying hadoop configuration ',negative
'possible for some request queued after main thread has decided kill this session the next iteration wed processing that request with irrelevant session ',negative
'there point trying validate further have type info about target field ',negative
'setup for input resultexpr select list ',negative
'not external table ',negative
'how run this test you can run this test via the command line mvn clean install java jar targetbenchmarksjar prof perf linux java jar targetbenchmarksjar prof perfnorm linux java jar targetbenchmarksjar prof perfasm linux java jar targetbenchmarksjar prof allocation counting via ',negative
'null ',negative
'infomessages ',negative
'with repeating value can finish all remaining rows ',negative
'views derive the column type from the base table definition the view definition can altered change the column types the column type compatibility checks should ',negative
'when people forget quote string opop null for example select from sometable where not ',negative
'close should just nothing ',negative
'get the available privileges from file system ',negative
'executor single thread can guarantee domain created before any ats entries ',negative
'called functions that transform the raw input this method invoked during translation and also when the operator initialized during runtime subclass must use this call setup the shape the raw input that fed the partitioning mechanics subsequent this call call getrawinputoi call the link must return the the output this function ',negative
'see this node toktableorcol find the value and put the list not recurse any children ',negative
'down the semaphore block until available ',negative
'update the count the number values seen far ',negative
'read type encoding ',negative
'these two structures track the list known nodes and the list nodes which are sending keepalive heartbeats ',negative
'cred provider doesnt have entry fall back conf ',negative
'add type params ',negative
'overwrite value ',negative
'aggregatedata already has the ndv the max all ',negative
'threadsafe ',negative
'seed with the buddy this block the first iteration would target this block ',negative
'null check because some test cases get null from msgetcatalog ',negative
'matching rule and passes the context along ',negative
'fraction digits continue into middle longword ',negative
'slow down the reducer that shufflebytes publishing and validation can happen adding sleep between multiple reduce stages ',negative
'thomas wangs integer hash function ',negative
'create resolver ',negative
'build aggregations ',negative
'its possible that parition column may have null value which case the row belongs the special partition ',negative
'order sourcecolumn ',negative
'the threshold size convert the join into mapjoin and dont create conditional task ',negative
'add views planner ',negative
'the the actual spark job ',negative
'class could not inited use our local copy ',negative
'test string literal string column comparison ',negative
'query info created sqloperation which will have start time the operation when jdbc statement not used queryinfo will null which case take creation driver instance query start time which also the time when query display object created ',negative
'reset the interrupt status ',negative
'inputsplitnum that contains the first row this block ',negative
'run with cascade ',negative
'restricted text for now this new feature only text files can sliced ',negative
'provide faster way write date without date object ',negative
'whether any error occurred during query compilation used for query lifetime hook ',negative
'preemption with ducks reversed ',negative
'this would attempt directory add watch and track ',negative
'add our conf file ',negative
'optimize for common case just one row for key container acts row ',negative
'when minor compacting write delete events separate file when splitupdate turned ',negative
'convert rexnode ',negative
'these arent real column refs instead they are special internal expressions used the representation aggregation ',negative
'should not happen ',negative
'help ',negative
'this has already been inspected and rejected ',negative
'binary type should not seen ',negative
'not created cannot remove ',negative
'',negative
'cant overwrite existing keys ',negative
'deleted the user will need call unarchive again clear those ',negative
'this expected these mock files are not valid orc file ',negative
'restore interrupt wont handle here ',negative
'mapside join exactly one table not present memory the client provides the list tables which can cached memory ',negative
'collect all branching operators ',negative
'create fake directory throw exception ',negative
'known ',negative
'map col keycol etc ',negative
'whether the cycle running ',negative
'event operators point table scan operators when cloning these need restore the original scan ',negative
'found least one children with mismatch ',negative
'unable find stats for column return null can build stats ',negative
'decimalstats ',negative
'data structures coming originally from qbjointree ',negative
'cancel the heartbeat ',negative
'append ',negative
'',negative
'the state has changed during the update lets undo what just did ',negative
'this should never happen only schedule one attempt once ',negative
'need get state transition updates for the vertices that will send events once have received all events and vertex has succeeded can move the pruning ',negative
'',negative
'allow implicit numeric string conversion ',negative
'the registrator jar should already when not test mode ',negative
'writeid ',negative
'create one input split for each segment ',negative
'special char ',negative
'',negative
'second data dir contains files ',negative
'weve killed something and may want wait for die ',negative
'size surpasses limit cannot convert ',negative
'default ',negative
'copy the first entries ',negative
'test month diff with fraction considering time components ',negative
'replace existing view ',negative
'tez only the hash map might already cached the container run ',negative
'undone presumption append ',negative
'element for key long hash table hashmultiset ',negative
'data ',negative
'the destination table ',negative
'get close enough ',negative
'look all methods that generate values for explain ',negative
'check for fatal error again case occurred after the last check before the job completed ',negative
'set correct scheme and authority ',negative
'reverse place ',negative
'required required required required required required required optional optional ',negative
'generate groupby operator ',negative
'abstract function add httpauth header ',negative
'tokenowner ',negative
'mybool ',negative
'should set false when using ',negative
'build the path from bottom ',negative
'start explicit txn that txnmgr knows ',negative
'nonjavadoc see javalangobject ',negative
'create jointree structures fill them later ',negative
'called once the client ',negative
'decompress the data ',negative
'see expr already present reducekeys get index expr reducekeys ',negative
'are here then have established that firstrecordinbatch deleterecord now continue marking records which have been deleted until reach the end the batch exhaust all the delete records ',negative
'more queries can added here the future test acid joins ',negative
'table names with schema name necessary ',negative
'when dynamic partitioning used the recordwriter instance initialized here isnt used can use null thats because records cant written until the values the dynamic partitions are deduced that time new local instance recordwriter with the correct outputpath will constructed ',negative
'generate dummy preupgrade script with errors ',negative
'user sets default queue now ',negative
'same sign just add the absolute values ',negative
'recreate refresh jobconf currtask context ',negative
'class ',negative
'which the correlator ',negative
'materialization allowed not view definition ',negative
'statuscode ',negative
'find the first nonzero digit the last digits all are zero ',negative
'get text input format here can not determine file text according its content can test other file format can accept one other file format can accept this file treat this file text file although maybe not ',negative
'invalid table not partitioned but endpoints partitionvals not empty ',negative
'bloom filter rest ',negative
'need separate table for acid testing since has bucketed and has acid ',negative
'move next valid index ',negative
'export table tablename partition partcolumnvalue exporttargetpath ',negative
'first look the column from the source against which resolved wed later translated this into the column from proper input its valid todo excludecols may possible remove using the same ',negative
'for druid storage handler ',negative
'count characters forward and watch for final run pads ',negative
'call open mockmocktbl ',negative
'',negative
'original method used deepcopy the same here ',negative
'constant null just return ',negative
'buildvdirectly use druid default need configured user ',negative
'ptf handling ptfinvocationspec ptfdesc ',negative
'test for string type ',negative
'hive conf changed need get the hive again with ',negative
'hive has been filed for this ',negative
'default partition key ',negative
'required required required required required required required required ',negative
'all input columns are repeating just evaluate function for row the batch and set output repeating ',negative
'unsupported aggregation ',negative
'udf ',negative
'each side better have more either side unbalanced cannot convert this workaround for now right fix would refactor code the maprecordprocessor and with respect the sources ',negative
'make sure matching name but wrong type doesnt return ',negative
'the deletedelta should not read because greater than the high watermark ',negative
'task not allocated ',negative
'add hiveexec jar ',negative
'smallbuffer might still out space ',negative
'construct using ',negative
'show locks ',negative
'',negative
'deserialize and check ',negative
'track the dependencies for the view consider query like select from where view the form select from ',negative
'hostnameport ',negative
'this class used read one field time simple fields like long double int are read into primitive current members the nonsimple field types like date timestamp etc are read into current object that this method will allocate this method handles complex type fields recursively calling this method ',negative
'generate the map the inputoutput column name for the keys are about ',negative
'how many records already buffered ',negative
'output privileges and asks for selectnogrant input ',negative
'what were reading from disk originally ',negative
'specify the columns deserialize into array ',negative
'try allocate using basebuffer approach from each arena ',negative
'when file system cache disabled you get different filesystem objects for same file system cant used such cases filesystem api doesnt have equals function implemented using the uri for comparison filesystem already uses uriconfiguration for equality its cache once equality has been added hdfs should make use ',negative
'verify that can drain the pool then cycle the state not corrupted ',negative
'bring the server only after all other components have started ',negative
'check this mapjoin not split ',negative
'add the new operator child each the passed operators ',negative
'logical loop over the rows the batch since the batch may have selected use ',negative
'for tez route data from upstream vertex correctly the following vertex the output name the reduce sink needs setup appropriately the case reduce side merge work need ensure that the parent work that provides data this merge work setup point the right vertex name the main work name this case the big table work has already been created can hook the merge work items for the small table correctly ',negative
'http transport mode set the thread local address thrifthttpservlet ',negative
'the sixth will not combined because delete delta files that desired hive ',negative
'potentially wait the cache entry entry pending status blocking here can potentially dangerous for example the global compile lock used this will block all subsequent queries that try acquire the compile lock should not done unless parallel compilation enabled might not want block for explain queries well ',negative
'theres fixed number partition cols that might have filters avoid joining multiple times for one column there are several filters will keep numcols elements the list one for each column will fill with nulls put each join corresponding index when necessary and remove nulls the end ',negative
'need handle tables unsupported path ',negative
'any the partition requests are null then need pull all partition locks for this table ',negative
'purge ',negative
'undone for now ',negative
'disable trash hadoop fstrashintervalkey hadoop ',negative
'table scan has the table object and pruned partitions that has information such bucketing sorting etc that used later for optimization ',negative
'dont overwrite user choice transactional attribute explicitly set ',negative
'output entry should not null for null input for this particular generic udf ',negative
'then determine the local offset that magical time ',negative
'write the results the file ',negative
'this api changed from this wont even compile with but doesnt need since only run this preupgrade ',negative
'one single call get all column stats ',negative
'project only the correlated fields out each inputrel and join the projectrel together make sure the plan does not change terms join order join these rels based their occurrence cor var list which ',negative
'null firstlast ',negative
'normally worry about the blanket false being passed here and that itd need integrated into abort call for outputcommitter but the underlying recordwriter ignores and throws away its irrelevant ',negative
'important restore the batchs selected array ',negative
'pigs schema contain type information about maps keys and values its new column assume stringstring its existing return whatever contained the existing column ',negative
'remove col stats ',negative
'logdebugclassname logical logical batchindex batchindex new key currentkey savejoinresultname ',negative
'send done event which llaprecordreader expecting upon end input ',negative
'check owner has write permission else will have copy ',negative
'create more staging data and test load data overwrite ',negative
'very expensive sometimes ',negative
'nonjavadoc see ',negative
'nonjavadoc see javaioinputstream ',negative
'check scan grammar prevents coexit noscancolumns ',negative
'shift the remaining bins left one position ',negative
'stop the cachedstore cache update service well start explicitly control the test ',negative
'see discussion yarn for the memory accounting discussion ',negative
'does any result need emitted ',negative
'enabling this will cause test failures mac ',negative
'file required parameter ',negative
'check for partition key change when have order keys ',negative
'write value object that can inspected ',negative
'need drop the table ',negative
'staging area for results avoid new calls ',negative
'should this close updaters ',negative
'test serialization and deserialization with different schemas ',negative
'candidate ',negative
'not rexcall continue ',negative
'size stdout buffer bytes ',negative
'todo parse make sure its the only compacts file and contains alter table defaulttacid compact majoralter table defaulttacidpart partitionpy compact major ',negative
'username must present ',negative
'should not happen have accounted for all types ',negative
'long ',negative
'given key find the corresponding column name ',negative
'are rounding may introduce one more integer digit ',negative
'static partitions specified and hence all are dynamic partition keys and need part temp table input data file ',negative
'the may not tablescan for mapjoins consider the query select mapjoina count from join akey bkey ',negative
'add viewbased rewriting rules planner ',negative
'inner big table only join hash multiset ',negative
'case not list ',negative
'earlier version hive ',negative
'remove incomplete outputs some incomplete outputs may added the beginning for for dynamic partitions ',negative
'get the registry ',negative
'write buffer full read buffer isnt used switch buffer ',negative
'since componentize windowing need translate the partition order specs individual wfns ',negative
'decimalab type ',negative
'getcolumnsstring catalog string schemapattern string ',negative
'evaluate filter expression and update statistics ',negative
'llap off dont output ',negative
'equalkey match bytes ',negative
'note for now llap only supported tez tasks will never come others may added here although this only necessary have extra debug information ',negative
'check hashmap disk memory ',negative
'default user hasnt provided any optional constraint properties ',negative
'unique constaint violation incl unique key ',negative
'and accepts the first one clazzgetmethods returns ',negative
'handle the case select from select from the currentinput null check above needed the alias list that case would avt lookup would return null need further find the view inside ',negative
'need close the dummyops well the operator pipeline not considered closeddone unless all operators are ',negative
'open base txn which allocates write and then committed ',negative
'the data source will produce data ever ending want see that memory pressure kicks and some ',negative
'the queue should ignored ',negative
'child the optional comment the column ',negative
'this only there pre event listener registered avoid unnecessary metastore api call ',negative
'have reset the conf when change that the change takes affect ',negative
'hive compiler going remove inner order disable that optimization until then ',negative
'could also allow cutting off versions and other stuff provided that sha matches ',negative
'requested host still alive but cannot accept task pick the next available host consistent order ',negative
'the query materialization validation check only occurs cbo thus only cache results cbo was used ',negative
'verify handle the key column types for optimized table this the effectively the same check used hashtableloader ',negative
'remove the ddltime gets refreshed ',negative
'testing with repeating and nulls ',negative
'set the bit value not null ',negative
'read keys from token store ',negative
'now register permanent function ',negative
'make sure are checking the right latest compaction entry ',negative
'returns first one matches all the params ',negative
'select algorithm with min cost ',negative
'check possible drop default database ',negative
'write byte size the string which vint ',negative
'get the row structure ',negative
'for ',negative
'separate the base files into acid schema and nonacidoriginal schema files ',negative
'already added this column select list ',negative
'equals ',negative
'should not happen ',negative
'bags always contain tuples ',negative
'instruct exprnodedesc list for the current table alias ',negative
'look getting rid fractional digits that will now below hivedecimalmaxscale ',negative
'each evaluator has constant java object overhead ',negative
'test andor more ',negative
'masks for quicker extraction pprime qprime values ',negative
'dont need the buffer anymore ',negative
'looks like subq plan todo can collapse this part tree into single ',negative
'compactions are not happening ',negative
'jdo ',negative
'this method also initializes the consolereader which ',negative
'preserved initialization time have session use during resize ',negative
'more data ',negative
'append colnum make unique ',negative
'that can cancelled later from completedelegator ',negative
'not sequential with next ',negative
'the same thing that writerimpl does when writing the footer but the footer ',negative
'start the input and wait for ready event number mrinput expected ',negative
'create reader look footer need check side file since can only streaming ingest delta ',negative
'initialize table properties from the table parameters this required because the table may define certain table parameters that may required while writing the table parameter one such example ',negative
'empty maybe because cbo did not run fall back full select query ',negative
'bounds the column type are written and values outside throw exception ',negative
'constructing conditional task consisting move task and map reduce task ',negative
'has dynamic well static partitions ',negative
'check its ',negative
'initialize has not been called initialize has been called and close has not been called close has been called but one its parent not closed ',negative
'since noscan true table name command ',negative
'retry with different dump should fail ',negative
'get deterministic count number tasks for the vertex ',negative
'join cost ',negative
'test basic operation ',negative
'specialized class for doing vectorized map join that outer join multikey using hash map ',negative
'fell through here this not valid type conversion ',negative
'assumptions precision scale scale ',negative
'statsobjold found can merge ',negative
'test addpartitions ',negative
'special case unsigned magnitude twos compliment adjustment ',negative
'remove all non alphanumeric letters replace whitespace spans with underscore ',negative
'preanalyze hook fired the middle these calls ',negative
'fail with good message ',negative
'check size data shuffle larger table less than given max size ',negative
'here need lock partition write and lock table read which should ',negative
'the prefix nonempty add the before set the mutation ',negative
'note israwformat invalid for nonorc tables will always return true were good ',negative
'singlecolumn long outer null detection ',negative
'set conf ',negative
'this test function that takes three different kinds arguments for use verify vectorized udf invocation ',negative
'default values ',negative
'exact type conversion get out ',negative
'create and load the input data into the hbase table ',negative
'output minus the distinct aggcalls input ',negative
'authorizer not set check for metastore authorizer ',negative
'keep the small table alias avoid concurrent modification exception ',negative
'tests with queries which can pushed down and executed with directsql but the number partitions which should fetched bigger than the maximum set the parameter ',negative
'need new run the constant folding because might have created lots and true and true conditions rather than run the full constant folding just need shortcut andor expressions ',negative
'fetchtask should not depend the plan ',negative
'sadly derby for update doesnt meant what should ',negative
'keeps track all events that need processed irrespective the source ',negative
'not zero ',negative
'should empty ',negative
'non default session nothing changes the user can continue use the existing session the sessionstate ',negative
'now the left join ',negative
'with feature multiple tasks may get into conflict creatingusing tmplocation and were generate the target dir the map task there easy way pass outputcommitter ',negative
'carrayarraystring ',negative
'create input stream given nameext and write sql statements ',negative
'hashcode ',negative
'ingest size bytes gets resetted flush whereas connection stats not ',negative
'account for maximum cache buffer size ',negative
'validate inputs and outputs have right protectmode execute the query ',negative
'have close the processors run method because tez closes inputs before calling close tez and might need read inputs when flush the pipeline ',negative
'only consider range operators havent already seen one ',negative
'all divides are the result column times col ',negative
'map type ',negative
'merge the target works the second dpp sink into the first dpp sink ',negative
'table exists ',negative
'flag indicate its the first time read parquet data page with this instance ',negative
'the fractional digits are gone clear remaining round digits ',negative
'alter partitioned tables partition set partition property ',negative
'already file with same checksum exists cmpath just ignore the copymove also mark the operation unsuccessful notify that file with same name already exist which will ensure the timestamp cmpath updated avoid cleanup cleaner ',negative
'check mapreduce path ',negative
'not included the input collations but can propagated this aggregate will enforce ',negative
'task requested host got host since host dead and host full ',negative
'verify that the whitlelist params can set ',negative
'find the positions the bucketed columns the table corresponding the select list consider the following scenario tkey value value bucketedsorted key into buckets tdummy key value value bucketedsorted key into buckets query like insert overwrite table select key value value from should optimized ',negative
'database database ',negative
'todo duplicated code for init method since vectorization reader path doesnt support nested column pruning far see hive ',negative
'vectorptfoperator native vectorized ',negative
'get all files from the src directory ',negative
'newcat ',negative
'set the log stream ',negative
'this will happen for count such cases arbitarily pick first element from srcrel ',negative
'ekoifman tree ext hiveunionsubdir hiveunionsubdir hiveunionsubdir directories files ',negative
'case column stats hash aggregation grouping sets ',negative
'count characters ',negative
'for outer joins should not exceed aliases short type ',negative
'using volatile instead locking updates this variable ',negative
'queryfile ',negative
'remove the reduce sink operator ',negative
'valuetypeptr ',negative
'where product the carry from ',negative
'get partition ',negative
'test that exclusive blocks read and exclusive ',negative
'set vector null verify correct null handling ',negative
'delayed tasks will not kick right now that will happen the scheduling loop ',negative
'total size the composite object ',negative
'undone method still under development ',negative
'rewrite logic pass along any correlated variables coming from the input ',negative
'definition this session not use and can longer use only affects the session pool can handle this inline ',negative
'required for jackson ',negative
'create test dbs and tables ',negative
'',negative
'statsobjold not found just use statsobjnew accurate ',negative
'make sure location string proper format ',negative
'this should really close zero ',negative
'normal case ',negative
'not need the lock for partitions since they are covered the table lock ',negative
'some logging and force log rollover ',negative
'the logical indices for reading with readfield ',negative
'must have the duck still should just the other task ',negative
'note may add async option future for now let the task fail for the user ',negative
'abortedbits ',negative
'all children are done need walk the children ',negative
'the row consists string columns double columns some unionint double columns only ',negative
'repeating null ',negative
'the event else noop ',negative
'give sequence number for all the partitions ',negative
'arbitrary ',negative
'object overhead bytes for long fasttime bytes for cdate ',negative
'create the filter operator and update the parents and children appropriately ',negative
'test alter database set location ',negative
'because row was updated and thus has different recordidentifier now ',negative
'time ',negative
'nonjavadoc see javalangstring ',negative
'recreate ',negative
'show all privileges ',negative
'formatteroff ',negative
'caching disabled for mapinput due hive ',negative
'derivedschema arraylist ',negative
'password may longer the conf use getpassword ',negative
'use the serialization option switch write primitive values either variable ',negative
'make sure the node got deleted ',negative
'see javadoc hbasecompositekey ',negative
'remainder ',negative
'nonstatic methods wrap the static methods enable testing ',negative
'match was found create new entries ',negative
'insert globally unique byte value every few entries that one can seek into the middle file and then synchronize with record starts and ends scanning for this value ',negative
'history file name ',negative
'this shows the relevant bits the original hash value and how the conversion moving bits from the index value over the leading zero computation ',negative
'new connection should able call describeuse function without issue ',negative
'partition must have least sdid and serdeid set nothing set its view ',negative
'whether series key null ',negative
'update number columns from sel ',negative
'get the union value ',negative
'returns the location disc the jar this class ',negative
'more ',negative
'skip trailing blank characters ',negative
'get binary service port ',negative
'truncation needed ',negative
'since rowids are not needed didnt create the columnvectors hold them but still have check the data being read committed far current reader transactions concerned since here are reading original schema file all rows have been created the same txn namely ',negative
'not found the cache must parameterized types create ',negative
'',negative
'open matches metastore state ',negative
'the original record was lost the deserialization just the correct way through objectinspectors ',negative
'isemptypartition false ',negative
'store table descriptor mapwork ',negative
'prepare empty routing table ',negative
'add partitions for the partitioned table ',negative
'pbb ',negative
'check are already current schema level ',negative
'the database name not changed during alter ',negative
'expected number droppartitions call ',negative
'get the cost the operator ',negative
'also ensures that heartbeat noop since client likely doing async ',negative
'set upper bound how much were willing push before should flush weve set the memory treshold each key distinct should not beyond keydata ',negative
'check streaming side ',negative
'source filesystem ',negative
'note that this depends the fact that noone this class calls anything but getconnection you want use any the logger wrap calls youll have implement them ',negative
'remove pwd from conf file that job tracker doesnt show this logs ',negative
'random test string ',negative
'check this subquery lateral view ',negative
'this for special case ensure unit tests pass ',negative
'its filesink bucketed files also use mrstyle shuffle ',negative
'dont constant folding here wait until the optimizer changed family related jiras hive hive and hive ',negative
'create dumpfile prefix needed create descriptor ',negative
'already deleted ',negative
'myenumlist ',negative
'create temp dir ',negative
'get first level array ',negative
'and does not call the real method explicitly unset the queue name here ',negative
'remember the input file formats validated and why ',negative
'add scale ',negative
'the kill failed and the user also thinks the session invalid restart ',negative
'epoch days since epoch ',negative
'working directory ',negative
'build the filter and add parameters linearly are traversing leaf nodes ltr ',negative
'identifier ',negative
'nonjavadoc see ',negative
'tez used the base for tez ats acls exists honor get the same acls for tez ats entries and hive entries ',negative
'for group type need build the projected group type with required leaves ',negative
'verify that the property has been properly set while creating the ',negative
'class private variables ',negative
'stream buffers are arranged enum order stream kind ',negative
'lastanalyzed stored per column but thrift has per several get the lowest for now nobody actually uses this field ',negative
'add what found our type and table tables ',negative
'column name can anything since will named udtf clause ',negative
'noop for now ',negative
'append the first group within pattern ',negative
'create two connections ',negative
'multiplication with overflow check overflow produces null output ',negative
'also include the stillinmemory sidefile before has been truely spilled ',negative
'meets all requirements ',negative
'test invalid case ',negative
'try the repeating null case ',negative
'operator list ',negative
'todo this assumes indexes getrowindexes would match column ids ',negative
'clean prep ',negative
'check there segments load ',negative
'already have struct node for the current index insert the constant value into the corresponding struct node ',negative
'call listlocatedstatus mockmocktbl call check side file for mockmocktbl ',negative
'job request got timed out job kill should have started return client with queueexception ',negative
'estimated count ',negative
'adjust the data bytes according any possible offset that was provided ',negative
'use construct ',negative
'rules how recurse the objectinspector based its type ',negative
'currently none ',negative
'can null for void type ',negative
'poll the operation status till the query completed ',negative
'locks not associated with txn ',negative
'the highwatermark should assuming currenttxn otherwise currenttxn and commits before then will see result which doesnt make sense for snapshot isolation course for read committed the list should include the latest committed set ',negative
'uncompressed sizes file and rows ',negative
'have found merge work corresponding this closing operator hook this work ',negative
'addpartitiondesc already has the right partition location ',negative
'check parts the error not the whole string not tightly couple the error message with test ',negative
'task state unkown ',negative
'call listlocatedstatus mockmocktbl call check side file for mockmocktbl ',negative
'specialized class for doing vectorized map join that outer join singlecolumn string using hash map ',negative
'for final and complete ',negative
'alters and replacements are not undoable theyve taken effect already they are retriable though creates are undoable but cannot differentiate between creates alters and replacements from command level ',negative
'nonjavadoc see ',negative
'mode ',negative
'align multiples from origin ',negative
'allocatemap one txn per aborted writeid and abort the txn mark writeid aborted ',negative
'init udf ',negative
'check forcing the location required ',negative
'pattern ',negative
'partially contained topoffset toplimit bottomlimit topoffset bottomlimit ',negative
'scope openedclosed times ',negative
'partitionname keyvaluekeyvalue ',negative
'create row file and empty ',negative
'three array two int array ',negative
'are variables from constant ',negative
'all protocols ',negative
'todo expose all wmcontexts via jmx use ',negative
'time doesnt matter ',negative
'column stats ',negative
'data for the split fits the middle one two slices ',negative
'try split bigger blocks ',negative
'todo can pass custom things thru the progress ',negative
'this impossible read from this chunk ',negative
'unpartitioned table ',negative
'used for legacy hivedecimalv setscale compatibility for binary display serialization ',negative
'read the template into string expand and write ',negative
'nonjavadoc see ',negative
'job callable task for job status operation overrides behavior execute get status job need override behavior cleanup there nothing done job sttaus operation timed out interrupted ',negative
'firstrow invoke underlying evaluator initialize skipnulls flag ',negative
'enum class ',negative
'creates connection hms and thus must occur after kerberos login above ',negative
'start server again ',negative
'create map for tracking gauges ',negative
'the time seconds converts milliseconds first ',negative
'either schema literal serialization class must provided ',negative
'get the relevant information for this column ',negative
'pass the validtxnlist and validtxnwriteidlist snapshot configurations corresponding the input query ',negative
'didnt see this lock when running delete stmt above but now showed should should never happen happened ',negative
'should get back null ',negative
'operation log configuration ',negative
'reserved much needed ',negative
'read prompt configuration and substitute variables ',negative
'returns true the readfield method supported ',negative
'integer parsing move next lower longword ',negative
'private final helper helper ',negative
'grimacing face ufc bytes ',negative
'return our known table name ',negative
'remove this backup server ',negative
'the planner will not include unneeded columns for reading but other parts execution may ask for them ',negative
'foreigntblname ',negative
'after constant folding child expression the return type udfcase might have changed recreate the expression ',negative
'value can anything use the obj inspector and respect binary ',negative
'divided max split size ',negative
'drop table ignore error ',negative
'generate the data ',negative
'the actual check should the compare the connection string the external tables ',negative
'columns from sel branch only and append all columns from udtf branch ',negative
'inner classes ',negative
'trim additional bytes ',negative
'generate parse context for optimizer physical compiler ',negative
'batch size and decaying factor ',negative
'not update metrics wed immediately add the session back are able remove ',negative
'return all the dependency urls ',negative
'outer and inner joins ',negative
'noop orcdatareaderref owned the parent object ',negative
'setup for input resultexpr select list ',negative
'test routines exercise vectorizedrowbatch filling column vectors with data and null values ',negative
'check whether the mapjoin bucketed mapjoin the above can ascertained checking the big table bucket small table buckets mapping the mapjoin descriptor first check this mapjoin operator already bucketmapjoin not not give ',negative
'runasync ',negative
'',negative
'bit mask generate positive bit longs from random signed longs ',negative
'have cases here all the data the cache always single slice disk read cache puts some data the cache always single slice disk read and single cache put data the cache multiple slices disk read and multiple cache puts ',negative
'can this happen delta cannot exceed ',negative
'the partition did not change the new partition should similar the original partition ',negative
'streaming evaluators fill their results during the evaluate call ',negative
'todo track stats rejections etc per host ',negative
'right now only support one special character more special characters can added accordingly the future note the following array updated please also sure update the configuration parameter documentation ',negative
'build keys grouping set starting position first add original keys ',negative
'inspect the test data ',negative
'this ast has only one child then column name specified ',negative
'nonjavadoc see ',negative
'end synchronized ',negative
'drop table event ',negative
'wrapper extends qlmetadatatable for easy construction syntax ',negative
'drop the table ',negative
'accumulo token already configuration but the token isnt the job credentials like the ',negative
'nonjavadoc see int ',negative
'specify the columns deserialize into list ',negative
'tag the original file name know where the file comes from note currently only track the last known trace xattr has limited capacity shall revisit and store all original ',negative
'',negative
'create some data ',negative
'right repeats ',negative
'use current for now ',negative
'processing will decref once and the last one will unlock the buffers ',negative
'dont support changing name type ',negative
'when type config set classic ',negative
'does the conversion string itself ',negative
'end ',negative
'this one can used deny permission for performing the operation ',negative
'process multikey inner join vectorized row batch ',negative
'datacolumnnums ',negative
'compare input ',negative
'setup qbjointree between subquery and its parent query the parent query the lhs the join the parent query represented the last operator needed process its from clause case single table query this will tablescan but can join operator the parent query contains join clauses case single source from clause the source could subquery ptf invocation setup the qbjointree with the above constrains place the lhs the qbjointree can another qbjointree the parent query operator joinoperator this case get its qbjointree from the joincontext the rhs always reference the subquery its alias obtained from the qbsubquery object the qbsubquery also provides the joining condition ast the joining condition has been transformed qbsubquery setup before this call the joining condition has any correlated predicates and predicate for joining the parent query expression with the subquery the qbsubquery also specifies what kind join construct given this information once initialize the qbjointree call the parsejoincondition method validate and parse join conditions ',negative
'and grand child ',negative
'this sql standard return statemaxnarray null the size zero ',negative
'test that existing sharedwrite partition with new sharedwrite coalesces ',negative
'this two functions are for use only the planner will fail task ',negative
'simple pattern ',negative
'cannot convert bucket map join cannot convert map join either based the size check can convert smb join ',negative
'create the mapjoin operator ',negative
'test repeating nonnull selection ',negative
'cardinality estimate from normalized bias corrected harmonic mean ',negative
'get the counters for the input vertex ',negative
'both classes access subclasses ',negative
'timeseries query results records types defined metastore ',negative
'private readstringresults readstringresults ',negative
'even the user isnt doing schema evolution cut the schema the desired size ',negative
'insert two rows into the table ',negative
'see hive ',negative
'set base the location that the input format reads the original files ',negative
'locks from different transactions detected from transaction and readonly query autocommit ',negative
'operator wants some work the beginning group the first group ',negative
'convert all nan values vector null should only used ',negative
'filter expression since will taken care partitio pruner ',negative
'dont store too many items the queue full well block the checker thread since the worker count determines how many queries can running parallel makes sense produce more work the backlog getting too long ',negative
'checking ',negative
'string query ',negative
'session handle should not null ',negative
'nothing will added the expression ',negative
'semijoin ',negative
'acending ',negative
'print foreign key containing parents ',negative
'iterate through children and push down not for each one ',negative
'realm ignored ',negative
'walk the operator tree the tablescan and build the mapping ',negative
'make ones complement masked only for the bytes read ',negative
'nothing done for filters the output schema does not change ',negative
'make initialdelay random number heartbeatinterval that lot queries land the server the same time and all get blocked lack resources that they all dont start heartbeating the same time ',negative
'arithmetic operations reset the results ',negative
'rewrite projects replace column references constants when possible ',negative
'resourceplans ',negative
'xxxx idx xxxx idx ',negative
'this primitive type ',negative
'add the log processor ',negative
'the alias already there then have conflict ',negative
'create provider ',negative
'this operator only process small tables read the keyvalue pairs load them into hashtable ',negative
'reset the bufferreader fetching from the beginning the file ',negative
'extract any drop privileges out required privileges ',negative
'without this becomes ',negative
'change for multiply value ',negative
'query should have fetch task ',negative
'cause cannot prune columns from udtf branch currently extract ',negative
'substituted ',negative
'count distinct with more that one argument not supported ',negative
'custom build arguments ',negative
'get node type ',negative
'has use linkedhashmap enforce order ',negative
'',negative
'will superceded credential provider will not superceded ',negative
'the bucketing and sorting positions should exactly match ',negative
'distinct lost position ',negative
'subquery rewriting needed ',negative
'',negative
'call function ',negative
'create ',negative
'string type never stored anything other than escaped string ',negative
'test params check that all the parameters empty table are retained asis may add beyond but not change values for any parameters that hive defines for empty table ',negative
'task requested host got host ',negative
'clear work from threadlocal after splits generated case thread reused pool ',negative
'can not use split function directly may quoted ',negative
'disabling vectorization this test yields incorrect results with vectorization ',negative
'the perbatch setup for left semi join ',negative
'get timestamp from string constant cast ',negative
'completion delete from tab txn ',negative
'construct map join and set the child operator tblscanop ',negative
'finished writing array contents ',negative
'for dynamic uris relookup there are new metastore locations ',negative
'unknown unknown ',negative
'cause timestamp object replaced buggy code with zerotimestamp ',negative
'gby for distinct after windowing ',negative
'this thismag thisscale right rightmag rightscale this right thismag rightmag thisscale rightscale need scale down thisscale rightscale newscale ',negative
'both children the expression should not literal ',negative
'empty ctor make jackson happy ',negative
'verify the directories table location ',negative
'try iso format ',negative
'this method converts from the string representation druid type the corresponding hive type ',negative
'indicates the maximum capacity the cache minimum value should the number threads ',negative
'note doesnt check for overflow could and with max refcount mask but the caller checks ',negative
'one time update issue when the new hive catalog created upgrade the script does not know the location the warehouse need update ',negative
'build the schema for this table which slightly different than the schema for the input table ',negative
'propagate null values for twoinput operator and set isrepeating and nonulls appropriately ',negative
'positioned first ',negative
'append leading needed ',negative
'longminvalue ',negative
'currently tez the flow events thus generate splits initialize vertex with parallelism info obtained from the generate splits phase the generate splits phase groups splits using the however for bucket map joins the grouping done this input format results incorrect results the grouper has knowledge buckets initially set the input format hiveinputformat dagutils for the case bucket map joins obtain ungrouped splits then group the splits corresponding buckets using the tez grouper which returns tezgroupedsplits ',negative
'stores each cells length column one dataoutputbuffer element ',negative
'pairwise columnhasnulls columnisrepeating columnhasnulls columnisrepeating ',negative
'groupingsets cube rollup were used account groupingid ',negative
'only need flip the msb ',negative
'service fresh conf for every testmethod ',negative
'note that the calls below will throw exception java securitymanager installed and configured forbid invoking setaccessible practice this not problem hive ',negative
'reset the execcontext for each new row ',negative
'the read field the union gives its tag ',negative
'disable expensive operations the metastore ',negative
'something else holds the lock the moment dont bother cleaning ',negative
'test null input ',negative
'read length ',negative
'adjacency list ',negative
'cluster worker end job submitted the cluster ',negative
'shutdown the current active one ',negative
'get the operation logs once and print then wait till progress bar update complete before printing the remaining logs ',negative
'nonjavadoc see ',negative
'get the cpu counters gctimemillis cpumilliseconds ',negative
'shutdown the timeout thread any while closing this operation ',negative
'evaluate row ',negative
'out range due time ',negative
'sort before comparing with expected results ',negative
'clearing this before sending kill since canfinish will change false ideally this should state machine where kills are issued the executor and the structures are cleaned once all tasks complete new requests however ',negative
'undone need get the table schema inspector from selfdescribing input file formats like orc modify the orc serde instead for now this works ',negative
'digits ',negative
'fall back regular api and create statuses without ',negative
'the table does not define any transactional properties return default type ',negative
'trigger query hook before compilation ',negative
'update columnar lineage for each partition ',negative
'this node was previous union inputs but not this one ',negative
'broadcast data ',negative
'stores explain output ',negative
'files size for splits ',negative
'different batch for vectorized input file format readers they can their work overlapped with work the row collection that vectorrow deserialization does this allows the partitions mix modes for flush the previously batched rows file change ',negative
'not always there sessionstate sometimes exedriver directly invoked ',negative
'table level event that matches ',negative
'swsw lock are examining shared write ',negative
'avoid copy ',negative
'last row last batch determines isgroupresultnull and decimal lastvalue ',negative
'now that have found real data emit sign byte necessary ',negative
'create executor ',negative
'the existing entry already has grant new priv does not have grant update needs done ',negative
'skip some piece data ',negative
'useful for class generation via templates ',negative
'normally statskeypref will the same dirname but the latter ',negative
'todo make sure cleanup created dirs ',negative
'',negative
'castexpr coltype colname ',negative
'full acid export goes thru updatedelete analyzer ',negative
'tests the missing element layer detected multifield group ',negative
'only done when bucket map join only smb ',negative
'friday august ',negative
'generate split strategy for acid schema files any ',negative
'replacemode creates are really alters using createtabledesc ',negative
'create export task and add root task ',negative
'core pool size max pool size direct handoff ',negative
'new record reader ',negative
'weve already satisfied the number events were supposed deliver end ',negative
'create environment for ',negative
'the first argument null return null its okay for other arguments null which case null will printed ',negative
'create success file requested ',negative
'get the top operator and its child all operators have single parent ',negative
'todo handle cast windowing agg call ',negative
'archiving was done this upper level its level would lesser equal specification size not which means archiving this upper level ',negative
'need iterate twice since byteswritable doesnt support append ',negative
'processing the message that the successful init has queued for ',negative
'get too crazy ',negative
'the lazy struct object inspector ',negative
'call check existence side file for mockmocktbl ',negative
'string including style literal characters ',negative
'always mark llap ',negative
'test that new columns gets added table schema ',negative
'verify proxy user privilege the realuser for the proxyuser ',negative
'both sides are constants there nothing propagate ',negative
'add this node the parent node ',negative
'todo use final fields ',negative
'check the score for this method any better relative others ',negative
'the operator not rexcall type fail fall through ',negative
'check out the types ',negative
'specialized class for doing vectorized map join that inner join singlecolumn string and only big table columns appear the join result hash multiset used ',negative
'when truncated included used its length must least the number source type infos ',negative
'cannot proceed and need tell the hive client that retries wont succeed either ',negative
'add new information from source target ',negative
'remove the subquery from the where clause tree return the remaining whereclause ',negative
'now deserialize ',negative
'here assume that upstream code may have parametrized the msg from errormsg want keep ',negative
'last value should present ',negative
'for each matching partition call getsplits the underlying inputformat ',negative
'from bit linear congruential generator ',negative
'constant propagation constant folding ',negative
'verify moveonlytask not optimized ',negative
'create add node for current pool ',negative
'and are sorted the same order ',negative
'get splits from accumulo ',negative
'determine two strings are equal from two byte arrays each with their own start position and length use lexicographic unsigned byte value order this whats used for utf sort order ',negative
'table using some custom format and its not the classpath wont mark the table for acid but today hive and earlier the only acid format ',negative
'',negative
'launch the parallel mode separate thread only for tasks ',negative
'build rel for limit clause ',negative
'lstring ',negative
'convert from java writable ',negative
'save the current script any ',negative
'messageformat ',negative
'column family qualifier dont want include the map ',negative
'case the sizes match preference given the table with fewer partitions ',negative
'the hiveserver instance running this service ',negative
'iterate through the index that add more children they dont get revisited ',negative
'write json the temp file ',negative
'should produce json ',negative
'skip not the one are looking for ',negative
'script preserves alias and value for columns related keys user can set this true ',negative
'deactivate currently active resource plan ',negative
'extra heartbeat logically harmless but ',negative
'otherwise write the file system implied the directory copy required may want revisit this policy future ',negative
'new new byte ',negative
'make the movetask the child the task ',negative
'message ',negative
'test getter for map object ',negative
'plug verifying metastore for testing directsql ',negative
'specialcase for orc ',negative
'the node should always known this point log occasionally not known ',negative
'currently sumdistinct not supported partitionevaluator ',negative
'assign row from array objects ',negative
'base path for repl load ',negative
'this vectorized code pattern says the input batch has nulls all nonulls true the input row not null copy the value otherwise have null input value the standard way mark null the output batch turn off nonulls indicating there least one null the batch and mark that row null when vectorized row batch reset nonulls set true and the isnull array zeroed grab the key index dont care about selected repeating since all keys the input batch are suppose the same ',negative
'dayssinceepoch ',negative
'test left input repeating ',negative
'this map task has filesinkoperator and bucketingsorting metadata can inferred about the data being written that operator these are mappings from the directory ',negative
'single table alias reference ignore and move the next expression node ',negative
'currently only support these noprecisionloss promotion data type conversions tinyint smallint tinyint int tinyint bigint smallint int smallint bigint int bigint float double since stare char without padding can become string implicitly char varchar string ',negative
'task requested host got host host and host are full ',negative
'test that only the fixed property forqueue used order determination not the dynamic call ',negative
'add support for configurable threads however should always enough ',negative
'confvarsscratchdir ',negative
'map that contains the rows for each alias ',negative
'might still able push the limit ',negative
'mapper can span multiple filespartitions the need changed the input file changed ',negative
'all files are needed meet the size limit disable optimization usually happens for empty tablepartition tablepartition with only one file disabling this optimization can avoid retrying the query there not sufficient rows ',negative
'find the parsed delta files ',negative
'add fake entries ',negative
'trigger clean errors for anyone who mixes identity with hosts ',negative
'same algorithm timestampwritable not currently importable here ',negative
'write nonnull element ',negative
'run cleaner should remove the delta dirs ',negative
'overflow checks ',negative
'nonjavadoc see ',negative
'obtain stats for partition cols ',negative
'list ',negative
'fallback default ',negative
'are using the fact the input sorted ',negative
'writing both acid and nonacid resources the same txn tab write dynamic partition insert ',negative
'external table are done ',negative
'create empty invalid side file make sure getlogicallength throws ',negative
'specify that the results this query can cached ',negative
'test null dbname works default used ',negative
'are aborting only the current transaction move the min range for heartbeat disable heartbeat the current txn last the batch ',negative
'return mocked serdeinfo ',negative
'nonjavadoc see javautilcalendar ',negative
'copy over the mandatory configs for the package ',negative
'generate row values ',negative
'update output row schema ',negative
'map from type name such int varchar the corresponding primitivetypeinfo ',negative
'everything qualifies rows all with value ',negative
'theres rexinputref the projected expressions return empty set ',negative
'check object ',negative
'expecting change the size internal structures ',negative
'proxy class within the tezapi package access package private methods ',negative
'estimate needed underlying aggbuffer for results for maxchain results underlying wdwsz maxchain underlying wdwsz ',negative
'need create the merge join work ',negative
'sort the splits that subsequent grouping consistent ',negative
'max length for varchar and char cases ',negative
'all are selected ',negative
'insert overwrite directory command there were bucketing list bucketing ',negative
'statementid ',negative
'save charvarchar string ',negative
'for null fields make valid max length ',negative
'trigger failover minihs without authorization header ',negative
'normilize table name for mapping ',negative
'dont currently allow imposition type ',negative
'array should have listtypeinfo within the list extract types ',negative
'locks txn outta here ',negative
'does the logger config look correct ',negative
'verifyrunselect from repldbname matview unptndata drivermirror ',negative
'delete remaining jars ',negative
'lock table dynamic partitions ',negative
'correlating variables are means for other relational expressions use fields ',negative
'serialize the row struct ',negative
'under the limit ',negative
'build partition strings ',negative
'not vrb mode the new cache data partially ready should use force the rest the data thru ',negative
'column not partition column for the table not allow partitions based complex list struct fields ',negative
'parse out ngrams update frequency counts ',negative
'remove the tag from key coming out reducer and store separate variable make copy for multiinsert with join case spark reuses input key from same parent ',negative
'this will take care mapping between input column names and output column names the returned column stats will have the output column names ',negative
'checkh subquery predicates cannot only refer outer query columns ',negative
'show cannot create child file ',negative
'iterate over the map and remove semijoin optimizations needed ',negative
'more than capacity ',negative
'check this udf has been provided with type params for the output varchar type ',negative
'execute final aggregation stage for simple fetch query fetch task ',negative
'try merge this join with the left child ',negative
'create the adaptor for this function call work vector mode ',negative
'get the vlong that represents the map size ',negative
'switch hivesitexml with remote metastore ',negative
'our one time process method initialization ',negative
'startdate sun letters day name ',negative
'delay with exponential backoff ',negative
'done with the row ',negative
'sleep for seconds ',negative
'udf like one user would create implementing the udf interface this used test the vectorized udf adaptor for legacystyle udfs ',negative
'file ',negative
'for table level load need not update replication state for the database ',negative
'dont think this can happen but just case ',negative
'this batch with the same column schema the big table batch that can used build join output results can create some join output results the big table batch will for better efficiency avoiding copying otherwise will use the ',negative
'perform some operations ',negative
'load the partition ',negative
'try with null args ',negative
'the fsop configuration for the fsop that going write initial data during ctas ',negative
'add all columns make vectorization context for the tablescan operator ',negative
'rounding fractional digits ',negative
'during mapreduce tasks there may not valid hiveconf from the sessionstate lookup and save any needed conf information during query compilation the hive conf where there should valid hiveconf from sessionstate plan serialization will ensure have access these values the mapreduce tasks ',negative
'leave the client time out ',negative
'',negative
'generate the right hand side the clause ',negative
'the windowingspec used for windowing clauses this ',negative
'trigger the transform ',negative
'processing completed ',negative
'multiple stripes ',negative
'insert the dummy store operator here ',negative
'load entire archive there some parallelism going you load more than partition the file name changes from run run between and and the data correct but this causes rowidbucketidfile names change ',negative
'here checked all parts and they are acid compatible make acid ',negative
'test behavior with nonchunked streams ',negative
'there are aborted txns then the minimum aborted txnid could the minuncommittedtxnid ',negative
'endtime ',negative
'the children type should converted return type ',negative
'unit tests can overwrite this affect default dump behaviour ',negative
'lower case rolename ',negative
'set our inputformat ',negative
'column types ',negative
'todo side the union has columns with the same name noone the higher level can refer them could change the alias the original node ',negative
'then make sure the file sink operators are set right ',negative
'there kryo which after timestamp becomes date get around this issue once kryo fixed the issue can simplify ',negative
'insert schema was specified ',negative
'describe how deserialize data back from user script ',negative
'sets the job state and result returns true status and result are set otherwise returns false ',negative
'table existed and okay replicate into not dropping and recreating ',negative
'locationuri ',negative
'most one alias unknown can safely regard big alias ',negative
'this genericudf cant pushed down ',negative
'only first stripe will satisfy condition and hence single split ',negative
'used for value registry ',negative
'view referring old database data ',negative
'for each table reference get the table name and the alias alias not present the table name ',negative
'hive this noop ',negative
'generate the service ticket for sending the server locking ensures the tokens are unique case concurrent requests ',negative
'make data consistent with encodings dont store useless information ',negative
'merges sampling data from previous and make partition keys for total sort ',negative
'resend existing value necessary ',negative
'only the admin allowed list privileges for any user ',negative
'nothing just retry ',negative
'create map capture object privileges corresponding privilege ',negative
'the method returns new writable ',negative
'include same hllastheartbeat condition case someone heartbeated since the select ',negative
'there can multiple instances per node ',negative
'int pos ',negative
'check make sure there are duplicate rowids hive ',negative
'oldreplstate lessthan newreplstate allow ',negative
'fixup numbers limit the range ',negative
'the first group ',negative
'check see this input job outputjob ',negative
'serialize the configuration once ',negative
'change correlator rel into join join all the correlated variables produced this correlator rel ',negative
'this secured cookie and the current connection nonsecured then skip this cookie need skip this cookie because the cookie replay will not transmitted the server ',negative
'avro requires nullable types defined unions some type and null this annoying and were going hide from the user ',negative
'drop partition after dump ',negative
'the max good the min too low ',negative
'list the loggers and their levels ',negative
'then create splits with the druid queries ',negative
'sort will try open the output file write mode windows need close first ',negative
'detect correlations ',negative
'now check the partition already exists not ahead error out immutable and mutable check that the partitions matches our current jobs tables check for compatibility compatible ignore and not add incompatible error out again ',negative
'otherwise try default timestamp parsing ',negative
'rowid ',negative
'try reduce ',negative
'bgenjjtree typemap ',negative
'the structure the ast for the rewritten insert statement tokquery tokfrom tokinsert tokinsertinto tokselect toksortby the following adds the tokwhere and its subtree from the original query child tokinsert which where would have landed had been there originally the string this way because its easy then turning the original ast back into string and reparsing have move the sortby over one grab and then push the second slot and put the where the first slot ',negative
'determine the user would need sign fragments ',negative
'write couple batches ',negative
'this constant bit misnomer since now always have txn context just means the operation such that dont care what tablespartitions affected doesnt trigger compaction conflict detection better name would nontransactional ',negative
'data ',negative
'integer ',negative
'',negative
'not supported ',negative
'unknown information judge ',negative
'finally hand off the stripe reader produce the data ',negative
'check permutation project ',negative
'heartbeat started ',negative
'there could case where join operators input are not map join with spark since following estimation statistics relies join operators having inputs reduced sink will not work for such cases should not try estimate stats ',negative
'pattern does not contain all date fields ',negative
'partition bucket file names and partition bucket number for ',negative
'verify drop table nonexisting table idempotent ',negative
'explaining this would really require picture basically the level lower than our level that means imagine tree are the leftmost leaf node the subtree under our sibling the tree wed need look the buddies that leftmost leaf block all the intermediate levels aka all intermediate levels the tree between this guy and our sibling including its own buddy its own level and for every subtree where our buddy not the same level does not cover the entire ',negative
'incorrect use this class ',negative
'write schema since need pull the data out see point above ',negative
'even for regular copy have use the same user permissions that distcp will use since hiveserver user might different that the super user required copy relevant files ',negative
'only release cache chunks not release proccachechunks they may not yet have data ',negative
'the indexes the delimiters ',negative
'after load shall see the overwritten data ',negative
'expected ',negative
'statementid ',negative
'process join values ',negative
'template classname valuetype outputtype outputtypeinspector ',negative
'create reducesinkop operator ',negative
'populate local work needed ',negative
'create fetchwork for partitioned table ',negative
'turn clientside authorization ',negative
'this the iow case ',negative
'the synchronization here not necessary but tests depend ',negative
'since theres close here maintain the initial read position between writes ',negative
'fields these aggregation classes ',negative
'actual result directory need move anything ',negative
'conversion possible for the reduce keys ',negative
'bugbug need deal with named type here look and proxy should raise exception this typedef since wont any children and thus can quickly find this comment and limitation ',negative
'dogetfooters ',negative
'the same day the month then time part should ignored ',negative
'with nulls and selected ',negative
'need clone some operator plans and remove union operators still ',negative
'',negative
'coming from bigtable side some bookkeeping and continue traversal ',negative
'about filtering ',negative
'must have most one child ',negative
'calculate result typeinfo ',negative
'print since otherwise exception lost ',negative
'update summary bitvector generate hash value the long value and mod bitvectorsize this implementation bitvectorsize ',negative
'hltxnid and hllockstate lockwaiting for multistatement txns where ',negative
'use get make sure variable substitution works ',negative
'meanwhile the init fails ',negative
'not need column reset since are carefully changing the output ',negative
'verify that ptned table property set worked ',negative
'this tmp table and thus session scoped and acid requires sql statement serial ',negative
'when there order specified add the partition expressions order expressions this implementation artifact for udafs that imply order like rank denserank depend the order expressions work internally pass the order expressions args these functions could change the translation that the functions are setup with partition expressions when the orderspec null but for now are setting orderspec that copies the partition expressions ',negative
'implicit type conversion hierarchy ',negative
'the data type primitive category the column being deserialized ',negative
'need include isinsideview inside digest differentiate direct ',negative
'will clone here will update bucket column key with its ',negative
'actually create the permanent function ',negative
'default list bucketing directory key internal use only not for client ',negative
'for queue size estimation purposes assume all columns have weight one and the following types are counted multiple columns this very primitive wanted make better ',negative
'were calling processop again process the leftover tuples know the row coming from the spilled matchfile need recreate hashmaprowgetter against new hashtables ',negative
'set the fetch formatter noop for the listsinkoperator since well read formatted thrift objects from the output sequencefile written tasks ',negative
'introduce select after the union ',negative
'from javasqltimestamp used vectorization serializable ',negative
'maxcomplexdepth ',negative
'local aliases need not hand over context further ',negative
'add the filter the queryid appender ',negative
'mapjoin table descriptor contains key descriptor which needs the field schema column type column name the column name not really used anywhere but needs passed use the string defined below for that ',negative
'set props for read ',negative
'read friendly string ',negative
'repeat times ',negative
'one each for min max and bloom filter ',negative
'make sure the ugi current ',negative
'reached the end the tag ',negative
'create view ',negative
'there remainder from numrowsnumbuckets then distribute increase the size the first rem buckets ',negative
'need convert the thrift type the sql type ',negative
'used handle skew join ',negative
'push down projections ',negative
'custom parameters ',negative
'construct using ',negative
'check julian days between jan and jan this method used test julian days between jan bce and jan since bce ',negative
'start monitoring the spark job returns when the spark job has completed failed ',negative
'count denserank and rank not care about column types the rest ',negative
'failure from not having permissions create table ',negative
'optimized for sequential key lookup ',negative
'security the path below the user path full access ',negative
'index into map ',negative
'maxrows ',negative
'detect queries the form select udtfcol looking for function the first child and then checking see the function generic udtf its not clean transform due ',negative
'convert from mapjoin bucket map join enabled ',negative
'hive values have copied and use ',negative
'test with multilevel scratch dir path ',negative
'clear the columnbuffers ',negative
'make cost based decision pick cheaper plan ',negative
'the dispatcher generates the plan from the operator tree ',negative
'keep track for error reporting ',negative
'ignored for some reason the bean was not found dont output ',negative
'save the current record the new extravalue for next time that ',negative
'now add cache ',negative
'todo ',negative
'for each node ',negative
'make sure originaldate midnight the local time zone since datewritablev will generate dates that time ',negative
'concurrency ',negative
'generated the log message ',negative
'index the next free block split ',negative
'any hive related operations like moving tables and files ',negative
'specification binary storage should not affect serde ',negative
'the automation the data warehouse the local file system based ',negative
'create mapred task for this work ',negative
'case ',negative
'current replication state must set the table object only for bootstrap dump event replication state will null case bootstrap dump ',negative
'tez were avoiding duplicate the file info fileinputformat ',negative
'sessionmanager initialized ',negative
'the parameter keys for the table statistics those keys are excluded from show create table command output ',negative
'the first position partition column ',negative
'sort the methods before omitting them ',negative
'inherit table properties into partition properties ',negative
'target isnull was copied beginning method ',negative
'lastfrom point the same text object which would make fromequalslastfrom always true ',negative
'prevnext are already checked the calls ',negative
'but soon became very fast decimal specific ',negative
'last partition key anything between key and end ',negative
'inclusive exclusive ',negative
'make random array byte arrays ',negative
'this makes that can back the tree later ',negative
'tables ',negative
'sortby desc ',negative
'test replicated drop should not drop because evid replstateid ',negative
'create the join predicate info object the object contains the join condition split accordingly the join condition not part the equijoin predicate the returned object will typed sqlkindother ',negative
'downloaded resources dir ',negative
'updatable map that holds instances the class ',negative
'first update buffer priority have just been using ',negative
'emulate biginteger serialization used lazybinary avro parquet and possibly others ',negative
'reload tables from the metastore ',negative
'set arguments ',negative
'dont want cache hits from llap for testing filesystem bytes read counters ',negative
'source ',negative
'since txnutilsgettxnstore calls txnhandlersetconf checkqfiletesthack which may change the values below two entries need avoid polluting the original values ',negative
'empty constructor for writable etc ',negative
'throw away lower digits ',negative
'mask udfs ',negative
'there was cluster state change make sure redistribute all the pools ',negative
'prepare ',negative
'mystringlist ',negative
'create default users ',negative
'notify have successfully copied the file ',negative
'set handling for low resource conditions ',negative
'caller should not try allocate another arena before waiting for the previous one ',negative
'this operator has materialized view below make its cost tiny and adjust the cost its ',negative
'our stats for ndv approx not accurate ',negative
'required optional required ',negative
'runas ',negative
'this method takes care bitflipping for descending order ',negative
'skip mode should not throw exception when invalid partition directory found should just ignore ',negative
'nonpartitioned table ',negative
'dont add this the resources because dont want read config values from but find because want remember where for later case anyone calls ',negative
'ignore the value got from optraits the logic below will fall back the estimate from numreducers ',negative
'the directory does not exist ',negative
'listhapeers ',negative
'hive jar ',negative
'cleanup session log directory ',negative
'pre all the fields are required and serialized order isrealthrift ',negative
'note the stats for acid tables not have any coordination with either hive acid logic like txn commits time outs etc nor the lower level sync metastore pertaining acid updates the are not themselves acid ',negative
'first partition key anything between key and first ',negative
'optional optional ',negative
'step reanalyze ',negative
'not understand why needed and wonder could combined with close ',negative
'found file depth which less than number partition keys ',negative
'preemption will finally registered deallocatetask result preemptcontainer that resets preemption info and allows additional tasks preempted required ',negative
'alter table tablename drop exists partition partitionspec partition partitionspec ',negative
'existing ngram just increment count ',negative
'the idea that this will use lockhandledbconn ',negative
'find out need throw away the tuple not ',negative
'copy current value not change current scale ',negative
'optional fragmentruntimeinfo fragmentruntimeinfo ',negative
'have storage specification for primitive column type ',negative
'',negative
'noop dont really write anything here ',negative
'theres race between removing the current task from the preemption queue and the actual scheduler attempting take element from the preemption queue make space for another task the current element removed make space that since the current task completing and will end making space for execution any kill message sent out the scheduler the task will ignored since the task knows has completed otherwise would not this callback the task removed from the queue result this callback and the scheduler happens the section where its looking for preemptible task the scheuler may end pulling the next preemptible task and killing extra preemption todo this potential extra preemption can avoided synchronizing the entire tryscheduling block this would essentially synchronize all operations would better see theres approach where multiple locks could used avoid single threaded operation checks available and preempts which could this task this task completes making space and removing the need for preemption ',negative
'find ',negative
'copy set deduped locks back original list ',negative
'none are null all are selected ',negative
'the number stripes should match the key index count ',negative
'special handling for timestamp column field name field type ',negative
'after the first child evaluated ',negative
'this nonnative table need set stats inaccurate ',negative
'thrift configs ',negative
'add partitions all tables ',negative
'here means open transaction but different queries ',negative
'bgenjjtree typei ',negative
'serde ',negative
'create dummy tablescanoperator for the file generated through filesinkop ',negative
'recheck locks which were waiting state should now acquired ',negative
'create views registry ',negative
'the plan needs broken only one the subqueries involve ',negative
'assertequals mapgetcapacity ',negative
'there group file need call chgrp ',negative
'not used for mock but ',negative
'buffers hold filter pushdown information ',negative
'either colstats null estimated ',negative
'check bigint implicitely cast double part comparison perform the check here instead guarantee only run once per operator ',negative
'dbtesttablep should also columns will fix separate ticket ',negative
'',negative
'exact match ',negative
'now have written some new data bkt and shows ',negative
'generate empty dyn part call ',negative
'shortcircuit quickly forward all rows ',negative
'tablehandle can null table doesnt exist ',negative
'this captures mapping hive type names hcat type names the long run should just use hive types directly but that larger refactoring effort for hcatpig mapping see for pighcat mapping see ',negative
'nonjavadoc see javasqlref ',negative
'nanosecond interval primitives produces type timestamp ',negative
'every line below this identical for evaluatelong evaluatestring ',negative
'calite bug calcite ',negative
'lets write more bytes the files test that actually working returning the file size not from the filesystem ',negative
'round the specified number decimal places using halfeven round function ',negative
'add backup task runnable ',negative
'more than thread should call this close function ',negative
'use this constructor when only ascending sort order used default for ascending order null first ',negative
'dont think event notifications case failures are necessary but other hms operations make this call whether the event failed succeeded make this behavior consistent this call made for failed events also ',negative
'add new column ',negative
'jline will detect tab regular character ',negative
'restrictionh correlated sub queries cannot contain windowing clauses ',negative
'check static partition appear after dynamic partitions ',negative
'same hashs default seed ',negative
'the jsonobject for this vertex ',negative
'the ndv the minimum the and the ',negative
'specialized class for doing vectorized map join that left semi join singlecolumn long using hash set ',negative
'findreadslot key key slot slot pairindex pairindex found key ',negative
'virtual columns ',negative
'fake two live session ',negative
'for each range have intersect them they dont overlap the range can discarded ',negative
'and produce the correlated variables the new output ',negative
'task contains operator which instructs ',negative
'perform another major compaction nothing should change both deltas and both base dirs should have the same name ',negative
'remove trailing ',negative
'todo should call this more often theory for date type time should never matter but ',negative
'this sync call that will feed data the consumer ',negative
'verify that rows were selected ',negative
'this transaction isnt skip over ',negative
'just check make sure base below not new ',negative
'files size for splits ',negative
'time ',negative
'mysql returns the string not wellformed double value but decided return null instead which more conservative ',negative
'matches rows matches rows ',negative
'timeout chosen make sure that even one iteration takes more than half the scripttimeout but less than scripttimeout will still ',negative
'update necessary ',negative
'suspect that like pushdown into jdo invalid see hive check for like here ',negative
'empty set cannot convert ',negative
'were using tss stats for mapjoin optimization check each branch and see theres any upstream operator join lateralview that can increase output data size ',negative
'each child should has its own outputobjinspector ',negative
'backtrack value columns crs prs ',negative
'the instance ',negative
'need get the columnaccessinfo and viewtotableschema for views ',negative
'remove the path with which alias associates ',negative
'sbappendchar ',negative
'recently evicted index used for next keyvalue count excluded rows from previous flush ',negative
'only columns present the batch and noncomplex types ',negative
'the same line ',negative
'one byte always available for writing ',negative
'internal name for expressions and estimate column statistics for expression ',negative
'only leader publishes instance uri endpoint which will used clients make connections via service discovery ',negative
'transfer columnvector objects from base batch outgoing batch ',negative
'update the output position for the cor vars only pass the cor ',negative
'make sure the vector was flattened ',negative
'test second argument with nulls ',negative
'create metrics directory not present ',negative
'stored directories dont care about the skew otherwise ',negative
'out range for whole batch ',negative
'simple distributeby goes here ',negative
'note may later have special logic pick old ams any ',negative
'get the job info from the configuration ',negative
'add the relevant database namespace writeentity ',negative
'nothing far ',negative
'context for current input file ',negative
'structentry ',negative
'table information yet looks like could valid ',negative
'egetkey alias can null case constant expressions see inputq ',negative
'check input can pruned ',negative
'operation not recognized set null and let upper level handle this case ',negative
'',negative
'then invalidate column stats ',negative
'need get footers ',negative
'test decimal scalar decimal column addition this used cover all the cases used the source code template ',negative
'regardless our matching result keep that information make multiple use ',negative
'there are aggregate functions grouping sets will need value generator ',negative
'assumption that environment has already been cleaned once globally hence each thread does not call cleanup and createsources again ',negative
'load the incremental dump and ensure does nothing and lastreplid remains same ',negative
'perform another major compaction ',negative
'this list may modified specific cli drivers mask strings that change every test ',negative
'generate input event ',negative
'there some information about the windowing functions that needs initialized during query compilation time and made available during the mapreduce tasks via plan serialization ',negative
'copied here until utility version this released orc ',negative
'the for the main map operator itself ',negative
'continue handle changes specific plan ',negative
'boolean ',negative
'various unsupported methods ',negative
'when the reducer encountered for the first time ',negative
'done for broadcast joins that includes the dummy parents ',negative
'continue range ',negative
'storage descriptor data ',negative
'for joinrs case its not possible generally merge child has less keypartition columns than parents ',negative
'gets swallowed remote mode ',negative
'need update the queryplans output well that postexec hook get executed this only needed for dynamic partitioning since for the the writeentity constructed compile time and the queryplan already contains that for writeentity creation deferred this stage need update ',negative
'although its likely valid exception will retry with cbo off anyway for tests would like avoid retrying catch cbo failures ',negative
'timestamp scalarscalar ',negative
'not forward compatible ',negative
'local mode outputcommitter hook not invoked hadoop ',negative
'setup mapjointables and serdes ',negative
'the way this works session pool will move back tez pool kill and will get reassigned back pool getrequest based user pool mapping only remove the session from active sessions list its pool will the queued getrequest processed ',negative
'get object cache ',negative
'make sure flow and double equality compare works ',negative
'negative test ',negative
'exprinfo the key ',negative
'its send cancel already completed future noop ',negative
'bail out empty list ',negative
'most the above will failed offers and takes due speed the thing ',negative
'there should call create partitions with batch sizes ',negative
'ignore changes the amount white space ',negative
'nonjavadoc see javalangobject ',negative
'new tai lue letter low bytes ',negative
'loop rewrite rest insert references ',negative
'the following steps seem roundabout but they are meant aid recovery failure occurs and keep consistent state the ',negative
'the value repeating use row ',negative
'match this configuration before merging else will not merged ',negative
'class expressionbuilder ',negative
'state close doesnt mean all children are also state close ',negative
'the vectorized mapoperator there are modes reading for vectorization one for the vectorized input file format which returns vectorizedrowbatch the row one for using deserialize each row into the vectorizedrowbatch currently these input file formats textfile sequencefile and one using the regular partition deserializer get the row object and assigning the row object into the vectorizedrowbatch with vectorassignrow this picks input file format not supported the other two ',negative
'probably not local filesystem need check ',negative
'enable trash can tested hadoop fstrashintervalkey hadoop ',negative
'case fail the reader sending back the error received from the reader event ',negative
'create the jetty server jetty conf file exists use that create server ',negative
'stripped down version fastsetfrombytes ',negative
'this hook verifies that the location every output table empty ',negative
'test lazybinaryserde ',negative
'the select list ',negative
'just rename the directory ',negative
'join has been automatically converted into sortmerge join create conditional task try mapside join with each table the big table similar hiveautoconvertjoin but only applicable joins which have been automatically converted sortmerge joins for hiveautoconvertjoin the backup task the mapreduce join whereas here the backup task the sortmerge join depending the inputs sortmerge join may faster slower than the mapside join the other advantage sortmerge join that the output also bucketed and sorted consider very big table say with buckets being joined with very small table say with buckets the sortmerge join may perform slower since will restricted mappers ',negative
'for now top level can have where clause predicate ',negative
'use this buffer hold columns cells value length for usages ',negative
'end ',negative
'ensure that dont try read any data case skip read ',negative
'doesnt matter wait for all inputs any input ready ',negative
'this the root the partition which the file located ',negative
'make sure the signature works ',negative
'root abstract class for hash table result ',negative
'verify droptable recycle table files ',negative
'are going use lbserde serialize values create for retrieval ',negative
'couldnt find the from that contains subquery replace with allcolref ',negative
'etl strategy requested through config ',negative
'hmmthis looks bit wierdsetup boots qtestutilthis part used beforeclass ',negative
'create rexnode for lhs ',negative
'nonjavadoc see ',negative
'this part reducekeys later used create column names strictly for nondistinct aggregates with parameters same distinct keys which expects col the end always append col the end instead coli ',negative
'load newthreshold newthreshold ',negative
'allocator uses memory manager request memory create the manager next ',negative
'reset ckpt and last repl keys empty set for allowing bootstrap load ',negative
'seems point the start the batch todo validate ',negative
'check reset operation ',negative
'set vector null object reference verify correct null handling ',negative
'join types should all the same for merging returns null ',negative
'communication error the default sql state override the sqlstate ',negative
'format the upgrade script name upgradexydbtypesql ',negative
'but the value rounded more scaling ',negative
'format are sure are getting the right value ',negative
'return all the known job ids for this user based the optional filter conditions example usages curl return all the job ids submitted hsubramaniyan curl return all the job ids that are visible hsubramaniyan curl return all the job ids for hsubramaniyan after job curl return the first atmost job ids submitted hsubramaniyan after job curl return the first atmost job ids submitted hsubramaniyan after sorting the job list lexicographically supporting pagination using jobid and numrecords parameters step get the start jobid jobxxx numrecords step issue curl command specifying the userdefined numrecords and jobid step list obtained from step has size equal numrecords retrieve the lists last record and get the job the last record jobyyyk else quit step set jobidjobyyyk and step param fields fields set the request will return full details the job fields missing will only return the job currently the value can only other values are not allowed and will throw exception param showall showall set true the request will return all jobs the user has permission view not only the jobs belonging the user param jobid jobid present the records whose job lexicographically greater than jobid are only returned for example jobid job the jobs whose job greater than job are returned the number records returned depends the value numrecords param numrecords the jobid and numrecords parameters are present the top numrecords records appearing after jobid will returned after sorting the job list lexicographically jobid parameter missing and numrecords present the top numrecords will returned after lexicographically sorting the job list jobid parameter present and numrecords missing all the records whose job greater than jobid are returned return list job items based the filter conditions specified the user ',negative
'get delegation tokens from hcat server and store them into the job these will used publish partitions hcat normally when the jobtracker hadoop mapreduce starts supporting renewal ',negative
'use the hints later top level ',negative
'combine the column field schemas and the partition keys create the whole schema ',negative
'this set not empty means need generate separate task for collecting ',negative
'need merge isdirect flag input even the newinput does not have parent ',negative
'supplying using this enforces identityequls matching which will most probably make the signature very unique ',negative
'the the jobhandle used track the actual spark job ',negative
'the lack special token ',negative
'number digits mantissa ',negative
'max time when waiting for read locks node list ',negative
'all fractional digits become integer digits ',negative
'most likely the user specified invalid partition ',negative
'dummy vertex for mergejoin branch ',negative
'best attempt shouldnt really kill dag for this ',negative
'number nodes stack current mark ',negative
'whether there tag added the end each key and the tag value ',negative
'copy the hive conf into the job conf and restore the backend context ',negative
'add some data and nulls ',negative
'get the table objects for this batch table names and get iterator ',negative
'adjust arrays ',negative
'nodes one them column and the other numeric const ',negative
'can happen with virtual columns would add the column its output columns but would not exist the grandparent output columns exprmap ',negative
'number elements map cannot determined this value will used ',negative
'child should join for this happen ',negative
'the only aba problem care about have another buffer there ',negative
'adjustable ',negative
'assuming returns all schemes that are accessed task side well not need way get all the schemes that are accessed the tez taskllap ',negative
'figure out subquery expression columns type ',negative
'individual columns are going pivoted hbase cells and for each row they need written out order column name sort the column names now creating mapping their column position however the first ',negative
'setup run concurrent operations ',negative
'scalar and trivial evaluate ',negative
'test that read columns are initially empty list ',negative
'found some columns user specified schema which are neither regular not dynamic partition columns ',negative
'the job argument passed that configuration overrides can used initialize the metastore configuration the special case embedded metastore hivemetastoreuris ',negative
'create selectlistoi ',negative
'principalgrants ',negative
'expected error ',negative
'cannot merge ',negative
'genericudf stateful have make copy here ',negative
'the record reader from which the record originated already seen and valid need reencode the record ',negative
'parameters for exporting metadata table drop requires the use the ',negative
'print parent where data comes from ',negative
'set and table ',negative
'according the javadoc getmax can return this case default this will probably never actually happen ',negative
'this hash function returns the same result stringhashcode when all characters are ascii while texthashcode always returns different result ',negative
'end struct ',negative
'load data ',negative
'custom composite key class provided return null ',negative
'for hbase storage handler ',negative
'likewise ',negative
'sign with different key ',negative
'gen join between outer operator and ',negative
'see class comment about refcounts ',negative
'determine type udaf this the genericudaf name ',negative
'left larger ',negative
'',negative
'verify that the names match the partitioned clause ',negative
'set the link between mapjoin and parent vertex ',negative
'newer versions and later support offsetfetch ',negative
'repeating ',negative
'did read all the data ',negative
'read the altered via cachedstore altered user from user user ',negative
'srctxntowriteidlist ',negative
'the join columns which are also skewed ',negative
'this succeeds aborttxn idempotent ',negative
'copy the bigtable values into the overflow batch since the overflow batch may not get flushed here must copy value ',negative
'use hiveinputformat any the paths not splittable ',negative
'check whether the join can combined with any its children ',negative
'case repeating nulls ',negative
'optional alias the column external name ',negative
'walk over all the sources which are guaranteed reduce sink operators ',negative
'when splitupdate enabled can choose not write any delta files when there are inserts such cases only the deletedeltas would written they are closed separately below ',negative
'sort all the inputs outputs lock needs acquired any partition read lock needs acquired all ',negative
'the input filesinkoperator dynamic partition enabled the tsmerge input schema needs include the partition column and the fsoutput should have ',negative
'note hive ast rowsrangephysical range values logical ',negative
'username ',negative
'',negative
'check permutation project ',negative
'update maxlength length greater than the largest value seen far ',negative
'instantiated ',negative
'array null column values input objectinspectors ',negative
'value becomes zero for rounding beyond ',negative
'format the storage format statements ',negative
'dynamic partition usecase partition values were null not all were specified need figure out which keys are not specified ',negative
'longer best match more than one ',negative
'just bitwiseor the bits together size functions should the same ',negative
'this restricts macro creation privileged users ',negative
'for now for simplicity are doing just one directory one database come back use multiple databases once have the basic flow chain creating tasks place for database directory ',negative
'disk ',negative
'reach runlength here use the previous length and reset runlength ',negative
'were moving files around for acid write then the rules and paths are all different ',negative
'only consider the materialized view outdated forceoutdated true rebuild otherwise passed the test and use ',negative
'create ',negative
'for leaf dont anything ',negative
'verify cmrecyclepath api moves file cmroot dir ',negative
'default location hiveserver ',negative
'there are multiple aliases source not know how merge ',negative
'alternate ',negative
'flush memory limits were reached ',negative
'now only try the first partition the first partition doesnt contain enough size change normal mode ',negative
'await future result with timeout check the abort field occasionally its possible that the interrupt which comes along with abort suppressed some other operator ',negative
'this event can never occur does fail ',negative
'when either name value null the set method below will fail and throw ',negative
'there will not any tez job above this task ',negative
'the following data used compute partitioned tables ndv based partitions ndv when true global ndvs cannot accurately derived from partition ndvs because the domain column value two partitions can overlap there overlap then global ndv just the sum partition ndvs upperbound but there some overlay then global ndv can anywhere between sum partition ndvs overlap and same one the partition ndv domain column value all other partitions subset the domain value one the partition lowerboundbut under uniform distribution can roughly estimate the global ndv leveraging the minmax values and also guarantee that the estimation makes sense comparing the upperbound calculated sumnumdistincts and lowerbound calculated maxnumdistincts ',negative
'dump and load only truncate records ',negative
'same string ',negative
'none ',negative
'weve got what need mark the queue ',negative
'the keyhash missing the bloom filter then the value cannot exist any the spilled partition return nomatch ',negative
'windowing spec include the list further will examine its children ast nodes check whether there are aggregation functions within ',negative
'original bucket files should stay until cleaner kicks ',negative
'right trim and truncate slice byte array maximum number characters and place the result into element vector ',negative
'children ',negative
'the child selectoperator does not have the columnexprmap not need update the columnexprmap the parent selectoperator ',negative
'far same javamathbigdecimal but the scaling below specific ansi sql numeric ',negative
'',negative
'this needed for tracking the dependencies for inputs along with their parents ',negative
'repartition new number splits ',negative
'directly deserialize with the caller reading fieldbyfield the lazysimple text serialization format the caller responsible for calling the read method for the right type each field after calling readnextfield reading some fields require results object receive value information separate results object created the caller initialization per different field even for the same type some type values are reference either bytes the deserialization buffer other type specific buffers those references are only valid until the next time set called ',negative
'match ',negative
'make sure map task environment points ',negative
'first parse the view query and create the materialization object ',negative
'represents collection rows that acted upon tablefunction windowfunction ',negative
'testing multibyte string with reference starting mid array ',negative
'group column found ',negative
'one the predicates then any other predicate with illegal add residual ',negative
'over all the destination tables ',negative
'second incremental ',negative
'schemaversion ',negative
'this compressed buffer need uncompress the buffer can comprise several disk ranges might need combine them ',negative
'simple rownum offset and rownum offset limit wont work will return nothing ',negative
'verify that there notifications available yet ',negative
'check each exprnodedesc ',negative
'move files ',negative
'update the connection ',negative
'nonjavadoc see javaneturl ',negative
'two cases srcs has only src directory rename src directory destf also need copymove each file under the source directory avoid delete the destination directory the root hdfs encryption zone srcs must list files ensured ',negative
'curreader closed for only need footer buffer info from prereader ',negative
'wrap the inner parts the loop catch throwable that any errors the loop dont doom the entire thread ',negative
'take the empty buffer out the free list ',negative
'start heartbeat thread ',negative
'get the transactional tblproperties value ',negative
'duplicate names this should ',negative
'create task for this local work right now this local work shared ',negative
'only distinct nodes that are not part the key should added distexprnodes ',negative
'have just locked buffer that wasnt previously locked ',negative
'can use alter table partition rename convertnormalize the legacy partition column values should not enable the validation the old partition spec passed this command ',negative
'time which needs thread protected ',negative
'boolean long done with identityexpression boolean double done with standard long double cast see for remaining cast vectorexpression classes ',negative
'one element ',negative
'for now bigint going cast double throw error warning ',negative
'distinct value estimator ',negative
'since are using thrift part will not have the create time and last ddl time set since does not get updated the addpartition call likewise part and part set correctly that equals check doesnt fail ',negative
'removes tasks from the runninglist and sends out preempt request the system subsequent tasks will scheduled again once the deallocate request for the preempted ',negative
'update the existing row newlyconverted acid table ',negative
'single long value map optimized for vector map join ',negative
'operators for which there chance the optimization can applied ',negative
'could not find common category return null ',negative
'temporary till the external interface makes use single connection per instance ',negative
'make sure that the table alias and column alias are stored the column info ',negative
'these tests inherently cause exceptions written the test output logs this undesirable since you might appear someone looking the test output logs something failing when isnt ',negative
'note this relies the fact that always evict the entire column have the column data assume have all the streams need ',negative
'reserve spaces for the byte size the map which integer and takes four bytes ',negative
'are overwriting disable existing sources ',negative
'skip daytime part both dates are end the month ',negative
'nonjavadoc see ',negative
'count the size for cost estimation later ',negative
'just created top level node for this jobid ',negative
'unique registered based submit response theoretically could get ping when the task valid but havent stored the unique yet tasknodeid null however the next heartbeats should get the value eventually and mark task alive ',negative
'hmm why dont many other operations here need locks ',negative
'can bail out ',negative
'saslkerberos properties ',negative
'for innersemi join for left outer join for right outer join ',negative
'optimize assuming that repeating listmap will run from from lengths the child vector sanity check the assumption that can start ',negative
'callback method used subclasses set the rawinputoi the evaluator ',negative
'optional string dagname ',negative
'reattempts are left upto the rpc layer theres failure reported after this mark all attempts running this node killed the node itself cannot killed from here thats only possible via the scheduler the assumption that theres failure communicate with the node will eventually timeout and more tasks will allocated ',negative
'now that have found real data emit sign byte necessary and negative fixup ',negative
'verify that new job requests have issues ',negative
'since warehouse path nonqualified the database should located second filesystem ',negative
'the rowid column string ',negative
'found suitable join keys add them key list ensuring that there nonequi join predicate appears the end the key list also mark the null filtering property ',negative
'used with primitive types ',negative
'metastore stats unavailable fallback old way ',negative
'age ',negative
'elt special case because can take variable number arguments ',negative
'varchar test ',negative
'reconfigure logj after settings via hiveconf are write into system properties ',negative
'try using both permanent functions ',negative
'need add all the estimates from the siblings this reduce sink ',negative
'does breadth first traversal the tasks ',negative
'',negative
'emptybuckets true ',negative
'generate single split typically happens when reading data out order queries order query returns rows files will exists input path ',negative
'todo ideally this should only need send the taskattemptid everything else should inferred from this passing parameters until theres some dag information stored and tracked the daemon ',negative
'the null union type should removed ',negative
'this percentage value between and ',negative
'required required optional optional optional optional ',negative
'keyvalueseparator seen all bytes belong key and ',negative
'todo these can eventually used replace ',negative
'want message sent when session commits thus run transacted mode ',negative
'note this sets loadfiletype incorrectly for acid that relevant for load ',negative
'has been rewritten apply rules postdecorrelation ',negative
'since updates the childoperators and parentoperators list place need make copy list iterator over them ',negative
'values should unique given how the checking and addormerge ',negative
'enums are one two avro types that hive doesnt have any native support for ',negative
'mssql specific parser ',negative
'lateral view outer not supported cbo ',negative
'todo ideally want tez use callablewithmdc that retains the mdc for threads created thread pool for now will push both dagid and queryid into ndc and the custom thread pool that use for task execution and llap will pop them ',negative
'define constants and local variables ',negative
'case broadcastjoin read the broadcast edge inputs possibly asynchronously ',negative
'extract name will need afterwards ',negative
'now check for overflow ',negative
'existence ',negative
'config parameter that suggests hcat that metastore clients not cached default false this parameter allows highlyparallel hcat usescases not gobble too many connections that ',negative
'remove the entry theres nothing left the specific priority level ',negative
'should still able get the session ',negative
'set values needed for numeric arithmetic udfs ',negative
'hasnext implies there some column the batch ',negative
'can not queue more requests queue full ',negative
'convert integer value milliseconds since the epoch timestamp value for use long column vector which represented nanoseconds since the epoch ',negative
'miscellaneous errors range ',negative
'found ugi perform doas ',negative
'check any right pair exists for left objects ',negative
'gen calcite plan ',negative
'test for valid values for both ',negative
'now working should sorted like delta delta delta delta for example and want end with the best set containing all relevant data delta delta ',negative
'use this copy method when the source batch may get reused before the target batch finished any bytes column vector values will copied the target value into the columns data buffer ',negative
'get array utf byte arrays from array strings ',negative
'and then compare the two tables ',negative
'special case date requires its own specific betweendynamicvalue class but derives from ',negative
'others should kept ',negative
'after that inputop the parent selop ',negative
'not real field ',negative
'initialize metrics first some metrics are for initialization stuff ',negative
'the maximum column length mfieldschemafname ',negative
'nonjavadoc see ',negative
'try widening conversion otherwise fail union ',negative
'look through the file with rows selected ',negative
'select none child none child and none ',negative
'rowmode the expected value ',negative
'move all data from destsequencefile dest ',negative
'split join condition ',negative
'register that have visited this operator this rule ',negative
'the following complex type for special handling ',negative
'mixture input columns and new scratch columns for the aggregation output ',negative
'verify that are indeed doing acid write import ',negative
'dbpatterns ',negative
'thru the blocks add stuff results and prepare the decompression work see below ',negative
'the lowest digit power ',negative
'arrange result has longer digit tail and lines will shift the shift digits our addition and them into the result ',negative
'this has state cant cached ',negative
'not want the start group clear the storage ',negative
'next locate all the iterate methods for each these classes ',negative
'set the config value empty string which should result all catalogs being cached ',negative
'create and put hiverc sample file default directory ',negative
'this purely for testing convenience has bearing operations such list ',negative
'ssl settings ',negative
'get the distinct values the group fields and the arguments the agg functions ',negative
'catchall due some exec time dependencies session state that would cause otherwise ',negative
'are revoking another duck dont wait could also give the duck ',negative
'todo lossy conversion distance considered seconds ',negative
'add new operator cache work group existing operator group exists ',negative
'store the given token the ugi ',negative
'calculate the parameters ',negative
'get the parent victimrs ',negative
'columncount ',negative
'for cast constant operator all members the input list and return new list containing results ',negative
'insert reduce side with reduce side input ',negative
'only one distinct aggregate and one more nondistinct aggregates ',negative
'select all with not null child none child and null with and then expect the child not invoked ',negative
'positions variable arguments columns nonconstant expressions ',negative
'just can get the output type ',negative
'reads through undesired field data values are valid after this call designed for skipping columns that are not included ',negative
'these members have information for deserializing row into the vectorizedrowbatch columns say source because when there conversion are converting deserialized source into target data type ',negative
'partition columns are appended end only care about stats column ',negative
'createdat ',negative
'list doubles ',negative
'not want the end group cause checkandgenobject ',negative
'when there are partition and order clauses will have different partition expressions ',negative
'have remember ',negative
'the character set for encoding constant can optimize that ',negative
'this time even more inaccurate ',negative
'replace the distinct with the count aggregation ',negative
'for hive script operator ',negative
'technique ',negative
'since wont able update this add for now estimate usage this shouldnt much and this cache should remove later anyway ',negative
'for nonllap mode most these are not relevant only used shared scan optimizer ',negative
'since row mode takes everyone ',negative
'does need additional job ',negative
'todo set correct vendorcode field ',negative
'the current expression node does not have virtualpartition column ',negative
'create ',negative
'the current char will written out later ',negative
'examine the last child could alias ',negative
'returnafteruse will take care this ',negative
'decimal longwords ',negative
'not need anything the expression ',negative
'extraction need parsed ',negative
'the type cast udf for parameterized type then should implement the settableudf interface that can pass the params not sure this the cleanest solution but there does need way ',negative
'list struct ',negative
'valid merge register set size gets bigger dense automatically ',negative
'looks like subq plan ',negative
'should used for either sort merge join bucket map join ',negative
'well ',negative
'primitive ',negative
'char not between ',negative
'generate map join task for the big table ',negative
'output type intervaldaytime ',negative
'property speficied file found local file system use the specified file ',negative
'same commitdroptable where always delete the data accumulo table ',negative
'callers duplicate the buffer they have for bufferchunk dont have ',negative
'all must selected otherwise size would zero repeating property will not change ',negative
'requires that current directory exists ',negative
'the isnull check and work has already been performed ',negative
'copy the current object contents into the output only copy selected entries ',negative
'check for delegation token present add the header ',negative
'preserving the old logic hmm ',negative
'have already explored the stack deep enough but not have matching ',negative
'for nway join only spill big table rows once ',negative
'nonjavadoc see javalangstring ',negative
'update aggregate partition column stats for table cache ',negative
'add udaf ',negative
'datetime type isnt currently supported ',negative
'',negative
'helper classes for connparam comparison logics ',negative
'handle case with nulls dont function the value null save time because calling the function can expensive ',negative
'should constant column ',negative
'delete remaining directories for external tables can affect stats for following tests ',negative
'note are creating brand new the partition this going valid for acid ',negative
'than the configured the header size ',negative
'eat ',negative
'node exists throw exception ',negative
'part part ',negative
'for locks associated with txn always heartbeat txn and timeout based that ',negative
'clean time out locks from the database not associated with transactions locks for readonly autocommittrue statements this does commit and thus should done before any calls heartbeat that will leave ',negative
'only creates the expiration tracker expiration configured ',negative
'look for row like info query ',negative
'find operators work ',negative
'adding rowid field ',negative
'check owid outside the range all owids present ',negative
'also has one outer join filter associated with ack which making ',negative
'add dummy node cache partnames tabparttabpart ',negative
'testperformancetest ',negative
'its wrapped toplevel select star query skip ambiguity check for backward compatibility ',negative
'initialize any entries that could used output vector have false for null value ',negative
'should since the lock ephemeral and will eventually deleted when the query finishes and zookeeper session closed ',negative
'use for maponly merging ',negative
'there project top due nullability ',negative
'running example ',negative
'any sensible way for now the lock going epic ',negative
'does the row match the pattern represented this symbolfunction ',negative
'matchstats for each candidate ',negative
'temporarily holds location exponent string ',negative
'bytes remaining the current chunk data ',negative
'does not add back task here because back task should the same type the original task ',negative
'for equalpriority rules user rules come first because they are more specific then apps ',negative
'make sure the dpp sink has output for all the corresponding part columns ',negative
'now get nonexistant entry ',negative
'tests multimap structure ',negative
'rollback done for the benefit postgres which throws sqlstatep errorcode you attempt any stmt txn which had error ',negative
'the configuration ',negative
'not reducesink skip ',negative
'optimize whole decimal fits one binary word ',negative
'need subtract nwinext would the next write allocated but need highest allocated write ',negative
'more files for current bucket ',negative
'the time and number counters become available only after the ',negative
'add minmax and bloom filter aggregations ',negative
'some cfcq ',negative
'limit reached batchsize reduces whichever comes earlier ',negative
'move files one one because source subdirectory destination ',negative
'hadoop only expects username query param not form param post request for backwards compatibility this logic get username when its sent form parameter this added hive and should desupported ',negative
'scan ',negative
'the number partitions aggregated per cache node the number partitions requested this value well fetch directly from metastore ',negative
'fkcolumnname ',negative
'',negative
'ctas with nonacid target table ',negative
'cannot static because default unique perinstance ',negative
'fractional part powered too high ',negative
'there ptf between crs and prs cannot ignore the order direction ',negative
'buckets means turn off bucketing ',negative
'same thing might deleted other nodes just ',negative
'launch upto maxthreads tasks ',negative
'allownull ',negative
'',negative
'read the first row parquet data page this will only happened once for this instance ',negative
'return true for exprnodecolumndesc ',negative
'not nullable union ',negative
'call open read data split mockmocktable call split mockmocktable call open read data split mockmocktable call split mockmocktable call read footer split mockmocktable get offset since its original file ',negative
'check second most significant part ',negative
'always need call reset the codec ',negative
'find the list scripts execute for this upgrade ',negative
'for now allow only createview with select with grant ',negative
'add additional overhead each map entries ',negative
'and now wander straight into the swamp when instead adding subtract from utc midnight supposedly get local midnight the above case utc course given ',negative
'',negative
'are done reading batch send consumer for processing ',negative
'try genericudf translation ',negative
'correct version stored metastore during startup ',negative
'there should calls create partitions with each batch size ',negative
'should not happen ',negative
'dynamic allocation enabled numbers for memory and cores are meaningless dont try get ',negative
'when longer assume the caller will default with nulls etc ',negative
'for small tables only get the big table position first ',negative
'note the list expected few items its longer may want ihm ',negative
'inject behaviour where throws exception insert event found dynamically add partition through insert into cmd should just add addpartition event not insert event ',negative
'column aliases defined query for lateral view output are duplicated ',negative
'then serialize again using hrsd and compare results ',negative
'show table level privileges ',negative
'assert values retrieved ',negative
'inject behaviour where some events missing from notificationlog table ',negative
'simply need remember that weve seen event operator ',negative
'add window functions ',negative
'path must reused ',negative
'bytes necessary store extra bits the second timestamp storing timestamp ',negative
'update table level column stats ',negative
'current joinoperaotr will stop traverse the tree when any parent reducesinkoperaotr this joinoperator not considered correlated reducesinkoperator ',negative
'',negative
'sum input decimal and output decimal any mode partial partial final complete ',negative
'addpartition event partitioned table ',negative
'task failed ',negative
'called one more times the client and ',negative
'must deterministic order map for comparison across java versions ',negative
'patch the optimized query back into original ctas ast replacing the original query ',negative
'this works because logically need lock nonacidorctbl read and lock write but ',negative
'check the bucketing andor sorting columns were inferred ',negative
'nonjavadoc see javasqlrowid ',negative
'mutator created ',negative
'try either yyyymmdd integer representing days since epoch ',negative
'',negative
'move the intermediate archived directory the original parent directory ',negative
'process the last byte ',negative
'either inittxnmgr from the sessionstate that order ',negative
'todo look repeating optimizations ',negative
'the partition spec not present ',negative
'non acid txn managers dont support txns but fwd lock requests lock managers acid txn manager requires all locks associated with txn end here open txn its because are processing something like use database which definition needs locks ',negative
'',negative
'columnreference node column the input row ',negative
'new threads ',negative
'operations involvingreturning yearmonth intervals ',negative
'estimate number reducers ',negative
'this type information specifies the data types the partition needs read ',negative
'read totalseconds nanos from datainput ',negative
'nonjavadoc see javasqldate ',negative
'filter will executed for constant ',negative
'handle repeated join key found ',negative
'stop the composite service ',negative
'holds the root the operator tree were currently processing this could table scan but also join ptf etc ',negative
'load the connection url properties from hivesitexml present the classpath ',negative
'extract drop privileges ',negative
'check the partitions exist the sourcetable ',negative
'not necessary here cause noone will looking these after set them for clarity ',negative
'ifexists ',negative
'need initialize those muxoperators first because first initialize other operators the states all parents those muxoperators are init including this demuxoperator but the inputinspector those muxoperators has not been set ',negative
'date part ',negative
'neginfinity end inclusive ',negative
'statement ',negative
'prevent instantiation ',negative
'set the table where were writing this data ',negative
'',negative
'stop ',negative
'now have probe the global hash and findorallocate ',negative
'populate reduce task ',negative
'for scale the minimum ',negative
'newcols are not specified use default ones ',negative
'find out the nullbytes ',negative
'its partial line continue collecting the pieces ',negative
'source ',negative
'tez can handle unpopulated buckets ',negative
'',negative
'assumption this point parse tree gen resolution will always true since started out that way ',negative
'instantiation ',negative
'populate stage ',negative
'this dummy assigner ',negative
'the trailing zeroes extend into the integer part only want eliminate the fractional zero digits ',negative
'for ctas runs late make table acid the initial write ends running nonacid ',negative
'semi join specific members ',negative
'join conditions ',negative
'',negative
'nonjavadoc see ',negative
'return true this one small set functions for which significantly easier use the old code path vectorized mode instead implementing new optimized vectorexpression depending performance requirements and frequency use these may implemented the future with optimized vectorexpression ',negative
'log exception this produces enough text force new logfile ',negative
'returncode ',negative
'get the power turn digits into the desired decimal with possible fractional part ',negative
'because use hives string type when avro calls for enum have expressly check for enumness ',negative
'the difference larger than then definitely larger than power increment ',negative
'once drop support for old hadoop versions change these getbytes and getlength fix the deprecation warnings not worth shim ',negative
'assert that and are not the same within epsilon tolerance ',negative
'nonjavadoc see ',negative
'check immediately after reducer assigned cae the abort came during ',negative
'use the rowresolver from the input operator generate input objectinspector that can used initialize the udtf then the resulting output object inspector can used make the rowresolver ',negative
'exchange operator introduced later make sort operator create new stage for the moment ',negative
'update this information sparkworkmap ',negative
'build operator ',negative
'oldernode tasks proactively for now let the heartbeats fail them ',negative
'multitable inserts not supported ',negative
'then just look the other locks ',negative
'nothing ',negative
'zone zone should already have been checked for nulls ',negative
'sanity check ',negative
'use varchars text field directly ',negative
'modify table schema add columns ',negative
'this method does not depend setting ',negative
'typedesc ',negative
'partition spec node present set partition spec ',negative
'undone ',negative
'test for publish with missing partition key values ',negative
'more accurate information about the original ndv the column before any filtering ',negative
'have replication state record for the obj allow replacement ',negative
'read the type ',negative
'column stats for group column ',negative
'read the second flush and make sure see all rows ',negative
'not used the direct access client native vector map join ',negative
'this can happen only top most limit not while visiting limit operator since that can within subquery ',negative
'execdriver well have proper local properties ',negative
'need skip seek here index wont used anymore ',negative
'had the entire pool other list couldnt exist exhausted the entirepoolsized list ',negative
'plans ',negative
'memory hashmap stores small table keyvalue pairs stores big table rows ',negative
'row null for delete events ',negative
'missing database name the query ',negative
'for the generation the values expression just get the inputs ',negative
'resultexpression select list with the following variation the select keyword optional the parser checks the expression doesnt start with select not prefixes window clauses are not permitted expressions can operate the input columns plus the psuedo column path which array structs the shape the struct the same the input ',negative
'check can convert map join bucket scaling ',negative
'this implementation vectorized join delegating all the work the rowmode implementation hijacking the big table node evaluators and calling the rowmode join processop for each row the input batch since the join operator not fully vectorized anyway atm due the use rowmode smalltables this reasonable tradeoff ',negative
'followed call ',negative
'remove local copy hdfs location from resource map ',negative
'all together there only one security check ',negative
'get the application the spark app ',negative
'testtable ',negative
'nonjavadoc see javasqlclob ',negative
'were replication scope its possible that were running the export long after the table was dropped the table not existing currently being different kind table not error simply means should noop and let future export capture the appropriate state ',negative
'involving local file system ',negative
'flip column references join condition specified reverse order join sources ',negative
'for simpler access make these members protected instead providing get methods ',negative
'nonjavadoc see ',negative
'only support single expression when its udtf ',negative
'rehome location now that know the rest the partvals table table liststring partitioncols new arrayliststring ',negative
'create ',negative
'even table location specified table creation should fail ',negative
'verify table for key long hash table hashmultiset ',negative
'use construct ',negative
'only allow one single ',negative
'walk through udaf collect distinct info ',negative
'and round may cause exceed our precisionscale ',negative
'start with capacity make sure expand every put ',negative
'uri ',negative
'set value for the union type ',negative
'create the parent znodes recursively ignore the parent already exists ',negative
'myint ',negative
'uninitialized bucket ',negative
'for dynamic partitioned hash join assuming table split evenly among the reduce tasks ',negative
'note specify dynamic partitions desttab for writeentity ',negative
'see rowlength javadoc ',negative
'called the beginning the compile phase have another chance optimize the operator plan ',negative
'datapath ',negative
'multikey hash map based the ',negative
'last partial batch ',negative
'',negative
'all columns are dynamic nothing ',negative
'multiple children ',negative
'set the locations starting from startindex and wrapping around the sorted array ',negative
'determine the big table retained mapping first can optimize out with projection copying inner join big table keys the subsequent small table results section ',negative
'internal vars ',negative
'codahale just include the pool name the counter name ',negative
'probably cross product ',negative
'should noop since exists ',negative
'integer value was interpreted timestamp inconsistently milliseconds comparing floatdouble seconds since the issue exists for long time and some users may use such inconsistent way use the following flag keep backward compatible the flag set false integer value interpreted timestamp milliseconds otherwise its interpreted timestamp seconds ',negative
'',negative
'mouthful but safe guaranteed have atleast destination dont support multi insert picking the first dest ',negative
'adding oracle jdbc driver exists ',negative
'cvalue map rowvalues assertequals cvaluesize map mapval map assertequals mapvalsize mapval map assertequals mapvalsize ',negative
'ensure only one final event ever sent ',negative
'notify any queries waiting this cacheentry become valid ',negative
'looks like owner unsupported type ',negative
'verify drop partition nonexisting partition idempotent and just noop ',negative
'get here know that weve archived the partition files but they may the original partition location the intermediate original dir ',negative
'add not null constraints ',negative
'has use full name make sure does not conflict with ',negative
'use toolrunner files option could considered here ',negative
'build tokinsert tokdestination ',negative
'continue with next table ',negative
'return the maximum absolute decimal value for precision ',negative
'generate the columns according the column mapping provided note the generated column names are same the the qualifier name null each column familynamecoli where the index the column ranging from where the size the column mapping the filter function removes any special characters other than alphabets and numbers from the column family and qualifier name the only special character allowed column name which used separator between the column family and qualifier name ',negative
'set first argument boolean flag ',negative
'cli ',negative
'workers run concurrently ',negative
'now compact ',negative
'check the pruner only contains partition columns ',negative
'need for overflow checks assume selectivity always ',negative
'the union the first time seen set current task genmrunionctx ',negative
'requesting less partitions than allowed should work ',negative
'position the cursor line ',negative
'',negative
'need execute this stage ',negative
'can happen that although therere some partitions memory but their sizes are all ',negative
'the join key table column create the exprnodedesc based this column ',negative
'result ',negative
'the dispatcher fires the processor corresponding the closest matching ',negative
'create new sort operator the corresponding input ',negative
'they set ifnotexists check for existence first and bail exists this ',negative
'return table name for column name column has been specified ',negative
'hive renumber the position after remove such that getposition column always returns value between schemasize ',negative
'will create the folder does not exist ',negative
'cant push predicate delete delta were push delete delta wed filter out all rows since the row always null for delete events and wed produce data the delete never happened ',negative
'set the timezone the object mapper ',negative
'append the third group within pattern ',negative
'can just restart the session have received one ',negative
'bgenjjtree commaorsemicolon ',negative
'handle file format check for table level ',negative
'implicit conversion accepting the source type and putting the same target type columnvector type ',negative
'safety limit for potential list bugs ',negative
'look for tables but not find any ',negative
'operator can use the same ',negative
'end additional steps ',negative
'overwhelmingly executes once maybe twice replacing stale value ',negative
'store char type stripped pads ',negative
'skip this and rest cmds the line ',negative
'the algorithm looks all the mapjoins the operator pipeline until hits and for each mapjoin examines has paralllel semijoin edge dynamic partition pruning ',negative
'run the optimizations that use stats for optimization ',negative
'consolidation for outer joins ',negative
'date ',negative
'max time when waiting for write locks node list ',negative
'random batchsize unique ordered integers indices this could smarter ',negative
'give pass optionally have literaldelegate provide getliteralclass check ',negative
'this not the datanucleus but the assigned the sequence ',negative
'preserve interrupt status ',negative
'note columnids below makes additional changes for acid dont use this var directly ',negative
'basic case ',negative
'skip default directory only all value false ',negative
'check the metastore key set first ',negative
'inputs are not the same bail out ',negative
'time ',negative
'are only processing partition reset our evaluators ',negative
'the value before the value record offset make byte segment reference absolute ',negative
'for now exclude char until determine why there difference blank padding serializing with and the regular serde ',negative
'skewedcolnames ',negative
'run rule fix windowing issue when done over ',negative
'bail out not enabled for rewriting ',negative
'merging from targetinner nodeouter ',negative
'linkedhashmap provide the same iteration order when selecting random host ',negative
'found the class this would hadoop version newer see hadoop hadoop ',negative
'the object for storing row data ',negative
'the time has come ',negative
'required required required required required required required ',negative
'interface for the case where there explicit name for the function ',negative
'the object reused during evaluating make copy here ',negative
'this exception indicates that code record could not parsed and the caller can decide whether drop send dead letter queue rolling back the txn and retrying wont help since the tuple will exactly the same when its replayed ',negative
'nulls left nulls right ',negative
'drop first and then create ',negative
'set result the quotient ',negative
'multiple file sink descriptors are linked use the task created the first linked file descriptor ',negative
'the row consists some string columns some arrayarrayint columns ',negative
'the type info each column being assigned ',negative
'remove values key exprs schema for value already fixed ',negative
'when have decimal the input parameter then have see there special vector udaf for not will need convert the input parameter ',negative
'filterg stuff that way this method and byfilter would just merged ',negative
'there only one ',negative
'set will only allocate memory the buffer result smaller than ',negative
'skip since not selected query ',negative
'verification passed encode the reply ',negative
'nothing find for this type ',negative
'even reduction lets still test the original predicate see was already constant which case dont need any runtime decision ',negative
'zookeeper related configs ',negative
'get hive aggregate info ',negative
'for each entry collection retrieve skewed column retrieve skewed map ',negative
'compose file text ',negative
'determine the default encoding type specified the table the global default none was provided ',negative
'add constraints need not deep retrieval the table column descriptor while persisting the constraints since this transaction involving create table not yet committed ',negative
'status has not changed continue waiting ',negative
'since have done exact match tsselgbyrsgbyselfs ',negative
'table operations ',negative
'dont want that ',negative
'field look for the record identifier field inside recid look for row field inside recid look for original write field inside recid look for bucket for the original row for the record identifier struct for the long row inside the recordidentifier for the original write inside the record identifer ',negative
'push down current ppd context newly added filter ',negative
'data ',negative
'granularity ',negative
'string length should work after readfields ',negative
'set min possible value ',negative
'operator wants some work the end group ',negative
'move clock forward and request task ',negative
'get outputfieldois ',negative
'for column column only toss date and intervalyearmonth ',negative
'verify create table not called table but called for and ',negative
'not known estimate that based the number entries ',negative
'convert the stub from the configuration back into normal token ',negative
'add props from params set table schema ',negative
'this has called before initializing the instance rawstore ',negative
'should not use this optimization sorted dynamic partition optimizer used ',negative
'statementexecute after resultsetclose should fine too ',negative
'first get the columns named columns ',negative
'then update pool allocations ',negative
'format the row format statement ',negative
'there should original bucket files and plus one delta directory and one deletedelta directory when splitupdate enabled update event split into combination delete and insert that generates the deletedelta directory the delta directory should also have bucket files bucket and bucket ',negative
'bytes per stripe bytes per split ',negative
'the given filtercondn refers only table alias the qbjointree return that aliass position otherwise return ',negative
'check there are compactions requests left ',negative
'ival ',negative
'precalculated offset values for each alias ',negative
'used ',negative
'skip skipsize rows batch ',negative
'sparse map ',negative
'erase both headers the blocks merge ',negative
'given the previous range and the current range calculate the new sum from the previous sum and the difference save the computation ',negative
'literal bigint ',negative
'list alrady loaded containers number partitions each table should have the partition spilled next ',negative
'configure the authfilter with the kerberos params iff security ',negative
'converts amt days milliseconds ',negative
'lock are examining exclusive ',negative
'remove the proxy privilege and the auth should fail reality the proxy setting should not changed the fly ',negative
'current transaction ',negative
'create schema with serde then remap ',negative
'scalarscalar ',negative
'recompose filter possibly pulling out common elements from dnf ',negative
'parse out words the sentence ',negative
'constructing the default mapredwork ',negative
'teseted oracle database express edition release bit production ',negative
'escaping ',negative
'todo expose this operation client useful for streaming api abort all remaining trasnactions batch ioexceptions caller must rollback the transaction not all transactions were aborted since this will not attempt delete associated locks this case param dbconn active connection param txnids list transactions abort param maxheartbeat value used link performtimeouts ensure this doesnt abort txn which were hearbetated after performtimeouts select and this operation param isstrict true for strict mode false for besteffort mode strict mode all txns are not successfully aborted then the count updated ones will returned and the caller will roll back besteffort mode will ignore that fact and continue deleting the locks return number aborted transactions throws sqlexception ',negative
'retrieve enabled not null constraint from metastore ',negative
'digits digits ',negative
'describeexplainshow commands ',negative
'repeating null value ',negative
'add another column ',negative
'need update the status the creation signature ',negative
'apply partition pruning ',negative
'operations that require insertdelete privileges ',negative
'evaluate the value ',negative
'get structfields for bucketed cols ',negative
'see sessioninitcontext javadoc ',negative
'only user belonging admin role can create new roles ',negative
'for the case when the output can have null values follow the convention that the data values must for long and nan for double this prevent possible later zerodivide errors complex arithmetic expressions like col col the case when some col entries are null ',negative
'insert the new task between current task and its child ',negative
'tracks running and queued allocated tasks cleared after task completes ',negative
'matches only forwardoperators which are reducers and are followed groupbyoperators ',negative
'process the row batch that has less than defaultsize rows ',negative
'add self the end the queue ',negative
'for each task completion event get the associated task job ',negative
'vcontextenvironment ',negative
'just last one ',negative
'filtering for outer join just removes rows available for hash table matching ',negative
'hiveconf ',negative
'test basic assign vector ',negative
'all the other cases can not merge ',negative
'this will true node was examined the vectorizer class ',negative
'the only allowed nonoverlapping option extra bytes the end ',negative
'normal check stoptimer works ',negative
'unsupported type ',negative
'pick random avail port ',negative
'the jdoexception the nucleus exception may wrapped further metaexception ',negative
'processor creation ',negative
'gbyrsgbyrsgby top bottom ',negative
'not already part the groupby ',negative
'tests for the partition partition method ',negative
'update the subcache ',negative
'this get should fail because its variance way past maxvariance ',negative
'strict admin check allows only set true when set true checks true and the logged user via pam spnego kerberos list ',negative
'look for the nonescapedsemicolon the query text not the table name which comes the result ',negative
'process method call ',negative
'optional string user ',negative
'drop testdatabaseops via objectstore ',negative
'right side ',negative
'create the column expr map ',negative
'first separated substring will txnid and the rest are ',negative
'create the routes group ',negative
'ignorenot needed but useful for testing ',negative
'pkname ',negative
'scale down rounding clear fraction ',negative
'',negative
'exclusive ',negative
'set that the row the mutation ',negative
'the output arrays start index for output evaluator aggregations ',negative
'need pointer the hash map since this class must static support having ',negative
'base object inspector start column number number columns ',negative
'complete first request second pending request should through ',negative
'alter database set property ',negative
'add selectop match the schema ',negative
'get names all tables under this dbname ',negative
'populate semijoin select needed ',negative
'hack off the last word and try again ',negative
'move the next root node ',negative
'use linkedhashmapstring operator extends operatordesc getaliastowork should not apply this for nonnative table ',negative
'check see the directory already exists before calling mkdirs because the file system readonly mkdirs will throw exception even the directory already exists ',negative
'copy the source files cmroot the client will move the source files another location should make copy the files cmroot instead moving ',negative
'addoverride properties found from hivesite with userspecific properties ',negative
'should only used ',negative
'theres usually nothing escape will optimistic ',negative
'order fix hive ',negative
'test that timestamp arithmetic done utc and then converted back local timezone ',negative
'when have splitupdate and there are two kinds delta directories the deltaxy directory one which has only insert events and the deletedeltaxy directory which has only the delete events the clever thing about this kind splitting that everything the deltaxy directory can processed base files however this left out currently improvement for the future ',negative
'loginfodiscover ptns called ',negative
'case the property the conf will not set ',negative
'exit the loop and check next lock ',negative
'let yarn pick the queue name isnt provided hivesite via the commandline ',negative
'theres authentication then directly substitute the user ',negative
'the joins have been automatically converted mapjoins however the joins were converted sortmerge joins automatically they should also tried mapjoins ',negative
'create single insert delta with rows with rowids per original transaction ',negative
'because inverse scaled ',negative
'would still empty because stats are actually populated ',negative
'lazysimple seems throw away everything but and ',negative
'queries rejected from being cached because they exceeded the max cache entry size ',negative
'just retrieve value from conf ',negative
'hybrid strategy ',negative
'dont redisplay warnings have already seen ',negative
'line table not found tablename ',negative
'save usedcacheentry ensure reader released after query ',negative
'all the data comes from disk the reader may have split into multiple slices also possible theres data the file ',negative
'ensure that the table properties were copied ',negative
'should initialize the serde with the typeinfo when available ',negative
'there can error ',negative
'are unsecure mode ',negative
'sent shoulddietrue ',negative
'',negative
'all children are descendants ',negative
'check bucketsort cols ',negative
'the first time see big key this key not the last table the last table can always streamed define that get ',negative
'assumes value written after key ',negative
'partitionkeys added order ',negative
'result ',negative
'for map check for virtual columns ',negative
'trailing zeroes rounding ',negative
'concurrenthashmap does not allow null use substitute value ',negative
'exchange multiple partitions using partial partitionspec only one partition column ',negative
'get tag value from object last list ',negative
'reached here either and are both nulls and are both not nulls this internal error and should not let this happen throw exception ',negative
'create row file copy and empty copy ',negative
'case the statement insert ',negative
'keep track colnametoposmap for new ',negative
'column node ',negative
'nonjavadoc see ',negative
'this method looks locations specified above and returns the first location where the file exists the file does not exist any one the locations returns null ',negative
'lists are compatible and onlyif the elements are compatible ',negative
'all inputs this unionoperator are the same reducer not need break the operator tree ',negative
'weve seen this terminal before and have created union work object just need add this work there will children this one since weve passed this operator before ',negative
'noop ',negative
'see need kill some sessions because the pool was resized down while bunch sessions were outstanding see also deltaremaining javadoc ',negative
'internal variable ',negative
'test first argument repeating ',negative
'generate the signer with secret ',negative
'check keys are copied from token store when token loaded ',negative
'null for default partition ',negative
'subquery case tmp may from outside ',negative
'this position parent constant now propagate the constant from the parent the child ',negative
'multibyte truncation ',negative
'get record reader for the idxth chunk ',negative
'make looks like has been compacted but not cleaned ',negative
'hive conf option hiveconf ',negative
'global all pools inherit ',negative
'this file descriptor linked other file descriptors one use case that unionselect starfile sink broken down for consider query like select from subq union all subqx where subq subq involves mapreduce job broken into two independent queries involving subq and subq directly and the subqueries write subdirectories common directory the file sink ',negative
'call the appropriate hive authorizer function ',negative
'test content summary ',negative
'task metrics ',negative
'when sync called will return the value signaling the end the file this should result call sync the beginning the block was searching and should continue normally ',negative
'validate the update ',negative
'closing open scope should ',negative
'hive stuff ',negative
'when deal with big data the vectorizedrowbatch will used for the different file split cache the data here the situation the first split only have rows and vectorizedrowbatch cache them meanwhile the size vectorizedrowbatch will updated the following code simulate the size change but there will when process the next split which has more than rows ',negative
'ordering constant and column expression important correct range generation ',negative
'alter view select requires the view must exist ',negative
'instantiate which this will use lookup the serialization class and the actual class being deserialized ',negative
'grant priv required ',negative
'these members have information for data type conversion not defined there conversion ',negative
'format the serde statement ',negative
'create the filterjoin with the new condition ',negative
'',negative
'lets split bit hashcode into two bit hash codes and employ the technique mentioned the above paper ',negative
'replacing directly the pool should unblock get ',negative
'there hint and operator removed then throw error ',negative
'for select distinct queries dont move any aggregations ',negative
'trim blanks because oldhivedecimal did ',negative
'create table will start and coomit the transaction ',negative
'optional signablevertexspec vertex ',negative
'ballot box with bytes ',negative
'uber llap container ',negative
'create new fetchwork reference the new cache location ',negative
'required required required optional optional optional optional ',negative
'check the old values are still there ',negative
'hiveservice refers logical service name for now hiveserver hostname will used give service actions name this used kill query command can authorized specifically service necessary ',negative
'throw away extra more than decimal places ',negative
'nonjavadoc see ',negative
'simulate show locks with different filter options ',negative
'answer must depending relative magnitude dividend and divisor ',negative
'inserted sort order hence explict sort ',negative
'reset the static variables hiveconf default values ',negative
'long ',negative
'for queries with windowing expressions the selexpr may have third child ',negative
'more bits should considered for finding longest zero runs set msb ',negative
'use input class read length ',negative
'sec months month days ',negative
'carefully check the children make sure they are decimal ',negative
'for unbucketed tables have exactly recordupdater until hive for each which ends writing file bucket see also link getbucketobject ',negative
'special handling grouping function ',negative
'fraction digits from lower longword ',negative
'add element listcolumnvector one one ',negative
'use final variable properly parameterize the closure ',negative
'null means dont return metadata wed need the array anyway for now ',negative
'the coltype not the known type long double etc then get ',negative
'the storage handler does not provide predicate decomposition support well implement the entire filter hive however still provide the full predicate the storage handler case wants any its own prefiltering ',negative
'clone thread local file system statistics ',negative
'',negative
'repeated iospecproto outputspecs ',negative
'for orc parquet all the following statements are the same analyze table partition compute statistics analyze table partition compute statistics noscan there will not any spark job above this task ',negative
'the byte comparison potentially expensive better branch null ',negative
'index the get requests make sure there are ordering artifacts when requeue ',negative
'load the options first can override the command line ',negative
'make batch test the trim functions ',negative
'kvtxt keys ',negative
'when mapping defined assumed that the hive column names are equivalent the column names the underlying table ',negative
'validwriteidlist ',negative
'this update need skip the first col since row ',negative
'marked skipped previously dont bother processing the rest the payload ',negative
'getpos should always return ',negative
'deserialize value into vector row columns ',negative
'get our own connection the database can get table and partition information ',negative
'store arraybyteend that can use the same formula compute the length ',negative
'various helper methods ',negative
'override the default delegation token lifetime for llap also set all the necessary settings defaults and llap configs not set ',negative
'column will removed from filter column will removed from filter ',negative
'the running class was loaded directly through eclipse rather than through ',negative
'init ',negative
'setting the comparison less the search should use the block ',negative
'hive states that should use run instead execute due hadoop known issue added hadoop ',negative
'test the idempotent behavior drop function ',negative
'note that may have two more duplicate partition names ',negative
'not clear replcopywork should inherit from copywork ',negative
'not supported ',negative
'vverbose ',negative
'find the constant origin certain column originated from constant ',negative
'select count from ',negative
'note here should use the new partition predicate pushdown api get list pruned list ',negative
'required check the original types manually primitivetypeequals does not care about ',negative
'ignore this expected ',negative
'that new one gets created for the next query the same ',negative
'element for key row and byte hash table hashmap ',negative
'converted table ',negative
'skip trailing blank characters ',negative
'merge numdistinctvalue estimators ',negative
'nonjavadoc see ',negative
'its set parserewrittenquery ',negative
'assume one separator depth needed ',negative
'set optraits for dynamically partitioned hash join bucketcolnames reuse previous joinops bucketcolnames parent operators should reduce sink which should have bucket columns based the join keys numbuckets set number reducers sortcols this unsorted join sort cols ',negative
'finalize paths ',negative
'any file claim remain trash would granted ',negative
'need preserve currentuser ',negative
'the create time set ',negative
'create database and table that database because the being used each attempt create something should require two calls the retryinghmshandler ',negative
'undone fall through for these they dont appear supported yet ',negative
'whether theres any error occurred during query execution used for query lifetime hook ',negative
'deserialize group key into vector row columns ',negative
'none are null none are selected ',negative
'walk through all our inputs and set them note that this read part update ',negative
'check needed for single byte read ',negative
'even there are results move least check that have permission check the existence zerorowspath the read using the cache will fail ',negative
'try narrow type constant ',negative
'responsible for the flow rows through the ptf chain invocation wraps tablefunction the ptfop hands the chain each row through the processrow call also notifies the chain when partition startsfinishes there are several combinations depending whether the tablefunction and its successor support streaming batch mode combination streaming streaming start partition invoke startpartition tabfn process row invoke process row tabfn any output rows hand next tabfn chain forward next operator finish partition invoke finishpartition tabfn any output rows hand next tabfn chain forward next operator combination streaming batch same combination combination batch batch start partition create reset the input partition for the tabfn caveat prev also batch and not providing output iterator then can just use its output partition process row collect row input partition finish partition invoke evaluate tabfn input partition function gives output partition set next invocations input partition function gives output iterator iterate and call processrow next invocation for last invocation chain forward rows next operator combination batch stream similar combination except finish partition behavior slightly different finish partition invoke evaluate tabfn input partition iterate output rows hand next tabfn chain forward next operator ',negative
'allow for future ctor mutabulity design ',negative
'column ',negative
'inside job can pull out the actual properties ',negative
'were here want more events and either batchiter null batchiter ',negative
'ignore provides invalid url sometimes intentionally ',negative
'add new column with cascade option ',negative
'intermittent failure ',negative
'impersonation turned this called using the using sessionproxy the currentuser will the impersonated user here oozie cannot create proxy user which represents oozies client user here since cannot authenticate using kerberosdigest trust the user which opened session using kerberos this case impersonation turned off the current user hive which can open ',negative
'ideally wed want worker try for every slot there are workers want reread probability falling back random however make slightly more probable avoid too much rereading this handwavy ',negative
'queue for disabled nodes nodes make out this queue when their expiration timeout hit ',negative
'look for functions but not find any ',negative
'return null failed find ',negative
'the token already removed ',negative
'chooses representative alias index and order use the string the first used because set the constructor ',negative
'specialized class for doing vectorized map join that inner join singlecolumn long using hash map ',negative
'nonjavadoc see ',negative
'field within complex type ',negative
'never lose pool session should still able get ',negative
'need compute the input size runtime and select the biggest the big table ',negative
'create new type for handling precision conversions from decimal doublefloat the type only relevant boxliteral and all other functions treat identically ',negative
'are looking based the original fsop use the original path ',negative
'now have delta with delete events ',negative
'this sel sel for ',negative
'vectorization only works with struct object inspectors ',negative
'close them all ',negative
'base file ',negative
'for thrift connects ',negative
'init the list object inspector for handling partial aggregations ',negative
'numtrues ',negative
'real struct ',negative
'pretty print the values ',negative
'use modified portions dofastscaledown code here since not want allocate temporary fasthivedecimal object ',negative
'exponential backoff for ndvs descending order sort ndvs denominator ndv ndv ndv ',negative
'retrieve partitions ',negative
'this only used orc derive the structure most fields are unused ',negative
'passing for currenttxn means this validtxnlist not wrt any txn ',negative
'importing into existing transactional table will create new transactional table because export was done from transactional table need writeid explain plan doesnt open txn and hence need allocate write ',negative
'snapshot subset tez session info for printing events summary ',negative
'now hive meta store uses the same configuration hadoop sasl configuration ',negative
'test rounding ',negative
'stats were available try reduce ',negative
'for casting floating point types boolean ',negative
'for set role none clear all roles for current session ',negative
'read the data isnt null ',negative
'optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional optional ',negative
'test remainder carry indicates result therefore too ',negative
'all participating instances uses the same latch path and curator randomly chooses one instance leader which can verified via ',negative
'generate struct for each the given prefixes ',negative
'using the second table since table called testtable exists both databases ',negative
'note thrift returns ssl socket that already bound the specified hostport therefore open called this would noop later hence any ttransportexception related connecting with the peer are thrown here bubbling them the call hierarchy that retry can happen opentransport dynamic service discovery configured ',negative
'registry uses ephemeral sequential znodes that are never updated now ',negative
'hive ',negative
'move the setuppool code ctor for now least hasinitialsessions will false ',negative
'groupby not distinct like disabling ',negative
'first authorize the call ',negative
'the required outputlength ',negative
'there can atmost one element eligible converted metadata only ',negative
'nonjavadoc see ',negative
'otherwise return the row ',negative
'the columns are same order ',negative
'nonjavadoc see ',negative
'identifier ifnotexists ',negative
'sanity check the config ',negative
'the interval wake and check the queue ',negative
'its column ',negative
'userprivileges ',negative
'provide the type params the type cast ',negative
'after load from this dump all target tablespartitions will have initial set data but source will have latest data ',negative
'pretend this partition exists ',negative
'create writable object inspector for primitive type and return ',negative
'clienttool end singleton instance the job client ',negative
'column ',negative
'call increasebufferspace will ensure that buffer points byte with sufficient space for the specified size this will either point smallbuffer newly allocated byte array for larger values ',negative
'set the fetch operator for the new input file ',negative
'subtract ',negative
'reason poll untill the job initialized ',negative
'cause them localized for the sqoop job tasks ',negative
'the type information for all fields ',negative
'incoming events can ignored until the point when shuffle needs handled instead just scans ',negative
'rss schema key max min ',negative
'there single discardable operator tablescanoperator and means that have merged filter expressions for thus might need remove dpp predicates from the retainable tablescanoperator ',negative
'column ',negative
'already committed aborted ',negative
'select columns that actually not exist the file ',negative
'translate the udaf ',negative
'check right valid ',negative
'from orcinputformat ',negative
'order not necessary but sql require use fetch ',negative
'nonjavadoc see ',negative
'drop role ignore error ',negative
'have readentity defaultacidtblpart writeentity defaultacidtblpart typetable writetypeinsert isdpfalse ',negative
'extract operator need rewrite ',negative
'trigger query hooks before query execution ',negative
'subscriber can get notification about drop database hcat listening topic named hcat and message selector string hcatevent hcatdropdatabase ',negative
'table creation should succeed even location specified ',negative
'this servlet based off the jmxproxyservlet from tomcat has been rewritten read only and output json format not really that close the original ',negative
'need get the partitions column names from the partition serde avro provides the table schema and ignores the partition schema ',negative
'apply join filters the row ',negative
'translate column names walking the ast ',negative
'dynamic partitions ',negative
'extract tables used the query which will turn used generate the corresponding txn write ids ',negative
'remove this cor var from output position mapping ',negative
'assumes the result set set valid row ',negative
'check this txn state already replicated for this given table yes then ',negative
'shouldnt really happen ',negative
'etl strategies will have start start first stripe ',negative
'analyzeexport creates tablespec which turn tries build public listpartition partitions looking the metastore find partitions matching the partition spec the export command these course dont exist yet since weve not ran the insert stmt yet ',negative
'tracks completed requests pre node ',negative
'successful task ids ',negative
'assumes the cache lock has already been taken ',negative
'session txnmanager that already use ',negative
'the histogram object ',negative
'nonjavadoc see ',negative
'generate applicationid for the llap splits ',negative
'are going build the name ',negative
'spark task were currently processing ',negative
'did not generate anything for the new predicate bail out ',negative
'hadoop queue information ',negative
'table import statement specified location and the tableunpartitioned already exists ensure that the locations are the same partitioned tables not checked here since the location provided would need ',negative
'event alter marker stats event insert alter stats update event ',negative
'sorry too many ifs but this form looks optimal ',negative
'general when can have unlimited branches currently only handle either branch ',negative
'needed ',negative
'forward the overflow batch over and over reference new one big table rows values each time cross product current batch small table values todo this could further optimized copy big table equal keys once and only copy big table values each time and not set repeating every time ',negative
'adding the credentials from hadoop config hidden ',negative
'trim trailing zeroes and readjust scale ',negative
'nonjavadoc see ',negative
'estimated number rows will product ndvs ',negative
'lazily create instances ',negative
'record current valid txn list that will used throughout the query compilation and processing only this transaction was already opened and the list has not been recorded yet explicit open transaction command ',negative
'first branch query second branch ',negative
'desc will null its create table like desc will contained createtablelikedesc currently hcat disallows ctlt prehook desc can never null ',negative
'well over allocate and then shrink the array for each type ',negative
'this includes the mandatory alias ',negative
'third group ',negative
'call out the real configure method ',negative
'second overflow error ansi sql numeric cast decimal throws overflow error ',negative
'some commands like show databases dont start implicit transactions ',negative
'the heartbeat has timeout double check whether can remove ',negative
'index for value from keys from values ',negative
'the udtf expects arguments object ',negative
'construct skewedvalue location map except default directory why query logic knows defaultdir structure and dont need get from map ',negative
'look thats the table for example drop table explicit txn not allowed some cases this requires looking more than just the operation for example hiveoperationload target table but not nonacid table ',negative
'this the original parent the currentrootoperator scan through the graph root operator might have multiple parents and just use this one remember where came from the current ',negative
'dont catch any execution exceptions here and let the caller catch ',negative
'sets permissions and group name partition dirs and files ',negative
'should have stats for all columns estimated actual ',negative
'new serde needs store fields metastore but the old serde doesnt save the fields that new serde could operate note that this may fail some fields from old serde are too long stored metastore but theres nothing can ',negative
'log warning row count missing ',negative
'use getperflogger get instance perflogger ',negative
'partition was found but was empty ',negative
'generate basic tez config ',negative
'set the hivedecimalwritable from bytes without converting string first for better performance ',negative
'files size for splits ',negative
'add new partition via objectstore ',negative
'query ',negative
'stored stats ',negative
'now new job requests should succeed all cancel threads would have completed ',negative
'nothing currently available for hash sets ',negative
'leaving some table from the list tables cached ',negative
'merge statements could have inserted cardinality violation branch need avoid that ',negative
'normalize prop name ',negative
'this can max never ',negative
'add default dir the end each list ',negative
'included will not null row options will fill the array with trues null ',negative
'setdays resets the isnulli false there parse exception ',negative
'fail silently ',negative
'test that existing exclusive partition with new sharedread coalesces ',negative
'could null for default partition ',negative
'construct join rel node and rowresolver for the new join node ',negative
'use ',negative
'its parent sparkworks for the small tables ',negative
'the modelertype attribute was not found the class name used instead ',negative
'make vectorized operator ',negative
'java version system property formatted find second dot instead last dot safe ',negative
'set empty text property set null ',negative
'group operators select the key cols need find them the values ',negative
'only mark output null when input null ',negative
'create the directories filesinkoperators need ',negative
'need way know what thread interrupt since this blocking thread ',negative
'this may possible when srctype string but desttype integer ',negative
'since overlaps with long running still open does nothing ',negative
'creates the configuration object necessary run specific vertex from map work this includes input formats input processor etc ',negative
'the file may not exist and just ignore this ',negative
'show cannot create child null directory ',negative
'bloom filter for the new node that will eventually add the cache ',negative
'convert stringutf ascii bytes methods ',negative
'some bookkeeping ',negative
'forwardoperator that can add multiple filtergroup operators children ',negative
'event dump each subdir individual event dump need guarantee that the directory listing got order evid ',negative
'try another slot ',negative
'nonjavadoc see javaioreader long ',negative
'the schema for intersect all like this countc cnt minc create input project for udtf whose schema like this minc ',negative
'the simple validity check see the file size not other checks maybe added the future ',negative
'dont mess with the cached object ',negative
'pathtablelocation year ',negative
'can clean ',negative
'get our singlecolumn long hash multiset information for this specialized class ',negative
'clean above throws ',negative
'this rel references corvar and now needs rewritten must have been pulled above the correlator replace the input ref account for the lhs the correlator ',negative
'for explain plan txn wont opened and doesnt make sense allocate write ',negative
'register function ',negative
'power ten way beyond our precisionscale ',negative
'move all the children the front queue ',negative
'test outputformat with complex data type and with reduce ',negative
'parsing error invalid url string ',negative
'settable recursively all the nested fields are also settable ',negative
'with constant folding then the result will ',negative
'use construct ',negative
'test select rootcol from ',negative
'initialize transient fields called after deserialization other fields ',negative
'this the threshold that the user has specified fit mapjoin ',negative
'tests location returned when file present the first directory lookup order ',negative
'binaryvalue ',negative
'simplify vector bruteforce flattening nonulls isrepeating this can used reduce combinatorial explosion code paths vectorexpressions ',negative
'skip entire the data ',negative
'set nothing for prepared sql ',negative
'all files were copied successfully last try can break from here ',negative
'timestamps with second vint storing additional bits the seconds field ',negative
'remember the count positions ',negative
'initialize rownums have row ',negative
'read back ',negative
'heartbeater should running the background every second ',negative
'create and return clause ',negative
'for pattern find digits ',negative
'theory the below call isnt needed non thriftmode but lets not get too crazy ',negative
'class loading thread safe ',negative
'started from main and nolog should not output any logs turn the log please set dtestsilentfalse ',negative
'explicitly misset the catalog name ',negative
'second incremental load ',negative
'now make sure get the stats expect for partition are going add data later ',negative
'the client did not specify qop then just negotiate the one supported server ',negative
'round using the halfup method used hive ',negative
'will strain out the record for the underlying writer ',negative
'load using same dump with view should fail not empty ',negative
'collect keys ',negative
'assume dont need fetch the rest the skewed column data have columns ',negative
'add the new task child each the passed tasks ',negative
'throw new not sort order and unique ',negative
'checkoutputspecs mightve set some properties need have context reflect that ',negative
'tablestats ',negative
'now look any jars weve packaged using jarfinder returns null when ',negative
'the super method will reload hash table partition one the small tables currently for native vector map join will only one small table ',negative
'swap the first element the metastoreuris with random element from the rest the array rationale being that this method will generally called when the default connection has died and the default connection likely the first array element ',negative
'new exception ',negative
'start with dummy vector operator the parent the parallel vector operator tree are creating ',negative
'string utilities ',negative
'aggregate this batch ',negative
'used for testing ',negative
'there should reader event available coming soon okay blocking call ',negative
'decode the value necessary ',negative
'runinternal which defers the close the called that method ',negative
'getconvertedoi without any caching ',negative
'locate the where the branch starts this function works only for the following pattern fil fil sel sel gby gby join sparkpruningsink sparkpruningsink ',negative
'call listlocatedstatus mockmocktbl call check side file for mockmocktbl call open mockmocktbl call check side file for mockmocktbl ',negative
'null ',negative
'make sure only mapjoins can performed ',negative
'safe assume else orc semantic analyzer will check for rcorc ',negative
'removed from make sure that can still get session from ',negative
'minutes ',negative
'for the failures the users have been notified just need clean theres session here its unused conflicts are possible just remove for successes the user has also been notified various requests are also possible however start wed just put the session into the sessions list and from there ',negative
'for xlint code will never reach here ',negative
'sets the job state completed and also sets the results value returns true completed status set otherwise returns false ',negative
'trigger kill threads and verify that get and expected message this should raise kill operations and ensure that retries keep the time out occupied for sec ',negative
'have reached the point where are transferring files across filesystems ',negative
'fill high long from middle long and middle long from lower long ',negative
'capture the cte definitions query ',negative
'equal key series checking ',negative
'optionally read current values big length big value len big value bytes ',negative
'long range bias for bit hashcodes ',negative
'',negative
'try find the class ',negative
'write the sync bytes flush header ',negative
'store the position the constant value for later use ',negative
'fast path first statement doesnt work rollback and the long version ',negative
'optional int withindagpriority ',negative
'using member variable the closure will not the right thing ',negative
'have scan the directory find min date greater than currentdir ',negative
'sets temp and task temp path ',negative
'join clause rewriting needed ',negative
'optional int amport ',negative
'this semijoin branch find this creating potential cycle with childjoin ',negative
'parameters for exporting metadata table drop requires the use the preevent listener ',negative
'temp tables just call underlying client ',negative
'now prunedcols are columns used child operators and columns ',negative
'first quickly check the two table scan operators can actually merged ',negative
'mybitint ',negative
'avro guarantees that the key will type string just need worry about deserializing the value here ',negative
'the entire seconds field stored the first bytes ',negative
'assuming that there transaction for read lock ',negative
'authorizer ',negative
'default outputtypeinfo long ',negative
'now create session specific dirs ',negative
'comma separated list classes batch ',negative
'replpolicy ',negative
'let writers release the memory for garbage collection ',negative
'reserve spaces for the byte size the struct which integer and takes four bytes ',negative
'the outputs intermediate map reduce jobs ',negative
'dont break this allocation failure was result localitydelay others could still allocated ',negative
'newer tasks first ',negative
'this just the directory need recurse and find the actual files but dont this until have determined there base this saves time plus possible that the cleaner running and removing these original files which case recursing through them could cause get error ',negative
'isreplace ',negative
'utf byte constants used stringutf bytes decimal and decimal stringutf byte conversion ',negative
'for better performance longdouble dont want the conditional statements inside the for loop ',negative
'directory where path resides ',negative
'theres more data ',negative
'higerbound when true ',negative
'hconf null unit testing ',negative
'either this arena being allocated already allocated next the ',negative
'these are the functions that have window fns window have already been processed ',negative
'push down filters ',negative
'keeps track the righthandside table name the leftsemijoin and ',negative
'files size for splits ',negative
'node marked failed node has capacity ',negative
'literal smallint ',negative
'can only used for kerberos user but not for the user logged via other authentications such ldap ',negative
'rowid ',negative
'calculate which writer use from the remaining values this needs done before delete cols ',negative
'this project will what the old input maps replacing any previous mapping from old input ',negative
'note this called under the epic lock ',negative
'reset for reading ',negative
'check this the best match far ',negative
'fall through ',negative
'always long ',negative
'such table the given database ',negative
'precompute normalization dont have deal with sqlexceptions later ',negative
'generate join operator ',negative
'besides the hiveshims jar which hadoop version dependent also always need include hive shims common jars ',negative
'locality delay ',negative
'need generate not null predicate for partitioning virtual column since those columns can never null ',negative
'take care view creation ',negative
'compare two strings from two byte arrays each with their own start position and length use lexicographic unsigned byte value order this whats used for utf sort order return negative value arg arg arg arg positive arg arg ',negative
'filesystem ',negative
'decimal columns use hivedecimalwritable ',negative
'delim ',negative
'not column need for the keys move ',negative
'walk through the ast replace all the toktabref adding additional masking and filter the table needs masked filtered for the replacement leverage the methods that are used for ',negative
'asserttrueve instanceof ',negative
'hivemetastore the deletedata flag indicates whether dfs data should removed drop ',negative
'values put above ',negative
'adding select operator top semijoin ensure projection only correct columns ',negative
'populate partition info ',negative
'the projection ',negative
'transform the table reference absolute reference dbtable ',negative
'cache extractobject ',negative
'case column stats grouping sets ',negative
'existing avro data ',negative
'key column name and the value the col stat object ',negative
'',negative
'check that has priority overr ',negative
'optional string tokenidentifier ',negative
'this boolean true bypass the cache ',negative
'over the schema and convert type thrift type ',negative
'user providing config could null ',negative
'exact limit can taken care the fetch operator ',negative
'not care about the transformation rewriting ast which following statement does only care about the restriction checks they perform plan get rid these restrictions later ',negative
'this demuxoperator can appear multiple times muxoperators parentoperators ',negative
'only one reducer this configuration does not prevents ',negative
'grantoption ',negative
'room ',negative
'required required required required required required required required required ',negative
'reparse text passing null for context avoid clobbering the toplevel token stream ',negative
'the list bytes used for the separators the column nested struct such arrayarrayint will use multiple separators the list separators escapechar are the bytes required escaped ',negative
'the root interface for vector map join hash set ',negative
'should return null when there column ',negative
'insert data into both tables ',negative
'the complete list output columns these should added the vectorized row batch for processing the index the row batch equal the index this array plus initialoutputcol ',negative
'the query needs rowid maybe explicitly asked maybe its part updatedelete statement either way need decorate original rows with rowid ',negative
'check arrayidx argument category list ',negative
'interpolation needed because position does not have fraction ',negative
'ignore leading zeroes ',negative
'delete column statistics present ',negative
'use left alias mysql postgresql try widening conversion otherwise fail union ',negative
'this means there second vint present that specifies additional bits the timestamp the reversed nanoseconds value still encoded this vint ',negative
'check constraint fails only evaluates false nullunknown should evaluate true ',negative
'production byte ',negative
'default argless run simply runs and does not care about failure ',negative
'starting tez session pool start here let parent session state initialize cliservice state avoid sessionstateget return null during createtezdir ',negative
'some udfs may emit strings variable length like pattern matching udfs its hard find the length such udfs return the variable length from config ',negative
'check stats need recalculated ',negative
'first promote the next group the current group reached new group the previous fetch ',negative
'test alter table with rename ',negative
'any more input ',negative
'partitions added now should inherit tableschema properties etc ',negative
'initialize singlecolumn long members for this specialized class ',negative
'check groupingid needs projected out ',negative
'rewrite logic rewrite join condition map output positions and produce cor vars any ',negative
'root task ',negative
'',negative
'methods ',negative
'bootstrap constraint dump shouldnt fail the table droppedrenamed while dumping just log debug message and skip ',negative
'testing for equality doubles after math operation not always reliable use this tolerance ',negative
'lock database operation acquire the lock explicitly the operation itself doesnt need locked set the writeentity writetype ddlnolock here otherwise will conflict with hives transaction ',negative
'following suggestion from gopal quickly read the bytes from the stream consider have orc read the whole input stream into big byte array with one call the readbyte int off int len method and then let this method read from the big byte array ',negative
'keeps existing output column map etc ',negative
'same session ',negative
'nonempty java opts with xmx specified twice ',negative
'use table properties case unpartitioned tables and the union table properties and partition properties with partition taking precedence the case partitioned tables ',negative
'not expected further down the pipeline see jira for more details ',negative
'reset port setting ',negative
'table second read entity ',negative
'each http request must have authorization header ',negative
'note for deallocateevicted not release the memory memmanager may happen that the evictor tries use the allowance before the move finishes retryingmore defrag should take care that ',negative
'note important retain the same key the export ',negative
'operations require select priv ',negative
'the partition got dropped before went looking for ',negative
'hadoop default week ',negative
'when transforming not subquery need check for nulls the joining expressions the subquery there are nulls then the subquery always return false for more details see basically sql semantics say that not null always false not operator equivalent all since not equal check with null returns false not predicate against aset with null value always returns false for not subquery predicates join null count predicate and the joining condition that the null count query has count ',negative
'created demand ',negative
'the operator stack ',negative
'well set tez combine spits for iff the input format hiveinputformat ',negative
'ignore potentially incorrect values ',negative
'prevent view from referencing itself ',negative
'generate binary sortable key for current row vectorized row batch ',negative
'used replication copy files from source destination possible source file changedremoved during copy double check the checksum after copy ',negative
'means serializing another instance ',negative
'not print duplicate status while still middle print interval ',negative
'bail out ',negative
'use utc date ensure reader date same all timezones ',negative
'always set the null flag false when there value ',negative
'read the template into string ',negative
'bootstrap dump with empty ',negative
'case ',negative
'sleep this many seconds between each retry ',negative
'first look for this alias from cte and then from catalog ',negative
'the partition columns cant all found the values then the data not bucketed ',negative
'desttablename ',negative
'binary operator ',negative
'portion the join output ',negative
'only valid partitions should added ',negative
'create client instance ',negative
'all the setup done genmapredutils ',negative
'old version thrift client should have false but they not you add default value variable isset for that variable true regardless the where the message was created for object variables works correctly for boolean vars test mode upgrades are not tested client version and server version thrift always matches see unset here means something didnt set the appropriate value ',negative
'should allocate ',negative
'runtime objects ',negative
'nonjavadoc see for fns that are not give them the remaining rows rows whose span went beyond the end the partition for rest the functions invoke terminate while numoutputrows numinputrows for each that doesnt have enough invoke getnextobj there then flag this error ',negative
'this helper object deserializes lazybinary format small table values into columns row ',negative
'disable datasource case insert overwrite have disable the existing druid datasource ',negative
'need concurrent weakhashmap weakkeys that when underlying transport gets out scope still can gced since value map has ref key need weekvalues well ',negative
'raw socket connection nonsasl ',negative
'allocate free every other one allocate ',negative
'the right token ',negative
'move from the tmp dir intermediate directory the same level ',negative
'multiple instances such classes ',negative
'unused ',negative
'shellcmd binbash shellcmd ',negative
'special module for tests the rootdir ',negative
'udf rowid rowid ',negative
'hcat will always prune columns based what ask the ',negative
'okay were going need these originals recurse through them and figure out what really need ',negative
'existing lock for this partition ',negative
'func ',negative
'now pick server node randomly ',negative
'test bad field names ',negative
'checkh for and not the subquery must implicitly explicitly only contain one select item ',negative
'round each physical with row selection and logical with row selection ',negative
'build new join ',negative
'some join alias could changed alias newalias ',negative
'just return smaller than ',negative
'infer schema ',negative
'set memory available for operators ',negative
'does pool exist for this path already ',negative
'unused unknown reason ',negative
'end union ',negative
'invalid schemes ',negative
'suppress multispaces ',negative
'create the walker and the rules dispatcher ',negative
'then reopened session will use user specified queue name else default cluster queue names ',negative
'interrupted ',negative
'default block size set most cache line sizes are bytes and also avx friendly ',negative
'wrapper class write hsconnectionconfig file ',negative
'complete one task host ',negative
'compare next part ',negative
'',negative
'all keys matched ',negative
'check query results cache maskingfiltering required then can check the cache now before generating the operator tree and going through cbo ',negative
'should not have tried print any thing ',negative
'returns the singleton instance cache ',negative
'check for queryhint expressions ast ',negative
'reset the location and table name and compare the partitions ',negative
'',negative
'january ',negative
'when reach here must have some data already because size need see there are any data flushed into file system not can directly read from the current write block otherwise need read from the beginning the underlying file ',negative
'fetch existing ingestion spec from druid any ',negative
'set isnull before calls case they change their mind ',negative
'use construct ',negative
'setting defaults the same zookeeper ',negative
'for each the tables that are part the materialized view where the transaction had committed after the materialization was created ',negative
'create instance hive order create the tables ',negative
'initiate minor compaction request the table ',negative
'the function should support both short date and full timestamp format time part the timestamp should not skipped ',negative
'primitive column types ignore nulls and just copy all values ',negative
'only input side has nulls ',negative
'updating bucket column should move row from one file another not supported ',negative
'loginfowriting offset tailoffset lrptroffset ',negative
'make sure get all deltas within single transaction multistatement txn generate multiple delta files with the same txnid range course maxwriteid has already been minor compacted all per statement deltas are obsolete ',negative
'the query contains more than one join ',negative
'batch batchcounter joinresultname batchsize batchsize somerowsfilteredout somerowsfilteredout ',negative
'matches skewed values ',negative
'use vector parent get ',negative
'here portion the above explain the filterexpr the tablescan the pushed predicate ppd the line simply not there otherwise the plan the same map operator tree tablescan alias acidtbl filterexpr type boolean filter operator predicate type boolean select operator ',negative
'note kerberos user appid doesnt have access ',negative
'mixing down into the lower bits this produces worse hashcode purely numeric terms but leaving entropy the higher bits not useful for bucketing scheme see jsr concurrenthashmap released under public domain note concurrenthashmap has since reverted this retain entropy bits higher support the level hashing for segment which operates higher bitmask ',negative
'need check the properties are valid especially for stats they might changed via alter table update statistics alter table set tblproperties the property not rowcount rawdatasize could not changed through update statistics ',negative
'for table replication reach the max number tasks then for the next run will try reload the same table again this mainly for ease understanding the code then can avoid handling loading partitions for the table given that the creation table lead reaching max tasks loading next table since current one does not have partitions ',negative
'nonjavadoc see javaioreader long ',negative
'prints short events information that are safe for consistent testing ',negative
'findreadslot returning not found know never went that far when were inserting findreadslot key key slot slot pairindex pairindex ',negative
'create the output array ',negative
'expected for permanent udfs this point ',negative
'use the current return type when creating new call for operators with return type built into the operator definition and with type inference rules such cast function with less than operands ',negative
'create tablescandesc ',negative
'column value provided replace column name with value ',negative
'hive udtf only has single input ',negative
'',negative
'output will also repeating and null ',negative
'all are selected ',negative
'schemaversions ',negative
'returns path the partition created any else path table ',negative
'may not exist depending the version hadoop being used build hive use reflection the following lines allow the test compile regardless what version hadoop update credname entry the credential provider credentialprovider provider providerflush ',negative
'comes from cluster wide configured queue names explicitly set user session sets tezqueuename user has specified one use the one from cluster wide queue names there way differentiate how this was set user system unset this after opening the session that reopening session uses the correct queue names client has not died and the user has explicitly set queue name ',negative
'number digits number digits ',negative
'table files dont have footer rows ',negative
'reload tables from the metastore and create data files ',negative
'transport specific confs ',negative
'for now only alter name owner class name type ',negative
'logicalcolumnindex ',negative
'try get cluster information once avoid immediate clusterupdate event ',negative
'found exact bin match for value then just increment that bins count otherwise need insert new bin and trim the resulting histogram back size possible optimization here might set some threshold under which just assumed equal the closest bin fabsvbinsbinx threshold then just increment bin this not done now because dont want make any ',negative
'counters for debugging cannot use existing counters cntr and nextcntr operator since want individually track the number rows from ',negative
'there may multiple selects chose the one closest the table ',negative
'look for single column optimization ',negative
'test alter table ',negative
'however must end after the split end otherwise the next one would have been read ',negative
'get partition metadata partition specified ',negative
'restriction subquery isnot allowed lhs ',negative
'aggr distinct the parameter name constructed ',negative
'and transformation creates nodes andor thus not triggered ',negative
'the user has not returned and the kill has failed are going brute force kill the whatever user does now irrelevant ',negative
'create new local work and setup the dummy ops ',negative
'cannot merge ',negative
'found conflict ',negative
'turn speculative execution for reducers ',negative
'can read more than need the actualcount not multiple the bytebuffer size and file big enough that case cannot use flip method but need set buffer limit manually trans ',negative
'remove unused table scan operators ',negative
'compare hive version and metastore version ',negative
'rowid ',negative
'pass configs and precreate parse context ',negative
'got exception during doing the unlock rethrow here ',negative
'both the classname and the protocol name are table properties the only hardwired assumption that records are fixed per table basis ',negative
'colname ',negative
'get the parameter values ',negative
'subqueryast ',negative
'now this shouldnt find the path the ',negative
'were replacing the current big table with new one need count the current one map table then ',negative
'get our singlecolumn string hash map information for this specialized class ',negative
'for the current context for generating file sink operator either insert into insert overwrite ',negative
'default which created during automatic logging initialization static initialization block changing contextselector runtime requires creating new context factory which will internally create new context selector based system property ',negative
'poll the operation status till the operation complete ',negative
'second column exists ',negative
'start insert statement transaction and roll back this transaction ',negative
'set the table transactional for compaction work ',negative
'update min counter new value less than min seen far ',negative
'the absence uncompressedraw data size total file size will used for statistics annotation but the file may compressed encoded and serialized which may lesser size than the actual uncompressedraw data size this factor will multiplied file size estimate ',negative
'restore the interrupted status since not want catch ',negative
'set all properties specified via command line ',negative
'operand oneoperand another result unknown ',negative
'now output this timestamps millis value the equivalent totz ',negative
'avoid having huge bloomfilter need scale false positive probability ',negative
'setup deserializerobj inspectors for the incoming data source ',negative
'estimate row count ',negative
'just made existing table full acid which wasnt acid before and passed all checks initialize the write sequence that can handle assigning rowids original files already present the table ',negative
'catching exceptions here makes sure that the thread doesnt die case unexpected exceptions ',negative
'check only the partition that exists all should well ',negative
'constant varchar projection ',negative
'selectoperator should use only simple castcolumn access ',negative
'load same data again additive ',negative
'the pool empty queue the request ',negative
'files size for splits ',negative
'build nonpartpartvirtual column info for new rowschema ',negative
'histogram used for quantile approximation the quantiles requested ',negative
'because only fractional digits its not this much accurate ',negative
'this test collector operator for mapjoin rowmode ',negative
'should this also just ignored throw for now doas unlike queue often set admin ',negative
'',negative
'its filesink bucketed files use the bucket count the reducer number ',negative
'collectionssortkvs ',negative
'determine input vector expression using the ',negative
'initialize the resources from command line ',negative
'source cmd ',negative
'fall through and look for other options ',negative
'the union task empty the files created for all the inputs are assembled the union context and later used initialize the union plan ',negative
'see ',negative
'first project all groupby keys plus the transformed agg input ',negative
'weve filled the buffer write out ',negative
'unknown category ',negative
'before cleaner there should items ',negative
'abort proactively that dont wait for timeout perhaps should add version abort which ',negative
'restore data ',negative
'branch ',negative
'fail misconfigured ',negative
'not every partition uses bitvector for ndv just fall back the traditional extrapolation methods ',negative
'flip inclusion ',negative
'monday july ',negative
'nonjavadoc see javaioinputstream ',negative
'blank byte smiling face with open mouth and smiling eyes bytes ',negative
'database ',negative
'asserting other partition parameters can also changed but not the location ',negative
'message bus related properties ',negative
'this method used check whether the subtype sub type the grouptype for nested attribute need check its existence the root path recursive way ',negative
'numslotsavailable can negative the callback after the thread completes delayed ',negative
'this expected ',negative
'check the limit ',negative
'this for conditionaltask ',negative
'get the information from calcite ',negative
'with timeout ',negative
'per jdbc spec the driver does not support catalogs will silently ignore this request ',negative
'check entries beyond one ',negative
'todo assert ikey ',negative
'need different record handler for mergefilework ',negative
'set constraint name null before sending listener ',negative
'the common case far ',negative
'the keyhash missing the bloom filter then the value cannot exist any the spilled partition return nomatch ',negative
'right now are few cheap redundant update calls lets just the simple thing ',negative
'update messages ',negative
'hiveconf hivestatsndverror default produces ',negative
'column family becomes map ',negative
'make sure hadoop credentials objects not reuse the maps ',negative
'setup the necessary metadata originating from analyze rewrite ',negative
'ensure pig schema correct ',negative
'either variables will never null because default value returned case absence ',negative
'allocate writeid from test txn and then verify validwriteidlist write ids committed and self test txn should valid but writeid open txn should invalid ',negative
'see bucketnumreducersq bucketnumreducersq todo try using set ',negative
'not supported for the test case ',negative
'this task contains sortmergejoin can converted mapjoin task this operator present the mapper for sortmerge join operator present followed regular join cannot converted auto mapjoin ',negative
'link the work with the work associated with the reduce sink that triggered this rule ',negative
'first handle the condition where the first fetch was never done big table empty ',negative
'first try extract long value from the strings and compare them ',negative
'undone for now all ',negative
'user did not specify partition ',negative
'for windows need pass hivehadoopclasspath java parameter while starting ',negative
'test the same day month ',negative
'class ',negative
'skewed value ',negative
'build row type from field type name ',negative
'simple truncation ',negative
'iterate through all and locate the one introduce enforce bucketing ',negative
'fail transactional set true but the table not bucketed ',negative
'verify that have got correct set deletedeltas ',negative
'not calculated since high integer always for our decimals ',negative
'possible have file with same checksum cmpath but the content partially copied corrupted this case just overwrite the existing file with new one ',negative
'skip when already eof ',negative
'test parent references from preparedstatement ',negative
'pos driver alias ',negative
'use mapping object here can build the projection any order and get the ordered output columns the end also avoid copying big table key into the small table result area for inner joins reference with the projection there can duplicate output columns ',negative
'requires exact types both sides setop ',negative
'physical optimizers which follow this need careful not invalidate the inferences made this optimizer only optimizers which depend the results this one should ',negative
'can use the whole batch for output matches ',negative
'return length characters utf string byte array beginning start that len bytes long ',negative
'source code for the strtod library procedure copyright regents the university california permission use copy modify and distribute this software and its documentation for any purpose and without fee hereby granted provided that the above copyright notice appear all copies the university california makes representations about the suitability this software for any purpose provided without express implied warranty ',negative
'for caching aggregate column stats for all and all minus default partition key column name and the value list col stat objects ',negative
'figure out which kind bloom filter want for each column picks bloomfilterutf its available otherwise bloomfilter ',negative
'bgenjjtree unflagargs ',negative
'all selected nothing ',negative
'hive behavior where double decimal decimal gone ',negative
'vectorudfadaptor usage ',negative
'default ',negative
'add all children ',negative
'required required required required optional optional optional ',negative
'always just write mono ',negative
'check all parents are finished ',negative
'this the base class for the output parser output parser will parse the output pig hivehadoop other job and extract jobid note hadoop jobid extract rely the api hadoop application submitting the job different api will result different console output the jobid extraction logic not always working this case ',negative
'use construct ',negative
'gbinfo already has exprnode for gbkeys ',negative
'testing druid queries row filtering present ',negative
'default noop implementation ',negative
'just remove deletedelta there have been delete events ',negative
'tableonly fetch partitions regular export dont metadataonly ',negative
'ptf function must provide the external names the columns the transformed raw input ',negative
'mutate data ',negative
'nullindicator now different location the output ',negative
'make sure only return valdecompressor once ',negative
'means there transaction select statement which not part explicit transaction iud statement that not writing acid table ',negative
'use tez combine splits ',negative
'new plan absolutely better than old plan ',negative
'isenableandactivate ',negative
'return the caller since long polling timeout has expired ',negative
'see the above release the headers after unlocking ',negative
'note just like the move path only one level recursion ',negative
'test repeating nulls case ',negative
'allocate the source deserialization related arrays ',negative
'nothing ',negative
'rexliterals equal only consider value and type which isnt sufficient providing custom comparator which distinguishes objects irrespective valuetype ',negative
'this should never happen does its bug with the potential produce incorrect results ',negative
'get estimation parameters ',negative
'the ieee floating point spec specifies that signed and should treated equal ',negative
'temporary singlebatch context used for vectorization ',negative
'consider query like select from where the view defined select from the inputs will contain and parent will marked indirect entity using isdirect flag this will help distinguishing from the case where direct dependency ',negative
'violation etl queue ',negative
'table created hive catalog should have been automatically set transactional ',negative
'operators ',negative
'kill queries ',negative
'allocate output column and get column number ',negative
'null big table partitioned ',negative
'clear completed instances this case dont want provide information from the previous run ',negative
'determine the transaction manager use from the configuration ',negative
'drop the tables when were done this makes the test work inside ide ',negative
'history file stream ',negative
'the version that was included with the original magic which mapped ',negative
'datatype column ',negative
'prove invariant the compiler len len all array access between start startlen and start startlen are valid more oob exceptions are possible ',negative
'rowresolver helper methods ',negative
'execute generated plan ',negative
'may field name return the identifier and let the caller decide whether not ',negative
'set joins already processed ',negative
'establish context ',negative
'such partition ',negative
'check the stripped property the empty string ',negative
'all the contain bitvectors not need use uniform distribution assumption because can merge bitvectors get good estimation ',negative
'create data container ',negative
'should not happen ',negative
'test that exclusive blocks exclusive and write ',negative
'when run task after the jar has been added ',negative
'whether the statement boolean expression was repeating ',negative
'split the children into vertices that make the union and vertices that are ',negative
'figure out correlation and presence nonequi join predicate ',negative
'appends might exist after the root message strip tokens off until ',negative
'array strings type array arrays strings ',negative
'this constant was created while doing constant folding foldedfromcol holds the name original column from which was folded ',negative
'override hive specific operators ',negative
'not the right host ',negative
'only need username for ugi use for groups getgroups will fetch the groups based hadoop configuration documented ',negative
'make sure not loose ',negative
'when bias correction enabled ',negative
'release the unreleased stripelevel buffers see class comment about refcounts ',negative
'excluded via property already has stats only expect two updates ',negative
'this run initiator doesnt add any compactionqueue entry ',negative
'extract the collations indices ',negative
'test that existing sharedread table with new sharedread coalesces ',negative
'run batch ',negative
'woe ',negative
'verify droptable recycle partition files ',negative
'avoid instantiation ',negative
'return just the single range ',negative
'only green qualifies and its entry ',negative
'add cast expression needed child expressions udf may return different data types and that would require converting their data types evaluate the udf for example decimal column added integer column would require integer column cast decimal ',negative
'note partcolsisnull only used for ptf which isnt supported yet ',negative
'possible reasons end here unable read version from manifestmf version string not the proper xxxxx format ',negative
'complex type constants currently not supported ',negative
'handle normal case ',negative
'current match may out order wrt the global name list add specific parts ',negative
'colnamespace ',negative
'need override assigns all assign ops will fail due size ',negative
'',negative
'for each row produced ',negative
'nonjavadoc see ',negative
'insert junk middle file assumes file local disk ',negative
'rename partitioned table ',negative
'all values are filtered out ',negative
'final check find size alreadycalculated mapjoin operators same work sparkstage ',negative
'current state each selected column current run length etc ',negative
'make sure the proper transaction manager that supports acid being used ',negative
'find the same key ',negative
'invalid app name checked later ',negative
'cannot apply the reduction ',negative
'accept int writables and convert them ',negative
'row results should null ',negative
'allow date string casts note suspect this the reverse what actually want but matches the code cant see how users would altering date columns into string columns the other easily see since hive did not originally support datetime types also the comment the hive code says string date even though the code does the opposite but for now keeping ',negative
'number aborted txns found exceptions ',negative
'launch task ',negative
'test single highprecision multiply random inputs ',negative
'each branch ',negative
'just checked the user specified schema columns among regular table column and found some which are not regular now check they are dynamic partition columns for dynamic partitioning given create table multiparta int int partitioned int int for insert into multipart partitioncdda values expect parse tree look like this tokinsertinto toktab toktabname multipart tokpartspec tokpartval tokpartval toktabcolname ',negative
'have found valid cookie the client request ',negative
'',negative
'any the other fields need set ',negative
'open txns ',negative
'the inputoi the same the outputoi just return identityconverter ',negative
'tsfiltsfilfil ',negative
'pretend like has fractional digits can get the trailing zero count ',negative
'the output path used the subclasses used final destination same outpath for tables ',negative
'topnhashes are active proceed not already excluded order limit ',negative
'need check again exists ',negative
'normally import trying create table partition that does not yet exist error condition however the case repl load possible that are trying create tasks create table inside that asofnow does not exist but there precursor task waiting that will create before this encountered thus instantiate defaults and not error out that case ',negative
'already done iulrindex initialized before normalization ',negative
'empty list ',negative
'this indicates original query was either correlated exists ',negative
'',negative
'future this may examine config return appropriate hcatreader ',negative
'specific files they exist ',negative
'this class authenticates web via pam authenticate use httpget with header name authorization and header value basic authbcode where authbcode base string for loginpassword ',negative
'indices ',negative
'convert the work the smb plan regular join note that the operator tree not fixed only the pathalias mappings the plan are fixed the operator tree will still contain the smbjoinoperator ',negative
'create file well import later ',negative
'local mode command like dfs ',negative
'have rely hive implementation filesystem permission checks ',negative
'generate operator prune the tablenames only count the tablenames that are not empty strings empty string table aliases only allowed for virtual columns ',negative
'not using byref now since its unsafe for text readers might safe for others ',negative
'for compare will convert requisite children for between skip the first child the revert boolean ',negative
'create table ',negative
'handle repeating null ',negative
'indicate used the last rows bytes for large buffer ',negative
'msck called drop stale paritions from metastore and there are stale partitions ',negative
'add signature ',negative
'key key key key ',negative
'turn off passwords enable sasl and set keytab ',negative
'outer join hash map ',negative
'clear the previous values recorded ',negative
'classify partitions within the table directory into groups based shared properties ',negative
'initialize the metrics system ',negative
'use minimrcluster mapreduce ',negative
'handle repeating case ',negative
'numreducers newnumreducers newnumreducers numreducers will not consider reducesinkoperator with this newnumreducer correlated reducesinkoperator ',negative
'perform major compaction ',negative
'verify table for key byte hash table hashmap ',negative
'now new job requests should succeed list operation has cancel threads ',negative
'hive added files and jars ',negative
'next node consistent order died does not have free slots rollover next ',negative
'note power can positive negative note power effectively multiply ',negative
'add the privs for roles curroles new roletopriv map ',negative
'',negative
'get our vector map join fast hash table variation from the vector map join table container ',negative
'load keycountadj keycountadj load threshold threshold load loadfactor loadfactor load wbsize wbsize ',negative
'insert rows both tables ',negative
'retrieve token and store the cache ',negative
'the antlr grammar looks like kwconstraint idfridentifier kwforeign kwkey kwreferences tabnametablename tokforeignkey idfr fkcols tabname parcols relyspec enablespec validatespec when the user specifies the constraint name childgetchildcount kwforeign kwkey kwreferences tabnametablename tokforeignkey fkcols tabname parcols relyspec enablespec validatespec when the user does not specify the constraint name childgetchildcount ',negative
'order keys ',negative
'replace bucketing columns with hashcode numbuckets ',negative
'create clauses ',negative
'this test verifies that the alter table set owner command will change the owner metadata the table hms ',negative
'release the copies made directly the cleaner ',negative
'convert our source object just read into the target object and store that the vectorizedrowbatch ',negative
'prefixing with mask ensure conflict with named columns the file schema ',negative
'loginforeturning ',negative
'simple join from relations denom maxv ',negative
'look for any valid versions this will also throw the schema itself doesnt exist which what want ',negative
'hive will reuse the configuration object that passes initialized therefore need make sure dont look for any ',negative
'this testfilter ',negative
'protected for testing and can pass conf for testing ',negative
'grantor ',negative
'get the list dynamic partition paths ',negative
'the start ',negative
'starting from the given rowidx scan the given direction until rows expr evaluates amt that crosses the amt threshold specified the boundarydef ',negative
'try read the dropped partition via cachedstore ',negative
'string ',negative
'partitioned table ',negative
'nothing add ',negative
'task has completed ',negative
'srcpositionstart cant accept negative numbers ',negative
'need three params differentiate between this and param method auto generated since some calls the autogenerated code use null param for param and thus ',negative
'note dont add added jars reloadable jars etc that design there are too many ways add jars hive some which are sessionetc specific env conf arg should enough ',negative
'create keyvalue tabledesc when the operator plan split into tasks the reduce operator will initialize extract operator with information ',negative
'weve opened transaction need commit rollback rather than explicitly ',negative
'char strategic blanks string length beyond max ',negative
'cmapintmapintint ',negative
'fallback regular logic ',negative
'extract all the columns ',negative
'know they are not equal because the one with the larger scale has nonzero digits below the others scale since the scale does not include trailing zeroes ',negative
'add supported protocols ',negative
'todo verify any escaping needed for values ',negative
'use method adddelegationtokens instead getdelegationtoken get all the tokens including kms ',negative
'the biggest output from parent ',negative
'note hive convention doesnt pushdown non deterministic expressions ',negative
'local plan not null want merge into smbmapjoinoperators local work ',negative
'there are elements the list ',negative
'get partial aggregation results and store reducevalues ',negative
'comes from tblproperties compact ttp ',negative
'create the lineage context ',negative
'source path subdirectory the destination path the other way around insert overwrite directory select srcvalue where srckey where the staging directory subdirectory the destination directory not delete the dest dir before doing the move operation assumed that subdir and dir are same encryption zone move individual files from scr dir dest dir ',negative
'find the move task ',negative
'serialize the hcatpartitionspec ',negative
'multikey specific lookup key ',negative
'make sure buffers are eligible for discard ',negative
'clear rounding portion middle longword and add right scale roundmultiplyfactor lower longword result ',negative
'the other overload should have been used ',negative
'factory method for serde ',negative
'middle word bits need multiplier longmaxvalue digit commad ',negative
'the table alias information map already contains the current table ',negative
'reached here have match for generated cookie ',negative
'deregister versionnumber ',negative
'are running using the same umbilical ',negative
'set the security for plugin endpoint will create the token and publish the registry note this application bogus and only needed for ',negative
'',negative
'this the start containerannotated logging ',negative
'construct using ',negative
'the antlr grammar looks like kwconstraint idfridentifier kwprimary kwkey tokprimarykey pkcols idfr when the user specifies the constraint name kwprimary kwkey tokprimarykey when the user does not specify the constraint name ',negative
'tests for partition tablename string dbname string name method ',negative
'currently all tez work the cluster ',negative
'verify preemption requests since everything the same priority ',negative
'hashcodes had conversion positive done different ways the past abs now obsolete and all inserts now use integermaxvalue the compat mode assumes that old data couldve been loaded using the other conversion ',negative
'init ',negative
'',negative
'while processing bucket columns atleast one bucket column missed this results different bucketing scheme add empty list ',negative
'add sortingbucketing needed ',negative
'longsize tablerowsize longsize ',negative
'skip class loading the class name didnt change ',negative
'can fully qualified name use default database ',negative
'the outputops this vertex ',negative
'for bucketed tables optimization ',negative
'not display vectorization objects ',negative
'',negative
'abort all the allocated txns that the mapped write ids are referred aborted ones ',negative
'bogus warnings despite closequietly ',negative
'fallback for unexpected relnode type ',negative
'this means that nothing was set for default stripe size previously should unset ',negative
'start the cache threads cache also serves buffer manager ',negative
'the data must type string ',negative
'this means the process will exit without waiting for this thread ',negative
'this shouldnt throw exception ',negative
'the serialized nonnull series keys these members represent the value ',negative
'streaming ingest writer with single transaction batch size which case the transaction either committed aborted either cases dont need flush length file but need flush intermediate footer reduce memory pressure also with hive streaming writer does automatic memory management which would require flush open files without actually closing ',negative
'now were use stale value would use ',negative
'query logresutsr explain logical ',negative
'logdebugclassname repeated key key ',negative
'for nonmm tables directory structure ',negative
'test greater ',negative
'init without the knowledge llap usage lack thereof the cluster ',negative
'handles cases where the query has predicate columnnameconstant ',negative
'have singleton and just return the child ',negative
'there bitvector but not adjacent the previous ones ',negative
'log warn given how unfortunate this ',negative
'load row ',negative
'handle string types properly ',negative
'',negative
'gets list job ids and calls getjobstatus get status for each job ',negative
'',negative
'override properties from tblproperties applicable ',negative
'before reparsing they are known semanticanalyzer logic ',negative
'both inputs the join are unique there nothing gained this rule fact this aggregatejoin may the result previous invocation this rule continue might loop forever ',negative
'make copy ',negative
'',negative
'number elements sum elements sumxavg this actually times the variance ',negative
'returns true the specified path matches the prefix stored ',negative
'set entry null ',negative
'substitute for any carriage return line feed characters line with the escaped character sequences param line the string for the crlf substitution return there were replacements then just return line otherwise new string with escaped crlf ',negative
'this stream can separated using index lets that ',negative
'since rowcount used later instantiate ',negative
'connect via kerberos with trusted proxy user ',negative
'add test parameters from storage formats specified table ',negative
'src scratch directory need trim the part key value pairs from path ',negative
'configure the broker ',negative
'when type config set hive ',negative
'testing multibyte string substring ',negative
'nonjavadoc see javalangobject int int ',negative
'drop the materialized view but not its data ',negative
'should also not able add fields different types with same name ',negative
'especially since llap prone turn off the mapjoindesc later ',negative
'async progress the callback will take care this ',negative
'get next batch table names this list ',negative
'failed set job status completed which mean the main thread would have exited and not waiting for the result kill the submitted job ',negative
'taskwrapper used structures well for ordering using comparators ',negative
'represents windowframe applied partitioning window can refer isourcei window name the source window provides the basis for this window definition this window specification extendsoverrides the isourcei window definition our the select expression sumpretailprice over translated into windowfunction instance that has window specification that refers the global window specification the functions specification has content but inherits all its attributes from during subsequent phases translation ',negative
'when are doing vector deserialization these are the fast deserializer and the vector row deserializer ',negative
'objectstore also stores name lowercase ',negative
'loglevel args are parsed the python processor ',negative
'the user asking the token same the owner then dont any proxy authorization checks for cases like oozie where gets delegation token for another user need make sure oozie authorized get delegation token ',negative
'generate the list bucketing pruning predicate ',negative
'set transitive true default ',negative
'just forward the row ',negative
'havent added anything should return all ',negative
'classification ',negative
'now apply sarg any sarg this will just initialize stripergs ',negative
'add columns from inprr ',negative
'through all map joins and find out all which have enabled bucket map ',negative
'clone the original join operator and replace with the ',negative
'restore input and output streams ',negative
'not updated yet ',negative
'connect jersey ',negative
'note that valid value corresponding nanosecond timestamp because the second vint present use the value reversednanoseconds the second vint ',negative
'operator file sink reduce sink something that forces new vertex ',negative
'want the driver try print the header ',negative
'leftright skip filtered valid skip filtered valid right alias has any pair for left alias continue for continue has pair but not this turn for inner join join and continue ',negative
'its all good create new entry the map update existing one ',negative
'case the job context not yet lets wait since this supposed synchronous rpc ',negative
'scratch objects ',negative
'got using clause previous join need generate select list per standard for will have joining columns first nonrepeated followed other columns ',negative
'has this filesink already been processed ',negative
'this constant for whole series ',negative
'first make sure through complete iteration the loop before resetting ',negative
'when running unit test mode pass this property hcat which will turn pass hive make sure that hive tries write directory that exists ',negative
'new table always created with new column descriptor ',negative
'year month day ',negative
'',negative
'set the bit element not null ',negative
'check the file format the file matches that the table ',negative
'use this map map the position arglist the position grouping set ',negative
'were interested specific partitions dont check for any others ',negative
'number rows not matching the regex ',negative
'attempting fix valid should not result new file ',negative
'loop through each the lists exprs looking for match ',negative
'check should turn into streaming mode ',negative
'test negative integers result ',negative
'attempt extended acl operations only its enabled but dont fail the operation regardless ',negative
'read each expression and save the value registry ',negative
'',negative
'skewed value has the same length ',negative
'the value read last time ',negative
'stop once see dynamic partition ',negative
'test when the session opened user ',negative
'then assume from its own vertex ',negative
'add rounding ',negative
'schema ',negative
'currently testproccedures always returns empty resultset for hive ',negative
'possible concurrent modification issues try remove cache entries while traversing the cache structures save the entries remove separate list ',negative
'otherwise fall back return null use local tmp dir only ',negative
'make sure nothing escapes this run method and kills the metastore large wrap big catch throwable statement ',negative
'these members have information for extracting row column objects from vectorizedrowbatch columns ',negative
'nonjavadoc see javalangobject ',negative
'compute statistics for columns viewtime ',negative
'nonjavadoc see ',negative
'generate test data ',negative
'take away the only session was expiring ',negative
'standardstruct uses arraylist store the row ',negative
'noarg ctor required for json deserialization ',negative
'create rules registry not trigger rule more than once ',negative
'wasnt create dbtable ',negative
'user provided fully specified partition spec but doesnt exist fail ',negative
'join occurs before the sortmerge join not useful convert the the sortmerge join mapjoin might simpler perform the join and then sortmerge join join converting the sortmerge join mapjoin the job will executed mapjoins the best case the number inputs for the join more than would difficult figure out the big table for the mapjoin ',negative
'construct output object inspector ',negative
'left input positions are not changed ',negative
'first pass identify and break tree necessary ',negative
'hcat will allow these operations performed ',negative
'insert two rows partitioned table ',negative
'identical strings ',negative
'this case are assuming that there single distinct function ',negative
'create and drop partition times events ',negative
'invalid character present return null ',negative
'directory ',negative
'location will vary system ',negative
'the form ',negative
'construct using ',negative
'integer part ',negative
'only have file done ',negative
'writable constant null then return size ',negative
'its map ',negative
'nonjavadoc see int ',negative
'same priority for all tasks ',negative
'create table target ',negative
'make left child right ',negative
'modifying filter condition the incremental rewriting rule generated clause where first disjunct contains the condition for the update branch tokwhere and disjunct for update toktableorcol hdt toktableorcol hdt toktableorcol hdt toktableorcol hdt and disjunct for insert tokfunction isnull toktableorcol hdt tokfunction isnull toktableorcol hdt ',negative
'exceptions the range ',negative
'assume the varchar maximum length was enforced when the object was created ',negative
'stub out the serializer ',negative
'test repeating logic ',negative
'',negative
'return immediately entries found for pruning verified via the timeout ',negative
'and ',negative
'allow multiple mappings the same columninfo when columninfo mapped multiple times only the first inverse mapping captured ',negative
'steps extract the archive temporary folder move the archive dir intermediate dir that the same dir originallocation call the new dir rename the original partitions dir intermediate dir call the renamed dir intermediatearchive rename the original partitions dir change the metadata delete the archived partitions files intermediatearchive ',negative
'skip mode msck should ignore invalid partitions instead throwing exception ',negative
'asserttrueexc instanceof hcatexception hcatexception excgeterrortype with dynamic partitioning this isnt error that the keyvalues specified didnt values ',negative
'class fspaths ',negative
'unnecessary dataputintboffset dataputintboffset ',negative
'for other tasks just return its children tasks ',negative
'lastheartbeat ',negative
'its update metrics for two tasks parallel but not for the same one ',negative
'todo cat for now always use the default catalog eventually will want see the user specified catalog ',negative
'the returned value hivedecimal assume maximum precisionscale ',negative
'class connectionimpl ',negative
'now inplace reversal resultgetbytes first reverse every ',negative
'binarystats ',negative
'set ddl time now not specified ',negative
'all tables partitions are bucketed and their bucket number stored bucketnumbers need check the number buckets ',negative
'scheme use default file system uri ',negative
'these arent column types they are info for how things are stored thrift ',negative
'any the type isnt exact double chosen ',negative
'hcat wants intercept following tokens and specialhandle them ',negative
'use result with some all fractional digits stripped ',negative
'for auth ',negative
'converting tofrom external table ',negative
'',negative
'nothing this should generate proper index ',negative
'need specify the reserved memory for each work that contains map join ',negative
'fixup parent and child relations ',negative
'look them make sure they are all there ',negative
'statementid from directory name there none ',negative
'using biggest small table calculate number partitions create for each small table ',negative
'need use new filesystem object here have the correct ugi ',negative
'position last sync random bytes ',negative
'set the staging directory use ',negative
'groupprivileges ',negative
'cache ',negative
'',negative
'determine the direction order ',negative
'different columns means different commands have run ',negative
'verify that the partitions specified are continuous subpartition value specified without specifying partitions value ',negative
'merge them together ',negative
'conditional node constructed its condition true all the nodes that have been pushed since the node was opened are made children the conditional node which then pushed the stack the condition false the node not constructed and they are left the stack ',negative
'the following two keys should ideally used control connect timeouts however ',negative
'after constant folding child expression the return type udfwhen might have changed recreate the expression ',negative
'uris are checked for string equivalence even spaces make them different ',negative
'check orc and not sorted ',negative
'only print the first line the stack trace contains the error message and other lines may contain line numbers which are volatile also only take the string after the first two spaces because the prefix date and and time stamp ',negative
'release the hms connection for this service thread ',negative
'add more variables required ',negative
'since count has return type big int need make literal type big int relbuilders literal doesnt allow this ',negative
'this table dynamic partition ',negative
'these functions only work the string type ',negative
'schedule cmclearer thread will invoked metastore ',negative
'call listlocatedstatus mockmocktbl call check existence side file for mockmocktbl call open mockmocktbl call check existence side file for mockmocktbl ',negative
'there are some txns the list which has write allocated and hence ahead and get the next write for the given table and update with new next write ',negative
'scheme specified but not authority then use the default authority ',negative
'stores result cache ',negative
'set output names reducesink ',negative
'llap cache purge requires admin privilege mutates state cache the cluster ',negative
'default put row into partition memory ',negative
'there should delta directories the new one the aborted one ',negative
'this method helps reuse session case there has been change the configuration session this will happen only the case nonhiveserver sessions for when cli session started the cli session could reuse the same tez session eliminating the latencies new and containers ',negative
'the caller has already stopped the session ',negative
'might want setxattr for the new location the future ',negative
'first argument the column transformed ',negative
'using property defined hiveconfconfvars test system property overriding ',negative
'indicates the initial capacity the cache ',negative
'the separator for the hive row would using the separator for this struct would ',negative
'grantor ',negative
'based the plan outputs find out the target table name and column names ',negative
'step check mapjointask has single child ',negative
'these are closurebound for all the walkers context ',negative
'should only downcast within valid range ',negative
'compile time ',negative
'delegationtoken ',negative
'list was recreated while were exhausting ',negative
'classname ',negative
'kerberos connections hms required ',negative
'had cache range already expect single matching disk slice given that theres cached data expect there some disk data ',negative
'now tasks would have passed verify that new job requests should succeed with issues ',negative
'close and release resources within running query process since runs under driver state compiling executing interrupt would not have race condition ',negative
'get the sel branch ',negative
'get the context info and set the shared tmp uri ',negative
'logdebugclassname currentkey ',negative
'leave this one for the next round ',negative
'writeid for data from nonacid table and writeidhwm would ensure those data are readable any txns ',negative
'case column stats hash aggregation grouping sets ',negative
'check whether have enough memory allocate for another hash partition need get the size the first hash partition get idea ',negative
'just act passthru between the session and allocation manager dont change the allocation target only thread can that therefore can this directly and actualstatebased sync will take care multiple potential message senders ',negative
'from this point the update motion someone changes the state again that ',negative
'nonjavadoc see int ',negative
'cleaner static object use static synchronized make sure its threadsafe ',negative
'zero out the bits above bitstowrite ',negative
'setting key same value should not trigger configchange event during shutdown ',negative
'bloom filter merge input and output are bytes just modes partial final ',negative
'dont worry about this likely just means its already been created ',negative
'remove subquery ',negative
'createtable insert truncate insert the result just one record ',negative
'this the new number rows after joining with ',negative
'switch the database ',negative
'not based arp and cannot assume uniform distribution bail ',negative
'restore index ',negative
'the hive type this column ',negative
'for nonprefix maps ',negative
'otherwise all null afterwards ',negative
'for primitive types use lazybinarys object for complex types make standard java object from lazybinarys object ',negative
'files size for splits ',negative
'restart sensitive instance ',negative
'other nodes may trying delete this the same time just log errors and skip them ',negative
'override this with noop subclass doesnt need treat nan null ',negative
'any expression child referencing parent column which result function ',negative
'abstract class for hash map result for reading the values onebyone ',negative
'prscrs ',negative
'cleanup the entries less than minuncommittedtxnid from the txntowriteid table ',negative
'create table associated with the import ',negative
'this way the only way the recursive stack fetchnextbatch returns got nonempty result and can consume reached the end the queue and there are more events when return from the fetchnextbatch stack have more results batch were done ',negative
'',negative
'this will cause next txn marked aborted but the data still written disk ',negative
'got match return the value ',negative
'getdelay means the task will evicted from the queue ',negative
'events can start coming the moment the inputinitializer created the pruner must setup and initialized here that sets its structures start accepting events setting initialize leads window where events may come before the pruner ',negative
'write sql statements file ',negative
'even the service hasnt started its make this invocation since this will only happen after the atomicreference address has been populated not adding additional check ',negative
'for now because subquery only supported filter ',negative
'put empty list null ',negative
'scope close ',negative
'evaluate will return text object ',negative
'drain the buffer ',negative
'row count using the classic formula ',negative
'can update all partitions with single analyze query ',negative
'end relshuttleimpljava ',negative
'dont restrict child expressions for projection always use loose filter mode ',negative
'the parent reduce sink the big table side has the same emit key cols its parent can create bucket map join eliminating the reduce sink ',negative
'job request execution time out seconds then request will not timed out ',negative
'alter the via cachedstore can only alter owner parameters ',negative
'test for merging configs ',negative
'start getting the work part the task and call the output plan for the work ',negative
'nonjavadoc see ',negative
'null values and values zero length are not added the cachedmap ',negative
'the rest ops are fks ',negative
'the values from ',negative
'along the way for the columns that the group uses keys ',negative
'testing substring starting from index ',negative
'the mathexpr class contains helper functions for cases when existing library ',negative
'initialize result ',negative
'copy partitions that will split into batches ',negative
'get the mode the lock encoded the path ',negative
'different result after clearing ',negative
'open new connection with these conf vars ',negative
'lockrequestbuilder dedups locks the same entity only keep the highest level lock requested ',negative
'compute the fixed size overhead for the keys ',negative
'precondition make sure this done after the rest the serde initialization done ',negative
'get arguments ',negative
'recreate cluster that picks the additional traitdef ',negative
'update the byte size the map ',negative
'any partition updated then update repl state partition object ',negative
'write the value ',negative
'and return false ',negative
'bucketized keys note that the order need not the same ',negative
'',negative
'',negative
'decrease then increase sessions should not killed return ',negative
'here the global state confined just this process ',negative
'',negative
'level create all keys sumc sumvcolc for ',negative
'preserve the original view definition specified the user ',negative
'using binary search ',negative
'end relshuttlejava ',negative
'property specified file not found local file system use default setting ',negative
'reusable output for serialization ',negative
'get all user jars from work input format stuff ',negative
'failure handling import command and repl load commands are different import will set the last repl before copying data files and hence need allow replacement loaded from same dump twice after failing copy previous attempt but repl load will set the last repl only after the successful copy data files and ',negative
'nothing ',negative
'all other tables are small and are cached the hash table ',negative
'get all the tasks nodes from root task ',negative
'txnid ',negative
'handle repeating case ',negative
'production setfieldtype ',negative
'try eat dot now since could the end remember saw dot can ',negative
'nonjavadoc see ',negative
'the login context name not set are the client and dont need auth ',negative
'when stopping the process are redirecting from the streams might closed during reading should not log the related exceptions visible level they might mislead the user ',negative
'columnar splits unknown size estimate worstcase ',negative
'remove from the too was pushed ',negative
'try infer the type the constant only there are two ',negative
'disable memory estimation for this test class ',negative
'',negative
'create split for the previous unfinished stripe ',negative
'test that existing sharedread table with new exclusive coalesces ',negative
'use linkedhashmap make sure the iteration order ',negative
'decimal division remainder ',negative
'how run this test you can run this test via the command line mvn clean install java jar targetbenchmarksjar prof perf linux java jar targetbenchmarksjar prof perfnorm linux java jar targetbenchmarksjar prof perfasm linux java jar targetbenchmarksjar prof allocation counting via java jar targetbenchmarksjar hasnullstrue isrepeatingfalse processmodehash evalmodepartial java jar targetbenchmarksjar ',negative
'addition with overflow check overflow produces null output ',negative
'create the demuxoperaotr ',negative
'',negative
'how get around that ',negative
'nonjavadoc see ',negative
'set one the roles user belongs ',negative
'compute the reducers run time statistics for the job ',negative
'modify conf using set commands ',negative
'base case its leaf ',negative
'map key separator ',negative
'error heuristic could have generated different errorandsolution for each task attempt but most likely they are the same plus one those probably good enough for debugging ',negative
'update null counter null value seen ',negative
'batch rows emit per processnextrecord call ',negative
'this mimic previous behavior where was thrown through this method ',negative
'version annotation ',negative
'fill the all the vector entries with provided value ',negative
'nonjavadoc see int ',negative
'case the job empty there wont jobstartjobend events the only way ',negative
'regenerate the valuetabledesc ',negative
'test udf considers the difference time components date and date ',negative
'create the objectinspectors for the fields ',negative
'operands ',negative
'sign stays the same ',negative
'the caller this method should guarantee this ',negative
'add sign byte since high bit off ',negative
'results ',negative
'oterwise may later release permit acquired someone else ',negative
'either the token should passed here ctor ',negative
'the table has property external set update table type accordingly ',negative
'not allowed ',negative
'write another large value this should use different byte buffer ',negative
'get detailed tableinfo from query desc extended tablename ',negative
'check ownership for all partitions ',negative
'pktablename ',negative
'this always replaced atomically dont care about concurrency here ',negative
'dont write first empty value get offset reduce the relative offset later there are more than value ',negative
'pull apart the kids the expression ',negative
'add value numdistinctvalue estimator ',negative
'get the path expression for the row only ',negative
'first scale with check overflow ',negative
'are not running this mapred task via child jvm ',negative
'boring scenario two concurrent increases ',negative
'cache administration ',negative
'add the test ',negative
'add another partition the source ',negative
'are the last initialize ',negative
'deep copy case downstream changes ',negative
'isetlongarg exprsetarg ',negative
'row with columns ',negative
'purpose ',negative
'alter table for perform schema evolution ',negative
'copy critical columns ',negative
'only the killed case requires message sent out the ',negative
'dont throw new exception for this just keep going with the next one ',negative
'hivedecimal float number ',negative
'helper method ',negative
'hadoop and hadoop hadoophome gone and replaced with hadoopprefix ',negative
'add another partitioning key based floorrand ',negative
'try ignoring the transaction and make sure works still ',negative
'serde for fetchtask ',negative
'requesttype ',negative
'project the relevant key column ',negative
'attempt locate existing jar for the class ',negative
'when compacting each split needs process the whole logical bucket ',negative
'everything now base ',negative
'return less than because rights digits below lefts scale ',negative
'build the locations predictable order simplify testing ',negative
'check that hook disable transforms has not been added ',negative
'columns are keys column the aggregate value input ',negative
'false not cache yet ',negative
'replace virtual columns with nulls see javadoc for details ',negative
'all well ',negative
'all are selected nothing ',negative
'write out from hive rcfile table and orc table ',negative
'currently support only raw data size stat ',negative
'this helper method copies the group keys from one vectorized row batch another but does not increment the outputbatchsize the next output position was designed for sorted reduce group batch processing mode copy the group keys startgroup ',negative
'this pluggable policy choose the candidate mapjoin table for converting join sort merge join the largest table chosen based the size the tables ',negative
'instantiating the hmshandler with will cause initialize instance the ',negative
'duplicate ',negative
'set partition and order columns overflowbatch can set ref since our last batch held ',negative
'the partitons are also the same check the fieldschema ',negative
'test both with field comments and without ',negative
'statuses can null ddl etc ',negative
'repeated otherinfo ',negative
'remove from list live operations ',negative
'insert the current table alias entry into the map not already present tablealiastoinfo ',negative
'evaluation the decimal constant vector expression after the vector ',negative
'make sure weve built the lock manager ',negative
'continue with next database ',negative
'was null the new authorization plugin must specified config ',negative
'see below the simple newtons equation ',negative
'handle aborted deltas currently this can only happen for tables ',negative
'the set object containing the list use hashset hivedecimalwritable objects instead hivedecimal objects can lookup decimalcolumnvector hivedecimalwritable quickly without creating hivedecimal lookup object ',negative
'the process choosing new blank works ',negative
'make sure dont collide with the source ',negative
'already existing semijoin branch reuse ',negative
'correlation optimizer will not try optimize this query ',negative
'dynamic partition insert case ',negative
'statementclose after resultsetclose should close the statement ',negative
'time based log retrieval may not fetch the above log line logging stderr for debugging purpose ',negative
'following two config keys are required fileoutputformat work correctly usual case hadoop jobtracker will set these before launching tasks since there jobtracker here set ourself ',negative
'normalize positive ',negative
'topn query ',negative
'bround with digits ',negative
'add bigint values ',negative
'since nulls can provide values for all rows ',negative
'need lookup the table and get the select statement and then parse ',negative
'does not make sense have any the metastore config variables ',negative
'this makes sure has the same downstream operator plan the original join ',negative
'compare the results fetched last time ',negative
'there may not base dir the partition was empty before inserts this partition just now being converted acid ',negative
'ever created for taskattempt ',negative
'listener parameters arent expected have many values far only will add parameter lets set low initial capacity for now find out many parameters are added then can adjust remove this initial capacity ',negative
'base class for mocking job operations with concurrent requests ',negative
'the letter sequence foo ',negative
'deferclose indicates the closedestroy should deferred when the process has been interrupted should set true the compile called within another method like ',negative
'could also check writeset but that seems overkill ',negative
'sign mark ',negative
'have data from this point could unneeded skip ',negative
'lets first test for default permissions this the case when user specified nothing ',negative
'oncreatetable alters the table add the topic name since this class generating that alter dont want notify that alter take quick look and see ',negative
'indicate that weve replaced the value ',negative
'ensure the table online ',negative
'write null element element field omitted ',negative
'get partitionlist from source ',negative
'defaultpoolpath ',negative
'make sure dont compact dont need compact ',negative
'cbo did not optimize the query might need replace grouping function special handling grouping function ',negative
'redact the sensitive information from the configuration values ',negative
'this noop return successfully ',negative
'check the conditions apply this transformation not meet them bail out ',negative
'todo delink from sessionstate tezsession can linked different hive sessions via the pool ',negative
'the element list ',negative
'expandandrehash new resizethreshold resizethreshold metricexpands metricexpands ',negative
'converts partnames into partname string partname string ',negative
'useexternalbuffer ',negative
'restart even theres internal error ',negative
'initialize the function localizer ',negative
'add another session ',negative
'check has sqcountcheck ',negative
'thread executing the query ',negative
'convert the field java class string because objects string type ',negative
'its retrying first regenerate the path list ',negative
'',negative
'for entry should set null ',negative
'delete jar and its dependencies added using query ',negative
'the vertex name longer than column width trim down ',negative
'common outer join result processing ',negative
'coordinator running overlord well ',negative
'finally remove the partition columns from the end derivedschema clearing the sublist writes through the underlying ',negative
'',negative
'need update the exprnode currently they refer columns the output the join they should refer the columns output the ',negative
'since left integer always some products here are not included ',negative
'also add the last vcol ',negative
'info from ',negative
'the number duplicates for each series key null nonnull ',negative
'dont check compatibility two object inspectors but directly pass them into users this class should make sure doesnt throw exceptions and returns correct results ',negative
'will try pushdown first make the filter this will also validate the expression ',negative
'used initialize streaming evaluator ',negative
'create table ',negative
'for the optimization that reduce number input file limit number files allowed more than specific number files have selected skip this optimization since having too many files inputs can cause unpredictable latency its not necessarily cheaper ',negative
'only permanent functions need authorized builtin function access allowed all users user can create temp function they should able use without additional authorization ',negative
'complete txn ',negative
'the result the swapping operation either project ',negative
'preempt only theres pending preemptions avoid preempting twice for task ',negative
'send these potentially large objects longer intervals avoid overloading the ',negative
'the foo nonexistent ',negative
'process singlecolumn string inner bigonly join vectorized row batch ',negative
'decimal types can specified with different precision and scales decimal opposed other data types which can represented constant strings the regex matches only the decimal prefix the type ',negative
'all the catalogs should cached ',negative
'finally get all the stuff for serdes just the params ',negative
'set config that will produce multiple queries ',negative
'key the database name value map from the qualified name the view object ',negative
'create fake root for local ',negative
'lefts signum wins dont need anything ',negative
'check see this table level request partitioned table ',negative
'for one element the variance always ',negative
'mapping from hadoop job the stack traces collected from the map reduce task logs ',negative
'create colinfo and then row resolver ',negative
'generic function node case operator udf node ',negative
'single split ',negative
'inmemory hdfs ',negative
'turn escape ',negative
'nodetype ',negative
'assume cache chunks would always match the way read check and skip ',negative
'this can never null empty ',negative
'case topn for windowing need distinguish between rows with null partition keys and rows with value for partition keys ',negative
'count all values seen far ',negative
'aggregation classes ',negative
'over all the input paths and calculate known total size known ',negative
'get the key ',negative
'extract column from the given exprnodedesc ',negative
'construct using ',negative
'testing negative substring index ',negative
'then partition number any ',negative
'there are nonnumeric arguments that dont match from one udf another give this point ',negative
'tracks pending preemptions per host using the hostname always accessed inside lock ',negative
'set columnaccessinfo for view column authorization ',negative
'map integer string ',negative
'remove entry for operator ',negative
'merge task could after dynamic partition insert ',negative
'use type promotion ',negative
'',negative
'equal sum small tables size ',negative
'there are aggregations order need remember them ',negative
'need run this get consistent filterop conditionsfor operator tree matching ',negative
'replace view ',negative
'for numeric well minimum necessary cast cast the type expression bad things will happen ',negative
'nothing but count ',negative
'get the transaction ',negative
'boolean that says whether slow start not ',negative
'need consolidate more buffers into one decompress ',negative
'initialize stats publisher necessary ',negative
'create client manage our transaction ',negative
'index rank function ',negative
'preempt any host ',negative
'determin which task has been preempted normally task would preempted based starting later however both may have the same start time either could picked ',negative
'directly serialize with the caller writing fieldbyfield serialization format the caller responsible for calling the write method for the right type each field calling writenull the field null ',negative
'first table union query with view parent ',negative
'test the validation incorrect null values the tables ',negative
'trace error not exists ',negative
'track cleaner metrics ',negative
'have exhausted our current batch read the next batch ',negative
'jump out the loop need input from the big table ',negative
'set fake input and output streams ',negative
'synchronous event processing loop wont return until all events have ',negative
'throw exception ',negative
'swallow the exception since wont affect the final result ',negative
'the session has delegation token obtained from the metastore then cancel ',negative
'append asis ',negative
'try authenticating with the httphost principal ',negative
'columncolumn ',negative
'get the names out ',negative
'since not used further the tree ',negative
'last row last batch determines isgroupresultnull and long lastvalue ',negative
'empty parameters are sent columnmapping ',negative
'external client currently cannot use guaranteed ',negative
'executes the callable task with help execute call and gets the result the task also sets job status completed state not already set failed and returns result future ',negative
'have initialization here because the supers constructor calls next and thus need initialize before our constructor ',negative
'install the configuration the runtime ',negative
'extract columns and values ',negative
'index entries table usage ',negative
'the field that passed not primitive and either the field not declared schema was given initialization the field declared primitive initialization serialize the data json string otherwise serialize the data the delimited way ',negative
'the parameters are checked manually not check them ',negative
'batch size and decaying factor ',negative
'werent able check ',negative
'there should delta directories ',negative
'the aggregation type sum scaleup ',negative
'should generate ',negative
'this called from hcat always allow embedded metastore was the default ',negative
'core logic load hash table using hashtableloader ',negative
'disabled for acid path ',negative
'the was called hmshandlershutdown would have already cleaned thread local rawstore otherwise now ',negative
'add shutdown hook for cleanup there are elements remaining the cache which were not cleaned this the best effort approach ignore any error while doing notice that most the clients would get cleaned via either the removallistener the close call only the active clients that are the cache expired but being used other threads wont get cleaned the following code will only clean the active cache ones the ones expired from cache but being hold other threads are the mercy finalize being called ',negative
'check the easy cases first ',negative
'make sure nothing really moved ',negative
'simulate emitting records processnextrecord with large memory usage limit ',negative
'previously when path empty null and default path specified was the return value for escapepathname ',negative
'practice dont really care about the data any these tables except far creates partitions the sql being test not actually executed and results the wrt acid metadata supplied manually via but having data makes easier follow the intent ',negative
'export case ',negative
'use multiple lines for statements not terminated the delimiter ',negative
'resultset output formatting classes ',negative
'primitiveentry ',negative
'for serialization ',negative
'its table alias ',negative
'our reading positioned the value ',negative
'the table has sample specified bail from calcite path ',negative
'try alternate config param ',negative
'serializescale fastscale ',negative
'record partitions that were written ',negative
'optimize the scenario when there are grouping keys only reducer ',negative
'other columns provided nonnull values can return repeated output ',negative
'partitionview does not have and not need update its column stats ',negative
'this will used the outputcommitter pass the metastore client which turn will pass the tokenselector that can select ',negative
'map container succeeded ',negative
'dont fail this besteffort ',negative
'skip the directory that have found ',negative
'arguments ',negative
'simple map join the whole relation goes memory ',negative
'the assign method will overridden for char and varchar ',negative
'have found map systematically deserialize the values the map and return back the map ',negative
'thread pool execute job requests ',negative
'the join does fetching next row groups itself ',negative
'authorization done just call super ',negative
'push the feed its subscribers ',negative
'start cleanup ',negative
'this point weve set all the tables and ptns were going test drops across replicate first and then well drop the source ',negative
'rowid always the first field ',negative
'note assume reuse only possible for the same user and config ',negative
'use ranges and duplicate multipliers reduce the size the display ',negative
'coldouble ',negative
'nonjavadoc see set null only because carryforwardnames true ',negative
'get aliastopath and pass the heuristic ',negative
'try insert times rehash that fails ',negative
'initialize fetchtask right here ',negative
'implementation note implement hivedecimal with the mutable fasthivedecimal class that class uses protected all its methods they will not visible the hivedecimal class even one casts fasthivedecimal you shouldnt able violate the immutability hivedecimal class ',negative
'not create identity project does not rename any fields ',negative
'change file modification time and look for cache misses ',negative
'keep open txn which refers the aborted txn ',negative
'update the attrs ',negative
'table deleted ',negative
'the hash code for each nonnull key ',negative
'pooltriggers ',negative
'muxoperator should only have single child ',negative
'determine the name our map reduce task for debug tracing ',negative
'group aggregate minmax and bloom filter ',negative
'password file contents are trimmed trailing whitespaces and newlines ',negative
'dont zerodivide one comes random ',negative
'only one result column verify the column name verify the column name ',negative
'extract stages ',negative
'not allow temp table rename the new name already exists temp table ',negative
'these types are not supported yet should define complex type date thrift that contains single int member and dynamicserde should convert date type runtime ',negative
'number digits retain from the end ',negative
'tablescan only available during compile ',negative
'check two arguments were passed ',negative
'and again one last time ',negative
'note may make sizeetc configurable later ',negative
'note dont pass the config reopen the session was already open would have kept running with its current config preserve that behavior ',negative
'calcite creates null literal with null type here but ',negative
'right full outer join need iterate through the row container that contains all the right records that did not produce results then for each those records replace the left side with null values and produce the records observe that only enter this block when have finished iterating through all the left and right records aliasnum numaliases and thus have tried evaluate the postfilter condition every possible combination ',negative
'firstname null firstname sue ',negative
'push down projections columnar store works for rcfile and orcfile ',negative
'uses level parallel implementation bfs recursive dfs implementations have issue where the number threads can run out the number nested subdirectories more than the pool size using two queue implementation simpler than one queue since then will have add the complex mechanisms let the free worker threads know when new levels are discovered using notifywait mechanisms which can potentially lead bugs not done right ',negative
'stop requested and handled inside ',negative
'load the properties from config file ',negative
'the fields that uses give information about plugin endpoint some these will removed when registry implemented will generate and publish them ',negative
'first try nonrandom values ',negative
'note inserts into new part this wont fail ',negative
'throw exception simulate issue with cleaner thread ',negative
'the file missing getting modified then refer path ',negative
'get the other one examining join ',negative
'make sure the columns does not already exist ',negative
'final reduction case column stats numrows case column stats grouping sets minnumrows ndvproduct sizeofgroupingset case column stats grouping sets minnumrows ndvproduct ',negative
'enable createasacid ',negative
'hadoopproxyuser set create delegationtoken using real user ',negative
'check specificfilterset for qtest specific ones ',negative
'nonjavadoc see ',negative
'when all inputs are accounted for the output forwarded appropriately ',negative
'directory creation otherwise within the writers ',negative
'binary decimal conversion ',negative
'originally copied from but seemed have bug ',negative
'list compactions clean ',negative
'get the output location the order partition keys are defined for the table ',negative
'start metrics ',negative
'storage information ',negative
'and condition needs computed ',negative
'nothing handled below types will mismatch ',negative
'bucketed sorted tablepartition they cannot merged ',negative
'blocks more llap queries can submitted ',negative
'cached successfully add policy ',negative
'note assume here the session before resolve killquery result here still use that because all the user ops above like return reopen etc dont actually returnreopen when kill query progress ',negative
'apply overlay query specific settings any ',negative
'get the collection separator and map key separator ',negative
'evaluate the aggregation input argument expression ',negative
'the initialcapacity which cannot provide reasonable positive number here ',negative
'col ',negative
'add the jar file ',negative
'ignoresee ',negative
'test different format types ',negative
'generate the groupbyoperator for the query block parseinfogetxxxdest the new groupbyoperator will child the param mode the mode the aggregation partial complete param not null this function will store the mapping from aggregation stringtree the this parameter can used the nextstage groupby aggregations return the new groupbyoperator ',negative
'boolval ',negative
'all partitions with should have columns ',negative
'expected the operation takes time continue the loop and wait for completion ',negative
'null args ',negative
'note the sinks and ddl cannot coexist this time but they could would ',negative
'map requires levels key separator and keypair separator ',negative
'close off the buffer with normal tag ',negative
'now check the table folder and see find anything that isnt the metastore ',negative
'logger jobs ',negative
'get ready for another round small table values ',negative
'get the list table scan operators for this join interface has been provided currently only enabled for simple filters and selects ',negative
'col ',negative
'need least server for testing ',negative
'here are now doing acid read must have orcsplit ',negative
'this helper object serializes lazybinary format reducer values from columns row ',negative
'check for the line how check the line invalid for any reason the job will fail ',negative
'',negative
'create cost metadata provider ',negative
'parse until pair separator currentlevel ',negative
'need state store eventually for current state measure backoffs ',negative
'construct the setfacl command ',negative
'files size for splits ',negative
'cols ',negative
'found the plan already connected which means this derived from the cache ',negative
'element for key byte hash table hashmultiset ',negative
'hcat requires alterdata privileges for alter table location statements for the old tablepartition location and the new location ',negative
'add for tag ',negative
'were continuing existing command ',negative
'wrapping for exception handling ',negative
'leadership state changes and sending out notifications listener happens inside synchronous method curator only lightweight actions mainevent handler thread time consuming operations are handled via separate executor service registered via ',negative
'this call fills the column names types and partition column count ',negative
'leading for month and day should work ',negative
'grant option ',negative
'handle the isnull array first tight loops ',negative
'extract stats keys from statstask ',negative
'should not used ',negative
'both branches are null ',negative
'col ',negative
'generation ',negative
'use construct ',negative
'writeentity defaultacidtblpart typetable writetypeinsert isdpfalse ',negative
'this collector just row counter ',negative
'targetpath path xyz here xyz present the file system create the structure till xyz work rename for multilevel directory and rename fails delete the path xyz targetpath have multilevel directories like xyz xyz the renaming the directories are not atomic the execution will happen one one ',negative
'assume the full acid table ',negative
'cant happen ',negative
'split exclusively serves alias which needs sampled add the split list the alias ',negative
'task requested host got host since host dead and host full ',negative
'create syntax tree for function call myisnullcol unknown ',negative
'map each aborted write with each allocated txn ',negative
'there should still four directories the location ',negative
'bucket mapjoin llap make sure the caches are populated get the subcache ',negative
'enablejobreconnect param was not passed user use cluster wide default ',negative
'optional optional optional optional optional ',negative
'this also provides completion information and possible notification when task actually starts running first heartbeat ',negative
'rows emitted with separate thread per processnextrecord call ',negative
'add grpset col ',negative
'serde property for how the hive column maps accumulo ',negative
'only allow constant field name for now ',negative
'this function checks whether all bucketing columns are also join keys and are same order ',negative
'test gettables ',negative
'test that separate partitions dont coalesce ',negative
'header ',negative
'copy bytes into scratch buffer ',negative
'block make sure move happened successfully ',negative
'make sure the cleanup doesnt leave the pool without session ',negative
'release attempt again succeed ',negative
'key columns ',negative
'',negative
'placeholder minimum value enforced ngramestimator ',negative
'there should only one columnstatistics ',negative
'sanity check restrictedconfig always set setup ',negative
'new tai lue letter tha bytes ',negative
'round towards negative infinity ',negative
'partitionvalues ',negative
'verify that columnstatsaccurate removed from params ',negative
'resultsignum ',negative
'get the field objectinspector and the field object ',negative
'constant can ignore ',negative
'note other parent readers init everything ctor but union does startstripe ',negative
'move pending and intervening discard the allocator can release ',negative
'hadoop missing public api check for snapshotable directories check with the directory name until more appropriate api provided hdfs ',negative
'the first child set the mode variable value otherwise the mode are working different bail out ',negative
'this keep track null literal which already has been visited ',negative
'reset the selection vector ',negative
'strip the leading provided this the assumption with which were going start configuring sessionconfext ',negative
'important note for multior the class will catch cases with more parameters ',negative
'create the serialized string for type ',negative
'decimals are stored biginteger convert and compare ',negative
'only write the record file are writing line otherwise might get garbage from backspaces and such ',negative
'map filter the new filter over join ',negative
'since expect files minor compacted job produce delta minor compacted job file obsolete make delta skipped and then the requested minor compaction combine delta delta and delta make delta major compaction create base ',negative
'this uses ',negative
'are going use cache change the path depend file for extra consistency ',negative
'accumulate the counts ',negative
'available copy state registry for optimization rules ',negative
'task just created ',negative
'write header ',negative
'least one task would have been added update the repl state ',negative
'assertassertequals locksgetlockid ',negative
'standard object ',negative
'false prune ',negative
'neither side repeating ',negative
'truncate the table missing either due droprename which follows the truncate the existing table newer than our update ',negative
'see javadoc ',negative
'',negative
'nonjavadoc see ',negative
'next check this table has partitions and get the list partition names well allocate ',negative
'leading spaces are significant ',negative
'these are null when evaluate called for the first time ',negative
'since the loop left txntowriteidssize ',negative
'acquire different locks different levels ',negative
'firstfetchhappened true reality almost always calls joinonegroup fix ',negative
'more matches expected ',negative
'follow ',negative
'also need delete partdate make consistent ',negative
'now have vector aggregation buffer sets use for each row can start computing the aggregates the number distinct keys the batch can use the optimized code path aggregateinput ',negative
'could not get status for some reason log and send empty status back with just the that caller knows even look the log file ',negative
'sample data ',negative
'assuming that valid ugi with kerberos cred created llap ',negative
'translate args ',negative
'possible the table deleted during fetching tables the database that case continue with the next table ',negative
'this happens when the code inside the jmx bean threw exception log and dont output the bean ',negative
'its hard distinguish union with null from null union ',negative
'use nullindicator decide whether project null nothing the literal null ',negative
'rip apart the object inspector making sure got what expect ',negative
'gather memory threshold ',negative
'the udtf ',negative
'instances are running ',negative
'make this client wait job tracker not behaving well ',negative
'were here then socketgetlocalport was the port exclude since both sockets were open together point time were guaranteed that socketgetlocalport not the same ',negative
'dont lose this bit weird dance here ',negative
'the end ',negative
'phase hold onto any cte definitions aliastocte cte definitions are global the query ',negative
'create cmroot with permission not exist ',negative
'nonjavadoc see ',negative
'test query where timeout does not kick set ',negative
'nonjavadoc this provides lazyfloat like class which can initialized from data stored binary format see int int ',negative
'wildcard bind ',negative
'fetch the tablescan operator ',negative
'call listlocatedstatus mockmocktbl ',negative
'bail muxoperator because mux operator masks the emit keys the constituent reduce sinks ',negative
'key expiration create already expired key ',negative
'visiblefortesting ',negative
'the following union operation returns union which traverses over the first set once and then then over each element second set order that not contained first this means doesnt replace anything first set and would preserve the writetype writeentity first set case outputs list ',negative
'weve not seen this terminal before need check rootunionworkmap which contains the information mapping the root operator union work union work ',negative
'someone allocating arena ',negative
'reenable timeouts ',negative
'this the set entities that the statement represented extlockid wants update ',negative
'session should not lost however the fraction should discarded ',negative
'batchindex classname nomatch currentkey currentkey ',negative
'check select expr constant current logic used look for there none then the expression constant ',negative
'not fetching from table directly but from temp location ',negative
'constructors various flavors follow ',negative
'old schemas within ',negative
'serialize json based field annotations only ',negative
'check implementation class ',negative
'list softreferences ',negative
'negotiation complete remove this handler from the pipeline and register with the kryo instance handle encryption needed ',negative
'this really just ',negative
'case column stats hash aggregation grouping sets ',negative
'lookup long the hash set param key the long key param hashsetresult the object receive small table values information match for spill has information where spill the big table row return whether the lookup was match match spilled the partition with the key currently spilled ',negative
'hex input also supported ',negative
'base ',negative
'decimal ',negative
'middle pattern ',negative
'join operator ',negative
'initialize map local work ',negative
'add all nonvirtual columns from the tablescan operator ',negative
'process multikey inner bigonly join vectorized row batch ',negative
'lazily create pathchildrencache ',negative
'group requires arraylist dont ask ',negative
'priority ',negative
'typespecific handling ',negative
'its not clear this rewrite always performant since extra map phase introduced for job may offset gains this multistage aggregation need cost model for enable this ',negative
'get table metadata ',negative
'key reducesinkkeyint reducesinkkeyint value colint ',negative
'sparsesparse merge ',negative
'table ownership for createdropalter index ',negative
'get the tables for the desired patten populate the output stream ',negative
'keep separate from the creating events case the send blocks ',negative
'the following tests will verify the deprecation variable still usable ',negative
'assume the worst ',negative
'tell the operator the status the next keygrouped vectorizedrowbatch that will delivered the process method reduceshuffle these semantics are needed ptf can ',negative
'add the actual source input ',negative
'move the pointer the next byte since have written ',negative
'will generate results for all matching and nonmatching rows ',negative
'get the group keys columninfo ',negative
'failed because object doesnt exist ',negative
'thread simulating user session hiveserver ',negative
'keeps track regular timed heartbeats primarily used timing mechanism send log counters ',negative
'verify the connection fails after canceling the token ',negative
'decode utf ',negative
'same object ',negative
'aware that result could the same object ',negative
'next should always return false ',negative
'optional string executionmode ',negative
'null plan means disabled via command could still reenabled ',negative
'set that parent initialization done and call initialize children ',negative
'note this thing should know nothing about acid schema reads physical columns index schema evolutionacid schema considerations should higher level ',negative
'get aggregation from calcite given name ret type and input arg ',negative
'are currently walking the big table side the merge join need create hook ',negative
'partition values are specified nonpartitioned table ',negative
'sort and pick partition keys ',negative
'remove requested quantiles from the head the list ',negative
'lock few blocks without telling the policy ',negative
'need add cast datetime family ',negative
'use case ',negative
'row count and where there are nulls means index disabled and dont have stats ',negative
'some cases were converted before calling emulate those cases first ',negative
'return aggregate collations ',negative
'this the last byte leave the high bit off ',negative
'add partitions located the tabledirectory default ',negative
'fileheader resulttype argtype argtype ',negative
'testing matched and with case statement using targeta breaks this ',negative
'old table the cache but the new table cannot cached ',negative
'null implies empty column qualifier ',negative
'',negative
'only log the first wait and check after wait the last iteration ',negative
'return sec difference ',negative
'use runlength encoding only record run length same prevvaluelen occurs more than one time and negative the run length distinguish runlength and normal value length for example the values lengths are record and for value lengths record ',negative
'the key the next lowest reader ',negative
'expected result entry the recordidentifier data entry file before compact ',negative
'tasks started after the addfile call completes ',negative
'the key found mapcolumnvector set the value ',negative
'this means there are tables something the database ',negative
'save the evaluator that can used the nextstage ',negative
'this plan projecta all gby keys rewritten nullable projexpr aggregate groupbyall left input refs aggrewritten expression agg projectb rewriten original projected exprs joinreplace corvar input ref from leftinputrel leftinputrel rightinputrel ',negative
'update the aggregations ',negative
'this arbitrary note that metadata may come from big scan and nuke all the data from some small frequently accessed tables because gets such large priority boost start with think the multiplier the number accesses after which the data becomes more important than some random readonce metadata purelfu scheme ',negative
'cancel ',negative
'negative numbers indicate column deserialize read from the small tables lazybinary value row ',negative
'string string equivalent considered numeric when used arithmetic operator ',negative
'string can used write char and varchar when the caller takes responsibility for truncationpadding issues ',negative
'lower word bits the lower bit used the sign and removed need multiplier digit commad ',negative
'the objectinspector for the row ',negative
'numreducers and newnumreducers ',negative
'continue null out the field ',negative
'those tables directory ',negative
'the serializer then requires that noopfetchformatter used but when isnt then either the thriftformatter the should used ',negative
'decimal rounding ',negative
'update the joinkeys appropriately ',negative
'perform major compaction ',negative
'add column change serde file formats ',negative
'store the changes ',negative
'set method this requires calcite change ',negative
'the combinations below definitely result overflow ',negative
'remove currtask from parenttasks ',negative
'dates are also valid dates the dates are within ',negative
'will store all the new changed properties the job the udf context the the method need not ',negative
'for now olderclass has version and newerclass versions ',negative
'allow add any counters they have collected ',negative
'from ',negative
'partition level column stats merging ',negative
'create new operator hashtable dummyoperator which share the table desc ',negative
'create few read locks all the same resource ',negative
'check whether current operators are equal ',negative
'lets create new list and copy dont have linked list ',negative
'validate and setup symbolinfo ',negative
'possible because querycomplete message from the can come first kill successful before the fragmentcomplete reported ',negative
'heartbeat ',negative
'given the keyindex these arrays return the columnvectortype the type specific index into longindices doubleindices etc ',negative
'must just column name ',negative
'varchar ',negative
'child can expr alias expr ',negative
'deletes for the bucket being taken into consideration for this split processing ',negative
'prevent instantiation ',negative
'for dynamic partitioned writes without all keyvalues specified create temp dir for the associated write job ',negative
'since ifexists was not set true trying create the same table again will result exception ',negative
'set values look for include the original blank value longminvalue make sure ',negative
'relative positions the blocks dont change over time priorities expire can only decrease only have one block that could have broken heap rule and always move down therefore can update priorities other blocks for part the heap correct any discrepancy wthe parent after expiring priority and any block expire the priority for already has lower priority than that its children ',negative
'table current event has partition flag different from existing table means some the previous events same batch have drop and create table events with same same but different partition flag this case should with current events table type and create the dummy table object for adding repl tasks ',negative
'verify udf reflect allowed exception will thrown ',negative
'bigtablecandidates can never null ',negative
'find tables which name contains tofind hidden the default database ',negative
'subscriber can get notification about addition table hcat listening topic named hcat and message selector string hcatevent hcataddtable ',negative
'independent hashtable and can modified need copy ',negative
'this integer because derby converts boolean char breaking sysdb ',negative
'open the log file and read the lines parse out stack traces ',negative
'file checksum not implemented for local filesystem rawlocalfilesystem ',negative
'handle initialization results ',negative
'create another table for incremental repl verification ',negative
'cancel the query ',negative
'have regular single rows from the input file format reader that will need deserialize ',negative
'determine which rows are left ',negative
'newschema ',negative
'incase this corresponds subquery then modify its point subquery alias ',negative
'remove valid columns ',negative
'find file instead dir dont change inputpath ',negative
'done processing the task ',negative
'the subqueries are maponly jobs ',negative
'there correlation condition anywhere the filter dont push this filter past project since some cases can prevent correlate from being decorrelated ',negative
'accessing order join cols bucket cols should same ',negative
'task the new task ',negative
'note this partitionspec has ordered map ',negative
'credential provider has entry for our credential then should used ',negative
'first value written without next relative offset next value length next value last flag next value length small flag etc ',negative
'need drop the primary metastore shared both primaryreplica constraints ',negative
'make power backing down the ',negative
'where product the carry from ',negative
'did not find the column ',negative
'store the bucket path bucket number mapping the table scan operator although one mapper per file used possible that any mapper can pick any file depending the size the files the bucket number ',negative
'rename the table missing either due droprename which follows the current rename the existing table newer than our update ',negative
'avro considers bytes primitive hive doesnt make them list tinyint ',negative
'set hiveconf statics default values ',negative
'updatecurrentkey needs called initialize the master key there should null check added the future rollmasterkey updatecurrentkey ',negative
'todo setup ssl shuffle ',negative
'vectors ',negative
'collection usage threshold not support worst case set memory threshold memory usage before ',negative
'helper determine what java options use for the containers falls back mapreduces map java opts tez specific options are set ',negative
'one work only one map join operator can bucketed ',negative
'check singleaggrel singlevalue agg ',negative
'nonjavadoc this provides lazybinary like class which can initialized from data stored binary format see int int ',negative
'table for which show locks being executed ',negative
'set pass property operation and check its set the query config ',negative
'timestamp between ',negative
'bypass only outerrr not null otherwise need look for expressions outerrr for subqueries select minbvalue from table group bkey ',negative
'ascending null first default for ascending order ',negative
'interface into the registry service ',negative
'this stage mapreduce processing the groupby tha mapside aggregates was just used reduce output data case distincts partial results are not used and iterate again invoked the reducer case nondistincts partial results are used and merge invoked the reducer ',negative
'see also ',negative
'try empty rows query ',negative
'returns addr ',negative
'wait for another iteration make sure event gets processed for receive allocation ',negative
'check that are cleaning the empty aborted transactions ',negative
'the rowids are the same after compaction ',negative
'these constants are also imported ',negative
'for run length byte encoding record the number bits within current byte consume ',negative
'remove reducer ',negative
'this still could dpp ',negative
'whole repeated key batch was filtered out ',negative
'this spawns separate thread walk through the cache and removes expired nodes only one cleaner thread should running any point ',negative
'step move the file destination ',negative
'handle the rest the aggregation that the bottom aggregate hasnt handled ',negative
'returns rows from possibly multiple bucket files small table ascending order utilizing primary queue borrowed from hadoop ',negative
'need preserve loggedinuser ',negative
'may have already connected work with childwork case for example lateral view lvf sel sel lvjudtf sel here can reached from via two different paths there any child work after dont want connect them with the work associated with more than once ',negative
'degree parallelism ',negative
'make power ',negative
'partition value cant end this suffix ',negative
'the planner gives subset virtual columns available for this table scan and only support some virtual columns vectorization create the intersection note these are available vectorizable virtual columns later remember which virtual columns were actually used the query just those will included the map that has the information for creating the map vectorizedrowbatch ',negative
'this instance will not added back since its services are not yet ',negative
'match ',negative
'compare cell value with constant value filter they match and cell value isnt other return true they dont match but cell other and value filter not skewed value return unknown why not true true not enough since not true false but not unknown unknown for example skewed column skewed value clause where not cell other evaluate notc other ture notc will false but wrong skip default dir but unknown notc will unknown will choose default dir all others return false ',negative
'default false ',negative
'setting true ensures that performs the query operation the connected user instead the user running ',negative
'found something before ran out components use ',negative
'verify the actual locations being correct should different location splits are supposed consistent across jvms the test setup verify different host make sure not hash the same host osos the test were fail because the host the same the assumption about consistent across jvm ',negative
'kills templeton job with multiple retries job exists returns true kill job attempt success otherwise returns false ',negative
'needrequirelock false the release here will nothing because there lock ',negative
'since equiv should get back first container ',negative
'queryplan here ',negative
'for spark nonlocal mode any added dependencies are stored which the executors working directory local mode need manually point the processs working directory ',negative
'return empty result since only constant desc exists ',negative
'before closing the operator check statistics gathering requested and provided record writer this different from the statistics gathering done processop processop for each row added serde statistics about the row gathered and accumulated hashmap this adds more overhead the actual processing row but the record writer already gathers the statistics can simply return the accumulated statistics which will aggregated case spray writers ',negative
'calculate unique skewed elements for each skewed column ',negative
'one serialized key for more rows for the duplicate keys reduceskiptag tag tag reducetagbyte int reducetagbyte keylength loginfoprocess offset length ',negative
'trailing space should ignored for char comparisons write stripped values for this serde ',negative
'setting these props match lazysimpleserde ',negative
'check column types ',negative
'check task started ',negative
'internal representation integer representing day offset from our epoch value ',negative
'not public since must have the serialize write object ',negative
'classloader invokes this static block when its first loaded lazy initialization ',negative
'constant null expr just return ',negative
'apply the transformation ',negative
'create dummy vertex for mergejoin branch for self join this ',negative
'future add intervalyearmonth etc desired ',negative
'advance the primary reader the next record ',negative
'walk through udaf collect udaf info ',negative
'',negative
'array was passed parameter make sure its array primitives ',negative
'number arguments this udf external name ',negative
'valid paths ',negative
'surrogate pair case ',negative
'fetch rows from splits ',negative
'not need anything bail out ',negative
'the numbers input columns and output columns should match for regular query ',negative
'raise custom exception like ioexception and verify expected message ',negative
'old test moved msckrepairq ',negative
'walk operator tree create expression tree for filter buckets ',negative
'the columns the group expressions should not intersect with the columns the distinct expressions ',negative
'have when not matched and boolean expr then insert ',negative
'the plan knows are reading this locks security ',negative
'note that this keyinterval may adjusted later due copyn files ',negative
'check the existing partition values can type casted the new column type ',negative
'table matcher ',negative
'nexttxnidntxnnext could minuncommittedtxnid ',negative
'right now assume only and hll are available ',negative
'since there collision index will used for the next value have the map point back original index ',negative
'add the auth filter ',negative
'fill the buffer with key value pairs ',negative
'this field null ',negative
'edge case ',negative
'this case abd join will executed first and abdc join will executed next ',negative
'generate absolute path relative current directory hdfs home directory ',negative
'reads the index file for each requested mapid and figures out the overall length the response which populated into the response header ',negative
'the actual deserialization may involve nested records which require recursion ',negative
'different queries the session may using the same lock manager ',negative
'lock operations themselves dont require the lock ',negative
'partition columns partition values ',negative
'update the leaf place ',negative
'create conditional work list and task list ',negative
'custom root specified update the parent path ',negative
'nope look see our home dir has been explicitly set ',negative
'object inspectors corresponding the struct returned terminatepartial and the long field within the struct count ',negative
'singlethreaded init case with this the ordering sessions the queue will with sessions queues there ensuring uniform distribution the sessions across queues least begin with then sessions get freed the list may change this ordering multi threaded init case its free for all ',negative
'because this file will fetched fetch operator ',negative
'reduce filter with stats information ',negative
'summary for column statistics ',negative
'clear out any rows may have processed rowmode for the current partition ',negative
'added project need produce new keys than the original input fields ',negative
'look for getfieldname isfieldname ',negative
'only used for dynamic partitioned hash joins mapjoin operator the reducer ',negative
'choose first full batch with selection ',negative
'implementation row container ',negative
'writeidhwm known query all writeids under the writeid hwm any writeid under hwm allocated txn txnid hwm belongs openaborted txns then will added invalid list the results should sorted ascending order based write the sorting needed exceptions list validwriteidlist would lookedup ',negative
'value will null ',negative
'test when first argument has nulls ',negative
'record move events need cluster fraction updates that happens step ',negative
'using the hook startup ensures that the hook always has priority over settings xml the thread local conf needs used because this point has already been initialized using conf ',negative
'iterate over the rest the children ',negative
'cbucketcols pbucketcols have constant node expressions avoid the merge ',negative
'insert clause ',negative
'ket partition values and the value wrapper around the partition object ',negative
'need collect statistics index columns ',negative
'put uncompressed data cache ',negative
'handle the single block case ',negative
'parent reduce sinks ',negative
'clear all inmemory partitions first ',negative
'notnullconstraints ',negative
'hcat doesnt support transactional tables ',negative
'round power here required writebuffers ',negative
'delta with files raw format are result load data opposed compaction streaming ingest must have interval length ',negative
'linear interpolation get the exact percentile ',negative
'drop databases created other test cases ',negative
'loginfofirst tail offset ',negative
'use case ',negative
'preinstall the database all the tables are there ',negative
'check that dropping database from wrong catalog fails ',negative
'first create expression from defaultvalueast ',negative
'writeid recently committed txn which was open when get validtxnlist snapshot should invalid well ',negative
'all columns ',negative
'since may calculation and produce scratch column need map the right batch column ',negative
'accepting object means accepting everything but there conversion cost ',negative
'different level the drop command specified ',negative
'linkedhashmap have repeatable iteration order ',negative
'cleanup the mapwork path ',negative
'jobs stages per job tasks per stage ',negative
'reset the resolver ',negative
'this the overwhelmingly common case ',negative
'first child should rowid ',negative
'link backtrack selectop filesinkop ',negative
'initialize reduce operator tree ',negative
'for example original max dist min ',negative
'constant constant expressions shouldnt getting this ',negative
'arbitrary column names used internally for serializing spill table ',negative
'groupingsets cube rollup ',negative
'the whole column vector has nulls this true otherwise false ',negative
'use case ',negative
'generate the intermediate aggregate the one the bottom that converts distinct call group call bottom aggregate the same the original aggregate except that ',negative
'operations read committed sufficient ',negative
'else fallback hlloriginal algorithm ',negative
'filesadded ',negative
'create the filter for the queryid appender ',negative
'with hive the dictionaries will disabled after writing the first stripe there are too many distinct values hence only stripes compared stripes version above test case ',negative
'the perbatch setup for outer join ',negative
'get the table from metastore ',negative
'already check rowcnt null and rowcnt means table empty ',negative
'reset the conf variable values that changed for this test ',negative
'should copy only those table parameters that are specified the config ',negative
'skip semijoin branch ',negative
'copy new slot table ',negative
'counts are cached avoid repeated complex computation register value ',negative
'stores the tablescan operators processed avoid redoing them ',negative
'number registers ',negative
'test submission concurrent job requests with the controlled number concurrent requests verify that get busy exception and appropriate message ',negative
'the child has different schema create project operator between them both cannot prune the columns the groupby operator ',negative
'through each element the map ',negative
'one serialized key for more rows for the duplicate keys ',negative
'for backwards compatibility since some threads used hard coded but only run frequency was ',negative
'sum all nonnull double column values maintain isgroupresultnull ',negative
'uses nontransactional table cannot considered ',negative
'',negative
'verify the validwriteidlist with one open txn this table write open txn should invalid ',negative
'hash aggregations for group ',negative
'its set set our own conf value ',negative
'equal ',negative
'checking against the partition question instead ',negative
'registrations and unregistrations will happen and when tasks are submitted are removed reference counting likely required connection needs established each app master ignore exceptions when communicating with the later point report back saying the dead that tasks can removed from the running queue race when task completes sends out its message via the regular taskreporter the after this may run another dag may die this may need consolidated with the llaptaskreporter try ensuring theres race between the two single thread which sends heartbeats appmasters events drain off queue ',negative
'end ',negative
'groupingc equivalent ',negative
'todo hive sampling task running ',negative
'stagecounters ',negative
'perform any bucket expressions results will into scratch columns ',negative
'all returned type will text ',negative
'index the last seen delimiter the given line ',negative
'over the expressions the project operator and separate the windowing nodes that are result ',negative
'get nanos between epoch tozone and local time tozone ',negative
'update the pending state for now release this lock take both ',negative
'this means the name null ',negative
'class hiveendpoint ',negative
'the setchildren method initializes the object inspector needed the operators based path and partition information ',negative
'knuth the art computer programming edition volii algd normalize high digit base that guarantee ',negative
'probably expression cant handle that ',negative
'same with the termination after the failed update should maintain the correct count ',negative
'start the session fireandforget manner when the asynchronously initialized parts the session are needed the corresponding getters and other methods will wait needed ',negative
'use the big table row output ',negative
'also try creating ugi object for the spnego principal ',negative
'hcatalog specific configurations that can put hivesitexml ',negative
'order predciates based ndv reverse order ndvcrossproduct ndvpe ndvpe ndvpe ndvpe ',negative
'remove the current task from its original parent tasks dependent task ',negative
'walk over the input schema and copy the output ',negative
'filter enabled injection enabled exception not expected ',negative
'the table not bucketed add dummy filter rand ',negative
'order does not matter below wide ',negative
'set the bucketing version ',negative
'use linkedhashmap provide deterministic order ',negative
'this intentionally does not include interval types ',negative
'test invalid case without version ',negative
'reescape any backtick characters the identifier ',negative
'merge the names from the imposed schema into the types from the derived schema ',negative
'start after group keys ',negative
'defining bunch configs here instead hiveconf these are experimental and mainly for use when retry handling fixed yarnhadoop ',negative
'couldnt sql filter pushdown get names via normal means ',negative
'copy whole value for strings ',negative
'state ',negative
'drop all partitions from tbl tbl tbl and add new partitions tbl and tbl ',negative
'nonjavadoc see ',negative
'test booleanvalued nonfilter expression for strings ',negative
'',negative
'default drivers txn manager txn manager specified ',negative
'its ddl query ',negative
'static fieldsinitializers ',negative
'longer term should always have txn and then wont need track locks here all ',negative
'new entry the hash table ',negative
'the routing appender which manages underlying appenders ',negative
'give preference tblproperties over serdeproperties really should only use tblproperties this just for backwards compatibility with the original specs ',negative
'flush partially full deserializerbatch return return true the operator tree not done yet ',negative
'since the log length the sql operation may vary during hive dev calculate proper maxrows ',negative
'use inspector get byte out lazybinary ',negative
'add the request interceptor the client builder ',negative
'explode ',negative
'add alias table name and partitions hadoop conf that their children will inherit these ',negative
'overflow batch ',negative
'specialized class for doing vectorized map join that left semi join multikey using hash set ',negative
'from the prunedcols list filter out columns that refer windowfns windowexprs the returned list set the prunedlist needed the ptfop ',negative
'the expired nodes did not result cache being cleanuntil size ',negative
'through these hijinxes because java considers systemgetenv readonly and offers way set env var from within process only for processes that subspawn ',negative
'monday august ',negative
'any aggregate functions not support splitting bail out ',negative
'build the new predicate and return ',negative
'float family timestamp are handled via descriptor based lookup int family needs ',negative
'directory empty doesnt have any that could have been produced load data ',negative
'execution stuff ',negative
'partitions updated and other entries ',negative
'nonjavadoc see int ',negative
'todo relying everywhere the magical constants and columns being together means acid columns are going super hard change backward compat manner can foresee someone cursing while refactoring all the magic for prefix schema changes exclude the row column ',negative
'expect that colid will the same for all many sds ',negative
'concurrent increase and revocation increase fails revocation needed ',negative
'represents udaf invocation the context window frame explained above sometimes udafs will handled window functions even explicit window specification this support queries that have group clause window function invocation captures the astnode that represents this invocation its name whether stardistinct invocation its alias and optional window specification ',negative
'make sure capture the same metrics hadoop metrics system via annotations ',negative
'relies the fact that cache does not actually store these ',negative
'points the last txn which dont want heartbeat ',negative
'only trigger major compaction for ttp deltapctthreshold because the newly inserted row actual pct ',negative
'tamil ubd bytes ',negative
'create temporary scratch dir ',negative
'all this completely bogus and mostly captures the following function foutput ieyeballedtheoutput theylookok its pretty much golden file ',negative
'next column has similar name previous but different casing this allowed druid but should fail hive ',negative
'need final pass inner class ',negative
'blockingdeque methods ',negative
'flush necessary ',negative
'support ',negative
'read database table via cachedstore ',negative
'update the aggs ',negative
'flag indicate there data parquet data page ',negative
'create standard copy the object ',negative
'preallocated members for storing information single and matches valuecounts number empty small table values allmatchindices logical indices into allmatchs the first row match possible series duplicate keys duplicatecounts the duplicate count for each matched key ',negative
'functionname ',negative
'next command should produce error ',negative
'merge with its right child ',negative
'primary key pkfk relationship unique constraint not null constraint ',negative
'multiple values ',negative
'ptf input represents the input ptf function input can hive subquery table another ptf function input instance captures the astnode that this instance was created from ',negative
'this method can replaced with filescopysource target replaceexisting once hive uses java ',negative
'store text the original query ',negative
'nonjavadoc see ',negative
'otherwise return null ',negative
'this not called when building hashtable dont expect called ever ',negative
'methods that need data object ',negative
'the end this function the stream should pointing the last token that corresponds the value being skipped this way the next call nexttoken will advance the next field name ',negative
'',negative
'for load you only add does exist you might loading outdated ',negative
'check behavior while change the order columns ',negative
'alter rebuild ',negative
'corvar change input ref ',negative
'throw hiveexception for nonrcfile ',negative
'the simd optimized form ',negative
'run sql operations ',negative
'revalidated the new version after upgrade ',negative
'sparsedense merge ',negative
'its possible that the job doesnt have the token its credentials this case ',negative
'either weve found multiple big table branches the current branch cannot big table branch disable mapjoin for these cases ',negative
'cols ',negative
'also need update the expr that the index query can generated ',negative
'unique the load func and input file name table our case ',negative
'the tasks from now are more important than the candidate ',negative
'defining partition names unsorted order ',negative
'format single cluster sort statement ',negative
'try combine next level works recursively ',negative
'set the start and stop rows only asked ',negative
'could just remove here and handle the missing tail during read but that can ',negative
'mapping ',negative
'loginfocreating list record copying lengthslength lrptroffset lrptroffset ',negative
'recursively extract fields from exprnodedesc deeply nested structs can have multiple levels fields them ',negative
'truncatepartition event partitioned table ',negative
'not setting ranges scans the entire table ',negative
'capture stdout and stderr ',negative
'add destination pool ',negative
'get list indexes for which the columns the schema are the same ',negative
'multiple parents find the right one based the table alias the parentexpr ',negative
'check that exception from getmetadata reported correctly ',negative
'class partitioniterator ',negative
'formatted ',negative
'there only table alias return ',negative
'this column coming from right input only then update num nulls ',negative
'not qualify this optimization ',negative
'check for required fields ',negative
'wrap the transport exception rte since ugidoas then goes and unwraps this for out the doas block then unwrap one more time our catch clause get back the tte ugh ',negative
'threadunsafe position used write time ',negative
'higher compute cost ',negative
'nulls case ',negative
'this query will give runtime error ',negative
'drop primary key ',negative
'getprogressupdate ',negative
'get aggregation evaluators ',negative
'remove column ',negative
'set the local work all the operator can get this context ',negative
'secret ',negative
'create dispatcher and graph walker ',negative
'mergesum ',negative
'note fsdelete will fail windows the reason outputcommitter hadoop still writing logshistory linux dont care file still open and remove the directory anyway but windows refuse remove directory containing open files windows will leave output directory behind when job fail user needs remove the output directory manually ',negative
'new part ',negative
'generate reducesinkoperator ',negative
'finally check serializable ',negative
'hadoop gets and hadoop gets sigh ',negative
'schema ',negative
'maximum table size ',negative
'run initiator execute ',negative
'helper methods ',negative
'the function being added under database namespace then add entity representing the database only applicable permanentmetastore functions also add second entity representing the function name the authorization api implementation can decide which entities wants use authorize the createdrop function call ',negative
'rightinputrel filter and contains correlated reference make sure the correlated keys the filter condition forms unique key the rhs ',negative
'round ',negative
'now recompute state since weve done minor compactions and have different best set deltas ',negative
'',negative
'given these lines dont need double check later ',negative
'load partition that doesnt exist there some parallelism going you load more than partition which dont understand thats reasonable since each partition loaded parallel why happens here beyond the file name changes from run run between and and the data correct but this causes rowidbucketidfile names change ',negative
'copy the group key output batch now well copy the aggregates the end the group ',negative
'start ',negative
'this does the testing using remote metastore that finds more issues thrift ',negative
'this string constant used indicate alterhandler that ',negative
'build col details used scan ',negative
'view column authorization again even triggered again ',negative
'literal tinyint ',negative
'timer shared across entire factory and must released separately ',negative
'char columns should have correct display sizeprecision ',negative
'rest tests just picked above ',negative
'ensure there partition dir ',negative
'case replication idempotent taken care gettargettxnid ',negative
'nonjavadoc see ',negative
'test that existing exclusive partition with new sharedwrite coalesces ',negative
'outer join cannot performed table which being cached ',negative
'this project has correlated reference create value generator ',negative
'return the type string the first argument argument ',negative
'this sel cols udtf cols ',negative
'path being passed table dump location ahead and load needed tblname null then default the table name specified metadata which good are both specified which case thats what are intended create the new table ',negative
'test lazyhcatrecord init and read ',negative
'map ',negative
'need read bucket number which the last column value after partition columns ',negative
'codahale artifacts are lazilycreated ',negative
'must consistent with uncompressed stream seek orc see call site comments ',negative
'the case altering table for its partitions dont need lock the table itself just the partitions but the table will have readentity mark that readentity lock ',negative
'get exception resolving partition could describe table key return null continue processing for describe table key ',negative
'expect the dags not super large store full dependency set for each vertex ',negative
'make stripes with rows each ',negative
'add udaf args deduped reduce values ',negative
'not comparing hashctx irrelevant ',negative
'implies netty default number available processors ',negative
'boolean value match for extended char field ',negative
'test when two jars are added with shared dependencies and one jar deleted the shared dependencies should not deleted ',negative
'caller must remember small value length ',negative
'must have one those this point ',negative
'write large value this should use different byte buffer ',negative
'rows will filtered ',negative
'add parents for the newly created operator ',negative
'called when the value the partition has changed update the currentrank ',negative
'get unionoperator right now only handle when can find correlated reducesinkoperators from all inputs ',negative
'for select count from where tds ',negative
'original files delta directories deletedelta directory and base directories ',negative
'get the partition object already exists ',negative
'errored ',negative
'this weirdness setting our conf and then reading back does two things one handles the conversion the timeunit two keeps the value around for later case need again ',negative
'test that read can acquire after write ',negative
'initialize pathtoaliases ',negative
'for uncompressed case need some special processing before read ',negative
'appended since there could multiple scalar subqueries and filter ',negative
'the positions rscolinfolst are follows grpkeydistkeyvalues but distudaf may beforeafter some nondistudaf their positions can mixed for all udaf first check see groupby key not distinct key ',negative
'main memory hashmap ',negative
'basic algorithm determine rounding part nonzero for rounding scale away fractional digits present rounding clear integer rounding portion and add ',negative
'version ',negative
'scalesignum ',negative
'init aggregationclasses ',negative
'whether skippruning depends the payload from event which may signal skip the event payload too large ',negative
'this wont have decimal part because the hasdecimalmask bit not set ',negative
'combining acute acent bytes ',negative
'may not true with correlation operators muxdemux ',negative
'get the function documentation ',negative
'foo ',negative
'bucket centered already exists this must checked the next step ',negative
'verify that there now only new directory basexxxxxxx and the rest have have been cleaned ',negative
'use this via command line arg decimal use this via command line arg decimal ',negative
'row count exists stats arent estimated return ',negative
'remember for additional processing later ',negative
'timestamp values are pst timezone for tests set pst default ',negative
'delete the data the database ',negative
'handle select distinct gby there exist windowing functions ',negative
'means ',negative
'now delete the rest tables ',negative
'table missing then partitions are also wouldve been dropped just noop ',negative
'captures how the input ptf function should partitioned and ordered refers partition and order instance ',negative
'need check paths and partition desc for mapworks ',negative
'return false just noop ',negative
'all must selected otherwise size would zero repeating property will not change ',negative
'have some disk buffers see have entire part etc ',negative
'will invoked anyway teztask doing early initialize triggers for nonpool tez session ',negative
'expecting this exception ',negative
'get the number columns the users rows ',negative
'verify that task kill went out for all nodes running the specified host ',negative
'order use order from the block above relnode has pointer parent hence need top down but each block really belong its srcfrom hence the need pass sort for each block from its parent limit ',negative
'rawlocalfilesystem seems not able get the right permissions for local file always returns hdfs default permission can not overwrite directory deleting and recreating the directory and restoring its permissions should delete all its files and subdirectories instead ',negative
'this smb join ',negative
'listcoord coord holds two doubles ',negative
'optional string user ',negative
'value out range other unknown cases ',negative
'dummy create table command mark proper last repl after dump ',negative
'nonjavadoc order update decimal fast allocation need expose access the internal storage bytes and scale return ',negative
'add the udtfoperator the operator dag ',negative
'validate input formats all the partitions can vectorized ',negative
'this may happen were able establish connection once but its longer valid ',negative
'ckpt property not set empty means bootstrap not run this object ',negative
'also set the connection between each parent work and child work ',negative
'alter table add column change the metadata ',negative
'add another partition without stats ',negative
'figure out table acid not ',negative
'close resultset ignore exception any ',negative
'set alias work and put into smalltablealiaslist ',negative
'validate that some progress being made ',negative
'this means there existing partition ',negative
'skip authorization skip checking inside view skip checking authorization flag not enabled skip checking ',negative
'round with digits ',negative
'now set the output for the history ',negative
'wait until rjiscomplete ',negative
'newaggrel ',negative
'open the session closed ',negative
'these errors happen the jni lib not available for your platform ',negative
'now add again add this before our own config files that the ',negative
'need carry the insideview information from calcite into the ast ',negative
'prefix for separate row keys ',negative
'the children after not might need cast get common types for the two comparisons casting for between handled here special case because the first child for not and doesnt need ',negative
'check for part log message well part progress information ',negative
'the following code for mapjoin ',negative
'null error message here means the user has access ',negative
'intervalyearmonth ',negative
'disable resource monitoring although should off default ',negative
'add the number partitions given the current batchsize ',negative
'rightinputrel has this shape filter references corvar filterinputrel ',negative
'configure web application contexts for the web server ',negative
'counters for task execution side ',negative
'lower case null used within json objects ',negative
'nulls are considered not matching for equality comparison add the position the most recently inserted key ',negative
'all rowids are unique read after conversion acid rowids are exactly the same before and after compaction also check the file name only after compaction for completeness ',negative
'base type name primitivetypeentry map ',negative
'this means the key didnt exist the insertion point negative minus ',negative
'the aggregation buffer not estimable then get all the declared fields and compute the sizes from field types ',negative
'allocate the buffers prepare cache keys this point have read all the cbs need read cachebuffers contains some cache data and some unallocated membufs for decompression todecompress contains all the work need and each item points one the membufs cachebuffers target the iter ',negative
'header row transactions rows ',negative
'this key will put the conf file when planning acid operation ',negative
'for fast check possible existence will checked again ',negative
'',negative
'must support offsets able split ',negative
'only applicable nway hybrid grace hash join ',negative
'despite having fixed schema from hive have sparse columns accumulo ',negative
'lookup type infos for our input types and expected return type ',negative
'make one have nonstandard location ',negative
'case any other exception retry this also fails report original error and exit ',negative
'set the perms readonly access and create acl entries allowing write access ',negative
'are skipping the cds table here seems totally useless ',negative
'long not between ',negative
'nonjavadoc see ',negative
'check that the columns referenced rightjoinkeys form ',negative
'analyze each side and let the left and right exprs the conjunct object return conjunct contains details the left and right side the conjunct expression ',negative
'log with double base ',negative
'currently this sole field affecting mergee task ',negative
'unit test for github howl issue ',negative
'get all valid partition paths and existing partitions for them any ',negative
'the consumer joinoutputprojrel nullindicator located ',negative
'far weve read the ones complement add turn into twos complement ',negative
'the output has some extra fields set them null convert ',negative
'input contains header skip header ',negative
'selectobjs hold the row from the select until receiving row from ',negative
'create task aliases mapping and alias input file mapping for resolver ',negative
'not equal convert all double and compare ',negative
'get the column names the aggregations for reduce sink ',negative
'for auto reduce parallelism minimum reducers requested ',negative
'this will create delta and deletedelta see mockrawreader ',negative
'provide facility set current timestamp during tests ',negative
'try get consistent view can make copy the headers ',negative
'recreate the partition existed before ',negative
'char ',negative
'the same applies files added with addfile theyre only guaranteed available ',negative
'even though the stats were estimated need warn user that stats are not available ',negative
'encoding must have data ',negative
'with data ',negative
'add fields used the condition ',negative
'describe the table populate the output stream ',negative
'recalculate the hdfs stats auto gather stats set ',negative
'the join key ',negative
'skip rowid ',negative
'all the integer types float double string char varchar ',negative
'order make the dependencies accessible ',negative
'gen would have prevented ',negative
'nothing here ',negative
'conversion avro primitive types hive primitive types avro hive null boolean boolean check int int check long bigint check float double check double double check bytes binary check fixed binary check string string check tinyint smallint ',negative
'constant means filter ignore when null ',negative
'might have connect parent work with this work later ',negative
'true true true false false false false true ',negative
'create file with all the job properties read sparksubmit change the files permissions that only the owner can read this avoid having the ',negative
'get the list tracking jobs these can used determine which jobs have expired ',negative
'implement reloptrule override the rule order union all branch elimination ',negative
'create parameter converters ',negative
'check there only one immediate child task and stats task ',negative
'ukname ',negative
'use the multichar delimiter parse the lazy struct ',negative
'here recursively check whether there are exact one limit the query whether there aggregation groupby distinct sort distributed table sampling any the subquery the query only qualifies both conditions are satisfied example qualified queries create table select col col from tbl limit insert overwrite table select col hashcol splitcol from limit select from select col col select from limit ',negative
'check semantic conditions ',negative
'tasks including all dependencies ',negative
'ptf node form tokptblfunction name alias partitioningspec expression ptf node guaranteed have alias here ',negative
'get bigkeysdirtotaskmap ',negative
'nonjavadoc see javasqltimestamp ',negative
'has already been initialized using hiveconf ',negative
'gets status job form job maximum concurrent job status requests are configured then status request will executed thread from thread pool job status request time out configured then request execution thread will interrupted thread times out and does action ',negative
'from load data into acid converted table ',negative
'thought had the entire part cache but dont convert start noncached since are the first gap the previous stuff must contiguous ',negative
'myenumstringmap ',negative
'clean tables default ',negative
'always send secure cookies for ssl mode ',negative
'default argumentcompletor strict mode meaning token only autocompleted all prior tokens match dont want that since there are valid tokens ',negative
'clear set capture new set functions ',negative
'make char and varchar type info parsable ',negative
'set context for ',negative
'return false only occurred error when execution the sql and the sql should follow the rules ',negative
'for now ',negative
'create split for the given partition ',negative
'for each dynamically created directory construct full partition spec and load the partition based that ',negative
'nonjavadoc see ',negative
'return value not checked due concurrent access ',negative
'output debug info ',negative
'these tasks should have come from the same job ',negative
'create view with name already exist just verify failure flow clears the added createtable event ',negative
'list status jobs request maximum concurrent job list requests are configured then list request will executed thread from thread pool job list request time out configured then request execution thread will interrupted thread times out and does action ',negative
'for all parents other than the big table insert dummy store operator ',negative
'clean the databases ',negative
'server args ',negative
'objecttype ',negative
'events ',negative
'create the project after for those repeated values select ',negative
'tracing down the operator tree from the table scan operator ',negative
'note the enum names match field names the struct ',negative
'the remaining parameters are starting parameter name ',negative
'specificationtitle ',negative
'how many levels ancestors keep the stack during dispatching ',negative
'first underflow not error ansi sql numeric cast decimal without error ',negative
'need run spark job make sure the jar added the class loader monitoring sparkcontextaddjar doesnt mean much can only sure jars have been distributed ',negative
'test when third argument has nulls and repeats ',negative
'this import being done for replication then this will managed table and replacements are allowed irrespective what the table currently looks like more checks are necessary ',negative
'flag indicate whether cancel the task when exception timeoutexception raised the default cancel thread ',negative
'the index ',negative
'insert mapside ',negative
'the test table has rows total query time should ',negative
'retrieve the mbean server ',negative
'query will try add more partitions already existing partitions but will get cancelled for violation ',negative
'running all the servers ',negative
'note could call for tables the recursive call usually needed for nonmm tables because the path management not strict and the code does whatever that should not happen for tables keep like this for now may need replacement find valid use case ',negative
'here means currently committing txn performed updatedelete and should check conflict ',negative
'readlock not updating any stats the moment ',negative
'seconds ',negative
'read the whole file ',negative
'publish configs for this instance the data the node ',negative
'checkh for and not the subquery must implicitly explicitly only contain one select item ',negative
'',negative
'this should not happen unless are evicting lot once buffers are large ',negative
'when introduce discrepancy the state give the task updater unless was already given one the updater already doing stuff would handle the changed state when its done with whatever its doing the updater not going give until the discrepancies are eliminated ',negative
'find functions which name contains tofind the dummy database ',negative
'events can start coming the moment the inputinitializer created the pruner must setup and initialized here that sets its structures start accepting events setting initialize leads window where events may come before the pruner initialized which may cause drop events ',negative
'statistics for the column already exist use ',negative
'note that need call getresults for simple fetch optimization however need skip all the results ',negative
'compute distance and store sorted map ',negative
'',negative
'the sort order ascendingdescending for each field set true when descending invert ',negative
'make sure dont collide with the source files tables dont support concat dont expect the merge merged files ',negative
'create vertexgroup ',negative
'case substring from index the end ',negative
'sleep for expiry time and then fetch again sleep twice the ttl interval things should have been cleaned then ',negative
'walk through the ast ',negative
'the process has not logged using keytab this should test mode cant use keytab authenticate with zookeeper ',negative
'range starts here ',negative
'check job config for overrides otherwise use the default server value ',negative
'fastbitset rather than using integers ',negative
'the user tries actually use this session and fails proceeding returndestroy ',negative
'insert into newtablename select from where partition spec ',negative
'these are things that goes through singleton initialization most queries ',negative
'one last check ',negative
'istbllevel ',negative
'verify that returns zero events there are more notifications available ',negative
'nulls ',negative
'then the current node with the previous one ',negative
'years months years years month years month ',negative
'create map source file system destination path list files copy ',negative
'remove begining ',negative
'qualified column access for which table was not found ',negative
'write out the plan local file ',negative
'change the resource plan that the session gets killed ',negative
'get the updated path list ',negative
'set really low batch size ensure batching ',negative
'unpartitioned table ',negative
'verify that partition was added correctly and properties were inherited from the hcattable ',negative
'some application depends the original value being set ',negative
'note that when serializing row the logical mapping using selected use has already been performed batchindex the actual index the row ',negative
'get task detail link from the jobtask page ',negative
'project the column corresponding the distinct aggregate project asis all the nondistinct aggregates ',negative
'since the start and the length the first call sync should with the value return that for getpos ',negative
'relative start position the windowing can negative ',negative
'initialize hcatinputformat ',negative
'infix operator ',negative
'rounding numbers that increase int digits ',negative
'impl note hive provides authorization with its own model and calls the defined from however hcat has additional calls the auth provider implement expected behavior for this means that the defined auth provider called both hive and hcat the following are missing from hives implementation and when they are fixed hive can remove the hcatspecific auth checks create databasetable add partition statements does not call with the candidate objects which means that cannot checks against defined location hiveoperation does not define sufficient privileges for most the operations especially database operations for some the operations hive semanticanalyzer does not add the changed object writeentity readentity see see ',negative
'need kill anything ',negative
'currtask roottasks remove and add its children the roottasks which currtask its only parent task ',negative
'size query will fail but least wed get see the query debug output ',negative
'not merge not know how connect two operator trees ',negative
'all arguments are known length then can keep track the max length the return type however the return length exceeds the ',negative
'create operator info list return ',negative
'the source column names for orc serde that will used the schema ',negative
'value null the type should also void ',negative
'setup the compression codec ',negative
'nothing output null ',negative
'construct outer struct ',negative
'create two tables one user foo and other user bar ',negative
'now both batches have committed but not closed for each primary file expect side file exist and indicate the true length primary file ',negative
'sql std authorization managing privileges the tableview levels only ignore partitions ',negative
'remove any semijoin branch associated with hashjoins parents operator ',negative
'assumptions about the range numeric data being analyzed ',negative
'preceeding work must set the newly generated map ',negative
'parameterstotalsize parametersnumfiles ',negative
'skip first child since struct ',negative
'second split delta ',negative
'reloadable jars ',negative
'cleanup the synthetic predicate the tablescan operator replacing with true ',negative
'the storage arrays for this column vector corresponds the storage hiveintervaldaytime ',negative
'double min and max ',negative
'allocate temporary output dir the location the table ',negative
'batches will sized ',negative
'checking mapjoin can converted bucket mapjoin ',negative
'used because that the correct move task for the merge and move use case ',negative
'the input file has changed every operator can invoke specific action ',negative
'round the specified number decimal places using the standard hive round function ',negative
'base encoded and stringified token for server ',negative
'rel offset word big value len next value small len value bytes and beyond records have relative offset word the beginning ',negative
'access the fields ',negative
'hbase input formats are not thread safe today see hive ',negative
'guaranteed there only list within listbucketcols ',negative
'nonjavadoc see javaioinputstream int ',negative
'largest size allowed smallbuffer ',negative
'disable auth the call should succeed ',negative
'nothing this was fixed mapreduce ',negative
'insert five rows nonacid table ',negative
'nonjavadoc see ',negative
'fulltablenames ',negative
'cap factrowcount because numerical artifacts can cause ',negative
'send the bucket ids associated with the tasks must happen after parallelism set ',negative
'there are elements the union ',negative
'once the source node reached stop traversal for this ',negative
'there should one base dir the location ',negative
'print the stack from all threads for debugging purposes ',negative
'type job request ',negative
'network cost map side join ',negative
'spill the big table rows into appropriate partition when the joinresult spill means the corresponding small table row may have been spilled disk least the partition that holds this row disk need ',negative
'build map not convert multiple times further remove already included predicates ',negative
'set the location the storagedescriptor ',negative
'hiveserver output consumed jdbcodbc clients ',negative
'need know count since this specialized for innot corr subqueries ',negative
'remove the value every key found matching ',negative
'convert each keyvaluemap appropriate expression ',negative
'this should not schedule new compaction due prior failures but will create attempted entry ',negative
'add tblid and empty bitvector ',negative
'means user specified table not partition ',negative
'convert requiredprivileges ',negative
'reserve bytes for the hash dont just reserve there may junk there ',negative
'otherwise gbevaluator and expr nodes may get shared among multiple ops ',negative
'minihs will continue leader ',negative
'lastanalyzed ',negative
'emulate serializationutils deserialization used orc ',negative
'translate the double into sign exponent and significand according ',negative
'return true both are null false one null and the other isnt ',negative
'replicate the remaining insert overwrite operations the table ',negative
'nonjavadoc see ',negative
'add the select operator ',negative
'for each basework with operator build sparkwork for its small table baseworks ',negative
'each partition may have different format have check them all before deciding make full crud table run batches prevent oom ',negative
'decimal math ',negative
'else common code ',negative
'skip the driver and directly loadable tables ',negative
'wait time milliseconds before another cancel request made ',negative
'',negative
'find the parsed deltas some them containing only the insert delta events ',negative
'the strict mode have provide partition pruner for each table ',negative
'test gtltltegtelike for strings ',negative
'perform logical optimization ',negative
'data array and masks array are then traversed together and checked for corresponding set bits ',negative
'update the maps note output for sortrel considered same its input may end not using that present sort rel also note that rowtype sortrel the type child child happens synthetic project that introduced then that projectrel would ',negative
'eliminate stripes that doesnt satisfy the predicate condition ',negative
'destination disk ',negative
'tables need custom handling for union suffix tables use parent too ',negative
'the same umbilical used multiple tasks problematic the case where multiple tasks ',negative
'return new operator ',negative
'third parameter has been specified should integer that specifies the number ',negative
'parse string select list this allows table functions passed expression strings that are translated the context they define invocation time currently used npath allow users specify what output they want npath allows expressions tpath column that represents the matched set rows this column doesnt exist the input schema and hence the result expression cannot analyzed the regular hive translation process ',negative
'takes instead taskattemptcontext cant use that here ',negative
'reset everything for the next arena assume everything has been cleaned ',negative
'multikey specific save key ',negative
'create deeply nested table which has more partition keys than the pool size ',negative
'create rowid column select clause from left input for the right outer join this needed for the update clause hence find the following node tokquery tokfrom tokrightouterjoin toksubquery tokquery tokinsert tokselect and then create the following child node tokselexpr toktableorcol cmvmatview ',negative
'create empty file which not valid rcfile ',negative
'smile mapper used read query results that are serilized binary instead json ',negative
'can still fold since here null equivalent false ',negative
'second ',negative
'nesting level limits ',negative
'interceptor for adding username pwd ',negative
'test varchar literal string column comparison ',negative
'all left data small tables are less than and equal the left data big table lets them catch ',negative
'has nulls not repeating ',negative
'use case ',negative
'static fieldsinitializers ',negative
'vectorization only supports primitive data types assert the same ',negative
'multikey check for repeating ',negative
'read permissionno permissions the expected user ',negative
'build mapside graph graph template either input ptfmap reducesink input reducesink here the exprnodedescriptors the querydef are based the input operators ',negative
'hive assumes that user names the files per the corresponding bucket for file names should follow the format etc here the file will belong bucket and bucket and ',negative
'flatten and ',negative
'reuse existing available column the same required type ',negative
'test capability for tests ',negative
'this code with branches and all not executed there are string keys ',negative
'modifiable ',negative
'test with nulls input ',negative
'there were grp and perms begin with ',negative
'start ',negative
'actualbatchsize half batchsize when exception expected ',negative
'set columnaccessinfo for ',negative
'partarchivelevel ',negative
'optional vertexorbinary workspec ',negative
'create new open session request object ',negative
'zeroes ',negative
'use try finally cleanup temp file something goes wrong ',negative
'all alters done now replicate them over ',negative
'delete the original node ',negative
'per acid write test nonacidacid conversion mixed with load data ',negative
'sort columns are not allowed for full acid table change insertonly table ',negative
'current value ',negative
'vector serde can disabled both client and server side ',negative
'more than operator ',negative
'length greater than max key prefix ',negative
'create row file copy ',negative
'first delete any mvs avoid race conditions ',negative
'determine the temp table path ',negative
'update for ',negative
'use the cache rather than full query execution this point the caller should return from semantic analysis ',negative
'the specified operator class ',negative
'reuse the partition specs from dest partition since they should the same ',negative
'optional string applicationidstring ',negative
'check see have seen this request before ignore not add our queue ',negative
'that can test old files ',negative
'enter ',negative
'future this may examine context return appropriate hcatwriter ',negative
'driver driver new driverconf clisessionstateconf ',negative
'case test might not want remove the log directory ',negative
'inputs cannot use with input formats ',negative
'make more inserts that have copy copy files export ',negative
'fill starting with highest digit highest longword and move down end will will shift everything down necessary ',negative
'use case ',negative
'multicolumn also covers table nondefault database ',negative
'ududed okhotsk atka mackerel kanji ',negative
'multikey outer null detection ',negative
'input array used fill the entire size the vector row batch ',negative
'drop any constraints the table ',negative
'format sorted statement ',negative
'descriptor ',negative
'restore the context ',negative
'fetch operator ',negative
'scriptoperator echo the output the select ',negative
'',negative
'have that the null does not interfere with the current equal key series there one not set savejoinresult let current match equal key series keep going let current spill equal key series keep going let current nomatch keep not matching ',negative
'table was created user specified location using the ddl like create table tbl location should treated like external table the table rename its data location should not changed can check the table directory was created directly under its database directory tell such table ',negative
'sets the field and tag the union returns the union ',negative
'sorted ',negative
'maximum number times cancel request sent job request execution thread futurecancel may not able interrupt the thread blocked network calls ',negative
'the acid state probably absent warning logged the get method ',negative
'pushed the predicate into the table scan need remove the ',negative
'use that task ',negative
'druid timestamp column ',negative
'logj configuration file not set could not found use default setting ',negative
'http today but might not ',negative
'use defaults partitions are put the table directory ',negative
'restrictionm allow only subquery expression per query ',negative
'only needed when grouping sets are present ',negative
'test that existing sharedread with new sharedwrite coalesces ',negative
'glorified cast from iterabletbase iterablepartition ',negative
'all tables good destination drop source ',negative
'mintxnid would nonzero txnid ',negative
'get all join columns from join keys ',negative
'make sure only return keydecompressor once ',negative
'filters dont change the column names need anything for them ',negative
'can this mapjoin converted bucketed mapjoin the following checks are performed the join columns contains all the bucket columns the join keys are not transformed the subquery all partitions contain the expected number files number buckets the number buckets the big table can divided buckets small tables ',negative
'location expected with ',negative
'fall through ',negative
'check vectorized orc reader against orc row reader ',negative
'value becomes null for rounding beyond ',negative
'open txn tested for validwriteidlist get the validtxnlist during open itself verify the validwriteidlist with openaborted write txns this table ',negative
'now are handling exact types base implementation handles type promotion ',negative
'required required required optional optional required optional optional ',negative
'dont support maskingfiltering against acid query the moment ',negative
'update file sink descriptor ',negative
'move process the next parseddelta ',negative
'',negative
'default threshold for using main memory based hashmap ',negative
'prefix for primary row keys ',negative
'for mapwork getallrootoperators not suitable since checks getpathtoaliases and will return null this empty here are ',negative
'and finally cache policy uses cache notify eviction the cycle complete ',negative
'create new projectsortproject sequence ',negative
'this request lower priority should not affect anything ',negative
'interface for vector map join hash table which could hash map hash multiset hash set for single long ',negative
'add aux jars ',negative
'delete something but make sure txn rolled back ',negative
'set the config value catalogs other than hive ',negative
'singlecolumn string specific repeated lookup ',negative
'create the struct needed ',negative
'null filtering does not work here currently doing filter thrifthttpservlet ',negative
'initialization fails and does the retry resource plan change ',negative
'not needed anymore ',negative
'zero length key not allowed block compress writer use byte writable ',negative
'the size flush the string buffer ',negative
'todo fix currently lpde can only carry single partitionspec and that defeats the purpose ',negative
'outside the inner loop results npeoutofbounds errors ',negative
'release locks from select from unblock hte drop partition retest the the drop partiton lock ',negative
'skipped over leading and xffs ',negative
'the expr column const will try cast the const string according the data type the column ',negative
'need check finishable here was set would already the queue ',negative
'perform some checks whether the node will become available not ',negative
'since were only creating view not executing dont need optimize translate the plan and fact those procedures can ',negative
'this will throw expected exception since clientserver modes are incompatible ',negative
'didnt find case when udf ',negative
'delimited way ',negative
'set this encounter condition were not expecting ',negative
'binarycolumns ',negative
'',negative
'note that this will invoked cases directsql was disabled start with directsql threw and was disabled ',negative
'note location check here the buffer always locked for move ',negative
'hive these decimal values should trimmed trailing zeros ',negative
'null last default for descending order ',negative
'calcite always needs the else clause defined explicitly ',negative
'change all the linked file sink descriptors ',negative
'also prefer missed heartbeat over stuck query case discrepancy ',negative
'create directory with permissions ',negative
'will ultimately instantiate the accumuloserde with null configuration have accept this and just fail late data attempted pulled from the configuration ',negative
'new hivesitexml file ',negative
'empty java opts ',negative
'remove any virtual cols ',negative
'',negative
'scaling down may have opened trailing zeroes ',negative
'vectorized implementation roundcol function ',negative
'first field always starts from even when missing ',negative
'ascending ',negative
'new operators ',negative
'have the dag now proceed get the splits ',negative
'need capture the timing ',negative
'begin walk through the task tree ',negative
'nonjavadoc see ',negative
'verify mergeonlytask not optimized merge task writes directly finaldirname then movetask executed ',negative
'remove operator and combine ',negative
'each headersi virtual byte minallocation ',negative
'note here should use the new partition predicate pushdown api get list pruned list ',negative
'cms parallel ggc other vendors like ibm azul etc use different names ',negative
'hive queryid not always unique ',negative
'make sure the user has not requested insane amount txns ',negative
'execute one instruction terminate executing script there error silent mode prevent the query and prompt being echoed back terminal ',negative
'create deltabucket with row and close the file ',negative
'gen optimized ast ',negative
'web port cannot obtained ',negative
'refs these comparisons are anded together ',negative
'uses generic jdbc escape functions add limit clause query string ',negative
'dont need recordshuffleinfo since the out sync unregister will not remove the credentials ',negative
'demuxoperator forwards row exactly one child its children list based the tag and newtagtochildindex processop method need not anything here ',negative
'bgenjjtree struct ',negative
'compact ttp running the worker explicitly order get the reference the compactor job ',negative
'can there acls theres some access get acls assume means free for all ',negative
'block make sure kill happened successfully ',negative
'map join dump file name ',negative
'same instance driver which can run multiple queries ',negative
'increase check that the pool grows ',negative
'may need linear interpolation get the exact percentile ',negative
'files size for splits ',negative
'this only your own peril and never the production code ',negative
'',negative
'marker track the previous character was escape character ',negative
'the default setting throw assume dovalidate doskip means throw ',negative
'not doing any check ',negative
'execute extended optimization for each check whether other same work could merge into this one they are merged operators the resulting work will considered ',negative
'signature for wrapped storer see comments ',negative
'constant node ',negative
'subclass must provide this method this method invoked during translation and also when the operator initialized during runtime subclass must use this call setup the shape its output subsequent this call call getoutputoi call the link must return the the output this function ',negative
'remove the path not present ',negative
'case multitable insert the path alias mapping needed for all the sources since there reducer treat plan with null reducer ',negative
'can optimized later that operator operator initclose performed only after that operation has been performed all the parents this will require initializing the whole tree all the mappers which might required for mappers spanning multiple files anyway future ',negative
'nonjavadoc this provides lazyinteger like class which can initialized from data stored binary format see int int ',negative
'calculate complete collection ',negative
'convert filternode ',negative
'already handled all delete deltas above and there should not any other deltas for any table type this was acid code path ',negative
'this vectorexpression special case dont return descriptor ',negative
'find the biggest reduce sink ',negative
'inputts the the local timezone for this udf want the timezone represented fromtz ',negative
'for each component this lock request add entry the txncomponents table ',negative
'setup exprnode ',negative
'would either return fqdn ',negative
'http server ',negative
'top level query ',negative
'used group dependent tasks for multi table inserts ',negative
'select from tab txn ',negative
'bootstrap ',negative
'unfortunately making prunedpartitions immutable not possible here with semijoins not all tables are costed cbo their ',negative
'use milliseconds parser pattern matches our specialcase millis pattern string ',negative
'perf this function called for every row setting the selectedprojected columns the first call and dont that for the following calls ideally this should done the constructor where dont need branch the function for each row ',negative
'dont send the parseddbname this method will parse itself ',negative
'close ',negative
'operator wants some work the beginning group ',negative
'modifier letter small bytes ',negative
'too many sessions are outstanding due expiration restarts should not happen with inuse sessions because already kills the extras will kill ',negative
'nonjavadoc see int ',negative
'try fold otherwise return the expression itself ',negative
'the mapping from the index child operator its corresponding ',negative
'for instance this the case when are creating the table ',negative
'create currjobcontext the latest gets all the config changes ',negative
'only stored update based the original fixtmppath only stored update based the original fixtmppath ',negative
'execute prepared statement ',negative
'column family ',negative
'multiple level folder are there fsrename failing first create the targetpathgetparent not exist ',negative
'test class read series values the designated input stream ',negative
'this implementation vectorized join delegating all the work the rowmode implementation hijacking the big table node evaluators and calling the rowmode join processop for each row the input batch since the join operator not fully vectorized anyway the moment due the use rowmode smalltables this reasonable tradeoff ',negative
'generates grouping set position grouping set generally the last keys declared grouping set values bitsets acquired from grouping set values ',negative
'have field and are positioned read ',negative
'now create the filter with the transactions information particular each table the materialization will only have contents such that rowidwriteid highwatermark and rowidwriteid not openinvalidids hence add that condition top the source table the rewriting will then have the possibility create partial rewritings that read the materialization and the source tables and hence produce incremental ',negative
'since cannot merge operators into single job from here should remove reducesinkoperators added into walked exploitjfc ',negative
'where the log files wll written ',negative
'original bucket files and delta directory should have been cleaned ',negative
'finally add the fixed acid key index ',negative
'this method finds any columns the right side set statement thus rcols and puts them ',negative
'there select following clone the select also useful for followon optimization where the union ',negative
'getdatasize tries estimate stats doesnt exist using file size would like avoid file system calls too expensive ',negative
'cache settings will need setup llapdaemonsitexml since the daemons dont read hivesitexml ',negative
'floor integer argument noop but less code handle this way ',negative
'perform upgrade ',negative
'lock associated with txn can only unlock its waiting state which really means that the caller wants give waiting for the lock ',negative
'sum all nonnull decimal column values for avg maintain isgroupresultnull after last row last group batch compute the group avg when sum nonnull ',negative
'get some nonliterals need punt ',negative
'try again with some different data values and divisor ',negative
'slow remainder with biginteger ',negative
'will load into directory and hide previous directories needed ',negative
'currently only handles one input input ',negative
'call open mockmocktbl ',negative
'value conversion methods ',negative
'these are mostly copied from the root pomxml ',negative
'will returning text object ',negative
'reset ',negative
'for grouping sets add dummy grouping key ',negative
'verify that some dummy param can set ',negative
'process singlecolumn long outer join vectorized row batch ',negative
'cannot merge would end with cycle the dag ',negative
'exception from the rpc layer communication failure consider killed service down ',negative
'end string ',negative
'maxposition the percentile ',negative
'create tables user ',negative
'need make sure dont get two write ids for the same table ',negative
'another small write smallbuffer should reused for this write ',negative
'dynamic partition pruning enabled only for map join false and true ',negative
'the offset bigger than our current number fields grow ',negative
'step connect the operator trees two mapredtasks ',negative
'already found variable this isnt sarg ',negative
'this will replace the old value there one overwrite the existing file ',negative
'vertex waiting for inputslots complete ',negative
'todo pause fetching ',negative
'verify the scenario when maxprobesize very small value doesnt fail ',negative
'rename unmanaged files conform hives naming standard example warehousetablepartm will get renamed staging directory already contains the file taskidcopyn naming will used ',negative
'register with the amreporter when the callable setup unregister once starts running ',negative
'exists want this error condition repl load not intended replace ',negative
'for entry should work but generate nan ',negative
'not equals ',negative
'this method used traverse the dag created tasks list and add the dependent task ',negative
'given candidate mapjoin can this join converted the candidate mapjoin was derived from the pluggable sort merge join big ',negative
'set the updated fetch size from the server into the configuration map for the client ',negative
'first see have sessions that were planning restartkill get rid those ',negative
'were generating the splits the just need set ',negative
'when the next value small was not recorded with the old next value and ',negative
'synchronized besteffort display the queue order ',negative
'emulate serializationutils serialization used orc ',negative
'cannot hold all map tables memory cannot convert ',negative
'then optimization should merge them ',negative
'else recurse the children ',negative
'return the wrapper the root node ',negative
'deserialize data column values and populate the row record ',negative
'',negative
'mark agg produces count which needs reference the ',negative
'files size for splits ',negative
'old containers which are likely shutting down new containers which launched between yarn service statusdiagnostics skip for this iteration ',negative
'create the tez tmp dir and directory for hive resources ',negative
'tbltypes ',negative
'string not between ',negative
'given that are trying reuse this session must some poolsessions kills that could have removed must have cleared sessiontoreuse ',negative
'rcfile write ',negative
'thread pool for taking entities off the wait queue ',negative
'test string double ',negative
'expected error should throw ',negative
'address size check check for something better than non zero ',negative
'return true when the child type and the conversion target type the ',negative
'get valid txn list ',negative
'split not include itself ',negative
'ignore exceptions from stop ',negative
'onetime setup make query able run with tez ',negative
'set high worker count get longer queue ',negative
'should get all partitions for partitioned table ',negative
'hiveconf has changed new object should returned ',negative
'default need the results from droppartitions ',negative
'setval with the same function signature righttrim lefttrim truncate etc below ',negative
'this vertex has multiple reduce operators ',negative
'verify invalid column error ',negative
'nonjavadoc see ',negative
'create client ',negative
'use array instead only one object case future hive does not the byte copy ',negative
'special case for unions these items translate vertexgroups ',negative
'todo hive cleanup may required exiting early ',negative
'note this recursive struct ',negative
'this should only true for copy tasks created from functions otherwise there should never ',negative
'when have partial partitions specification must assume partitions lie standard place they were custom locations putting them into one archive would involve mass amount copying full partition specification case allow custom locations ',negative
'someone not done both user and the kill have returned ',negative
'map from primitivetypeinfo ',negative
'schedule task should get the only duck the one the same pri doesnt get one when the first one finishes the duck goes the and then becomes unused ',negative
'mutate operations ',negative
'get table metadata ',negative
'adding them restricted list ',negative
'set this read because cant overwrite any existing partitions ',negative
'case max list members max query string length and exact members single clause ',negative
'make the union operator ',negative
'statsindexes ',negative
'field length difference between positions hence one extra ',negative
'source table scan ',negative
'note use instead ',negative
'for consistency with tables ',negative
'these values come from setvalueresult when finds key these values allow this ',negative
'called runtime initialize the custom edge ',negative
'want isolate any potential issue may introduce ',negative
'look for unlikely database name and see either metaexception texception thrown ',negative
'weve reached our limit throw the last one ',negative
'since this test runs local file system which does not have api tell files open not are testing for negative case even though the bucket files are still open ',negative
'the same reported ',negative
'months produces type date via calendar calculation ',negative
'create delta cant push predicate ',negative
'dont want cancel the delegation token think the callback going retried for example because the job not complete yet ',negative
'config name used find the number concurrent requests ',negative
'need add connection status listener what will that ',negative
'nonjavadoc see ',negative
'this should the case only this create partition the privilege needed the table should alterdata and not create ',negative
'use tcompactprotocol read serialized tcolumns ',negative
'trivial case nothing read ',negative
'different table name ',negative
'null ',negative
'gather the operators that will used for next iteration extended optimization ',negative
'series equal keys ',negative
'register session first for backward compatibility ',negative
'europefrance ',negative
'most recent instance the pmf ',negative
'figure out the partition spec from the input this only done once for the first row when stat null since all rows the same mapper should from the same partition ',negative
'regardless the above should have the key weve signed with ',negative
'fallthrough throw exception its not expected for execution reach here ',negative
'all partition column type should string partition column virtual column ',negative
'all nonprimitive ois are writable need only check this case ',negative
'used only for explain ',negative
'cant have relative path there schemeauthority ',negative
'null ',negative
'definition here copy the limit the buffer ',negative
'temporary typesafe casting ',negative
'truncate slice byte array maximum number characters and place the result into element vector ',negative
'for split sampling shrinkedlength checked against which from recordreadergetpos some inputformats which does not support getpos ',negative
'previously this was handled filtertablenames but cant anymore because can longer depend mapping between table name and entry the list ',negative
'nanos ',negative
'check the new configs are added ',negative
'try dropping table user should succeed ',negative
'the big table has reached new key group try let the small tables ',negative
'enable dynamic partitioning ',negative
'integer and boolean types require conversion use noop ',negative
'use self alias ',negative
'double quote seen and the index not inside single quoted string and the previous character was not escape then update the flag ',negative
'localizing files for submitting dag ',negative
'set later with setoutput methods ',negative
'this used the future make sure disable grouping the payload isnt already disabled ',negative
'get the string value and convert interval value ',negative
'try single stripe ',negative
'event methods ',negative
'add the transformation that computes the lineage information ',negative
'assumption this will run last after col pruning the prejoinorder optimizations projectrel not synthetic then ppd would have already pushed relevant pieces down and hence point running ppd again for synthetic projects dont care about non deterministic udfs ',negative
'subscriber can get notification about addition database hcat listening topic named hcat and message selector string hcatevent hcatadddatabase ',negative
'returns code true such base file can used materialize the snapshot represented this code validwriteidlist param writeid highest write given basexxxx file return true the base file can used ',negative
'return ',negative
'update the vectorptfdesc with data that used during validation and that doesnt rely lookup column names etc ',negative
'verify that new sessionstate has default ',negative
'add new vector child the vector parents children list ',negative
'rare case ',negative
'subclasses can override this step for example for temporary tables ',negative
'sort trivial ',negative
'drop table idempotent ',negative
'make sure compare them entity that its the same table partition etc ',negative
'construct using ',negative
'boolean that tells the hivemetastore remote server being used can used determine the calls metastore api hmshandler are being made with embedded metastore remote one ',negative
'verify that objectstore fetches the latest notification event ',negative
'list bucketing then bail out ',negative
'create columnstatistics obj ',negative
'test that there lead second adjustment ',negative
'store column name mapwork ',negative
'get partition metadata ',negative
'should come back null ',negative
'format the properties statement ',negative
'test one random highprecision subtract ',negative
'calling close unopened session probably harmless ',negative
'get the needed columns and name ',negative
'finally check the filter for nonbuiltin udfs these are present cannot ',negative
'the output the lateral view join will the columns from the select parent followed the column from the udtf parent ',negative
'compaction doesnt filter deltas but may have reader for base ',negative
'testing substring index starting with and zero length ',negative
'setop rewrite ',negative
'everything the batch has already been filtered out ',negative
'inherent most properties from table level schema and overwrite some properties the following code this mainly for saving cpu and memory reuse the column names types and ',negative
'source replication not set ',negative
'bounds check for oob exception ',negative
'count nonnull column rows ',negative
'logged info multiple other places ',negative
'require admin privilege ',negative
'pos outer join aliaspos other aliasnum filters outer join aliasxn for example left outer join akbk and full outer join akck and and that means apos there are overlapped filters associated with bpos and cpos has one filter and also has one filter making filter map for ',negative
'change file length and look for cache misses ',negative
'nonjavadoc see ',negative
'check that there one datasource with the published segment ',negative
'special case rare the segment buffer boundary ',negative
'test translation both filters and booleanvalued expressions nonfilters ',negative
'this isnt one track just return whatever matches the string ',negative
'represents the total memory that this join operator will use mapjoin operator ',negative
'one last test are enabling the rewrite need check that query ',negative
'try invalid alter table with partition key name ',negative
'udtf not handled yet the parent selectop udtf should just assume all columns ',negative
'check delegation token job conf any ',negative
'test upper case ',negative
'rollup and cubes are syntactic sugar top grouping sets ',negative
'create the groupby operator ',negative
'cbo related ',negative
'need plugins handle llap and uber mode ',negative
'number the test rows with collection order ',negative
'avoid long overflow will divide the max row count denominator and use that factor multiply with other row counts ',negative
'for the char and varchar data types the maximum character length the columns otherwise ',negative
'none the columns need cast theres need for additional select operator ',negative
'limit compensation arrays for keyvaluehashcodes ',negative
'junk after exponent ',negative
'super hack city notice the mod plus only happens after firstfield hit right ',negative
'will estimate map object only its field ',negative
'after merge the sparse switching threshold exceeded then change dense encoding ',negative
'objectinspector for input data ',negative
'same for char ',negative
'determine minimum all nonnull double column values maintain isgroupresultnull ',negative
'smith lastname ',negative
'the reducer ',negative
'undone trim trailing zeroes ',negative
'errormessage ',negative
'tablenames filter short assertequals tablenamessize ',negative
'all small aliases are staged need full bucket context ',negative
'set can verify they are reset operation ',negative
'should need for child vector expressions which would imply castingconversion ',negative
'eltindex string returns the string columnexpression value the specified index expression the first argument expression indicates the index the string retrieved from remaining arguments return null when the index number less than index number greater than the number the string arguments ',negative
'there should expectedcallcount calls drop partitions with each batch size ',negative
'first child should operand ',negative
'insert the additional http headers ',negative
'jar found ',negative
'row ',negative
'logj ',negative
'case verify the difference ',negative
'process the grouping sets ',negative
'not need apply the optimization ',negative
'optional required required ',negative
'noop are process sending have the correct value ',negative
'last param complete ',negative
'sparse registers are delta and variable length encoded ',negative
'when are running current query ',negative
'',negative
'seems that loadtabledesc has operationinsert only for ctas ',negative
'unexpected ',negative
'this position constant ',negative
'use the default field delimiter replace the multiplechar field delimiter but cannot use parse the row since column data can contain well ',negative
'one call per root input ',negative
'check the inspectors ',negative
'use case ',negative
'only columns can selected for both sorted and bucketed positions ',negative
'entry point aliasnum ',negative
'need the expr that generated the key the reduce sink ',negative
'add cache same group tsop ',negative
'use case ',negative
'nodeid can null the task gets unregistered due failure being killed the daemon itself ',negative
'nonjavadoc see ',negative
'col ',negative
'both old and new params are not null merge them ',negative
'reserve space for the int length ',negative
'executequery should always throw sqlexception ',negative
'intentionally overwrites anything the user may have put here ',negative
'false for continue has pair but not this turn ',negative
'replace original stddevpopx with sqrt sumx sumx sumx countx countx ',negative
'not confused with vectorizer level which represents the value configuration ',negative
'try the different getters ',negative
'convert such ',negative
'physical optimizer stages ',negative
'now that exceptions aka abortedtxnlist sorted ',negative
'blockedbyintid ',negative
'first try reuse from the same pool should just work ',negative
'for those stmthandle passed from instead statement ',negative
'bail out there nothing push ',negative
'test dropping tables and trash behavior ',negative
'insert some data new schema ',negative
'methods that does not need data object ',negative
'row ',negative
'col ',negative
'perform any key expressions results will into scratch columns ',negative
'initialize wfns ',negative
'uses noop proxy ',negative
'orc creates batch size make memory check align with instead ',negative
'initialize using projection the column range fieldssize ',negative
'private void out throws ioexception ',negative
'create file with blocks spread around the cluster ',negative
'possible that the row got absorbed the operator tree ',negative
'supports acidinputformat which not use the key pass rowid info ',negative
'check that right rows are selected ',negative
'cluster state changes will notify and wed update the queries again ',negative
'request two messages ',negative
'coming from smalltable side some bookkeeping and skip traversal ',negative
'serializationformat property has the default value will not included serde properties ',negative
'first try full match ',negative
'join key origin has been traced table column check the table external ',negative
'write the remaining part the array ',negative
'for outer joins contains the potential nulls for the concerned aliases ',negative
'nonjavadoc see int ',negative
'not lock the list for this and use volatile lastaccesstime instead ',negative
'created case the mapjoin failed ',negative
'table should not null ',negative
'constant from accumulos ',negative
'one produce these will the same using any other ',negative
'drop the table first case some previous test created ',negative
'pass ',negative
'the base compositeservice already stopped dont anything again ',negative
'run execdriver another jvm ',negative
'now start concurrent select from tab txn ',negative
'check there log file without the suffix ',negative
'update col stats map with col stats for columns from right side ',negative
'will only overwritten close errors out ',negative
'cred provider has entry and conf does not cred provider used ',negative
'sre lock are examining exclusive ',negative
'this entry output not present the output schema first check the table schema see part col ',negative
'fetch the column expression there should atleast one ',negative
'this should block behind the lock ',negative
'default all users authorizations when configuration provided ',negative
'publish the new partitions ',negative
'incrementalrows constructor should buffer the first rows ',negative
'todo constraintcache ',negative
'job hash map ',negative
'the object was added later for the same class see addtoprocessing ',negative
'now the add with java bigdecimal ',negative
'parts the partition ',negative
'lazysimple seems work better with row object array instead java object ',negative
'timeseries query results records ',negative
'create remote dirs once ',negative
'change body overridden methods use file settings file templates ',negative
'spot check null propagation ',negative
'have base work from ',negative
'add list saved historic operations ',negative
'basic sanity check other cases are not skipped because similar the case for long ',negative
'number spilled partitions only one last one partition left memory how often rows apart check memory full configuration for nway join write buffer size for ',negative
'reader will check for the event queue upon the end the input stream need interrupt ',negative
'populate vectormapjoininfo ',negative
'constructor with the individual addinitialcolumn method ',negative
'ignore index tables those will dropped with parent tables ',negative
'helper object that efficiently copies the big table key columns input key expressions ',negative
'remember the event operators weve seen ',negative
'first find the select closest the top ',negative
'join different keys different tables can longer apply multijoin conversion this longer valid star join bail out this the case ',negative
'for spark job with empty source data its not submitted actually would never receive jobstartjobend event jobstatelistener use javafutureaction get current job state ',negative
'hive ',negative
'read the altered tbl via cachedstore ',negative
'now add the keywords from the current connection ',negative
'owner information unchanged then properties wouldve changed ',negative
'currently smb broken cannot check its compatible with elevator dont use the below code that would get the correct mapwork see hive ',negative
'are compacting and its acid schema create reader for the bucket file that not empty ',negative
'not materialized view not rewrite ',negative
'tailing zeroes difference ',negative
'check columns ',negative
'closing the operator can sometimes yield more rows hive ',negative
'extract information from reference word from slot table ',negative
'optional int deletedelay default ',negative
'note that the pool per edc within edc cvbs are expected have the same schema ',negative
'extract type for the arguments ',negative
'this expected fail ',negative
'restrict with any filters found from where predicates ',negative
'after this optimization the tree should like fil skewed rows join fil skewed rows union fil skewed rows join fil skewed rows ',negative
'for windows paths ',negative
'user running the test belongs ',negative
'set the collection fields some code might not check presence before accessing them ',negative
'replace each the position alias groupby with the actual column name ',negative
'old table the cache and the new table can also cached ',negative
'multiple threads could try initialize the same time ',negative
'number elements list cannot determined this value will used ',negative
'default any additional data columns null once for the file they are present ',negative
'bgenjjtree constmap ',negative
'common analysis the statement boolean expression the following protected members can examined afterwards boolean boolean int thenselectedcount int thenselected int elseselectedcount int elseselected ',negative
'create ',negative
'',negative
'add the following strings column name table name tablenamecolumnname ',negative
'prscrscgby ',negative
'first remove the input operators the expression that ',negative
'row column information ',negative
'for now always convert double cant find common type ',negative
'create default route ',negative
'validate skewed information ',negative
'insert the value corresponding the current expression ',negative
'should technically update memory usage updating the old object but dont for now there mechanism properly notify the cache policyetc wrt parallel evicts ',negative
'the data not escaped reference the data directly ',negative
'some point these should inserted ',negative
'replace the original selectop the parents with selectunionop ',negative
'create new conf object bypass metastore authorization need retrieve all materialized views from all databases ',negative
'asc nulls first ',negative
'authorize this call the schema objects ',negative
'data structures ',negative
'first time registration new register comes before the previous unregister ',negative
'rely the caller supply reasonable total could log warning this doesnt match the allocation the last session beyond some threshold ',negative
'shouldnt hit digits until year ',negative
'the column has been read from disk ',negative
'reset value case any date fields are missing from the date pattern ',negative
'handle dual nature ',negative
'test nulls propagation ',negative
'clone the table ',negative
'check metrics during semantic analysis ',negative
'print out last part buffer ',negative
'find the extra table ',negative
'offset relative the beginning the stream where this ends ',negative
'required optional optional optional optional optional ',negative
'based update above ',negative
'this the overwrite case not care about the accuracy ',negative
'the only allowed flag newalloc and that only are not discarding ',negative
'for leftsemi join generate additional selection groupby operator before reducesink ',negative
'attempt match oracle semantics for timestamp arithmetic where timestamp arithmetic done utc then converted back local timezone ',negative
'the tables location currently unset left unset allowing the metastore fill the tables location note that the previous logic for some reason would make special case the was the default database and actually attempt generate location this seems incorrect and uncessary since the metastore just able fill the default table location the case the default for nondefault dbs ',negative
'end listiterator ',negative
'since metastore connections dont require the url this allowable ',negative
'ugi information not available connection setup time will set later via setugi rpc ',negative
'sleep until all threads with clean tasks are completed seconds completing task and sec grace period ',negative
'makes the message more informative helps find bugs client code ',negative
'get objinspectors for entire record and bucketed cols ',negative
'anything else fail ',negative
'note copywork supports copying multiple files but replcopywork doesnt ',negative
'split since mutatetransaction txn just does deletes ',negative
'for the case when the output can have null values follow the convention that the data values must for long and nan for double this prevent possible later zerodivide errors complex arithmetic expressions like col col the case when some col entries are null ',negative
'set needed columns for this dummy tablescanoperator ',negative
'undone provide isrepeated selected isnull ',negative
'catch the exception caused missing jpamso which otherwise would crashes the thread and causes the client hanging rather than notifying the client nicely ',negative
'now localtask the parent task the current task ',negative
'confirm grouping ',negative
'one scheduler pass from the nodes that are added startup ',negative
'normalize the columns sizes ',negative
'validate the third parameter which should integer represent ',negative
'move the last bytes the prefix area ',negative
'waitonprecursor determines whether not nonexistence dependent object error for regular imports for now the only thing this affects whether not the exists ',negative
'avgdecimal ',negative
'this the min number reducers for the bottom layer reducesinkoperators avoid query ',negative
'run load primary itself ',negative
'verify hit error while connecting ',negative
'flag for bucket map join one usage set ',negative
'are here when the left and right are nonzero and have the same sign ',negative
'bit packing ',negative
'replace with actual dir existed only want the absolute path remove the header such hdfslocalhost ',negative
'forgive error ',negative
'mapping once established not dependent upon the file channel that was used create delete file and hold onto the map ',negative
'for each dir get all files under the dir getsplits each individual file and then create ',negative
'validwriteidlist with hwmmaxlong include the data for aborted txn ',negative
'insert reduceside ',negative
'this selstarnocompute then this select operator treated like default operator just call the super classes process method ',negative
'array bitvectors where each entry denotes whether the element used not whether null not the size the bitvector same the number inputsaliases under consideration currently ',negative
'hadoop ipc wraps grrr ',negative
'intended predicate removed ',negative
'adds tables only for create view ppd filter can appended outer query ',negative
'only vectorized orc input cached theres reason ',negative
'use the same mechanism copy filesotherfiles and libdir but only want put contents libdir sqooplib thus pass the list names here ',negative
'this must leadlag udaf ',negative
'allocated ',negative
'trim trailing zeroes but only below the decimal point ',negative
'unlikely thrown ',negative
'need make sure that the list element type settable ',negative
'create database without location clause ',negative
'',negative
'the query has enforced that sortmerge join should performed for more details look filesinkdescjava ',negative
'that always true now but wasnt some day the below would throw getcolumndata ',negative
'allow debugging disabling column reuse input cols are never reused design only ',negative
'field because cannot multiinherit ',negative
'kill previously launched child jobs started this launcher prevent having ',negative
'test delete column stats col name passed all column stats associated with the ',negative
'these need based the target ',negative
'first kill any running jobs ',negative
'private ',negative
'the return type the genericudf boolean and all partitions agree result update the state the node true false ',negative
'synchronize the cache entry that one else can invalidate this entry ',negative
'fraction digit parsing move next lower longword ',negative
'one the children left right union identity projection followed union merge with ',negative
'now that the primary reader has advanced need see ',negative
'this cache range prefix the requested one the above also applies the cache may still contain the rest the requested range dont set gotalldata ',negative
'setup stdout and stderr ',negative
'note scratchdir reused implicitly because the sessionid the same ',negative
'the value has schema and not fieldschema ',negative
'validate sort columns and bucket columns ',negative
'llap cache can disabled via config isplancache ',negative
'does not contain limit operation bail out ',negative
'outside range ',negative
'correlate does not have clause for left correlate predicate must evaluated first for inner can defer ',negative
'partcount not equally divided into batches last batch size will less than batch size ',negative
'suppress leading zeroes ',negative
'extra element make sure have the same formula compute the length each element the array ',negative
'the other list doesnt exist create the first index our ',negative
'have statistics for the table size appropriately ',negative
'add original direcotries obsolete list any ',negative
'the context for creating the vectorizedrowbatch for this map node that the vectorizer class determined ',negative
'enabled get the final output writers and prepare the real output row ',negative
'register jvm metrics ',negative
'lock was outdated and was removed then maybe another transaction picked changed its state ',negative
'skip combine for all paths ',negative
'test decimal column decimal scalar division this used cover all the cases used the source code template the template used for division and modulo ',negative
'normal case variablelength arguments ',negative
'account for potential partial chunks ',negative
'groupingid groupingidoi ',negative
'register the pending events sent for this spec ',negative
'get the for the next entry the queue ',negative
'this method takes object accepts whatever types that are passed ',negative
'just copy the payload link has already been populated ',negative
'matching oracle behavior ',negative
'for orc case send the boundaries the stripes dont have send the footer ',negative
'convert skewdata contain exprnodedesc the keys ',negative
'the cost the result ',negative
'when people forget quote string opop null for example select from sometable where and ',negative
'fetch the table marked the message and compare ',negative
'for now this only used determine the bucketingsorting outputs the future this can removed optimize the query plan based the bucketingsorting properties ',negative
'realign the positionmap for the columns appearing after hcatfieldschema ',negative
'very simple counter keep track number rows processed operator dumps every million times and quickly before that ',negative
'for non partitioned table this will represent the last tablename replicated else its the name the ',negative
'logging inside ',negative
'cache key ',negative
'first try known drivers ',negative
'cache size should now ',negative
'test feb leap year ',negative
'submits the request and returns ',negative
'crttbldesc ',negative
'mapping from constraint name list default constraints ',negative
'some keys need left null corresponding that grouping set ',negative
'write cost ',negative
'die ',negative
'remove this branch ',negative
'synonym some places the code ',negative
'padding needed ',negative
'the operation atomic ',negative
'need check the total size local tables under the limit here are using strong condition which the total size local tables used all input paths actually can relax this condition check the total size local tables for every input path example unionall mapjoin mapjoin big big this case have two mapjoins mapjoin and mapjoin big and big are two big tables and and are four small tables hash tables and will only used map tasks processing big hash tables and will only used map tasks processing big bigbig should only check the size under the limit and the size under the limit but right now are checking the size under the limit bigbig will only scan path once mapjoin and mapjoin will executed the same map task this case need make sure the size ',negative
'initialize the keys and values ',negative
'enabled accept quoting all character backslash qooting mechanism ',negative
'user has fully specified partition validate that partition exists ',negative
'the user trying insert into external tables ',negative
'however the fastbigintegerbytes can take trailing zeroes make larger ',negative
'create table ',negative
'stub out the zki mock ',negative
'check based the hive integer type need test with isbyte isshort isint islong not use corrupted truncated values for the hive integer type ',negative
'reopen the hms connection ',negative
'whether this for the columnlevel schema opposed nested column fields ',negative
'remember the event operators weve abandoned ',negative
'base tables set lets replicate them over ',negative
'convert from bucket map join sort merge bucket map join enabled ',negative
'doesnt require additional jobs ',negative
'input produces correlated variables move them the front right after any existing group fields ',negative
'initialize args ',negative
'must use raw local because the checksummer doesnt honor flushes ',negative
'and shouldnt even and are even then none the values will set bit thus introducing errors the estimate both and can even the times and result the bit vectors could inaccurate avoid this always pick odd values for and ',negative
'validate the materialized view statement ',negative
'update jobconf using mrinput info like filename comes via this ',negative
'best effort attempt write all output from the script before marking the operator ',negative
'assumes row schema stringintstring ',negative
'get max split size for ',negative
'map work should start with our ',negative
'because scaling down this could happen ',negative
'first multiple elements ',negative
'check the hints see the user has specified mapside join this will removed later once the costbased ',negative
'show tables dbname invalid show tables syntax hive does not return any tables this case ',negative
'assume stream list sorted column and that nondata streams not interleave data streams for the same column ',negative
'hive this creates deltabucket ',negative
'currently only used during reoptimization related parts ',negative
'add mapreduce job tag placeholder ',negative
'there credential provider configured for hadoop jobconf should not contain credstore password and provider path even env set ',negative
'bit before restart the loop ',negative
'bytesfieldbyteend separator ',negative
'lazysimpleserde doesnt support projection ',negative
'for given work descriptor extracts information about the reducesinkops the work for tez you can restrict reducesinks for particular output vertex ',negative
'get the input and prepare the output ',negative
'are skipping the skewedstringlist table here seems totally useless ',negative
'experiment ',negative
'few checks determine eligibility optimization look select list see its min max count etc connect metastore and get the stats compose rows and add fetchwork ',negative
'dont try log anything when appender stopped ',negative
'and then see what happens based the provided schema ',negative
'test between ',negative
'validate the third parameter which should also integer ',negative
'the hive config values ',negative
'use smallint outputtypeinfo ',negative
'inputformat ',negative
'the vertex that should inlined operator list vertex that ',negative
'start hiveserver with given config fail server doesnt start ',negative
'should not update the following values serdeinfo contains these this keep backward compatible with getschema where these keys are updated after serdeinfo properties got copied ',negative
'retrieve skewed columns ',negative
'tests concurrent modification and that results are the same per input across threads but different between inputs ',negative
'rule requires that aggregate key the same the join key the way neither superset nor subset would work ',negative
'either were interrupted one handleevent which case there reader error event waiting for the queue some other unrelated cause which interrupted which case there may not reader event coming either way should not try block trying read the reader events queue ',negative
'split pairs delimiter ',negative
'arbitrary tokens the renewer should the principal the jobtracker ',negative
'add test parameters from official storage formats registered with hive via ',negative
'static partition and list bucketing ',negative
'ignore the char after escapechar ',negative
'data and can just ignore them ',negative
'have already updated the metrics for the failure change the state ',negative
'this needs return live connection used operation that follows thus only closes connection failureretry ',negative
'otherwise compare with power and half ',negative
'return buildtestdata dfs ',negative
'acidop flag has checked use java hash which works like identity function for integers necessary read recordidentifier incase acid updatesdeletes ',negative
'try fold the expression remove cast constant ',negative
'the other list will exhausted when commits create new one pending that commit ',negative
'astring ',negative
'this multiply produces more than digits overflow ',negative
'eventually want this richer description having table role etc scope for now have trivial impl having only and table scopes determined whether not the tablename null ',negative
'wrap thrift connection with sasl for secure connection ',negative
'dont pass the parsed name will parse itself ',negative
'this not transactional ',negative
'flip the sign bit and unused bits the highorder byte the sevenbyte long back ',negative
'this truncate column command ',negative
'these are the vectorized batch expressions for filtering key expressions and value ',negative
'number tokens drop between sleep intervals ',negative
'verify table has been the target replication and check hiveconf were allowed override not fail ',negative
'are going add splits for these directories with recursive false ignore any subdirectories deltas original directories and only read the original files the fact that theres loop calling addsplitsforgroup already implies its the real input format multiple times however some split concurrencyetc configs that are applied separately each call will effectively ignored for such splits ',negative
'removed from heap without evicting ',negative
'ignored ',negative
'limit nothing propagate just bail out ',negative
'for now dont support joins using decimal ',negative
'not conversion function ',negative
'',negative
'set proxy user privilege and initialize the global state proxyusers ',negative
'partial time time part will skipped ',negative
'need extrapolate this partition based the other partitions ',negative
'dummy handle for thriftcliservice ',negative
'there are some required privileges missing create error message sort the privileges that error message deterministic for tests ',negative
'walk the list and acquire the locks any lock cant acquired release all locks sleep ',negative
'remove small table ailias from aliastoworkavoid concurrent modification ',negative
'initialize output vector buffer receive data ',negative
'not sure need this exec context but all the operators the work will pass this context throught ',negative
'replace stderr and run command ',negative
'case dynamic queries possible have incomplete dummy partitions ',negative
'the cases that have multistage insert skew join can happen that want multiple commits into the same directory from different tasks not just task instances nonmm case ensures unique names could the same here but this will still cause the old file deleted because has not been committed this fsop are going fail safe potentially could implement some partial commit between stages this ',negative
'all row indexes are null then indexes are disabled ',negative
'this should now throw some useful exception ',negative
'execute query ignore exception any ',negative
'print the column names ',negative
'not expected encountered for hive fail ',negative
'clears the dest dir when src subdir dest ',negative
'word size choose bits stay below the bit sign bit need multiplier digit commad ',negative
'int ',negative
'partitioning spec ',negative
'virtual relation generated the reduce sync ',negative
'set value minvalue that minvalue overflows and gets set minvalue again ',negative
'test getalltables ',negative
'hive servers session input stream not used open persession file autoflush mode for writing temp results and tmp error output ',negative
'write the size the map which vint ',negative
'stagelist ',negative
'the first bounds check requires least one more byte beyond for int hence ',negative
'there should original bucket files and base directory plus two new delta directories and one deletedelta directory that would created due ',negative
'normalize prop name ',negative
'the total size local tables localworki unknown ',negative
'need get new one see the comment wrt threadlocals ',negative
'first incremental ',negative
'check the stlevel children and simple semantic checks ctlt and ctas should not coexists ctlt ctas should not coexists with column list target table schema ctas does not support partitioning for now ',negative
'datestats ',negative
'execute the test query ',negative
'',negative
'combine two lists ',negative
'for each alias add object inspector for filter tag the last element ',negative
'may contain duplicates remove duplicates ',negative
'use separate metastore client for heartbeat calls ensure heartbeat rpc calls are isolated from the other transaction related rpc calls ',negative
'this could brittle ',negative
'assume ranges ranges are nonoverlapping thus will save next advance ',negative
'the virtual columns available under vectorization they may not actually used this query unused columns will null just like unused data and partition columns are ',negative
'traverse through all the source files and see any file not copied partially copied yes then add the retry list source file missing then retry with path path ',negative
'initialize fastsetfrom ',negative
'production typedef definitiontype thisname ',negative
'check that all calls were recorded ',negative
'operations involvingreturning daytime intervals ',negative
'project ',negative
'insert overwrite unpartitioned table ',negative
'mstringstring ',negative
'tests for int partitionspec method ',negative
'for stripelevel streams dont need the extra refcount the block ',negative
'object inspector for serializing input tuples ',negative
'handle reducesinkoperator here can safely ignore table alias and the current comparator implementation does not can ignore table alias since when compare reducesinkoperator all its ancestors need match down table scan thus make sure that both plans are the same ',negative
'distcp currently does not copy single file distributed manner hence dont care about the size file there only file dont want launch distcp ',negative
'additional static classes defined after this point ',negative
'initialize second mocked filesystem implement only necessary stuff ',negative
'error was expected ',negative
'arithmetic specializations are done convoluted manner mark them builtin ',negative
'clear any existing databases ',negative
'set that can leverage columnpruner ',negative
'full constant propagation only perform expression shortcutting remove unnecessary andor operators ',negative
'write value element ',negative
'used keep positionlength for complex type fields note the top level uses startpositions instead ',negative
'jobid job new jobid ',negative
'linear search since this wont take much time from the total execution anyway lower has the range total ',negative
'make sure the path normalized expect validation pass since just created ',negative
'propagate input format necessary ',negative
'bean methods ',negative
'this for backward compatibility the user did not specify the output column list assume that there are columns key and value however the script outputs col col col seperated tab the requirement key col and value col tab col ',negative
'dont create new separate filter most cases there will only one ',negative
'same primitive category but different qualifiers ',negative
'toss all other exceptions related reflection failure ',negative
'the original may have settable info that needs added the new copy ',negative
'note this code very similar the code ',negative
'before cleaner there should items ',negative
'has failed because the query was killed from under ',negative
'planner rule that creates code semijoinrule from link top link todo remove this rule and use calcites semijoinrule not possible currently since calcite doesnt use relbuilder for this rule and want generate hivesemijoin rel here ',negative
'table giving binary powers entry ',negative
'nonjavadoc see javaioreader ',negative
'process the third child nodeif exists get partition specs ',negative
'optional string srcname ',negative
'copy the test files into hadoop required ',negative
'test submission concurrent job requests ',negative
'supports keeping timestampwritable object without having import that definition ',negative
'process the normal splits ',negative
'check that the columns referenced these comparisons form ',negative
'position where wed write position where wed read unsafely write time ',negative
'only need apply negation all words when there are words etc ',negative
'thrown when the table altered does not exist ',negative
'set mock warehouse ',negative
'for partial and partial ',negative
'value correct ',negative
'import external table partition partcolumnvalue from sourcepath location importtargetpath ',negative
'hive server mode are not able retry the fetchtask case when calling fetch queries since execute has returned ',negative
'inner join ',negative
'throw new hiveexceptionremove not sort order and unique ',negative
'does not need unique just nonzero distinct value test against ',negative
'whether are using acid compliant transaction manager has already been caught are updating deleting and getting nonacid here means the table itself doesnt support ',negative
'just making sure datevalueof works ',negative
'these are used ',negative
'how many columns ',negative
'adding constant memory for the rabit hole deep implement memoryestimate interface also constant overhead ',negative
'set the fetch operator for the new input file ',negative
'insert overwrite ',negative
'project with the output our operator ',negative
'reallocate larger multiple defaultsize ',negative
'there should calls create partitions with batch sizes ',negative
'init output object inspectors the return type for partial aggregation still list strings the return type for final and complete full aggregation result which ',negative
'create temp table with current connection ',negative
'drain unused sessions the close sync delegate the caller ',negative
'variables for llap hash table loading memory monitor ',negative
'iterate and update masks array ',negative
'are the system registry and this feature enabled try get from metastore ',negative
'try push the full filter predicate iff the filter top tablescan the filter top ptf between ptf and filter there might select operators otherwise push only the synthetic join predicates note pushing filter top ptf necessary the for rank functions gets enabled ',negative
'corresponding branch since only that branch will factor the reduction ',negative
'after all the perf changes that might was well hardcode them separately ',negative
'this fast path for query optimizations can find this info quickly using directsql point failing back slow path here ',negative
'with non null value before trying alter the partition column type ',negative
'root the start the operator pipeline were currently ',negative
'extract date special handling since function hive does include timeunit observe that timeunit information implicit the function name thus translation will proceed correctly just ignore the timeunit ',negative
'this known incomplete caused orc end boundaries being estimates ',negative
'see comment next the field ',negative
'the input file has changed load the correct hash bucket ',negative
'user specified the memory for local mode hadoop run ',negative
'restore the local job tracker back original ',negative
'dont log the stack this normal ',negative
'hadoopclientopts appended hadoopopts hadoopsh should remove the old hadoopclientopts which might have the main debug options from current hadoopopts new hadoopclientopts created with child jvm debug options and will appended hadoopopts agina when hadoopsh executed for the child process ',negative
'now new job requests should succeed status operation has cancel threads ',negative
'take all the driver run hooks and postexecute them ',negative
'else cast newinput end ',negative
'now hook the children ',negative
'not repartition take number splits from children ',negative
'construct the inner struct ',negative
'cvalue map rowvalues assertequals cvaluesize assertequalsv cvaluegetk ',negative
'avoid copy when newtmpjars null empty ',negative
'either the user the kill not done yet ',negative
'find first virtual column and clip them off ',negative
'this correlated need make and type and type should retrieved from outerrr ',negative
'final return type that goes back hive list structs with ngrams and their estimated frequencies ',negative
'update path iocontext ',negative
'splits ',negative
'collect newer entry superset existing entry ',negative
'',negative
'must fill high word from both middle and lower longs ',negative
'next translate the tezwork tez dag ',negative
'assume only parent for operator ',negative
'add smallint values ',negative
'theres extensive need for this have its own type mirrors the intent copy enough this might change later though ',negative
'the getsession call should also fail ',negative
'verify standard case ',negative
'the length compactor only keep the rest ',negative
'required optional optional required ',negative
'one time initialization ',negative
'top level ',negative
'assert that the source and target partitions are equivalent ',negative
'for backward compatibility let the above parameter used ',negative
'verify that getoperationstatus returned only after the long polling timeout ',negative
'trying connect with nonexistent user should still fail with login failure ',negative
'this also gets around the enum issue since just take the value ',negative
'table will queried directly llap acquire locks necessary they will released during session cleanup the read will have readcommitted level semantics ',negative
'return itself should noop the pool went from with session the pool ',negative
'prepare the field objectinspectors ',negative
'only process equivalences found the join conditions processing equivalences from the left right side infer predicates that are already present the tree below the join ',negative
'set unique constraint name null before sending listener ',negative
'process grouping set for the reduce sink operator ',negative
'operations other than table rename ',negative
'ignore nulls ',negative
'would expect entries txntowriteid each insert would have allocated writeid including aborted one ',negative
'system environment variables ',negative
'return the value ',negative
'should look take the parent fsops task the current task ',negative
'using commontreeadaptor because the adaptor parsedriver doesnt carry the token indexes when duplicating tree ',negative
'round towards positive infinity ',negative
'check have visited this operator ',negative
'test submission concurrent job requests with the controlled number concurrent requests and job request execution time outs verify that get appropriate exceptions and exception message ',negative
'nonjavadoc see ',negative
'variables for metrics ',negative
'all resources including hdfs are session based ',negative
'use instead ',negative
'when set ',negative
'compaction can only done the whole table the table nonpartitioned ',negative
'commit ',negative
'nonjavadoc see ',negative
'newtbl ',negative
'since our probing method totally bogus give after some time ',negative
'combo both set none ',negative
'the only time this condition should false the case dynamic partitioning where the data bucketed dynamic partitioning column and the filesinkoperator being processed this case the dynamic partition column will not appear colinfos and due the limitations dynamic partitioning they will appear the end the input schema since the order the columns hasnt changed and new columns have been addedremoved safe assume that these will have indexes greater than equal colinfossize ',negative
'the join columns should contain all the sort columns the sort columns all the tables should the same order ',negative
'add spark job the hive history ',negative
'set the usernamepasswd for the accumulo connection ',negative
'map values may serialized binary format when they are primitive and binary ',negative
'oracle cannot have over expressions inlist ',negative
'the child selectoperator has the columnexprmap need update the columnexprmap the parent selectoperator ',negative
'partition names are url encoded decode the names unless hive configured use the encoded names ',negative
'all are filtered out ',negative
'and and and and and ',negative
'these should have viable defaults ',negative
'this point show compactions should have failed initiated explicitly user ',negative
'convert udaf params exprnodedesc ',negative
'make create table statement ',negative
'compiles the given pattern with proper algorithm ',negative
'execute the given sql statement ',negative
'add more failed compactions that the total exactly ',negative
'now remove all baseworks all the childsparkworks that created ',negative
'serialize each field ',negative
'cookie passes the validation return null the caller ',negative
'first allocation write hwm should add the table the nextwriteid meta table ',negative
'all correlation variables are now satisfied skip creating value ',negative
'set second argument ',negative
'tolerance for long range bias and for short range bias ',negative
'stripped down version this adapted for hive but should eventually deleted from hive and make use above ',negative
'get the input output file formats ',negative
'work boundary stop exploring ',negative
'force reread the configuration file this done because ',negative
'the reason use comparecommand rather than simply getting the serialized output and comparing for partitionbased commands that the partition specification order can different different serializations but still effectively the same ababc should the same babca ',negative
'view entity ',negative
'longdouble arithmetic ',negative
'use case ',negative
'close the stripe reader are done reading ',negative
'commit ',negative
'make clone existing hive conf make clone existing hive conf ',negative
'choose random sign ',negative
'todo hive potential blocking call mrinput handles this correctly even interrupt swallowed ',negative
'not support grouping set right now ',negative
'checkconstraintcols ',negative
'the storage arrays for this column vector corresponds the storage timestamp ',negative
'different parts the code rely this being set ',negative
'failed init remove from cache ',negative
'mysql parser ',negative
'return provider ',negative
'string type affinity ',negative
'the init method hmshandler throws exception all the times should retried until reached before giving ',negative
'dynamic partition ',negative
'remember the output name the reduce sink ',negative
'state has not changed already registered for notifications ',negative
'the types the expressions for the lateral view generated rows ',negative
'the methods ',negative
'for hybrid grace hash join need see there any spilled data processed next ',negative
'should not happen this should not get called before thisstart called ',negative
'rewriting was produced will check whether was part incremental rebuild try replace insert overwrite insert ',negative
'acquire lock ',negative
'add hbase related configuration spark because security mode spark needs generate hbase delegation token for spark this temp solution deal with spark problem ',negative
'nonbean ',negative
'the fourth could combined again ',negative
'field name field type ',negative
'try merge join tree from inner most source was merged from outer most inner which could invalid join tree abcd where not mergeable with can merged with into single join and only and has same join type ',negative
'note grouping sets are not allowed with map side aggregation set false dont have worry about ',negative
'descriptors for subq and subq are linked ',negative
'close one connection verify still two left ',negative
'writable ',negative
'incremental repl with alters dbtablepartition ',negative
'something else wrong ',negative
'run some queries ',negative
'perform any partition expressions results will into scratch columns ',negative
'hhelp ',negative
'from precision minps max scale maxs ',negative
'thread local configuration needed many threads could make changes ',negative
'bailout select involves transform ',negative
'verify that new job requests should succeed with issues ',negative
'test that existing sharedread partition with new sharedwrite coalesces ',negative
'perform major compaction ',negative
'adding headerlogix getsecondintlogix before logix ',negative
'add the function name writeentity ',negative
'check the given sparktask has child sparktask that contains the target mapwork does not then remove the target from dpp ',negative
'this valid executable command then add the buffer ',negative
'this helper object deserializes known deserialization input file format combination into columns row vectorized row batch ',negative
'construct new decimalformat only new dvalue ',negative
'rows with rank ranklimit are output only the first row with rank ranklimit output ',negative
'the cinfo for astnode this function returns the astnode that for ',negative
'dpp indeed set parallel edges true ',negative
'this noop there column assign and val expected null ',negative
'for the moment pretend all matched are selected can evaluate the value expressions since may use the overflow batch when generating results will assign the selected and real batch size later ',negative
'update delete merge there need cardinality check ',negative
'create dummy key for searching the owidbucket the compressed owid ranges ',negative
'immediate retry ',negative
'create matcher for custom path ',negative
'add aggregate see the reference example above the top aggregate ',negative
'reread length ',negative
'create new batch with one char column for input and one long column for output ',negative
'causing each thread get different client even the conf same ',negative
'additional rows corresponding grouping sets need created here ',negative
'todo test changes mark cleaned clean txns and txncomponents ',negative
'write the size the list vint ',negative
'can put multiple group bys single reducer determine suitable groups expressions otherwise treat all the expressions single group ',negative
'this assumes ranges passed cache fetch have data beforehand ',negative
'save join type ',negative
'the sorting order the child more specific than that the parent assign the sorting order the child the parent ',negative
'',negative
'when last txn finished the currenttxnindex pointing that txn need start from next one any also batch was created but ',negative
'not overwrite ',negative
'with the values generated and propagated from the right input ',negative
'skip the colinfos which are not for this particular alias ',negative
'for mapreduce job ',negative
'create hivesessionimpl object ',negative
'pipeline which can cause cycle after hashjoin optimization ',negative
'revokegrantoption ',negative
'return true current min has next row ',negative
'translate work vertex ',negative
'get the positions for partition bucket and sort columns ',negative
'initialize the config ',negative
'traversing operator tree ',negative
'the rpc server will take care timeouts here ',negative
'handle child tasks here could add them directly whereever need ',negative
'positive unix time ',negative
'bucketcols ',negative
'available and grammar check there the language itself ',negative
'intwritable can just sum the reduce ',negative
'full paths are replaced with base filenames ',negative
'test the user session specific conf overlaying global init conf ',negative
'due the limitation that can only have one instance persistence manager factory jvm are not able create multiple embedded derby instances for two different metastore instances ',negative
'couldnt find reference expression ',negative
'consider query like select from subq has filter join subq has filter some key let assume that subq the small table either specified the user inferred automatically the following operator tree will created tablescan subq select filter dummystore smbjoin tablescan subq select filter ',negative
'non partitioned table ',negative
'using signatures and encryption ',negative
'try valid alter table ',negative
'use table properties case unpartitioned tables and the union table properties and partition properties with partition ',negative
'find first nonsign xff byte input ',negative
'insert into partitions and get the last repl ',negative
'retain this digit ',negative
'for use from within ',negative
'particular use size data number uses ',negative
'singleton using dcl ',negative
'used groupby ',negative
'insert this map into the stats ',negative
'over the subqueries and getmetadata for these ',negative
'with grant also implies without grant privilege add without privilege well ',negative
'get the existing table ',negative
'case views the underlying views tables are not direct dependencies and are not used for authorization checks this readentity represents one the underlying tablesviews skip see description the isdirect readentity ',negative
'fill much the overflow batch possible with small table values ',negative
'regular create table create table like ctlt create table select ctas ',negative
'set ssl ',negative
'now trim the overstuffed histogram down the correct number bins ',negative
'indicates that are test mode ',negative
'drop table event ',negative
'tablescan nonhive table dont support for materializations ',negative
'returning true does not guarantee that the task will run considering other queries may running the system also depends upon the capacity usage configuration ',negative
'tests not setting maxrows ',negative
'dont have column stats just assume hash aggregation disabled ',negative
'sparkwork ',negative
'increase target list pos target list being drained set delta and refcount ',negative
'look for hint not run test some hadoop versions ',negative
'helper function create jobconf for specific reducework ',negative
'methods that create group keys and aggregate calls ',negative
'required required required required optional optional optional optional ',negative
'run cleaner this run doesnt anything for the above aborted transaction since the current compaction request entry the compaction queue updated have highestwriteid when the worker run before the aborted transaction specifically the for the entry but the aborted transaction has writeid this run does transition the entry ',negative
'contains aliases from subquery are just converting common merge join operator the shuffle join mapreduce case ',negative
'the overwhelming majority cases will here read bytes tada ',negative
'',negative
'the entire current slice part the split note that split end equals lastend the split would also read the next row need look the next slice any although wed probably find cannot use note also that not treat endoffile differently here cause not know any such thing the caller must handle lastend end split end file match correctly terms how lrr handles them see above for startoffile ',negative
'the return values are capped return and ',negative
'launchable task one that hasnt been queued hasnt been initialized and runnable ',negative
'has the user enabled merging files for maponly jobs for all jobs ',negative
'partitioned table stats not updated ',negative
'look for inputfileformat serde combinations can deserialize more efficiently using and deserialize class with the deserializeread interface the vectorized rowbyrow deserialization into vectorizedrowbatch the ',negative
'this the last file for this bucket maxkey null means the split the tail the file want leave blank make sure any insert events delta files are included conversely its not the last file set the maxkey that events from deltas that dont modify anything the current split are excluded ',negative
'case max list members max query string length ',negative
'all bits are set expected should ',negative
'rxrx and and rand ',negative
'creator ',negative
'minus here otherwise driver also counted executor ',negative
'let uval the value the unsigned long with the same bits twos complement uval max uval max now use the fact abc acbcc ',negative
'batchindex classname nomatch duplicate ',negative
'schema info ',negative
'prepare the list databases ',negative
'batching not enabled try drop all the partitions one call ',negative
'move any incompatible files final path ',negative
'check external table property being removed ',negative
'remove values key exprs for value table schema value expression for hashsink will modified ',negative
'this check recursively all the parent roles currole ',negative
'deserializer null case vectormapoperator ',negative
'todo when will the queue ever null pass queue and default constructor parameters and make them final ',negative
'set memory usage for the operator ',negative
'the names the columns the input matchpath used setup the tpath struct column ',negative
'create proxy for the local filesystem the schemeauthority serving the proxy derived from the supplied uri ',negative
'get the sort positions and sort order for the table ',negative
'adjust the aggregator argument positions note aggregator does not change input ordering the input output position mapping can used derive the new positions ',negative
'skewed keys which intersect with join keys ',negative
'leaf ',negative
'sort does not change input ordering ',negative
'false for the following cases name list which matches the spec name bag which indicates existing hive pig data ',negative
'the fromindexinclusive and toindexexclusive for each unique owid ',negative
'create syntax tree for simple function call longudfcol ',negative
'tenscale can make this long comparison ',negative
'look for matches time based counters ',negative
'both are numeric make sure the new type larger than the old ',negative
'the database newer than the create event then noop ',negative
'noop ',negative
'use the maximum parallelism from all parent reduce sinks ',negative
'the groupbyoperator not initialized which means there data since initialize the operators when see the first record just nothing here ',negative
'use the sign the reversednanoseconds field indicate that there second vint present ',negative
'also collect table stats while collecting column stats ',negative
'findwriteslot slot slot tripleindex tripleindex existing ',negative
'simple one long key map join benchmarks build with mvn clean install dskiptests pdistitests main hive directory from itestshivejmh directory run java jar targetbenchmarksjar inner innerbigonly leftsemi outer rowmodehashmap rowmodeoptimized vectorpassthrough nativevectorfast ',negative
'nonjavadoc see byte ',negative
'object inspectors corresponding the struct returned terminatepartial and the fields within the struct maxlength sumlength count countnulls ',negative
'only one belows notnull total length sample prunes splits exceeded percent total input prunes splits exceeded row count per split not prune splits ',negative
'hive acidorc requires hiveinputformat ',negative
'retrieve all partitions generated from partition pruner and partition column pruner ',negative
'the type the hive column cannot store the actual typeinfo because that would require ',negative
'check this udf has been provided with type params for the output char type ',negative
'alter table add columns ',negative
'right larger ',negative
'bacabc ',negative
'test variations callbacks increases revokes not update the same task again then increase decrease and decrease increase the call coming after the message sent the message callback should undo the change ',negative
'append this container the loaded list ',negative
'captures the window processing specified query query may contain udaf invocations window leadlag function invocations that can only evaluated partition for queries that dont have group all udaf invocations are treated window function invocations for queries that dont have group the having condition handled post processing the rows output windowing processing windowing container all the select expressions that are handled windowing these are held lists the functions list holds windowfunction invocations the expressions list holds select expressions having leadlag function calls may also contain astnode representing the post filter apply the output window functions windowing also contains all the windows defined the query one the windows designated the default window the query has distribute bycluster clause then the information these clauses captured partitioning and used the default window for the query otherwise the first window specified treated the default finally windowing maintains map from alias the astnode that represents the select expression that was translated window function invocation window expression this used when building rowresolvers ',negative
'cannot use integermaxvalue which ',negative
'because pairs give java the vapors ',negative
'returns null always ',negative
'level the return all filesdirectories under the specified path ',negative
'its not this property wont any others ',negative
'equivalent acidsinks but for ddl operations that change data ',negative
'rewritten grouping function ',negative
'remove currtask from childtasks ',negative
'and integermaxvalue the check for varchar precision done hive ',negative
'maximum reasonable defragmentation headroom mostly kicks very small caches ',negative
'storing nanosecond interval longs produces timestamp ',negative
'the the bucket care about here ',negative
'start inclusive infinity inclusive ',negative
'table comment ',negative
'make sure cross some buffer boundaries ',negative
'xmx speficied ',negative
'have override with the new conf since this where prewarm gets the conf object ',negative
'scale down again ',negative
'need new object obey our immutable behavior ',negative
'while initializing this need done here instead constructor ',negative
'release but keep the lock present ',negative
'case column stats ',negative
'first call createpartitions should throw exception ',negative
'haruri used access the partitions files which are the archive the format the something like ',negative
'are creating filter here should not returning null not sure why calcite return null ',negative
'check edge case throw exception can not build single query for not clause cases mentioned the method comments ',negative
'were sure this part smaller than memory limit ',negative
'replace essentially renaming plan the name existing plan with backup ',negative
'successful ',negative
'todo create this centrally case ',negative
'decimal note the scale parameter for text serialization that creates trailing zeroes output decimals ',negative
'now make exporttask from temp table ',negative
'pending pending pending ',negative
'for missing wdw frames for frames with only start boundary completely ',negative
'not able acquire the lock within that time period ',negative
'dictionary encoding ',negative
'the output columns for the destination table should match with the join keys this handle queries the form insert overwrite table select tkey tkey udftvalue tvalue from join tkey tkey and tkey tkey where and are bucketizedsorted key and key assuming the table which the mapper run the following true the number buckets for and should same the bucketingsorting columns for and should same the sort order should match with the sort order for partitioned only single partition can selected the select list should contain with tkey tkey tkey tkey ',negative
'consolereader will the substitution and only there exactly one valid completion ignore other cases ',negative
'table ddl ',negative
'the current expression node column see the column alias already part the return set not and already have entry set this invalid expression ',negative
'use case ',negative
'validate the first parameter which the expression compute over this should ',negative
'default false ',negative
'open txns are already sorted ascending order this list may may not include hwm but guaranteed that list wont have txn hwm but overwrite the hwm with currenttxn ',negative
'get the nulls ',negative
'utility get patterns from url every array element match for one ',negative
'the stack the table scan operator ',negative
'should have rolled over next transaction batch ',negative
'first quickly check the two operators can actually merged already know that these two operators have the same parent but need check whether both are actually equal further check whether their child also equal any these conditions are not ',negative
'format ',negative
'set results returned ',negative
'next put row into corresponding hash partition ',negative
'list locks protect the above list ',negative
'partitions ',negative
'they will different values ',negative
'flush any catalog objects held the metastore implementation note that this does not flush statistics objects this should called the beginning each query ',negative
'concurrent increase and revocation then another increase after the message sent ',negative
'add all the names for previous batches ',negative
'represents column information exposed queryblock ',negative
'the struct null and level dynamicserde will call writenull ',negative
'cached use serialize data ',negative
'the group operator has null key ',negative
'all remaining functions simply delegate objectstore ',negative
'column not found target table its new column its schema mapstringstring ',negative
'printstream that stores messages logged list ',negative
'set default spark configurations ',negative
'test that fetching nonexistent dbname yields objectnotfound ',negative
'break into multiple batches remove duplicates first ',negative
'this second attempt create should throw ',negative
'recursively remove this task from its childrens parent task ',negative
'replace flag not set caller then default set true maintain backward compatibility ',negative
'fill host with tasks leave host empty try running both tasks host single preemption triggered followed allocation followed another preemption ',negative
'reduce sink row resolver used generate map join ',negative
'theres some access get acls assume means free for all ',negative
'cleanup ',negative
'this expected noop will return null when use local metastore ',negative
'introduce project rel above original leftright inputs cast ',negative
'with data unsupported for the test case ',negative
'test that overflow produces null ',negative
'outer join specific members ',negative
'tracks vertices for which notifications have been registered ',negative
'descendants tasks who subscribe feeds from this task ',negative
'dont fail the query just return null caller should skip cache lookup ',negative
'consider validation type information ',negative
'for unit testing harm hardcoding allocator ceiling longmaxvalue ',negative
'create parquet file with specific data ',negative
'get writable object ',negative
'loginfoencrypted shuffle enabled sslfactory new conf sslfactoryinit ',negative
'cache ',negative
'are now positioned the end this fields bytes ',negative
'clean ',negative
'the exception has been logged the lower layer ',negative
'try construct object ',negative
'nonjavadoc see javaioinputstream ',negative
'what the index the row beyond the set rows that match this pattern ',negative
'array level array level ',negative
'minihs cluster let run until someone kills the test ',negative
'type ',negative
'check all field inspectors are initialized ',negative
'larger scale will knock off lower digits and round ',negative
'context handler ',negative
'ignore the first child since the variable ',negative
'demuxoperator and directly connects childop will add this muxoperator between demuxoperator and childop ',negative
'add partition key ensure its reflected the schema ',negative
'framework expects mapwork instances that have physical parents union parent fine broadcast parent isnt ',negative
'getxxx returns for numeric types false for boolean and null for other ',negative
'with that mind determine disk ranges readget from cache not stream ',negative
'guard against poor configuration noconditional task size let hash table grow till memory available for containerexecutor ',negative
'this processing only needs happen for the small tables ',negative
'verify that the created table identical sourcetable ',negative
'required string destinputname ',negative
'row the column name ',negative
'round fractional must not allowed throw away digits ',negative
'direct heap allocations need safer ',negative
'intialize the tables ',negative
'null all tablesviews are returned ',negative
'passed the test load ',negative
'happen the normal course business need log them errors all the time ',negative
'instance fields ',negative
'not cast ',negative
'are done ',negative
'authtype ',negative
'aggregate above sum the subtotals ',negative
'oldpath subdir destf but could not cleaned ',negative
'can always equality predicate just need make sure get appropriate representation constant filter condition can other comparisons only storage format hbase either binary are dealing with string types since there lexicographic ordering will suffice ',negative
'from the value arrays and our isrepeated selected isnull arrays generate the batch ',negative
'mixed sign cases ',negative
'must honored ',negative
'make sure the parent directory exists not error recreate existing directory ',negative
'scope openedclosed once multiple opens not count ',negative
'for now dont take any pool action future might restore the session based this and get rid the logic outside the pool that replacesreopensetc ',negative
'call the executor which will execute the appropriate command based the parsed options ',negative
'order expressions which will only get set and used for range windowing type ',negative
'phase verify get the expected objects created all threads ',negative
'check whether output works when merge the operators will collide work work merge work work work cannot merge the reason that tez currently does not support parallel edges multiple edges from same work ',negative
'stores big table rows bytes for native vector map join ',negative
'forward compatible ',negative
'try outer row resolver ',negative
'check value argument ',negative
'nonjavadoc see int javalangstring ',negative
'test setting fetch size above max ',negative
'test that read blocks exclusive ',negative
'not authorize dummy readentity writeentity ',negative
'are the leftmost child ',negative
'divide tenscale check for rounding ',negative
'definitely not int ',negative
'from tableacidtbl where select from tablenonacidorctbl ',negative
'using utility method above that doesnt have used here this helps avoid adding jdo dependency for hcatalog client uses ',negative
'try regular properties ',negative
'this because the pending events will sent via the succeededfailed messages taskdone set before are sent out which what causes the thread exit ',negative
'read notification from metastore ',negative
'required string vertexname ',negative
'roundpower fastscale ',negative
'noone can take this buffer out and thus change the level after lock and they take out before lock then will fail lock same ',negative
'defer allocation until really need since the common case there crlf substitution ',negative
'concern isrepeating ',negative
'getpartitions will parse out the catalog and names itself ',negative
'super set the grouping columns are abc and the sorting columns are grouping columns sorting columns grouping columns are superset sorting columns similarly means subset intersection between sort columns and bucketcols sort cols group cols partial match group cols sort cols match group cols sort cols partial match bucketcols sortcols bucket columns either same prefix sort columns sort cols group cols complete match group cols sort cols match group cols sort cols complete match group cols bucketcols partial match otherwise bucketcols sortcols bucket columns superset sorting columns group cols sort cols partial match group cols sort cols match one exception this rule groupbycols sortcols and all bucketing columns are part sorting columns any order complete match ',negative
'see planindexreading only read nonrowindex streams involved sargs ',negative
'the partition was dropped before got around cleaning ',negative
'clean ',negative
'for multistatement txns you may have multiple events for the same row the same current transaction want collapse these just the last one regardless whether are minor compacting consider insertupdateupdate the same row the same txn there benefit passing along anything except the last event did want pass along wed have include statementid the row returned that compaction could write out make minor minor compaction understand how write out delta files deltaxxxyyystid format there doesnt seem any value this todo this could simplified since acid even you update the same row times txn will have different rowids there such thing multiple versions the same physical row leave for now since this acid reader should away altogether and will used ',negative
'show cannot create child the directory with permissions ',negative
'serialization methods ',negative
'defined with skewed columns and skewed values metadata ',negative
'for hive the only thing missing the last stripe index information get the information from the last stripe and append the existing index the actual stripe data can written asis similar ',negative
'tests the beeline behaves like default mode there userspecific connection configuration file ',negative
'note dont take the scheduling lock here although the call queue still ',negative
'schema retriever has been provided well attempt read the write schema from the retriever ',negative
'equal groups return what can handle ',negative
'keep buckets from the streaming relation ',negative
'wait for startup complete ',negative
'alter testdatabaseops via objectstore ',negative
'get list aliases for the same column ',negative
'wont try too strict checking this because were comparing table create intents with observed tables created does have location though will compare with external tables ',negative
'modify below part imposing view column names ',negative
'only tablename pattern and ',negative
'will put fork the plan the source the reduce sink ',negative
'standard overrides methods ',negative
'move lockstep one entry added each the same time ',negative
'this likely indicates that instance has recently restarted ',negative
'',negative
'floatcompare treats and different ',negative
'the types array tells the number columns the data ',negative
'get partition column stats for this table ',negative
'one them null replace the old params with the new one ',negative
'map keys are required primitive and may serialized binary format ',negative
'skip this column ',negative
'create separate thread send the events ',negative
'this the genericudaf name ',negative
'when column name specified describe table ddl colpath will will tablenamecolumnname ',negative
'test that really the maximum value the ',negative
'nonjavadoc see ',negative
'order matters assuming later that timegranularity comes first then druidpartitionkey ',negative
'read the contents and make sure they match ',negative
'fraction digit continue into highest longword ',negative
'table dropped when dump progress just skip partitions dump ',negative
'find the appropriate storage descriptor ',negative
'will bail out not want end with limits all over the tree ',negative
'this should probably never happen see ',negative
'runctx and ctx share the configuration but not isexplainplan ',negative
'outpath will nonunion case union case ',negative
'round the next ',negative
'this should work comments are stripped hivecli ',negative
'location set null here can overwritten the import stmt ',negative
'lets see can one step further and just uber this puppy ',negative
'create and delete temp file ',negative
'case theres delay for the heartbeat txn should able commit ',negative
'finalization ',negative
'principal specified authorize ',negative
'original bucket files and delta directory should stay until cleaner kicks ',negative
'use util function aggr stats ',negative
'now correct error and rounding ',negative
'store the new joincontext ',negative
'construct list bucketing location mappings from subdirectory name ',negative
'setup uncaught exception handler for the current thread ',negative
'changing the tags ',negative
'for partitioned tables get the size all the partitions after pruning ',negative
'the reason that set the txn manager for the cxt here because each query has its own ctx object the txn mgr shared across the ',negative
'this the one are expecting ',negative
'not visited yet ',negative
'',negative
'mock operation and ophandle for operationmanager ',negative
'confirm that the function now gone ',negative
'most tests are done with ansi sql mode enabled set back true ',negative
'since hivemetastoreclient not threadsafe hive clients are not shared across threads thread local variable containing each threads unique used one the keys for the cache ',negative
'update min min lesser than the smallest value seen far ',negative
'this rather convoluted simplify for perf could call getrawkeyvalue instead writable and serialize based java type opposed ',negative
'used datumreader applications should not call ',negative
'materialized view not heuristic approach normal costing ',negative
'update the work name which referred operators following works ',negative
'skew the table take care ',negative
'try with ascii chars ',negative
'recall setup that get object store with the metrics initalized ',negative
'note must handle downgradedtask after this the end outside the lock ',negative
'note that this not typical list accumulator theres call finalize the last list instead add list first well locally add elements ',negative
'deserialize the ondisk hash table ',negative
'buildup the expr ast ',negative
'where mapreduce not present ',negative
'localize the nonconf resources that are missing from the current list ',negative
'the result decimal precision ',negative
'insertdata ',negative
'not sure how get around that ',negative
'ignore the first static partition ',negative
'keep copy hiveconf session conf changes may need get new hms client ',negative
'get number partitions ',negative
'descending ',negative
'clean request ',negative
'trim line ',negative
'nonjavadoc see int int ',negative
'generates sequence list indexes ',negative
'check random one these can change whive versions ',negative
'string entirely whitespace need apply trailingspaces ',negative
'',negative
'the above members are initialized the constructor and must not transient ',negative
'generate sqloperator for merging the aggregations ',negative
'update the sql string with parameters set setxxx methods link preparedstatement param sql param parameters return updated sql string throws sqlexception ',negative
'fill the column vector with the provided value ',negative
'store list have consistent order between gettests and the test argument generation ',negative
'',negative
'process help ',negative
'init file ',negative
'fill host with tasks leave host empty ',negative
'actually temp table does not support partitions cascade not applicable here ',negative
'drop tables before dropping ',negative
'fetchtype ',negative
'where the first event refers source path and second event refers path ',negative
'generate spark plan ',negative
'dont allow query that returns everything will blow stuff ',negative
'check the configure job properties called from input output for setting asymmetric properties ',negative
'there should one exception message per file ',negative
'after the stats phase might have some cyclic dependencies that need ',negative
'insert search the beginning would have failed these parents didnt exist ',negative
'columnexprmap has the reverse what need mapping the internal column names the exprnodedesc from the previous operation find the keyvalue where the exprnodedesc value matches the column are searching for ',negative
'conflict updates else update deletes from insert and new part ',negative
'only valid true ',negative
'for base type nothing other types like structs may initialize internal data structures ',negative
'value found ',negative
'every thread created this thread pool will use the same handler ',negative
'time part ',negative
'test ',negative
'check are llap needs determined should use bmj dphj ',negative
'names ',negative
'inlined ',negative
'skip all the insert events ',negative
'serdename ',negative
'kryo docs say are taken strange things happen you dont set when registering classes ',negative
'create map join tasks ',negative
'username and password are added the http request header ',negative
'this writer will created when writing the first row order get information about how inspect the record data ',negative
'constructors ',negative
'lastly should dedupednondistirefs ',negative
'the dispatcher finds smb and there semijoin optimization before removes ',negative
'generates initial key ',negative
'write uri locking done for this ',negative
'production ',negative
'populate the cache ',negative
'update the keys use operator name ',negative
'could start move its being evicted but lets not for now ',negative
'test that two shared read locks can share partition ',negative
'hive job credstore location not set but hadoop credential provider set jobconf should contain hadoop credstore location and password should from ',negative
'over all the tasks and dump out the plans ',negative
'round double decimal places ',negative
'store the row see comments above for why need new copy the row ',negative
'group name counter name outputrecords ',negative
'over the list and find reducer not needed ',negative
'array hash map results can lookups the whole batch before output result ',negative
'finally add the files the existing any the old code seems this twice first for all the new resources regardless type and then for all the session resources that are not type file see branch calls from updatesession and with resourcemap from submit ',negative
'add proj top ',negative
'figure out table level partition level ',negative
'read the record with existing record reader and same evolved schema ',negative
'user did not specfied alias names infer names from outputoi ',negative
'its standard map reduce task check what anything inferred about ',negative
'returns true long there more data mockresultdata array ',negative
'generate reducesink operator ',negative
'this data structure needed create the new project ',negative
'arithmetic with type date longcolumnvector storing epoch days and type intervalyearmonth longcolumnvector storing ',negative
'report progress for each stderr line but more frequently than once per minute ',negative
'setboolean setboolean setshort setint setfloat setdouble setstring setlong setbyte setbyte setstring settimestamp ',negative
'rows should returned ',negative
'run other optimizations that not need stats ',negative
'confirm default managed table paths ',negative
'reenable the node task completed due preemption capacity has become available and may have been able communicate with the node ',negative
'assign simplify hashcode ',negative
'need sort tuples based the value some their columns ',negative
'unescape the partition name ',negative
'local scratch dir ',negative
'cancel ',negative
'byteval ',negative
'all output columns used for bucketingsorting the destination table should belong the same input table insert overwrite table select tkey tkey udftvalue tvalue from join tkey tkey and tkey tkey not optimized whereas the insert optimized the select list either changed tkey tkey udftvalue tvalue tkey tkey udftvalue tvalue ',negative
'they both require dbtxnmanager and both need recordvalidtxns when acquiring locks driver ',negative
'mapping from expression node expression containing only partition virtual column constants ',negative
'bit set element was already cache should have been replaced ',negative
'add some multibyte characters test length routine later ',negative
'spark has its own config for merging ',negative
'generate pruned column list for all relevant operators ',negative
'for replicating hcatpartition definition ',negative
'operator native ',negative
'generate reducesinkoperator ',negative
'from ',negative
'set the conf variable values for this test ',negative
'blank byte cyrillic capital dje bytes ',negative
'operation handle guid string ',negative
'infrastructure place ',negative
'nodemap registration not required since theres taskid association ',negative
'has open txn ',negative
'set the operation handle information driver that thrift api users can use the operation handle they receive lookup query information ',negative
'first branch query second branch ',negative
'any operator which does not allow mapside conversion present the mapper dont convert into conditional task ',negative
'this configuration used only for client side configuration ',negative
'the basic premise here that will rsync the directory first working drone then execute local rsync the node the other drones this keeps from executing tons rsyncs the master node conserving cpu ',negative
'date comparisons ',negative
'create directory ',negative
'remove the last ',negative
'',negative
'its list join keys ',negative
'check what happens when ignore these errors ',negative
'connection secret show the child processs command line ',negative
'else ptn already exists but nothing with ',negative
'consdier need type name parser for typedescription ',negative
'lru cache using linked hash map ',negative
'message kept around for debugging ',negative
'objtorefresh ',negative
'lengths stream could empty stream already reached end stream before present stream ',negative
'the catalog name isnt set need through and set ',negative
'may have already created the tables and thus dont need redo ',negative
'result jobcallable task after successful task completion this expected set the thread which executes jobcallable task ',negative
'failure happens here the intermediate archive files wont ',negative
'make sure the map does not expand should able find space ',negative
'all the insert update and delete tests assume two tables and each with columns and partitioned additional column these are created parseandanalyze and removed cleanuptables ',negative
'remove the tasks above without state checks just reset all metrics ',negative
'the outputtype float cast the arguments float replicate the overflow behavior nonvectorized udf genericudfposmod ',negative
'for cardinality values use numrows default try use colstats available ',negative
'singlecolumn long hash table import ',negative
'extract the real user from the given token string ',negative
'allows for unescaped ascii control characters json values ',negative
'the same without specifying writer time zone this tests deserialization older records ',negative
'skip over any leading xffs ',negative
'skip the last partitioning column since always nonnull ',negative
'make sure put the instance back again case was removed part ',negative
'rank the tables ',negative
'cars ',negative
'used for union all copartitionedge partitionedge ',negative
'original transaction ',negative
'hive pending rename before ',negative
'setup stats the operator plan ',negative
'capture arguments authorizer impl call and verify addresses passed ',negative
'update the nextwriteid for the given table after incrementing number write ids allocated ',negative
'nonvectorized validates that explicitly during udf init ',negative
'unset distribution keys fixed can change reducer count order can concat adjacent buckets can redistribute into buckets uniformly group can not wait for downstream tasks ',negative
'set our current entry null since its done and try again ',negative
'resourceplanname ',negative
'add hacks for wellknown collections and maps avoid estimating them ',negative
'read dpp outputs ',negative
'minopenwriteid ',negative
'case describe formatted tablename columnname statement colpath will contain tablenamecolumnname columnname not specified colpath will equal tablename this how can differentiate are describing table column ',negative
'detect there are attributes join key ',negative
'this happens you change bucketcount ',negative
'for now totalresource taskresource for llap ',negative
'will come here exception was thrown map reduce hadoop always call close even exception was thrown map reduce ',negative
'millisperday ',negative
'autogenerate column aliases ',negative
'compressed ',negative
'recurse ',negative
'compaction preserves location rows wrt bucketstranches for now ',negative
'the ndv the side dont match and the side filter the key column then scale the ndv the side described peter boncz such cases can off large margin the join cardinality estimate the provides the join storesales and datedim the tpcds dataset since the datedim populated for years into the future while the storesales only has years worth data there are times fewer distinct dates storesales general hard infer the range for the foreign key arbitrary expression for the ndv for dayofweek the same irrespective ndv the number unique days whereas the ndv quarters has the same ratio the ndv the keys but for expressions that apply only columns that have the same ndv the key implying that they are alternate keys can apply the ratio the case storesales datedim joins for predicate the ddate column can apply the scaling factor ',negative
'change the plan this structure note that the aggregaterel removed projecta replace corvar input ref from the join join replace corvar input ref from leftinputrel leftinputrel filterinputrel ',negative
'serves lock for itself ',negative
'remote spark context property ',negative
'this reduce sink has been processed already the work for the parentrs exists ',negative
'order matters all these block ',negative
'handles nulls items ',negative
'return hadoopnative available otherwise construct our own copy its logic ',negative
'set start with this will decremented for all columns for which events are generated this source which eventually used determine number expected ',negative
'determines whether the current node was actually closed and pushed this should only called the final user action node scope ',negative
'infer foreign key candidates positions ',negative
'generate the second groupbyoperator for the group plan parseinfogetxxxdest the new groupbyoperator will the second aggregation based the partial aggregation results param mode the mode aggregation final param the mapping from aggregation stringtree the return the new groupbyoperator throws semanticexception ',negative
'java primitive class ',negative
'signature xxx ',negative
'couldnt convince you otherwise well then lets llap ',negative
'best effort ',negative
'local scratch dir ',negative
'only for incremental load need validate event newer than the database ',negative
'partkey constant can check whether the partitions have been already filtered ',negative
'try query stats for column for which stats doesnt exist ',negative
'the the size because the main work ',negative
'copy conf file ',negative
'mydouble ',negative
'get the bucketing version ',negative
'never locked for eviction java object ',negative
'expected exception will occur the partitions have custom location ',negative
'verify that the two are equal ',negative
'initialize aux data structures ',negative
'determine type udaf ',negative
'make easy write unit tests instead unique generation however this does mean that writing tests have aware that repl dump will clash with prior dumps and thus have clean properly ',negative
'grouping sets this point ',negative
'the replacement changed make sure redo tostring again ',negative
'store the total memory that this mapjoin going use which calculated totalsizebuckets with totalsize ',negative
'failure hooks are run after hivestatement closed wait sometime for failure hook execute ',negative
'tries optimize from clause multiinsert attempt optimize insert clauses the query returns true rewriting successful false otherwise ',negative
'have reached new key group ',negative
'write delta ',negative
'retry with same dump with which was already loaded should resume the bootstrap load ',negative
'descending null last default for descending order ',negative
'root must exist already ',negative
'total running ',negative
'this enables vectorization rowid ',negative
'invariant reducerhash null ',negative
'check incompatible versions ',negative
'nonjavadoc see ',negative
'depending the server setup ',negative
'need update the mapcorvartocorrel update the output position for the cor vars only pass the cor vars that are not used ',negative
'char between ',negative
'setup the actual ports the configuration ',negative
'now disregard null second pass ',negative
'link the list record the first element ',negative
'dummy final assignments ',negative
'schema that exists the avro data file ',negative
'remove the current active server ',negative
'create test database and base tables once for all the test ',negative
'nonempty java opts with xmx specified ',negative
'fetch the partition referred the message and compare ',negative
'perform dynamic partition pruning ',negative
'and restore the state before walking each child ',negative
'todo create immutable copies all maps ',negative
'careful with the range here now not want read the whole base file like deltas ',negative
'records whether delta dir type deletedeltaxy ',negative
'processing files ',negative
'note the following section metadataonly import handling logic interleaved with regular replimport logic the rule thumb being followed here that mdonly imports are essentially alters they not load data and should not creating any metadata they should replacing instead the only place makes sense for mdonly import create the case table thats been dropped and recreated the case unpartitioned table all other cases should behave like noop pure alter ',negative
'for tez dont use appid distinguish the tokens ',negative
'theres small number buffers and they all live the heap ',negative
'did not kill this session expect everything present ',negative
'make copy that not modify hookcontext conf ',negative
'generate the map join operator already checked the map join ',negative
'stores negative values count columns eventually set tasks columns after the source vertex completes ',negative
'end reldecorrelatorjava ',negative
'get the tablesviews for the desired pattern populate the output stream ',negative
'task startup ',negative
'right border the max ',negative
'otherwise this not sampling predicate need process ',negative
'should detect double ',negative
'fail try access offset out bounds ',negative
'llap dynamic value registry might already cached ',negative
'via the environment context ',negative
'theres rpc waiting for reply exception was most probably caught while processing the rpc send error ',negative
'ranges use the ranges from the child ',negative
'put something writeset ',negative
'datarelated configuration properties ',negative
'remove expression node desc and all children from mapping ',negative
'the generic sqlstate for syntax error ',negative
'determine all digits below power zero ',negative
'now populate structure use apply delete events ',negative
'testing for equality doubles after math operation not always reliable use this tolerance ',negative
'all must selected otherwise size would zero repeating property will not change ',negative
'very simple counter keep track number rows processed the reducer dumps every million times and quickly before that ',negative
'how many records the writer buffers before writes disk ',negative
'clear all the reading variables ',negative
'were going update the average variable row size sampling the current batch ',negative
'mark task failed due comm failure ',negative
'the lock manager still null then means arent using ',negative
'this count transform countnullindicator the null indicator located the end ',negative
'for this one dont specify location make sure gets put the catalog directory ',negative
'comletor add space after last delimeter ',negative
'end synchronized ',negative
'recursively the first phase semantic analysis for the subquery ',negative
'flushed ',negative
'custom dynamic location provided need rename final output path ',negative
'check that the table partition isnt sorted dont yet support that ',negative
'get partitionpath for gridxyz place the partition outside the tablepath ',negative
'output strips off the partition columns and retains other columns ',negative
'rewrite all insert references all the node values for this key ',negative
'note compaction creates delta wont replace existing base dir the txn the base dir wont part deltas range otoh compaction creates base dont care about this value because bases dont have min txn the name however logically this should also take base into account its included ',negative
'the cookie secured where the client connect does not use ssl ',negative
'ahead with the estimation ',negative
'below value for bucket for txn logically ',negative
'replace any occurrence dummyvectoroperator with our tablescanoperator ',negative
'see heapifydown comment ',negative
'dont exit the loop early because want extract the error code corresponding the bottommost error coded exception ',negative
'parsecontext ',negative
'catchall rule when none the others apply ',negative
'register tasks nodes with dependency vertex completing ',negative
'setup the map work for this thread pruning modified the work instance potentially remove partitions the same work instance must used when generating splits ',negative
'perform repl ',negative
'here are some positive cases which can executed below ',negative
'here means concrurrent acquirelock inserted the key ',negative
'input name position map ',negative
'true here indicates that sqcountcheck for innot subqueries ',negative
'the third one has the base file shouldnt combined but could base ',negative
'read field that under complex type may primitive type deeper complex type ',negative
'add replstatelogtask only pending table load tasks left for next cycle ',negative
'nonjavadoc see javasqltime ',negative
'are making new output vectorized row batch ',negative
'but have nulls initially ',negative
'for caching tablewrapper objects key aggregate database name and table name ',negative
'the outdated see whether should consider for rewriting not ',negative
'cctxnextoperatorid ',negative
'check srcf contains nested subdirectories ',negative
'put compaction request the queue ',negative
'this should not null because were allowed bind with this username safe check case were able bind anonymously ',negative
'otherwise wouldnt here ',negative
'any aggregate call has filter bail out ',negative
'schema diff return false ',negative
'control characeters according json rfc ',negative
'files size for splits ',negative
'access ',negative
'this will guarantee file name uniqueness ',negative
'must sync with toperationstate order ',negative
'note the vectorptfdesc has already been allocated and populated ',negative
'propagate new conf meta store ',negative
'release buffers are done with all the streams also see torelease comment ',negative
'for local file and hdfs key and value are same ',negative
'this may invoked before container ever assigned task allocatetask app decides ',negative
'try with regular decimal output type ',negative
'disabled hive disabled hive disabled hive disabled hive disabled hive disabled hive disabled hive ',negative
'myenumstructlistmap ',negative
'private static properties props ',negative
'partitioned tables dont have tabledesc set the fetchtask instead they have list partitiondesc objects each with table desc lets try fetch the desc for the first partition and use its deserializer ',negative
'construct using ',negative
'partial aggregation performed the mapper distinct processing the reducer ',negative
'taking precedence the case partitioned tables ',negative
'with nulls ',negative
'get the escape information ',negative
'note this writes into scratch buffer within hivedecimalwritable ',negative
'the root interface for vector map join hash map ',negative
'nonempty java opts with xmx specified ',negative
'the correlate ',negative
'dummy group ',negative
'add the finalop after the union ',negative
'its hard tell wsome code paths like udfsois etc that are used many places ',negative
'ideally hadoop should let know whether map execution failed not ',negative
'for lazystruct ',negative
'delim does not exist str ',negative
'neginfinity end exclusive ',negative
'must escaped binarysortable ',negative
'note this readlock prevent race with querycomplete operations and mutations within this lock need concurrent structures ',negative
'for the char and varchar data types the maximum character length the column otherwise ',negative
'this semaphore provides two functions forces cap the number outstanding async writes channel ensures that channel isnt closed there are any outstanding async writes ',negative
'this subdir under the hdfssessionpath will removed along with that dir ',negative
'statsobj ',negative
'compare hosts ',negative
'get stats for the metastore does not break see hive for motivation ',negative
'set the necessary accumulo information ',negative
'test reverse order ',negative
'insertonly tables dont have conform acid requirement like orc bucketing ',negative
'hive more restrictive hivepig works ',negative
'for the argument ',negative
'finally create session paths for this session local nonlocal tmp location configurable however the same across ',negative
'walk through the task graph and invoke ',negative
'dont want acquire read locks during update delete well acquiring write locks instead also theres need lock temp tables since theyre session wide ',negative
'get the input format ',negative
'perform reconnect with the proper user context ',negative
'generator ',negative
'test multithreaded implementation checker find out missing partitions ',negative
'this can happen for view index ',negative
'shuffle needed for union only hashpartition shuffle keys are not sorted any way rangepartition shuffle keys are total sorted hashpartition shuffle keys are sorted partition ',negative
'since have result for all rows dont need conditional null maintenance turn off nonulls ',negative
'reader count already incremented during cache lookup ',negative
'this not set default value set during config initialization default value cant set this constructor would refer names other confvars ',negative
'add the new readentity that were added readentitymap planutilsaddinput ',negative
'functionality remove semijoin optimization ',negative
'this method only returns the result when activating resource plan could also add boolean flag specified the caller see when the result might needed ',negative
'generates the plan from the operator tree ',negative
'abcabc ',negative
'used for ',negative
'extract only split hdfs ',negative
'logerrorgot ',negative
'check dbtablepartition doesnt have replsourcefor props also ensure ckpt property ',negative
'make sure the jar containing the custom compositerowid included the mapreduce jobs classpath libjars ',negative
'throw timeoutexception caller ',negative
'the queue esp conjunction with and rerun them ',negative
'verify when first argument boolean flags repeating ',negative
'tables ',negative
'pass flag hide prefixes ',negative
'evaluate the key expressions once ',negative
'noarg constructor make kyro happy ',negative
'boolean ',negative
'',negative
'put back and one additional table ',negative
'estimate the number hash table entries based the size each entry since the size entry ',negative
'use construct ',negative
'rundroppartitions the main function that gets called with different options partcount total number partitions that will deleted batchsize maximum number partitions that can deleted batch based the above the test will check that the batch sizes are expected exceptionstatus can take values noexception exception expected oneexception first call throws exception since will retry this will succeed after the first failure allexception failure case where everything fails will test that the test fails after retrying based maxretries when specified based decaying factor ',negative
'use case ',negative
'this will trigger new calls metastore collect metadata ',negative
'not required ',negative
'try find the default postfix dont check two last components least there should table and file could also try throw away partitionbucketacid stuff ',negative
'weve encountered new key must save current one cant forward yet the aggregators have not been evaluated ',negative
'for null just write out the type ',negative
'make sure get exceptions strategies might have thrown ',negative
'used dummy root operator attach vectorized operators that will built parallel the current nonvectorized operator tree ',negative
'should use for columnnamedelimiter column name contains comma but should also take care the backward compatibility ',negative
'for now old class ',negative
'allowing this increased via config breaks the merge impl per vector smaller ',negative
'fix the query for materialization rebuild ',negative
'',negative
'bail out encountered ',negative
'distinct aggregate rewrite ',negative
'add constant size for unions tags ',negative
'also minhistorylevel wont have any entries reference for open txns ',negative
'wont write the set expressions the rewritten query well patch that later the set list from update should the second child index ',negative
'runtime constants deterministic functions can folded ',negative
'during repl load foreign key shall ignore the foreign table may not part the replication ',negative
'end tests that check values from pig that are out range for target column ',negative
'process listhapeers ',negative
'required required required required required ',negative
'log with int input and double base ',negative
'resfile pctx roottasks fetchtask analyzer explainconfig cboinfo ',negative
'vectorized row batch reader ',negative
'subclasses must override this with function that implements the desired logic ',negative
'write information about the old value which becomes our next the beginning our new value ',negative
'multifile load for dynamic partitions when some partitions not need merge and they can simply moved the target directory ',negative
'dont validate column count encodings for vectors ',negative
'case spark the credential provider location provided the jobconf when the job submitted ',negative
'this checked ddlsemanticanalyzer ',negative
'for wdw specs that refer window defns inherit missing components ',negative
'row resolvers for input output ',negative
'lookup byte array key the hash multiset param keybytes byte array containing the key within range param keystart the offset the beginning the key param keylength the length the key param hashmultisetresult the object receive small table values information match for spill has information where spill the big table row return whether the lookup was match match spilled the partition with the key currently spilled ',negative
'the previous rules can pull projections through join operators ',negative
'create dummy partitions ',negative
'with extra structs ',negative
'call redundant cast then bail out casttrueboolean ',negative
'need remember the input object inspector that need know the input type ',negative
'negative range bigger than positive range there risk overflow here ',negative
'need remove those branches that ended with reducesinkoperator and the reducesinkoperators name not the same childreducername also the cloned work not the first remove all leaf operators except ',negative
'add original files obsolete list any ',negative
'delim chars ',negative
'parallelism shouldnt set for cartesian product vertex ',negative
'comparison ',negative
'already have the merge work corresponding this merge join operator ',negative
'the else expression either identityexpression column ',negative
'bigint ',negative
'would nice there was way determine quotes are needed ',negative
'debatable this correct but thats how its implemented ',negative
'have prefix with wildcard ',negative
'replace right key input ref ',negative
'vectorization ',negative
'scheduling run will happen which may may not pick this task the test ',negative
'using system classloader the parent using thread context ',negative
'use boundarytype boundaryamt sort key order behavior case current row any any scan forward until row such that rsk rsk end ridx following unb any any end partitionsize ',negative
'nonjavadoc see javaioinputstream long ',negative
'test the idempotent behavior open and commit txn ',negative
'stats aggregator not present clear the current aggregator stats for merge being performed stats already collected aggregator numrows etc are still valid however load file being performed the old stats collected aggregator are not valid might good idea clear them instead leaving wrong and old stats since hive maintain the old stats although may wrong for cbo purpose use flag columnstatsaccurate show the accuracy the stats ',negative
'local file system using pfile link ',negative
'nonjavadoc see javaioreader ',negative
'all the pending get requests should just requeued elsewhere note that never queue session reuse sessiontoreuse would null ',negative
'partitionname ',negative
'get the string representing action type its non default action type ',negative
'but kept unchanged throughout the operator tree for one row ',negative
'verification ',negative
'initialize stats publishing table for noscan which has only stats task the rest task following stats task initializes execdriverjava ',negative
'anchor the pattern the startend the whole string ',negative
'list need refactored out done only once ',negative
'shortcut length means fields ',negative
'get big table ',negative
'all checks passed return null ',negative
'dpp check actually refers same target column etc ',negative
'replace the output expression with the input expression that ',negative
'bitset array ',negative
'replace the synthetic predicate with true and bail out ',negative
'for serialization only ',negative
'make the conditional task the child the current leaf task ',negative
'unfortunately quick path lets scale updown ',negative
'pkfq relationship ndv selcolsourcestat superset what tscolstat ',negative
'not outer join the postcondition filters are empty the row passed them ',negative
'actually temp table does not support partitions cascade not applicable here ',negative
'precondition should always directory ',negative
'flag indicate these counters are subject change across different test runs ',negative
'numrecords fetch all records hence skip all the below checks when numrecords ',negative
'catchall due some exec time dependencies session state that would cause otherwise ',negative
'write the items the output stream ',negative
'test using the same cache where first rows are inserted then cache cleared next reuse the same cache and insert another rows and verify the cache stores correctly this simulates reusing the same cache over and over again ',negative
'the output partial aggregation list doubles representing the histogram being constructed the first element the list the userspecified number bins the histogram and the histogram itself represented pairs following the first element the list length should always odd ',negative
'revert default keystore path ',negative
'create partitions for the partitioned table ',negative
'min allow tez pick ',negative
'have data all for this part the stream could unneeded skip ',negative
'global contains includes individual modules can only contain additional includes ',negative
'need provide the minimum number columns read separator parser does not waste time ',negative
'since are closing the previous fsps record writers need see can get ',negative
'task currently running ',negative
'list and expressions that need distribute ',negative
'verify null output data entry not but rather the value specified design ',negative
'this class ',negative
'string length should work after enforcemaxlength ',negative
'safety check for get parentop although very unlikely that stack size less than there only one mergejoinoperator the stack ',negative
'the incoming vectorization context describes the input big table vectorized row batch ',negative
'export metadata delta bucket ',negative
'one more major compaction ',negative
'limit that for now ',negative
'column names lose the enumness this schema ',negative
'offset length ',negative
'get txn tables that are being written ',negative
'lookup byte array key the hash set param keybytes byte array containing the key within range param keystart the offset the beginning the key param keylength the length the key param hashsetresult the object receive small table values information match for spill has information where spill the big table row return whether the lookup was match match spilled the partition with the key currently spilled ',negative
'point for linear scan has been identified which point this value unset ',negative
'extrapolation not needed for columns noextracolumnnames ',negative
'config settings ',negative
'update the byte size the struct ',negative
'try merge rest operators ',negative
'this test requires disruptor jar classpath ',negative
'map join operator default has bucket cols and num reduce sinks ',negative
'type intervaldaytime storing nanosecond interval primitives ',negative
'matches only forwardoperators which are preceded some other operator the tree particular cant reducer and hence cannot one the forwardoperators ',negative
'only set the default catalog the client ',negative
'required required optional optional ',negative
'this the same the setchildren method below but for empty tables ',negative
'test that attempting unlock locks associated with transaction generates error ',negative
'move the clock forward and trigger run ',negative
'there are more than one skewed values ',negative
'run the cleaner thread until cache cleanuntil occupied ',negative
'copyfrom ',negative
'done ',negative
'contained within set msb ',negative
'update the stats which not require complete scan ',negative
'the bottom the call stack the front the array the elements are follows getstacktrace shouldskip caller test method ',negative
'input value serde needs array support different serde for different tags ',negative
'calculate filter propagation directions for each alias ',negative
'event ',negative
'tez will only localize file script operators the ',negative
'noop hmshander not needed this impl ',negative
'call listlocatedstatus mockmocktbl ',negative
'this test for llap command authz added hive which require access for pass ',negative
'set timestamp before moving cmroot can avoid race condition remove the file before setting ',negative
'key design point for repl dump not have any txns older than current txn which dump runs this needed ensure that repl dump doesnt copy any data files written any open txns mainly for streaming ingest case where one delta file shall have data from txns may also have data inconsistency the ongoing txns doesnt have corresponding openwrite events captured which means catchup incremental phase wont able replicate those txns the logic wait for configured amount time see all open txns current txn getting abortedcommitted not then forcefully abort those txns just like ',negative
'evict all results grouped with this index cannot any key further the batch evict key from this batch the keys grouped with cannot earlier that that key ',negative
'total entries valid fake ',negative
'custom vertex edge custom vertex and custom edge but single input custom vertex custom edge and multi input custom vertex custom edge multi input ',negative
'optimize the loops pulling special end cases and global decisions like isescaped out ',negative
'the correct result that the blank value not there ',negative
'this will hit theres large number mapids single request determined the cache size further which case disk again ',negative
'expressions macro table used when deserialize the query from calcite plan ',negative
'specialized class for doing vectorized map join that inner join multikey and only big table columns appear the join result hash multiset used ',negative
'not supposed compactable table ',negative
'inserted enforce bucketing sorting need remove since will not merge them single inserted enforce bucketingsorting will have bucketing column reduce sink key whereas inserted this optimization will have partition columns followed bucket number followed sort columns the reduce sink key since both key columns are not prefix subset will not merge them together resulting jobs ',negative
'temporarily ',negative
'nonjavadoc see ',negative
'have bytes data for this part for now ',negative
'create new source files with same filenames ',negative
'confirm the batch sizes were the three calls create partitions ',negative
'call listlocatedstatus mockmocktbl ',negative
'typeinfo ',negative
'comments ',negative
'test with txnabort ',negative
'expression does not have where clause there can common filter ',negative
'writer for producing row from input batch ',negative
'this map maintains the ptfinvocationspec for each ptf chain invocation this ',negative
'simulate unknown type ',negative
'check that files produced compaction still have the version marker ',negative
'are trying adding map joins handle skew keys and map join right now does not work with outer joins ',negative
'blah foo bar form ',negative
'after closing the path set null ',negative
'comma separated list work names used prefix ',negative
'create table and insert two file the same content ',negative
'only support changing the serde mapping and the state ',negative
'nonjavadoc see ',negative
'looks for the most encrypted table location may return null there are not tables encrypted are not part hdfs ',negative
'lru extreme frequency accesses should ignored only order matters ',negative
'this the first pptf the chain and there partition specified then assume the user wants include the entire input partition ',negative
'filters are using index which should match rows ',negative
'test case sensitivity ',negative
'done processing far ',negative
'are just relay send pause encoded data producer ',negative
'equivalent java type for the backing structure need recurse and build list ',negative
'bgenjjtree fieldrequiredness ',negative
'close with null reporter ',negative
'these flags are intended only for the bigkey map work ',negative
'can only push down stuff which appears part pure conjunction reject case etc ',negative
'invoking init method basehandler this way since adds the retry logic case transient failures init method ',negative
'only support countsumminmax for the single count distinct optimization ',negative
'for bucket join testing ',negative
'not completely accurate since oob heartbeats could out ',negative
'this get should succeed because its variance within past maxvariance ',negative
'convert dphj ',negative
'maintain list nonnull column ids ',negative
'reporter member variable the operator class ',negative
'week ',negative
'want end the binary search ',negative
'for each bucket file big table get the corresponding bucket file name the small table more than partition the big table the mapping for each partition ',negative
'find all leaf tasks and make the ddltask dependent task all them ',negative
'weve found and its already been marked acquired ',negative
'then definitely this will end zero ',negative
'create the map needed ',negative
'note interestingly this would exclude llap app jars that the session adds for llap case course doesnt matter because vertices run llap and have those jars and moreover anyway dont localize jars for the vertices llap but theory this still crappy code that assumes theres one and only app jar ',negative
'update error for some session that was actually already killed ',negative
'check all fields start with key value then unflatten adding additional level nested key and value structs example keyreducesinkkeyint keyreducesinkkey int valuecolint becomes ',negative
'return null since this will handled special case ',negative
'check the contents the first row ',negative
'filter any has filter expression ',negative
'was empty stream ',negative
'consider approximate map side parallelism table data size ',negative
'giving ',negative
'main entrypointname ',negative
'strings are interned and can thus compared like this ',negative
'and should the deletedelta directory ',negative
'return the current blocks length ',negative
'should have some ast ',negative
'binary join ',negative
'name ',negative
'try the first again would not combined and wed retain the old base less files ',negative
'the sargs are closely tied ',negative
'delete the table from the database ',negative
'the new partition should similar the original partition ',negative
'note above looks funny because seems like were instantiating static var and then nonstatic var the rule but the reason this required because rules are not allowed static but wind needing initialized from static context bcompat initialzed static context but this rule initialized before the tests run and will pick initialized value bcompat ',negative
'big table value expressions apply all matching and nonmatching rows ',negative
'this set copy the arguments objects avoid serializing ',negative
'check table only should not exist ',negative
'the result not null the buffer was evicted during the move ',negative
'insert reduceside ',negative
'case parenthesis ',negative
'that create empty bucket files when needed but see hive ',negative
'execute all implementation variations ',negative
'param partinfolist return the size the list partitions throws see ',negative
'null projection ',negative
'join selectivity ndv ',negative
'the operator groupby and are referencing the grouping ',negative
'fast pass worked all txns were asked heartbeat were open expected ',negative
'multiply the integer quotient back out can subtract from the original get ',negative
'nonjavadoc see ',negative
'group contains the columns needed need aggregate from children ',negative
'serialize the struct ',negative
'columnname column position map ',negative
'insert reduceside ',negative
'singlecolumn string specific declarations ',negative
'add primitive types ',negative
'',negative
'append filter tag ',negative
'generate sql stmts execute ',negative
'get some number ',negative
'connect any edges required for minmax pushdown ',negative
'uniform autoparallel maxed out ',negative
'only remove information column not key ',negative
'test that overflow produces null ',negative
'partitionnames ',negative
'constructs standard group plan there other subquery with the same group bydistinct keys there are aggregations representative query for the group and there group that representative query the data skewed the conf variable used control combining group bys into single reducer false ',negative
'this table needs converted ',negative
'collect name output columns which result function ',negative
'set backup task ',negative
'clienttool end the tasks have completed control back the tool ',negative
'logging configuration ',negative
'lower this for big key testing ',negative
'killthreads option has already done force shutdown need again ',negative
'nulls possible ',negative
'now trigger some needed optimization rules again ',negative
'test mode want the operation logs regardless the settings ',negative
'for har files ',negative
'create conf for nway ',negative
'test right input repeating ',negative
'sync start ',negative
'test failure user not set ',negative
'grantorname ',negative
'',negative
'add map all the referenced positions relative each input rel ',negative
'scale the raw data size split level based ratio split wrt file length ',negative
'sqlstate errorcode should set appropriate values ',negative
'just translate ',negative
'only right input repeating ',negative
'get the partitions for the table and populate the output ',negative
'create new sparktask for the specified sparkwork recursively compute ',negative
'see the path fsop that calls fsexists finalpath ',negative
'handle minimum integer case that doesnt have abs ',negative
'remembered the offset just after the key length the list record read the absolute offset the value ',negative
'the session cannot have been killed just now this happens after all the kills the current iteration would have cleared sessiontoreuse when killing this ',negative
'strings including single escaped characters ',negative
'remember the bad estimates for future reference ',negative
'the metric value must zeroed ',negative
'ctas ',negative
'forward conditional the survival the corresponding key currently indexes ',negative
'runs ',negative
'single row ',negative
'nonjavadoc see ',negative
'probably dont have jobconf here but can still try ',negative
'check that destination does not exist otherwise will overwriting data ',negative
'this run pre post execution hook writes message sessionstateerr causing cached cachingprintstream being used run failure hook will write what has been cached the cachingprintstream sessionstateout for verification ',negative
'push next ',negative
'update creation metadata ',negative
'overflow ',negative
'note not rename serviceacl hadoop generates hosts setting name from this resulting collision with existing and bizarre errors these are read hadoop ipc you should check the usage and naming conventions blocked string hardcoded hadoop and defaults are enforced elsewhere hive ',negative
'nontemp tables should use underlying client ',negative
'expressions subq that are joined the outer query ',negative
'link the update repl state task with dependency collection task ',negative
'sourcetask for not changed currently but that might changed various optimizers autoconvertjoin for example ',negative
'delete some data this will generate only delete deltas and insert deltas deletedelta ',negative
'now lost finishable state ',negative
'figure out how many tasks want for each bucket ',negative
'repeated string groupvertices ',negative
'char tests ',negative
'when deleterecordid currrecordidinbatch have move look the next record the batch but before that can shortcircuit and skip the entire batch itself checking the deleterecordid lastrecordinbatch ',negative
'for file sink operator change the directory name ',negative
'task for given input return empty list with index ',negative
'update tab ',negative
'renamepartition event partitioned table ',negative
'should not reach here ',negative
'nonjavadoc see int int ',negative
'check these belong the same task and work with withindagpriority ',negative
'make sure dont reference any old buffer ',negative
'copy the data over that the internal state text will set ',negative
'prevent infinite loop ',negative
'truncate ascii maximum length large ',negative
'avgcollen ',negative
'create list with variables that match some the regexes ',negative
'required required required optional required ',negative
'nonrepeating input column use any nonnull values for unassigned rows ',negative
'note this not the calling user but rather the user under which this session will ',negative
'note that output storage handlers never sees partition columns data schema ',negative
'after analyzeinternal hiveop get set query since are passing ast for select query reset ',negative
'retrieving can expensive and unnecessary only when required ',negative
'containers likely come soon ',negative
'for each directory add once ',negative
'find the jdbc driver ',negative
'for bucket columns all the columns match the parent put them the bucket cols else add empty list for sort columns keep the subset all the columns long order maintained ',negative
'multikey value hash map optimized for vector map join the key stored the provided bytes uninterpreted ',negative
'determine partition level privileges should checked for input tables ',negative
'null considered numeric type for arithmetic operators ',negative
'besteffort check cannot good check against caller thread since refcount could still someone else locked this used for asserts and logs ',negative
'find the jar local maven repo for testing ',negative
'shall never happen ',negative
'this case current task the root tasks add this new task into root tasks and remove the current task from root tasks ',negative
'any event there and name known then dump the start and end logs ',negative
'create list operator nodes start the walking ',negative
'now handle actual returns sessions may returned the pool may trigger expires ',negative
'each column the row ',negative
'connect the work correctly ',negative
'this being read because dependency view ',negative
'either have user functions metastore old version filter names locally ',negative
'first call resultsetnext should return true ',negative
'handle null with selected use ',negative
'insert the number elements plus trigger evictions ',negative
'lesser than both nexttxnidntxnnext and minminhistorylevel mhlminopentxnid ',negative
'builds and starts the mini dfs and mapreduce clusters ',negative
'cannot backtrack any the columns bail out ',negative
'fact were adding this table map table ',negative
'process these first that can instantiate sessionstate appropriately ',negative
'map string string lazymap allows emptystring style key null but not null style key null ',negative
'the table name might null are retrieving the constraint from the side ',negative
'try store the first key topnhashes arent active always forward ',negative
'remove them from current spark work ',negative
'even with tez some jobs are run disable the flag the conf that the backend runs fully ',negative
'references keys the hashtable the index hash the key collisions are resolved using open addressing with quadratic probing reference format offset into writebuffers state byte has list flag part hash used optimize probing offset tail offset the first record for the key the one containing the key not necessary store bits particular optimize probing fact when always store the hash not necessary but have nothing else with them todo actually could also use few bits store for each wed stop earlier read collision need profile real queries ',negative
'',negative
'scale ',negative
'writes data out series chunks the form chunk sizechunk byteschunk sizechunk bytes closing the output stream will send final length chunk which will indicate end input ',negative
'add temp table info current session ',negative
'check the existing entry contains the new ',negative
'query specific info ',negative
'add ctrlb delimiter between the fields this necessary because for structs case delimiter provided hive automatically adds ctrlb default delimiter between fields ',negative
'orc store rows inside root struct hive writes this way when populate column vectors skip over the root struct ',negative
'batch batch ',negative
'store the vertex name the operator pipeline ',negative
'this column doesnt come from any table ',negative
'note these references are with respect the output ',negative
'todo reduce the duplicated code ',negative
'get the last repl corresponding all insert events except rename ',negative
'should try tolerate corruption default ',negative
'alias alias alias all can selected but overriden biggest one alias ',negative
'write the current set valid write ids for the operated acid tables into the conf file ',negative
'partition the bucketing column ',negative
'',negative
'create the consumer encoded data will coordinate decoding cvbs ',negative
'nonjavadoc see javaioinputstream long ',negative
'nonjavadoc see ',negative
'comparisons using strings for event ids wrong this should numbers since lexical string comparison and numeric comparision differ this requires broader change where return the dump long and not string fixing this here for now was observed one the builds where compareto results failure the assertion below ',negative
'aint ',negative
'method deserialize allocwriteidmessage instance ',negative
'the current buffer contains multiple parts split ',negative
'avoid storing headers with data since expect binary size allocations ',negative
'make null safe check the job submission has gone through and job valid ',negative
'drivercontext could released the query and close processes same ',negative
'result object ',negative
'done processing the operator ',negative
'this place compatible with the shufflehandler requests from shuffleinputs arrive with job prefix ',negative
'this tablescanoperator could part semijoin optimization ',negative
'hdfs counters should relatively consistent across test runs when compared local file system counters ',negative
'after processing all the nonstreaming groups batches with evaluategroupbatch and isgroupresultnull false the aggregation result value based ',negative
'call the operator specific close routine ',negative
'set the data structures before any notifications come ',negative
'class hcatpartitionspec ',negative
'finds column name hcatschema not found returns null ',negative
'there are any leadlag functions this expression tree setup duplicate evaluator for the arg the llfuncdesc initialize using the inputinfo provided for this expr tree set the duplicate evaluator the lludf instance ',negative
'first process the current key ',negative
'generating the final row count relying the basic comparator evaluation methods ',negative
'put record into compactionqueue and nothing ',negative
'files size for splits ',negative
'otherwise need make sure that there subquery any level ',negative
'cleanup structures ',negative
'this operator the last operator summarize the noninlined ',negative
'sample path worker the sequence number which will retained until session timeout worker does not respond due communication interruptions will retain the same sequence number when returns back session timeout expires the node will deleted and new addition the same node restart will get next sequence number ',negative
'define schema ',negative
'nonjavadoc see ',negative
'use serialize the nonpartitioning columns the input row ',negative
'the logger ',negative
'create information for vector map operator the member onerootoperator has been set ',negative
'tables with various types ',negative
'for partitioned tables get the size all the partitions ',negative
'col col ',negative
'start the heartbeat after delay which shorter than the hivetxntimeout ',negative
'error code indicates proper shutdown ',negative
'all the getters try the metastore value name first not set try the hive value name ',negative
'the cluster running mode return null ',negative
'the number hive operations that are waiting enter the compile block ',negative
'benefit rows filtered from selectivity rows ',negative
'element for key byte hash table hashset ',negative
'',negative
'input fulltablename expected format dbnametablename ',negative
'grantor type for public role hence the null check ',negative
'get the largest table alias from order ',negative
'check only when its terminal state ',negative
'table node ',negative
'support some virtual columns vectorization for this table scan ',negative
'fetch the data from parquet data page for next call ',negative
'hive code has assertionerrors some cases want record what happens ',negative
'find the record identifier column there and return possibly new objectinspector that ',negative
'now rememember what supported for this query and any support that was ',negative
'test mode then the created extra log file containing only ',negative
'take filter pushdown into account while calculating splits this allows prune off regions immediately note that although the javadoc for the superclass getsplits says that returns one split per region the implementation actually takes the scan definition into account and excludes regions which dont satisfy ',negative
'the waitqueue avoid object comparison ',negative
'optional string eventtype ',negative
'datasource the table properties insertinsert overwrite the datasource name was already persisted otherwise ctctas and need get the name from the job properties that are set the druidstoragehandler ',negative
'clean the mapred work ',negative
'token remove the placeholder arg ',negative
'maptask and currtask should merged and joinunion operator genmrunion which has multiple topops assert maptask currtask maptaskid maptaskgetid currtaskid currtaskgetid ',negative
'add inputs ops remove ',negative
'major compaction check data files ',negative
'implied ',negative
'transformation the join operation ',negative
'make sure the hidden keys didnt get published ',negative
'testtablekey ',negative
'event ',negative
'maps application identifiers jobids the associated user for the app ',negative
'for now expecting single row minmax aggregated bloom filter rows ',negative
'verify that the new configs are effect ',negative
'this could overridden thru the jobconf ',negative
'',negative
'check input objects length doesnt match then output new primitive with the correct params ',negative
'fetchnext execute the same sql again ',negative
'check the table file has header skip ',negative
'add constants there select top ',negative
'trigger failover minihs ',negative
'setup our outer join specific members ',negative
'see ',negative
'digit measuring stick ',negative
'return the child directly the conversion redundant ',negative
'set output column ',negative
'optional bytes initialeventbytes ',negative
'for cbo ',negative
'alternate ',negative
'which could lead lot extra unnecessary scratch columns ',negative
'were starting new command ',negative
'convert the absolute big decimal string ',negative
'form result from lower and high words ',negative
'window based aggregations are handled differently ',negative
'dynamic partition pruning ',negative
'add string value numdistinctvalue estimator ',negative
'execution mode ',negative
'check for map which occupies levels key separator and keyvalue pair separator ',negative
'firstname null ',negative
'',negative
'most the cases this column reference ',negative
'load hivesitexml from classpath ',negative
'should careful when authorizing table based just the table name columns have separate authorization domain ',negative
'numdvs ',negative
'note assume this never happens for serde reader the batch would never have vectors ',negative
'fill values down for equal value series ',negative
'methods for metrics integration each threadlocal perflogger will openclose scope during each perflog method ',negative
'oops this should have been caught before trying componentize ',negative
'cannot convert complex types ',negative
'test invalid table name ',negative
'constructors ',negative
'programmatically set root logger level info default logjtestproperties not available root logger will use error log level ',negative
'temporarily reusing the tez view acls property for individual dag access control hive may want setup its own parameters wants control per dag access setting the tezproperty per dag should work for now ',negative
'type ',negative
'exponent that derives from the fractional part under normal circumstatnces the negative the number digits however very long the last digits get dropped otherwise long with large negative exponent could cause unnecessary overflow alone this case fracexp incremented one for each dropped digit ',negative
'need some conversions here ',negative
'host dead before scheduling ',negative
'this also ensures that txn still there expected state ',negative
'nothing migration just validate that the tables automatically determine the table should managed external migrate tables external tables migrate tables managed transactional tables ',negative
'all partitions are filtered partition pruning ',negative
'there should new directory basexxxxxxx ',negative
'all the entries map are representing null then return true ',negative
'were done processing events ',negative
'semanticanalyzer inject sel before aggregation the columns this sel are derived from the table schema and not reflect the actual columns being selected the current query this case skip the merge and just use the path from the child ops ',negative
'load the defaults ',negative
'unknown field return well continue from the next field onwards ',negative
'the first version rcfile used the sequence file header ',negative
'',negative
'moving files depends the parenttask still want the dependencytask depend the parenttask ',negative
'check that aborted operation didnt become committed ',negative
'srsrwait lock are examining waiting this case keep looking its possible that something front blocking ',negative
'each iteration this loop tries split blocks from one level the free list into target size blocks cannot satisfy the allocation from the free list containing the ',negative
'this prevent dropping archived partition which archived ',negative
'dont test all the combinations because least currently the logic inherited the same testcollower which checks all the cases ',negative
'isnull ',negative
'note not change without changing the corresponding reference ',negative
'this stream will restarted with the same random seed over and over ',negative
'create some partitions ',negative
'use case ',negative
'integer digits stop zeroes above ',negative
'last one for union column ',negative
'junk the destination for the pass ',negative
'with bucketing using two different versions version for exiting tables and version for new tables all the inputs the smb must from same version this only applies tables read directly and not ',negative
'the merge file being currently processed ',negative
'get the clusterby aliases these are aliased the entries the select list ',negative
'have row for current group indicate not the last batch ',negative
'copy data values over ',negative
'query might have been canceled stop the background processing ',negative
'create invalid table which has wrong column type ',negative
'initialize all forward operator ',negative
'add field collations ',negative
'finally add project project out the columns ',negative
'',negative
'source complexpbproto ',negative
'timebased ',negative
'alias confict should not happen here ',negative
'wait while for tasks respond being cancelled ',negative
'nonjavadoc see javaioreader long ',negative
'aggregate had count aggregate call the corresponding aggregate aggregate must sum for other aggregates remains the same ',negative
'initialize with data row type conversion parameters ',negative
'pass the null value along the escaping process determine what the dir should ',negative
'create embedded metastore ',negative
'recursively clone the children ',negative
'case the statement create table ',negative
'not expected access the same class ',negative
'determine need read this slice for the split ',negative
'convert dynamic arrays and maps simple arrays ',negative
'cached copy valid ',negative
'dot not affected ',negative
'offset within compression buffer where the row group begins ',negative
'this case both should dhj operators prschildmj can only guarantee ',negative
'hashjoin ',negative
'failed event should not create new notification ',negative
'database ',negative
'there branch remove prune sink operator branch the basework there branch remove the whole basework ',negative
'configured not ignore this ',negative
'traverse the txntowriteid see any the input txns already have allocated write for the same dbtable yes then need reuse else have allocate new one the write would have been already allocated case multistatement txns where ',negative
'grouping sets are involved early return ',negative
'uses string type for binary before ',negative
'lets just the safe expensive way ',negative
'consider should this for all vector expressions that can ',negative
'emit the rest word ',negative
'nonjavadoc see javasqltime ',negative
'test some extreme cases ',negative
'need this get the task path and set for mapred implementation since cant done automatically because mapreducemapred abstraction ',negative
'divide power equal scale logically shift the digits places right scale positions eliminate them ',negative
'have create the tables here rather than setup because need the hive connection which conveniently created the semantic analyzer ',negative
'reply copy only happens for jars hdfs not otherwise ',negative
'need close because some filesystem implementations flush noop close the file handle hdfs file but stderrstdout skip since webhcat not supposed close ',negative
'this the first batch process after switching from hash mode ',negative
'default lexicographicalasc ',negative
'check that the partition folders exist disk ',negative
'nonjavadoc see ',negative
'http mode use nosasl the default auth type ',negative
'implicit casts possible ',negative
'name the class report for ',negative
'verify whether the sql operation log generated and fetch correctly ',negative
'note the following test will fail you are running this test root setting permission the database folder will not preclude root from being able create the necessary files ',negative
'row ',negative
'this default case with setugi off for both client and server ',negative
'comment passed table params ',negative
'xxx from oazktclientbase ',negative
'avoid the npe ',negative
'overwhelmingly executes once ',negative
'utility udfs ',negative
'inplace progress update related variables ',negative
'with nulls ',negative
'provide iterator the rows partition iterator exposes the index the next location client can invoke leadlag relative the next location ',negative
'ranges just return the empty list ',negative
'when converting from char stringvarchar strip any trailing spaces ',negative
'realign the columns appearing after startpostionsay column such that column becomes column offset column becomes column offset and ',negative
'classify leaf predicate equi non equi ',negative
'all outside elements should ignored from stat estimation ',negative
'one live session ',negative
'move the next child from tree ',negative
'wait for all futures complete check for abort while waiting for each future any the futures cancelled aborted cancel all subsequent futures ',negative
'new number register bits for higher accuracy ',negative
'emit the rest word ',negative
'lbstruct needs bytearrayref ',negative
'brute force may discard twice many buffers ',negative
'the field the configured string representing null ',negative
'there should delta dirs the location ',negative
'hive streaming ingest settings ',negative
'for select createasselect query populate the partition column parcols table columns mapping tabcols ',negative
'currently druid supports only mapbasedrow jackson serde should safe cast without check ',negative
'lint ',negative
'row ',negative
'minimum number clauses needed transform into clauses ',negative
'evaluate result for position using bytes avoid storage allocation costs and set position the output vector the result ',negative
'check that mpart does not exist but mpart still does ',negative
'use bucketized hive input format that makes sure that one mapper reads the entire file ',negative
'doesnt have notion tiny and saves the full value int overflow expectednull but was ',negative
'must the same file system the current destination ',negative
'atlas would interested lineage information for insertloadcreate etc ',negative
'row ',negative
'supervised kafka tasks should respect instead ',negative
'construct using ',negative
'methods that need data object this function key has the same structure the map expects most cases key will primitive type its rare cases that key not primitive the user responsible for defining ',negative
'clear out the set dont need anymore ',negative
'nonjavadoc see javaioinputstream int ',negative
'all the code paths below propagate nulls even arg has nulls this reduce the number code paths and shorten the code the expense maybe doing unnecessary work neither input has nulls this could improved the future expanding the number code paths ',negative
'check the join columns contains all bucket columns table bucketized column but the join key and easy see joining different buckets yield empty results ',negative
'since the exceptions and the range question overlap count the ',negative
'strip leading zeroes although there shouldnt any for decimal ',negative
'still valid nothing more ',negative
'simple case ',negative
'there are nulls the structcolumnvector ',negative
'initialize transaction table properties with default string value ',negative
'based ',negative
'calling close explicitly clean the staging dirs ',negative
'verify table for key byte hash table hashmultiset ',negative
'str ',negative
'add dummy data for not removed etc ',negative
'the mutex pools should ideally somewhat larger since some operations require connection from each pool and want avoid taking connection from primary pool and then blocking because mutex pool empty there only thread any hms trying mutex each mutexkey except mutexkeychecklock the checklock operation gets connection from connpool first then connpoolmutex all others the opposite order not very elegant number connection requests for connpoolmutex cannot exceed size connpool ',negative
'the collist the output columns used child operators they are different from input columns the current operator need find out which input columns are used ',negative
'which big table and small table columns are bytecolumnvector and need have their data buffer manually reset for some join result processing ',negative
'prepend column names with starts with ',negative
'this mapper operator used initialize all the operators ',negative
'minimum values ',negative
'hive external tables should not considered have uptodate stats ',negative
'possible that some other hms instance could have created the guid the same time due which this instance could not create guid above such case return the guid already generated ',negative
'host ',negative
'set kafka producer ',negative
'neither side repeating ',negative
'extract rows and call process per row ',negative
'the first small table ',negative
'this exception has been handled ',negative
'show that their positions are recorded ',negative
'exception thrown scheduled tasks further tasks will scheduled hence this ugly catch ',negative
'thread killing the query ',negative
'any additions the subqueries select clause ',negative
'offsetlimit smaller than rowcount the input operator thus return the offsetlimit ',negative
'add the part lastbatch track the parition being dropped ',negative
'there are more nodes signal timeout monitor start timer ',negative
'checksum failure ',negative
'test that separate databases dont coalesce ',negative
'all parsing done were now good start the export process ',negative
'check access columns from readentity ',negative
'second child node could partitionspec column ',negative
'figure out and encode what files need read this here rather than getsplits below because part this discover our minimum and maximum transactions and discovering that getsplits too late then have way pass our mapper ',negative
'write operation always start txn ',negative
'just ran major compaction should have basex tblname that has the new files ',negative
'copy full bucket context ',negative
'',negative
'need disable these that automatic merge doesnt merge the files ',negative
'existing table not view ',negative
'this semijoin branch the stack should look like ',negative
'try delete other directories possible ',negative
'process the list ',negative
'divide rows array into different sized batches modify the rows array for isrepeating null patterns provide iterator that will fill vrb with the divided rows ',negative
'add dependency between the two work items ',negative
'test that existing sharedwrite partition with new sharedread coalesces ',negative
'close shouldnt matter ',negative
'stringscompleter matches against predefined wordlist ',negative
'any filter sorted filter its sorted filter ',negative
'the intersection only support both kept ',negative
'make sure there dynamic addition virtual cols ',negative
'can converted tez event this sufficient decide preemption ',negative
'with vectorization ',negative
'accumulo iterators werent getting serialized into the inputsplit but can compensate because still have that info ',negative
'orc cannot anything here for now all the logic the proxy ',negative
'for stringcharvarchar buffering when there are escapes ',negative
'column names also can inferred from result udtf ',negative
'note tokwindowvalues means range type tokwindowrange means rows type ',negative
'build the final custom path string replacing each column name with ',negative
'will move all the files the tablepartition directories into the first directory then commit the first write ',negative
'shortcircuit quickly eat all rows ',negative
'gather the common factors and return them ',negative
'create list topop nodes ',negative
'caller looking for temp table handle here otherwise pass underlying client ',negative
'reference already accounted for the directsize ',negative
'this fulfill the contract the interface which states that exception shall thrown when saslserver cannot created due error but null should returned when server cant created due the parameters supplied and the only thing plainsaslserver can fail nonsupported authentication mechanism thats why return null instead throwing the exception ',negative
'production ',negative
'normal ',negative
'for external tables not need anything else ',negative
'simulate concurrent session ',negative
'wait for the delete complete ',negative
'gather columns used aggregate operator ',negative
'after insert into operation get the last repl ',negative
'the sender will take care this the value didnt change ',negative
'obtain the byte buffer from the input string can traverse code point code point ',negative
'reset the parameters can compare ',negative
'antisymmetric ',negative
'overwrite buffer size and stripe size for delta writer based basedeltaratio ',negative
'for arrow serde ',negative
'make sure minihs not leader ',negative
'write estimated count ',negative
'have propagated the value the task ',negative
'call open read data split mockmocktable ',negative
'make sure nullscanfilesystem can loaded hive ',negative
'get the forward ',negative
'this the last range for this compression block yay ',negative
'not dynamic partitioning then bail out ',negative
'folder ',negative
'null then the major version number should match ',negative
'must deterministic order set for consistent qtest output across java versions ',negative
'conversion requires source placed writable can call upon vectorassignrow convert and assign the row column ',negative
'walk the tree long the operators between the union and the filesink not involve reducer and they can pushed above the union makes sense push them above the union and remove the union interface has been added the operator denote whether ',negative
'instanceof instanceof instanceof ',negative
'this version hadoop does not support getpassword just retrieve password from conf ',negative
'create materialized view ',negative
'write data the target ',negative
'some tests control the execution the background update thread ',negative
'copy within row ',negative
'this without checking for parents ',negative
'tail ',negative
'boolean ',negative
'just alter the table ',negative
'the input the gby has tab alias for the column then add entry based that tabalias for this query select count from group needs tabaliasb colaliasx the gby tabaliasb comes from looking the rowresolver that the ancestor before any gbyreducesinks added for the gby operation ',negative
'',negative
'write null field ',negative
'add empty string the list aliases some operators groupby add ',negative
'mark partitions with new schema with different blurb ',negative
'filecache might empty see can remove trywritelock ',negative
'backing store for this cache ',negative
'assume that there are locked blocks the list they are they can dropped therefore always evict one contiguous sequence from the tail can find one pass splice out and then finalize the eviction outside the list lock ',negative
'this zero need check might become zero after scaling down this not easy because there might rounding ',negative
'committed anymore ',negative
'rewrite logic filter references correlated field its filter condition rewrite the filter filter joincross product originalfilterinput distinct sets correlated variables and rewrite the correlated fieldaccess the filter condition reference the join output filter does not reference correlated variables simply rewrite the filter condition using new input ',negative
'nothing ',negative
'dont need update tmp paths when there depth difference paths ',negative
'end addpartitions tests ',negative
'assumes that getmapvalueelement object will work with key from object the equality implemented fully the greaterthanlessthan values not implement transitive relation ',negative
'convert nonacidorctbl acid table with splitupdate enabled txnpropsdefault ',negative
'populated column struct array map types struct type contains schema the struct array type contains schema one the elements ',negative
'example dump dirs need able handle for hivereplrootdir staging then repl dumps will created stagingdumpdir singledbdump stagingblah will contain dir for the specified blah default metadata tbl metadata files tbl tbl unptntbl metadata files multidbdump stagingbar will contain dirs for each covered staging bar default sales single tabledump stagingbaz will contain table object dump inside staging baz metadata files incremental dump stagingblue will contain dirs for each event inside staging blue ',negative
'files exists input path then has this code path gets triggered only order queries which expected write only one file written one reducer ',negative
'null null ',negative
'batches will sized ',negative
'first give this task and put back the original list ',negative
'complains you reset already set value just dont care ',negative
'only support querytime merge for tables dont handle this ',negative
'ddl time munging thrift mode ',negative
'parse out the number histogram bins only once havent already done before need least bins otherwise there point creating histogram ',negative
'data stream could empty stream already reached end stream before present stream this can happen all values stream are nulls last row group values are all null ',negative
'make sure fail the channel something goes wrong internally handle all the expected exceptions log lot information here ',negative
'',negative
'cant find hadoop home can least try usrbinhadoop ',negative
'delete any contents the warehouse dir ',negative
'nothing done here ',negative
'optional string groupname ',negative
'big input cumulative row count ',negative
'tasks qualify preemptable ',negative
'not removing this here will removed when taken off the queue and discovered have pending tasks ',negative
'for condition independently compute and update stats ',negative
'read table stats via cachedstore ',negative
'comment out these checks when are happy ',negative
'check metaexception has one any exception thrown from objectstore which means that the authorization checks passed ',negative
'disable filter pushdown for mapreduce when there are more than one table aliases ',negative
'columns ',negative
'clone the filesinkdesc the final filesink and create similar filesinks ',negative
'set havent done before ',negative
'permissions set ',negative
'these are insert events original txn current txn for all rows ',negative
'add all data for element listcolumnvector get out the loop there data the data for new element ',negative
'rebuilt the querydef why that the exprnodedescriptors the querydef are based the select operators rowresolver ',negative
'expected nextwriteid doesnt have entry for this table and hence directly insert ',negative
'user privileges testdb testtable testtable testtable testtable testdb ',negative
'checklock makes diagnostics easier ',negative
'dont release the buffer contains any data beyond the acceptable boundary ',negative
'not using the path child cache here there could more than path per host worker and slot znodes ',negative
'include shim and admin specified libjars ',negative
'cannot merge bail out ',negative
'ignore file with same content already exist cmroot ',negative
'try serialize ',negative
'normalized display width based maximum display size and label size ',negative
'first lookup the file sink operator from the load work ',negative
'test code path ',negative
'always create partition and virtual columns ',negative
'the validateptfoperator method will update vectorptfdesc ',negative
'ordinal position this column the schema ',negative
'entityname ',negative
'sanity check all tasks must submit events for succeed ',negative
'null signifies nodes that are irrelevant the generation ',negative
'this config contains all the configuration that master node wants provide ',negative
'having key select where minbvalue ',negative
'oid for kerberos principal name ',negative
'next verify that the destination table not offline nonnative table ',negative
'failed try clean the metastore ',negative
'here there attempt set transactional something other than true and not the same value was before ',negative
'unit test convenience method for putting key and value into the hash table using the actual types ',negative
'batchindex classname match duplicate ',negative
'last function name replicated null function replication was progress when created this state ',negative
'used for listprovided cases ',negative
'swsracquired lock are examining acquired need keep looking because there may may not another shared write front ',negative
'binary keys values and hashcodes rows lined index ',negative
'prepare new query string ',negative
'and convert string yay ',negative
'call this reinitialize the node stack called automatically the parsers reinit method ',negative
'both are not empty merge two lists ',negative
'left and right repeat and right null ',negative
'transaction manager used for the query this will set compile time based ',negative
'get the path names for the row only ',negative
'here each group looks something like map reducer just parse the numbers and ignore one from map and from its there ',negative
'there only one element tuple contained bag throw away the tuple ',negative
'put the script content temp file ',negative
'new state not running user not active any more ',negative
'bucketedsorted columns for the destination table ',negative
'default that its not repl scope default full exportimport not metadataonly ',negative
'nothing lock mode ',negative
'set nonzk lock manager avoid trying connect zookeeper ',negative
'data structures store original values ',negative
'not all partitions are scanned all mappers this could null ',negative
'two int ',negative
'this wraps instance exprnodedesc and makes equals work like issame see comment ',negative
'success file required for oozie indicate availability data source ',negative
'create the databases and reload them from the metastore ',negative
'start all mrmultimr inputs ',negative
'file dir for each for now plus export data dir this could changed flat file list later ',negative
'update ',negative
'same ',negative
'old api assumed all partitions belong the same table keep the same assumption ',negative
'the writing thread has given object wait ',negative
'astringintint can matched with methods like astringintint astringintinteger astringintegerint and ',negative
'open connections ',negative
'tez session settings ',negative
'now vary isrepeating nulls possible only left ',negative
'case external table drop the table contents well ',negative
'only one result column ',negative
'the heartbeat consistent with what have ',negative
'get the order aliases these are aliased the entries the select list ',negative
'primarykeycols ',negative
'',negative
'inner classes defined after this point ',negative
'the authorization check ',negative
'determine are dealing with numeric date arithmetic operation ',negative
'maintain count outstanding requests for tokenidentifier ',negative
'remove entries from txncomponents there may aborted txn components ',negative
'for better performance longdouble dont want the conditional statements inside the for loop ',negative
'acquire lock for the given materialized view only one rebuild per materialized view can triggered given time otherwise might produce incorrect results incremental maintenance triggered ',negative
'prevent view cycles ',negative
'aid testing through files authenticator passed argument the interface this helps being able switch the user within session need check the user has changed ',negative
'second request for host single invocation since the last has not completed ',negative
'nonjavadoc see ',negative
'next one new key bail out from the inner loop ',negative
'loop iulrindex obtaining digits onebyone paper ',negative
'return new hash set result implementation specific object the object can used access access spill information when the partition with the key currently spilled ',negative
'descending ',negative
'ignore the outdated updates for the same version ignore nonnull updates because assume that removal the last thing that happens for any given version ',negative
'getconvertedoi with caching store settable properties the object inspector caching might help when the object inspector contains complex nested data types caching not explicitly required for the returned object inspector across multiple invocations since the already takes care ',negative
'files size for splits ',negative
'check the comparison supported for this type ',negative
'convert the constants available strings the corresponding objects ',negative
'try preempting lower priority task any case ',negative
'support decimal ',negative
'abstract class hold info required for the implementation ',negative
'add listener handle any cleanup for when the connection closed ',negative
'float ',negative
'for after tname for the end version between ',negative
'creates scratch directories needed the context object ',negative
'the following conditions are for native vector reducesink ',negative
'handles expr like structkeyvaluekey follows same rules which equivalent version parsing such expression from ast ',negative
'does not need type conversions because ',negative
'setup the hadoop vars specify the user ',negative
'that already exist existingparts will not generate notifications ',negative
'since integer always some products here are not included ',negative
'support for statistics yet ',negative
'write the empty base ',negative
'queryload can contain insert overwrite ',negative
'calculate selectivity ',negative
'werent interrupted just propagate the error ',negative
'hiveobject ',negative
'ssl conf ',negative
'optional ',negative
'for both graceful forceful shutdown wait for tasks terminate such that appropriate exceptions are raised and stored ',negative
'generate the map work for this aliasid pass both confirmed and unknown partitions through the mapreduce ',negative
'safety checks ',negative
'url was not specified with but was present use that ',negative
'timeseries query results ',negative
'sets delimiter tab ascii ',negative
'throw error the user asked for bucketed mapjoin enforced and ',negative
'reset the time only want count the loop ',negative
'set permissions appropriately for each the partitions just created ',negative
'this only there for the use case where are doing table only replication and not database level ',negative
'setting repl dump and repl load all requiring admin privileges might wind loosening this the future but right now not want individual object based checks every object possible and thus asking for broad privilege such this the best route forward repl status should use privileges similar describe dbtable and asks for ',negative
'this class used verify that hivemetastore calls the nontransactional listeners with the current event set the class ',negative
'the group operator has already been processed ',negative
'spilled tables are loaded always sharing clear ',negative
'truncate byte array maximum number characters and return byte array with only truncated bytes ',negative
'set lineage info ',negative
'remove the tag for inmemory side mapjoin ',negative
'the data does not belong recognized chunk split wrong ',negative
'referencing another table instead self for the primary key ',negative
'derby fails creating multiple stats aggregator concurrently ',negative
'mybinary ',negative
'headernames ',negative
'few over ',negative
'get information from yarn service ',negative
'metaexception ',negative
'mssql this means communication link failure ',negative
'initially all columns are projected and the same order ',negative
'maxweight ',negative
'lock ',negative
'are good ',negative
'while updating local structures note this actually called under the epic writelock ',negative
'each parent ',negative
'this works assuming the executor running within yarn ',negative
'done ',negative
'clear vertices list ',negative
'print out the uncompressed sizes each column ',negative
'here only deals with nonpartition columns deal with partition columns next ',negative
'test the last day month ',negative
'subtract for twos compliment adjustment ',negative
'gives list any new paths that may have been created maintain the persistent ephemeral node ',negative
'mybitint ',negative
'write the delimiter the stream which means dont need outputformatstring anymore ',negative
'unionmstringstring ',negative
'red hat centosubuntudebian oel sles ',negative
'couldnt create the tracker node dont create the main node ',negative
'new session ',negative
'for left outer join join and skip further ',negative
'not boolean columns possible which case return false count ',negative
'executed twice once with the typed setters once with the generic setobject ',negative
'the last columns are vcol and ',negative
'verify outputs ',negative
'the avg result type has the same number integer digits and more decimal digits ',negative
'logj utilize gcfree layoutencode method taken care superclass ',negative
'test valid case ',negative
'the roundpower same scale means all zeroes below round point ',negative
'get empty container when the small table empty ',negative
'todo extend taskrunner see api with callbacks will work ',negative
'set the value the writable from the decimal digits that were written with dot ',negative
'asian character bytes ',negative
'mixture input big table columns and new scratch columns ',negative
'add some margin the wait avoid rechecking close the boundary ',negative
'the data not cache ',negative
'drive the doprocessbatch logic with the same batch but different grouping set and null variation performance note not try reuse columns and generate the keywrappers anew ',negative
'invoke delete leader endpoint for failover ',negative
'check the schema table with one field partition keys ',negative
'walk hive ast and translate the hive column names their equivalent mappings this basically cheat ',negative
'resolve parse tree ',negative
'whether the method takes array like object string etc the last argument ',negative
'delete source overwrite destination ',negative
'otherwise countnull should directly return ',negative
'note only used from index related classes ',negative
'store the partitions temporarily until processed ',negative
'all represented seconds ',negative
'boolean long min and max ',negative
'the number tez tasks executed the hiveserver since the last restart ',negative
'numtxns ',negative
'lvmerge the above order ',negative
'force spark configs cloned default ',negative
'canhandleqbforcbo returns null the query can handled ',negative
'temporary work arrays ',negative
'get list with the current classpath components ',negative
'specifies ',negative
'replace the commar ',negative
'keycount ',negative
'pad output ',negative
'localdatetime immutable ',negative
'this case can happen with llap able deserialize and cache data from the input format will deliver that cached data vrbs ',negative
'quote always for fields that mimic sql keywords like desc ',negative
'need register minimum open txnid for current transactions into minhistory table ',negative
'get the vector description from the operator ',negative
'divide and round ',negative
'best answer would ',negative
'path file specified can relative treat target file name hadoop put behavior ',negative
'delete partition level column stats exists ',negative
'required required optional required optional optional ',negative
'new method that distributes the scan query creating splits containing information about different druid nodes that have the data for the given query ',negative
'nunknown hive type infoot map ',negative
'shallow copy aliastoopinfo wont want clone the operator tree here ',negative
'describe table partition ',negative
'',negative
'the writer local the process ',negative
'generate the list bucketing pruning predicate separate clauses containing the partitioning and nonpartitioning columns ',negative
'clean the temp files ',negative
'could null theres race between the threads processing requests with dag finish processed earlier ',negative
'counters for debugging cannot use existing counters cntr and nextcntr ',negative
'metastore has record this lock most likely timed out either way there point tracking here any longer ',negative
'',negative
'this would refresh any conf resources and also local resources ',negative
'the jobtracker has exceeded its threshold and doing ',negative
'are modifying divisor here make local copy ',negative
'dcname ',negative
'check for list found recursively init its members ',negative
'this thread was interrupted this case the call might return sooner than long polling timeout ',negative
'update description ',negative
'nonjavadoc see javalangobject int int ',negative
'future this may examine readentity andor config return appropriate hcatreader ',negative
'',negative
'within the proxy object wrap the method call ',negative
'token found ',negative
'offset truncation ',negative
'the beginning the list record will the value length ',negative
'opentxns ',negative
'all keys vcol vcolc ',negative
'check for this pattern the pattern matching could simplified rules can applied during decorrelation correlaterelleft correlation condition true leftinputrel projecta rexnode aggregate groupby agg agg ',negative
'see paper for alpha initialization ',negative
'size sparse map excess the threshold convert the sparse map dense register and switch dense encoding ',negative
'query not compiled state executing state which carried over from combined compileexecute runinternal throws the error ',negative
'heartbeat the lockid first assure that our lock still valid then look the lock info hopefully the cache these locks ',negative
'number distribution keys crs chosen only when numdistkeys prs less all other cases distribution the keys based the prs which more generic than crs examples case prs sort key and crs sort key and number distribution keys are and resp then after merge the sort keys will while the number distribution keys will case prs sort key empty and number distribution keys and crs sort key and number distribution keys then after merge new sort key will and number distribution keys will ',negative
'test stringstring version ',negative
'get pushdown predicates for this operators predicate ',negative
'check there exists bloom filter size entry ',negative
'initialization isnt finished until all parents all operators are initialized for broadcast joins that means initializing the ',negative
'rowid ',negative
'validate principals ',negative
'first shot without waiting ',negative
'have more input but did start with something valid ',negative
'precisionscale exceed maximum precision result must adjusted ',negative
'',negative
'check the group operator has already been processed ',negative
'merge the result last evaluate previous evaluate ',negative
'throws exception not initialized ',negative
'evaluate the aggregation functions over the group batch ',negative
'this entry the pathenv directory see the required file this directory ',negative
'insert row nonacid table ',negative
'can not judge assuming replication not enabled ',negative
'list ',negative
'this simply verify that the hooks were fact run ',negative
'row resolver for the operator ',negative
'yarn ats ',negative
'this just for smb join usecase the numbuckets would equal that the big table and the small table could have lesser number buckets this case want send the data from the right buckets the big table side for big table has buckets and small table has buckets bucket small table needs sent bucket the big table ',negative
'nonjavadoc see ',negative
'two objects ',negative
'decimal ',negative
'writable class ',negative
'test functionality ',negative
'',negative
'invariant bucketcol literals type bucketfield ',negative
'this breaks all the tests files ',negative
'the one big table rows values repeat ',negative
'keydne ',negative
'map avros primitive types hives for those that are supported both ',negative
'this close function does not need synchronized since called its parents main thread ',negative
'assumption batchindex increasing was called ',negative
'insert overwrite ',negative
'use complexnewbuilder construct ',negative
'tblnames ',negative
'eof ',negative
'select query results ',negative
'zero dividend ',negative
'',negative
'adding mysql jdbc driver exists ',negative
'should also remove when pulling from accumulo ',negative
'get input path and remove this alias from pathtoalias ',negative
'column indexes corresponding data storage layer ',negative
'hive attempt fold expression like where case sssolddate when then else null end where sssolddate ',negative
'indicates read buffer has data number rows the temporary read buffer cursor during reading total number objects output ',negative
'verify ',negative
'add jars containing the specified classes ',negative
'this method takes input operator and subset its output column names and generates the input column names for the operator corresponding those outputs the mapping from the input column name the output column name not simple the method returns false else returns true the list output column names modified this method the list corresponding input column names ',negative
'after load from this dump all target tablespartitions will have initial set data ',negative
'this method returns the flip bigendian representation value ',negative
'since integer always some products here are not included ',negative
'only precision specified ',negative
'make deep copy the aliases that they are not changed the context ',negative
'rewrite logic permute the group keys the front the input aggregate produces correlated variables add them the group list change aggcalls reference the new project ',negative
'some the strings can passed unicode for example the delimiter can passed first check the string unicode number else back the old behavior ',negative
'has the user explicitly asked not sample this table ',negative
'nonjavadoc see ',negative
'children means were the bottom there are more operators scan ',negative
'maxed out capacity this move should fail the session ',negative
'this will also move the iterator ahead one code point ',negative
'end the month behavior ',negative
'look for file which can move the existing file with external services its possible for the service marked complete after each fragment ',negative
'continue ',negative
'doclipping ',negative
'converts timestamp timestamptz ',negative
'reset tableprops with reloaded tblproperties ',negative
'prspjoincrs ',negative
'production ',negative
'',negative
'tables have been replicated over and verified identical now couple alters the source ',negative
'skip check for import here because already handle above ctas check ',negative
'store the ugi transport and then continue usual ',negative
'for unpartitioned table partitionvals are not specified ',negative
'delete ',negative
'extract the launcher job submitstart time and use that scope down the search interval when look for child jobs ',negative
'finishtime ',negative
'then finishable must always precede nonfinishable ',negative
'still the local jvm use the usernamepassword kerberos credentials ',negative
'init ',negative
'override with user defined properties ',negative
'apply isnull and instr not supported pushdown via name filtering ',negative
'locks present ',negative
'lets take look the operators were checking for user code those will not run that llap ',negative
'the registration znode ',negative
'first try quickly lock some the correctsized free lists and allocate from them ',negative
'adjust file column index for orc struct ',negative
'remove this server instance from zookeeper dynamic service discovery set ',negative
'not want keep state two separate places remove from hive table properties ',negative
'done ',negative
'for temporary tables set the location something the sessions scratch dir has the same life cycle the tmp table ',negative
'toepochmilli returns utc time regardless ',negative
'resulting output object inspector can used make the rowresolver ',negative
'the new objectinspector the same the old one directly return true ',negative
'power with double power ',negative
'replace back ',negative
'orcrecordupdaterrow this somewhat fragile note this guarantees that physical column ids are order ',negative
'dont want include the root struct acid case would cause the whole struct get read without projection ',negative
'refer ',negative
'negative means the key didnt exist the original stream changed the tree ',negative
'determine minimum all nonnull decimal column values maintain isgroupresultnull ',negative
'cant escaped the separator ',negative
'skip the auth embedded mode the auth disabled ',negative
'same file different offset ',negative
'restore repeating and nulls indicators ',negative
'for inner joins may apply the filters now ',negative
'',negative
'need create some extra sessions wed just like startup does ',negative
'improved later ',negative
'todo this test method the first run then the parameters does not contain totalsize and numfiles this runs after other tests setupdropdatabase successful then the ',negative
'weve added insert clauses order when items whenclauses ',negative
'most operations cannot run asynchronously ',negative
'should emit exception ',negative
'callback separate thread that when task completes the thread the main queue ',negative
'filter the list locations those that have least the ',negative
'events for events for ',negative
'ignore empty strings ',negative
'when partition column type not string the values from will null ',negative
'apply the plan again enable ',negative
'unpartitioned table ',negative
'test february nonleap year ',negative
'class partitionspecproxy ',negative
'get failed attempts from jobfailuresjsp ',negative
'simd loop ',negative
'get list temp table names ',negative
'case thread count set use single thread ',negative
'then iterate through all the operators that have candidate fks again assume the first joining with the that just selected and apply the pkfk relationship when compute the newrows and ndv after that join the result with all the other fks not assume the pkfk relationship anymore and just compute the ',negative
'case error reset the file system object ',negative
'replicate all the events happened after bootstrap ',negative
'executorruntime etc ',negative
'runasync querytimeout makes sense only for sqloperation pass the original statement sqloperation sql parser can remove comments itself ',negative
'the big table has more buckets than the current small table use mod get small table bucket names for example the big table has buckets and the small table has buckets then the mapping should ',negative
'the reader pointer has moved the end next block the end current record ',negative
'need some extra checks get the running owner ',negative
'yearsmonths represented months ',negative
'buddy block allocated higher level allocation than are have reached the top level add whatever have got the current free list ',negative
'not public since must have the field count column sort order information ',negative
'create simple table and test create drop get ',negative
'repeated nulls skip this input column ',negative
'changes the owner group and verify the change ',negative
'the default different the client and server its null here ',negative
'compare set fields ',negative
'the cases create partition the time this event fires the partition object has not yet come into existence and thus will not yet have location but these are needed create qlmetadatapartition use the tables the only place this used the authorization hooks will not affect code flow the metastore itself ',negative
'and finally return them flat array ',negative
'the following deltas includes all kinds delta files including insert delete deltas ',negative
'drop table will clean the table entry from the compaction queue and hence cleaner have effect ',negative
'many restrictions ',negative
'check user have erroneously specified nonexistent partitioning columns ',negative
'manually modify the underlying metastore reflect statistics corresponding ',negative
'the rangeinputsplit should have all the necesary information contained which alleviates from reparsing our configuration from the and resetting into the configuration like did getsplits thus should unnecessary reinvoke configure ',negative
'for asc nulls first for desc nulls last ',negative
'all good ',negative
'fully specified partition spec ',negative
'ensure data structures are updated the main taskscheduler ',negative
'retry with path ',negative
'filter are the output and the same position ',negative
'txnididtxnupdate ',negative
'execution mode vectorized ',negative
'such abc ',negative
'special case both constants are not equal then return ',negative
'honor custom location for external table apart from what metadata specifies ',negative
'dont use the caches copy the buffer ',negative
'the decimal value currently valid ',negative
'update pathsi from queryid ',negative
'read should get rows immutable mutable ',negative
'search like binary search minimize comparisons ',negative
'this doing lot copying here this could improved enforcing length the same time escaping rather than separate steps ',negative
'when data prematurely ended the fieldposition will more than the end ',negative
'see the alias has lateral view chain the lateral view operator ',negative
'element for key long hash table hashset ',negative
'need openssl ',negative
'only input side has nulls ',negative
'sort columns make output deterministic ',negative
'crs being used for distinct the two reduce sinks are incompatible ',negative
'replicate the changes the replicatedtable ',negative
'add the distinct aggregate columns the groupby columns ',negative
'design note the future this function can implemented directly translate input output without creating new objects performance can probably improved significantly its implemented the simplest way now just calling the existing builtin function ',negative
'bgenjjtree flagargs ',negative
'txnididtxnupdate ',negative
'not need connect its parent its counterpart they have the same parents ',negative
'archived partitions have hartoharfile their location the original directory was saved params ',negative
'patterns that are included execution logging level execution mode show only select logger messages ',negative
'return constant vector expression ',negative
'not include the dummy grouping set column the output pass outputkeylength instead ',negative
'the tokens should ignore when are trying table masking ',negative
'this batch full break out for loop execute ',negative
'given byte array consisting serialized bloomkfilter gives the offset from for the start the serialized long values that make the bitset ',negative
'not read thus records ',negative
'txnididtxnupdate ',negative
'the jsonobject for this operator ',negative
'basic case ',negative
'intentionally using deprecated method ',negative
'disable llap wrapper doesnt propagate extra acid columns correctly ',negative
'default executors max slots noconditional task size will oversubscribed ',negative
'delete the incorrectly copied file and retry with path ',negative
'map integer integer ',negative
'hiveconfdir not defined file not found hiveconfdir then check hivehomeconf ',negative
'create partcount dummy partitions ',negative
'nothing done ',negative
'over all the aggregation classes and and get the size the fields fixed length keep track the variable length ',negative
'check how much memory left memory ',negative
'unregister for the dirwatcher for the specific dagidentifier either case ',negative
'txnididtxnupdate ',negative
'tests with queries which cannot executed with directsql because type mismatch the type the num column string but the parameters used the where clause are numbers after falling back orm the number partitions cannot fetched the method they are fetched the method ',negative
'while done ',negative
'current vector map operator read type and context ',negative
'set needed cols tsdesc ',negative
'serialize some data the schema before altered ',negative
'logger with default base ',negative
'another single call get all the partition objects ',negative
'rows qualify ',negative
'only user belonging admin role can list role ',negative
'this should not happen ',negative
'demuxoperator should have least one child ',negative
'currently not support merging windowing functions with other windowing functions embedding windowing functions within each other ',negative
'set values look for ',negative
'check whether will end with same operators inputing same work work merge work work work work cannot merge the reason the same above currently tez does not support parallel edges the check exclude the inputs the root operator that are trying ',negative
'both were the same and can replaced the new were loading into ',negative
'this join has been converted smb join the hive optimizer the user did not give mapjoin hint the query the hive optimizer figured out that the join can ',negative
'test that existing sharedwrite table with new exclusive coalesces ',negative
'binary conversions supported genericudfdecimal genericudftimestamp ',negative
'todo couldshould trace seek destinations pps needs peek method ',negative
'source prep ',negative
'get the scale factor turn big decimal into decimal this relies the bigdecimal precision value which hive ',negative
'cancel the timer ',negative
'cache the results for table authorization ',negative
'only set output dir partition fully materialized ',negative
'column buffers otherwise the end when closeop called things get printed multiple times ',negative
'fall through delta ',negative
'not not ',negative
'keep column expression map explain plan uses this display ',negative
'this set ',negative
'proceed with binary search ',negative
'for once actually want reference equality java ',negative
'should convert the datetime the format hive understands default either yyyymmdd hhmmss seconds since epoch date crdatetimeepoch dgettime ',negative
'',negative
'first row all nulls ',negative
'dont need this for now not support ',negative
'following mapping enable udfname udf while generating expression for default value operator tree ',negative
'have repeated value the sum increases value batchsize ',negative
'',negative
'always set the explain conditions ',negative
'verify that the table does not already exist dumptable only used check the conflict for nontemporary tables ',negative
'objectinspector ',negative
'data from delta ',negative
'setup cache enabled ',negative
'the init method hmshandler throws exception for the first time while creating retryinghmshandler should retried ',negative
'',negative
'setting ',negative
'use this constructor when there output column ',negative
'generate groupby operator hash mode for mapside partial ',negative
'return true this task ancestor itself parameter desc ',negative
'skip all the events belong other dbstables ',negative
'lookup key hashcode hashcode ',negative
'need merging the move local file system ',negative
'the sequence must prsselcrs ',negative
'these operators need linked enable runtime statistics gatheredused correctly ',negative
'remove bloomfilter stats generated ',negative
'for now disable operating decimal column vectors for semijoin reduction have make sure same decimal type should used during bloom filter creation and bloom filter probing ',negative
'since repeating only happens when offset length ',negative
'optional authzid ',negative
'testlazybinaryfast source rows serde serdefewer primitivetypeinfos useincludecolumns true dowritefewercolumns true ',negative
'exact same state above ',negative
'nonjavadoc see ',negative
'optional string hivequeryid ',negative
'groupby for the grouping set corresponding the rollup ',negative
'optimize running value expressions only over the matched rows ',negative
'not have location locked someone else ',negative
'look the current sessions setting ',negative
'tree form tokptblfunction name alias partitioningspec arguments can tablereference subquery another ptf invocation ',negative
'boolean that says whether tez auto reduce parallelism should used ',negative
'this plugin avoid getting serialized event runtime ',negative
'leaving this the hive catalog rather than choosing the default from the configuration because all the default udfs are that catalog and think thats would people really want here ',negative
'byte ',negative
'needs refresh param passed should return new object ',negative
'create ssl socket and connect ',negative
'hivetablescan not found not sequence project and filter operators execute the original getuniquekeys method ',negative
'inppartspec mapping from partition column name its value ',negative
'writer should match the orc configuration from the original file ',negative
'will remember completed dag for hour avoid execution outoforder fragments ',negative
'get column type ',negative
'backtrack key exprs child parent and compare with parents ',negative
'highest priority this point should have come out ',negative
'nonjavadoc see javaneturl ',negative
'always positive ',negative
'remove branch ',negative
'are going create the map for each view the given database ',negative
'',negative
'returns true the aggregation result will streamed ',negative
'old bucketing logic ',negative
'hive insert overwrite local directory uses task dir name setting the jobconf helps have the similar dir name ',negative
'make sure one calls this ',negative
'else osx gives ugly temp path which screws approvals ',negative
'group the list dedup ',negative
'blank byte blank byte blank byte asian character bytes ',negative
'this will stop run from pushing records along with potentially blocking initialization ',negative
'how much can read from current read buffer out what need ',negative
'input path operator context ',negative
'wait for scheduling run few times ',negative
'enforce certain order when the reutilization particular use size table number reads ',negative
'singlecolumn long get key ',negative
'message must transacted before return ',negative
'push filter top children for retainable ',negative
'mark any scratch small table scratch columns that would normally receive copy the key null too ',negative
'the input should textinputformat ',negative
'set nonzk lock manager prevent from trying connect ',negative
'retry logic ',negative
'test ',negative
'verify moveonlytask optimized ',negative
'keep the dynamic partition context conditional task resolver context ',negative
'public boolean droppartitionstring dbname string tablename string partname boolean deletedata boolean ifpurge throws metaexception texception return droppartitiondbname tablename partname deletedata ifpurge null ',negative
'lowest field ',negative
'other failure testcases ',negative
'cannot validate the list may unset ',negative
'new state changed running from something else user active ',negative
'deal with the last entry ',negative
'the number reduce tasks per job hadoop sets this value default setting this property hive will automatically determine the correct ',negative
'set completed there nothing the server has report for example ddl statements ',negative
'step remove this mapjoin task ',negative
'nonjavadoc see ',negative
'the string has more code points make sure traverse too ',negative
'the log ',negative
'app base dir dagdir appbaseoutput ',negative
'the method not exposed and dont use ',negative
'name required for routing error out not set ',negative
'this call accessed from client jdbc side ',negative
'use construct ',negative
'process grouping set for the reduce sink operator for consider select key value count from group key value with rollup assuming mapside aggregation and skew the plan would look like tablescan select groupby reducesink groupby select filesink this function called for reducesink add the additional grouping keys introduced ',negative
'files size for splits ',negative
'',negative
'coalesce special case because can take variable number arguments nvl specialization the coalesce ',negative
'use case ',negative
'try render place update progress bar only the operations logs update least once this will hopefully allow printing the metadata information like query application etc have remove these notifiers when the operation logs get merged into getoperationstatus ',negative
'convert the first param into datewritablev value ',negative
'tries get lock and gets waiting state ',negative
'sem input ',negative
'source table input format ',negative
'change the location and see the results ',negative
'the current child struct expression constant struct ',negative
'',negative
'try create table with all the parameters set ',negative
'theres base file major compaction ',negative
'now replace the old evaluators with our own ',negative
'recurse over the subqueries fill the subquery part the plan ',negative
'dont want push cross join ',negative
'nothing for null struct ',negative
'mapoutputinfomap used share the lookups with the caller ',negative
'query ',negative
'attempt create the table taking external into consideration ',negative
'found all possible rows which will not filtered ',negative
'first stripes will satisfy the predicate and merged single split last stripe will ',negative
'finally throw exception ',negative
'test partition listing with partial spec specified but not ',negative
'this operator this used for connecting them later ',negative
'this the first call open the session ',negative
'set these two invalid values any attempt use them ',negative
'set fastscale ',negative
'startix inclusive endix exclusive ',negative
'not sure these large cols could resultschema ignore this for now ',negative
'now run lower priority task ',negative
'remote dirs ',negative
'keep backward compat explain for singlefile copy tasks ',negative
'exists and not empty exists and empty doesnt exist ',negative
'integer digit fraction digits trailing zeroes are suppressed ',negative
'text byte value ',negative
'query ',negative
'resourceuris ',negative
'sleep for longer than servers idletimeout and execute query ',negative
'can stored string text some other classes ',negative
'can add current batch ',negative
'nothing ',negative
'test that existing exclusive with new sharedwrite coalesces ',negative
'longrunning application ',negative
'nonjavadoc see ',negative
'request task ',negative
'the globalhook stuff theres proper way insert this add everywhere ',negative
'create rows file and empty ',negative
'right now assume that the group arraylist object may ',negative
'since tasks themselves can graphs want limit the number created ',negative
'the serialize routine uses this build ',negative
'empties the projection columns ',negative
'implies fetch everything ',negative
'partitions dropped this batch ',negative
'this called only during move session handling removing session already checks this this not expected remove failing will not even invoke this method ',negative
'the collectable stats for the aggregator needs cleared for file being loaded the old number rows are not valid ',negative
'its parent hierarchy indicates how many these have completed ',negative
'same above ',negative
'req ',negative
'drop them from the proper catalog ',negative
'more reliable then attempting parse the error message from the sqlexception ',negative
'check can handle the subquery ',negative
'',negative
'singlecolumn string specific save key and lookup ',negative
'read cost ',negative
'',negative
'cannot open the lock file for writing must held live process ',negative
'repl noop export nonexistant table has replnoop does not error import repl noop dump error ',negative
'call open read split mockmocktable ',negative
'rely ndc information the logs map counters attempt that not available appid should either passed extracted from ndc ',negative
'aggregation buffer methods ',negative
'for smb the key columns should same bucket columns and sort columns ',negative
'make sure delta appears before delta ',negative
'',negative
'javameta javaref javaarraymeta javaref ',negative
'replicate drop partition event and verify ',negative
'test batchsize not divisible decaying factor ',negative
'all new versions acid tables created after the introduction acid versiontype system can have property defined this parameter can used change the operational behavior acid however this parameter not defined the new acid tables will still behave the old ones this done preserve the behavior case rolling downgrade ',negative
'these are the possible types referenced type below ',negative
'since take the rhs set exactly was input dont need deal with quotingescaping columntable names ',negative
'create union above all the branches the schema union looks like this ',negative
'let hashtable the child this parent ',negative
'fill high long and middle from some lower long ',negative
'visible for testing ',negative
'project nulls for the extra fields maybe subclass table has ',negative
'invariants ',negative
'cancel the deleg tokens that were acquired for this job now that are done should cancel the tokens were acquired hcatoutputformat and not they were supplied oozie the latter case the property the conf will not set ',negative
'multiple transactions only happen for streaming ingest which only allows inserts ',negative
'overlap between the two ranges ',negative
'found source which not stored memory ',negative
'evaluate children ',negative
'topn query results records types defined metastore ',negative
'sequence the following insert with segments original segment still present the datasource insert overwrite with segments datasource empty insert overwrite with segments datasource empty insert with segments datasource empty insert with one segment datasource has one segment insert overwrite with one segment datasource has one segment insert with one segment datasource has two segments insert overwrite with segments datasource empty ',negative
'same depth using natural order ',negative
'context for using deserialize each row from the input file format into the vectorizedrowbatch ',negative
'map that keeps track work that need linked while ',negative
'use only reducer order present ',negative
'',negative
'cant this with before because want able control when the thread starts ',negative
'long avro ',negative
'try expire the session its not use use bail ',negative
'intermediate reduction case column stats hash aggregation grouping sets numrows case column stats hash aggregation grouping sets numrows sizeofgroupingset case column stats hash aggregation grouping sets minnumrows ndvproduct parallelism case column stats hash aggregation grouping sets minnumrows sizeofgroupingset ndvproduct parallelism sizeofgroupingset case column stats hash aggregation grouping sets numrows case column stats hash aggregation grouping sets numrows sizeofgroupingset ',negative
'with big table ',negative
'mapside ',negative
'initialize map operator ',negative
'put them and also roll them while were ',negative
'all the tasks should have deallocated their stuff make sure can allocate everything ',negative
'regular vertices ',negative
'depending whether are using beeline sqlline the line endings have handled differently ',negative
'reset key and value columns and batchsize ',negative
'extrapolation needed for extracolumnnames ',negative
'assume can only used the beginning line ',negative
'cors check ',negative
'set ',negative
'computes the sig every object ',negative
'have just added new node signal timeout monitor reset timer ',negative
'configure header size ',negative
'field not included query ',negative
'set operator plan ',negative
'left outer join produced list with values ',negative
'the default encoding for this table when not otherwise specified ',negative
'recreate the database ',negative
'illformed query like select from having ',negative
'and firstname john sue and ',negative
'query does not contain cube rollup grouping sets and thus grouping should return ',negative
'kick out previous overflow batch results ',negative
'output nonulls set false need reset the isnull array ',negative
'storage vars ',negative
'process per vertex counters that are available only via vertex progress ',negative
'this arraylist holds the maxmin this the ',negative
'canretainbyteref ',negative
'start with the following locks path shared path exclusive path shared path shared ',negative
'select query results records types defined metastore ',negative
'add the new aggregate column and recompute data size ',negative
'alter the table ',negative
'column level statistics are required only for the columns that are needed ',negative
'read the warehouse dir which can changed multiple metastore tests could run ',negative
'this point are going make copy needed avoid array boundaries ',negative
'how this different from the outputshape set the tabledef this the the object coming out the ptf put output partition whose serde usually lazybinaryserde the next ptf operator the chain gets lazybinarystruct ',negative
'only primitive fields supports for now ',negative
'case max list members max query string length ',negative
'linuxyes windowsno ',negative
'the user assumptions ',negative
'avoid copy when oldtmpjars null empty ',negative
'semijoin case ',negative
'current random sampling implementation inputsampler always returns value index which can same with previous partition key that induces split points are out order exception causing hive ',negative
'vertex ',negative
'now clone the tree above selop ',negative
'populated for smb joins only for all the small tables ',negative
'nontransitive ',negative
'reset the bytebuffer ',negative
'set the specific parameters needed ',negative
'run the map join task set task tag ',negative
'instantiate the chosen transaction manager ',negative
'run count times and get average ',negative
'cannot swap the inputs can try ',negative
'ide support for running tez jobs ',negative
'need set the clients keyprovider the nns for jks else the updates not get flushed properly ',negative
'name child class ',negative
'view column access info carried ',negative
'too large have effect ',negative
'the table containing the partitions not yet loaded cache ',negative
'use reflection set lockmanager since creating the object using the relection dummytxnmanager wont take mocked object ',negative
'mapping from operator the columns which its output bucketed ',negative
'generating join output results ',negative
'the token file location initial hiveconf arg ',negative
'first add original keys ',negative
'first try without qualifiers would resolve builtintemp functions otherwise try qualifying with current name ',negative
'reducesink also needs mapjoin child ',negative
'',negative
'multiply and subtract some digits ',negative
'this resets vectors both batches ',negative
'the storage root ',negative
'the subquery has where clause there nothing rewrite decompose subquery where clause into list top level conjuncts for each conjunct break down the conjunct into leftexpr leftexprtype rightexpr rightexprtype the top level operator equality operator will break down into left and right all other case there only lhs the exprtype based whether the expr refers the parent query table sources refers the subquery sources both assume unqualified column refers subquery table source this because require parent column references qualified within the subquery the lhs rhs expr refers both parent and subquery sources flag this unsupported the conjunct whole only refers the parent query sources flag this error conjunct correlated the lhs refers subquery sources and rhs refers parent query sources the reverse say the lhs refers subquery and rhs refers parent query sources the other case handled analogously remove this conjunct from the subquery where clause for the subquery expressionlhs construct new alias the correlated predicate replace the subquery expressionlhs with the alias ast add this altered predicate the join predicate tracked the qbsubquery object add the alias ast list this list used the case outer joins add null check predicates the outer querys where clause add the subquery expression with the alias selectitem the subquerys selectlist case this subquery contains aggregation expressions add this subquery expression its groupby add the front the groupby predicate not correlated let remain the subquery where clause additional things for having clause correlation predicate may refer aggregation expression this introduces twists the rewrite when analyzing equality predicates need analyze each side see aggregation expression from the outer query for this valid correlation predicate minry where outer table reference and subquery table reference when hoisting the correlation predicate join predicate need rewrite the form the join code allows the predict needs contain qualified column references handle this generating new name for the aggregation expression like rgbysqcol and adding this mapping the outer querys row resolver then construct joining predicate using this new name our the condition would rgbysqcol ',negative
'the list table names could contain duplicates only guarantees returning duplicate table objects one batch need ',negative
'check that compacted base dir has version file with expected value ',negative
'nonjavadoc see ',negative
'performed smb join based all the tablespartitions being joined ',negative
'this the last key need process ',negative
'initialize set unprocessed small tables ',negative
'this code has been only added for testing ',negative
'singlecolumn long specific members ',negative
'create input bytearrayref ',negative
'nonjavadoc see ',negative
'',negative
'need convert these the generated column names can see the join operator ',negative
'spot check correctness decimal scalar subtract decimal column the case for addition checks all the cases for the template dont that redundantly here ',negative
'deliver vector batch the operator tree the vectorized input file format reader has already set the partition column values reset and filled the batch etc pass the vectorizedrowbatch through here return return true the operator tree not done yet ',negative
'one the digits was the point ',negative
'dummy private constructor since this class collection static utility methods ',negative
'remember which parent belongs which tag ',negative
'introduce derived table above one child this selfjoin since user provided aliases are lost this point ',negative
'get column names and types ',negative
'math methods ',negative
'cast string ',negative
'whether the files output this filesink can merged they are put into ',negative
'reset the data ',negative
'create converters beforehand that the converters can reuse the same object for returning conversion results ',negative
'first branch should have the query with write filter conditions ',negative
'test set random divisions high precision ',negative
'nonjavadoc see ',negative
'use default values from fsdefaultname ',negative
'also check config that has default hiveconf ',negative
'initialize input ',negative
'pass lineagestate when driver instantiates another driver run compile another query ',negative
'select with grant for exporting contents ',negative
'neither select nor compaction which select wil work after this ',negative
'statistics object that combination statistics from all ',negative
'the wrapper for byte array ',negative
'overlay the confvars note that this ignores confvars with null values ',negative
'code below does not deal with the connection serverobject ',negative
'there transaction then this lock will released commitabortrollback instead ',negative
'ignore this exception actually this exception wont thrown from ',negative
'that beeline wont kill the jvm ',negative
'finally remove the role ',negative
'write the given version metastore ',negative
'test http mode with ssl properties specified url ',negative
'test getcolumns ',negative
'this the key less than the lowest key need process ',negative
'map ',negative
'predicate expression userid and subtype ',negative
'literals ',negative
'',negative
'its multiexpr filter and with previous exprs ',negative
'this the first expression ',negative
'generate vectorized expression ',negative
'force the cache clear know its empty ',negative
'udf requiring additional jars cant determine the result ',negative
'since there can multiple rounds this run all which will tied the same query generated compile phase adding additional uuid the end print each run separate files ',negative
'extract input refs they will serve input for the function invocation ',negative
'here can one states map join work null implies that have not yet traversed the big table side just need see can find reduce sink operator the big table side this would imply reduce side operation dont find reducesink has the case that map side operation have already created work item for the big table side need see can find table scan operator the big table side this would imply map side operation dont find table scan operator has reduce side operation ',negative
'max this table either the big table cannot convert ',negative
'create the lazystruct from the lazystructinspector ',negative
'the layout for acid files will always only writing delta files except iow which writes basex the buckets created filesinkoperator will look like extdeltabucket need move that into the above structure for the first mover there will delta directory can move the whole directory for everyone else will need just move the buckets under the existing delta directory ',negative
'for join identify key then can infer the join cardinality rowcountt selectivityt this like semijoin where the tfact sidefk side filtered factor based the selectivity the pkdim table side both and are keys then use the larger one the side case outer joins the side should the null preserving side doesnt make sense apply this heuristic case dim loj fact fact roj dim the selectivity factor applied the fact table should ',negative
'find the next chunk size ',negative
'the data written and read withing the same job thus should never written one version hive and read another see ',negative
'unable parse the use command ',negative
'',negative
'the table partitioned have put the partition clause ',negative
'for updatedelete always write exactly most actually the partitions read ',negative
'the can used for multiple queries this indication that single query complete dont have good mechanism know when app ends removing this right now ensures ',negative
'error already occurred but still want get the error code the child process possible ',negative
'based index which row are ',negative
'this particular sparksessionmanager implementation dont recycle returned sessions references session are still valid ',negative
'make fully qualified path for further use ',negative
'deduce resultset schema ',negative
'have valid pruning expressions only retrieve qualifying partitions ',negative
'take the pattern and split the get all the composing patterns ',negative
'just individual column ',negative
'there change here prev version had transadtional one beofre acid ',negative
'get the last operator for processing big tables ',negative
'the authorization ',negative
'based the new key count keycount smaller than threshold then just load the entire restored hashmap into memory ',negative
'this call checks under lock can actually preempt the task possible have race where the update thats also under lock makes the task finishable guaranteed between the remove and kill but its the same timing ',negative
'when true indicates that this object being read part update delete this important because that case shouldnt acquire lock for authorize the read ',negative
'make sure theres default database associated with each catalog ',negative
'add order token ',negative
'now prepare partnames with partitions tabparttabpart but with overlap ',negative
'combined all good dont combine with that but may combine with others dont combine with with that and make that base for new combines ',negative
'bgenjjtree headerlist ',negative
'the partition that has exchanged ',negative
'represents the column name corresponding distinct aggr any ',negative
'multi parents cant handle that right now not remove projection top lateralviewforward operators ',negative
'allmatchesindex allmatchesindex duplicatecount duplicatecount count count ',negative
'trimming again necessary because rounding may introduce new trailing ',negative
'test for timestamp type ',negative
'populate byte from timestamp ',negative
'get the last repl corresponding all insertaltercreate events except drop ',negative
'may used acid table ',negative
'lastaccesstime ',negative
'test for both colnames and partnames being empty ',negative
'client proxies connection use forwarded ipaddresses instead just the gateway ',negative
'for now just use this hold the object inspector there are writevalue setvalue etc methods yet ',negative
'log which resources were adding apart from the hive exec ',negative
'javadoc for statement interface states that the value zero then fetch size hint ignored this case means reverting the default value ',negative
'run exhaustive ppd add not null filters transitive inference ',negative
'properties ',negative
'since the user didnt supply customized typechecking context use default settings ',negative
'mingle existing tblproperties with those specified the alter table command ',negative
'left and right repeat ',negative
'validation the bitset size hash functions ',negative
'one less integer digit ',negative
'enabled check ',negative
'should only downcast and elimination precision within valid range ',negative
'ignore the char after escapechar ',negative
'positive test ',negative
'get the big table row bytes container for native vector map join ',negative
'need maintain the unique that target map works can read the output ',negative
'the function should support both short date and full timestamp format ',negative
'tabledesc needed for dynamic partitioned hash join ',negative
'send out the preempted request outside the lock ',negative
'only small table values appear join output result ',negative
'modulo operator with overflowzerodivide check ',negative
'now both txns concurrently updated tab but different partitions ',negative
'scale with this done via ',negative
'use list bucketing prunners path ',negative
'test extra field names ',negative
'because that point need access the objects ',negative
'',negative
'skip the row ',negative
'todo for now get the secure username out ugi after signing can take ',negative
'write the necessary config info hadoopsitexml ',negative
'for udtfs skip the function name get the expressions ',negative
'esw lock are examining shared write ',negative
'this may happen one the following case fil sel dummystore mergejoin fil sel ',negative
'destdb ',negative
'make possible for tests check that the right type was ',negative
'join operator contains big subtree there chance that its ',negative
'ptf function must provide the external names the columns its output ',negative
'add this parent the children ',negative
'record reader ',negative
'nope look see hives home dir has been explicitly set ',negative
'release all transient locks simply closing the client ',negative
'this point know that the extracted files are the intermediate extracted dir the the original directory ',negative
'you are admin you have all privileges missing privileges ',negative
'initialize unionexpr for reduceside reduce key has union field the last field there are distinct ',negative
'now really scan ',negative
'the calculation strongly dependent the assumption that all splits came from the same file ',negative
'the delimiter seen and the line isnt inside quoted string then treat lineindex single command ',negative
'all the code paths below propagate nulls even arg has nulls this reduce the number code paths and shorten the code the expense maybe doing unnecessary work neither input has nulls this could improved the future expanding the number code paths ',negative
'support the old syntax hivemetastore port but complain ',negative
'wrap the static accumuloinputformat methods with methods that can verify were correctly called via mockito ',negative
'shared plan utils for spark ',negative
'need ',negative
'use this function make the union flat for both execution and explain ',negative
'addpartitions ',negative
'guid ',negative
'match for workeridentity ',negative
'tresolver ',negative
'checks default connection configuration file present and uses connect found noop the file not present ',negative
'the implementation may may not set output isrepeting ',negative
'can ignore the check failed ',negative
'availableslots waves desired slots fill weight for particular bucket weights add ',negative
'implementation row iterator ',negative
'subquerytorelnode might null subquery expression anywhere other than expected filter wherehaving should throw appropriate error message ',negative
'can happen race condition where another process adds zlock under this parent just before about deleted should not problem since this parent can eventually deleted the process which hold its last child zlock ',negative
'given byte array consisting serialized bloomfilter gives the offset from for the start the serialized long values that make the bitset ',negative
'called many times ',negative
'sqlstate ',negative
'setup the context for reading from the next partition file ',negative
'even the state has changed dont log twice ',negative
'now process the delta files for normal read these should only delete deltas for compaction these may any deltaxy the files inside any deltaxy may acid format with acid metadata columns original ',negative
'checkcorrect codec checkcorrect codec ',negative
'else recurse the parents ',negative
'skip compaction theres delta files and theres original files ',negative
'real object ',negative
'not relevant for load ',negative
'for this may null should default which most restrictive ',negative
'been processed ',negative
'consider the query select count from group with cube where being executed single mapreduce job the plan tablescan groupby reducesink groupby filesink groupby already added the grouping part the row this function called for groupby add grouping part the groupby keys ',negative
'noarg ctor required for kyro ',negative
'todo could also vectorize some formats based llap enabled and are going run llap however dont know end llap not this stage dont this now may need add force option ',negative
'alter the tablepartition stats and also notify truncate table event ',negative
'byte ',negative
'assumed throughout the code that reducer has single child add ',negative
'reference hdfs based resource directly use distribute cache efficiently ',negative
'druids time column always not null ',negative
'find the last stripe ',negative
'this means that not need value generator ',negative
'hadoop might return null cannot locate the job may still able retrieve the job status ignore ',negative
'for runtime query may have finished ',negative
'writeentity typepartition writetypeinsert isdpfalse ',negative
'determine maximum all nonnull long column values maintain isgroupresultnull ',negative
'tests expect configuration changes applied directly metastore ',negative
'this piece code runs master node and gets necessary context ',negative
'release the background thread ',negative
'were dealing with input that array strings ',negative
'avoid allocationcopy nulls because potentially expensive branch instead ',negative
'all the versions should place this list ',negative
'drop tablepartition corresponding records txncomponents and should disappear ',negative
'for multiplicands with scale trim trailing zeroes ',negative
'constant projection ',negative
'the modification cannot affect active plan ',negative
'for each the the logical this should called seperately ',negative
'',negative
'generate reduce sink and project operator ',negative
'both are partitioned tables ',negative
'bucketing and sorting keys should exactly match ',negative
'distinct partitions modified ',negative
'could derive the expected number ams pass note pass null token here the tokens talk plugin endpoints will only known once the ams register and they are different for every unlike llap token ',negative
'start third batch abort everything dont properly close ',negative
'store credentials private hash map and not the udf context ',negative
'overlay the sasl transport top the base socket transport ssl nonssl ',negative
'files size for splits ',negative
'filter disabled injection enabled exception not expected ',negative
'only used for semijoin with residual predicates ',negative
'and udf rowid and rowid rowid ',negative
'all sources are initialized front events from different sources will end getting added the same list pruning disabled either source sends event which causes pruning skipped ',negative
'rest cases ',negative
'get the operationlog object from the operation ',negative
'first comparison unsigned ',negative
'nonjavadoc see ',negative
'test that the all columns will read default ',negative
'optional int ',negative
'',negative
'test that setting read column ids set read all columns false ',negative
'process wdw functions ',negative
'add column info for non partion cols object inspector fields ',negative
'optimizing for readfield ',negative
'create the event and send tez tez will route appropriate processor ',negative
'test readfully ',negative
'add maxlength parameter udfs that have char varchar output ',negative
'change textinputformat ',negative
'the existing entry and newer entry are subset one another ',negative
'projection mode not yet supported for not between return null vectorizer knows revert rowatatime execution ',negative
'trim the trailing zero fraction digits dont cause unnecessary precision overflow later ',negative
'function class will not cause exception ',negative
'vectorized row batch being created ',negative
'anonymous element arrayelement ',negative
'the position this table ',negative
'verify scratch dir paths and permission ',negative
'uses pattern ',negative
'properties for remote driver rpc and yarn properties for spark yarn mode ',negative
'convert mapreduce job ',negative
'the cached buffer the middle the requested range the remaining tail the latter may still available further ',negative
'all good ',negative
'leave works output may read further sparkworkfetchwork should not combine leave works without notifying further sparkworkfetchwork ',negative
'filter enabled injection disabled exception expected ',negative
'not supported for tables for now ',negative
'scanning the filesystem get file lengths ',negative
'sortbased aggregations ',negative
'our hash tables are immutable can safely reference string charvarchar etc ',negative
'this insert into statement might need add constraint check ',negative
'want remove the dpp with bigger data size ',negative
'this can only come from brute force discard for now dont discard blocks larger than the target block could discard and add remainder free lists definition are fragmented there should smaller buffer somewhere ',negative
'adjust right input fields nonequiconds previous call modified the input ',negative
'hcat input format related errors ',negative
'run hcat expression and return just the json outout ',negative
'looks much possible like original query ',negative
'continue merging with next alias ',negative
'open the log file and read line then feed the line into ',negative
'all rows from left side will present resultset ',negative
'todo periodically reload new hiveconf check stats reporting enabled ',negative
'regrettable that have wrap the hcatexception into runtimeexception but throwing the exception the appropriate result here and hasnext signature will only allow runtimeexceptions iteratorhasnext really should have allowed ioexceptions ',negative
'from insert for each update stmt ',negative
'some the information the source complete dont need fetch from the context ',negative
'back seconds ',negative
'this internalname represents constant parameter aggregation parameters ',negative
'its parents also ',negative
'test with same schema with include ',negative
'least know they are not equal the one with the larger scale has nonzero digits below the others scale since the scale does not include trailing zeroes ',negative
'authorize for the old location and new location ',negative
'get our multikey hash map information for this specialized class ',negative
'today acid tables are only orc and that format vectorizable verify these assumptions ',negative
'add the hadoop token back the job the configuration still has the necessary ',negative
'todo for ordinal types you can produce range between ',negative
'make sure the accumulo token set the configuration only stub the accumulo authentiationtoken serialized not the entire token configurejobconf may called multiple times with the same jobconf which results error from accumulo ',negative
'walk through the tree decide value example skewed column index expression tree and ',negative
'the size present the metastore use ',negative
'implicit conversion from source type target type ',negative
'this branch had already cookie that did expire therefore need resend valid kerberos challenge ',negative
'need separate stmt for executeupdate otherwise will close the resultsethive ',negative
'boolean array instead single number ',negative
'wrap around the end buffer ',negative
'this required otherwise correct work object repl load wont created ',negative
'',negative
'the last line didnt match pattern probably error message part string stack traces related the same error message add the stack trace ',negative
'map valid write ids list for all the tables read the current txn ',negative
'send the delegationtoken down the configuration for accumulo use ',negative
'for caching column stats for partitioned table ',negative
'map tasks and reduce tasks are finishable state then priority given the task the following order dag start time within dag priority attempt start time vertex parallelism ',negative
'order events dagstart and fragmentstart was guaranteed could just create the cache when dag starts and blindly return execution here ',negative
'the logjconfiguration property hasnt already been explicitly set use hives default logj configuration ',negative
'there least one record put the map ',negative
'add the entry mapredwork ',negative
'now clean them and check that they are removed from the count ',negative
'have found the colname ',negative
'its countcol case ',negative
'genericudfcase and genericudfwhen are implemented with the udf adaptor because their complexity and generality the future variations these can optimized run faster for the vectorized code path for example case col when then one when then two else other end example genericudfcase that has all constant arguments except for the first argument this probably common case and good candidate for fast specialpurpose vectorexpression then the udf adaptor code path could used catchall for nonoptimized general cases ',negative
'start column for columns array values corresponding columns ',negative
'level top level will have rexcall kept map ',negative
'create reducesink operator ',negative
'interrupt the runner thread ',negative
'union hard handle for instance the following case fil fil sel sel union join treat this case then after the removed would create two mapworks for each the each these mapwork will contain operator which wrong otherwise could try break the tree the union and create two mapworks for the branches above then will the following reducework ',negative
'the join alias modified before being inserted for consumption sortmerge join queries the join part subquery the alias modified include ',negative
'this represents subquery predicate then this will point the subquery object ',negative
'were dealing with input that array arrays strings ',negative
'cqstate ',negative
'the join keys are available the reducesinkoperators before join ',negative
'setup the table column stats ',negative
'create database specific location absolute nonqualified path ',negative
'second table union query with view parent ',negative
'base means originals longer matter ',negative
'load from modified dump event directories ',negative
'optional ',negative
'amount time thread can alive thread pool before cleaning this core threads will not cleanup from thread pool ',negative
'create expressions for project operators before and after the sort ',negative
'the map overloaded keep track mapjoins also ',negative
'not found ',negative
'',negative
'add added archives ',negative
'remove task from the pending list ',negative
'init output object inspectors ',negative
'indexes have only one entry per value could linear from here want use this for any sorted table well need continue the search ',negative
'cases where column expression map row schema missing just pass the parent column stats this could happen cases like fil where fil does not map input column names ',negative
'need new connection object well check the cache size after connection close ',negative
'future ',negative
'store the required fields information the udfcontext that ',negative
'the number reducers set user inferred ',negative
'are going validate the schema make sure dont create ',negative
'object inspectors for the tags for the input and output unionss ',negative
'start the service ',negative
'when token created the renewer the token stored shortname this seems like inconsistency because while cancelling the token uses the shortname compare the renewer while does not use shortname during token renewal use getshortusername until its fixed hadoop ',negative
'fallback mapjoin bucket scaling ',negative
'convert array typeinfo using library routine since parses the information and can handle use different separators etc cannot use the raw type string for comparison the map because the different separators used ',negative
'compute keys and values standardobjects use nonoptimized key ',negative
'this file sink desc has been processed due linked file sink desc ',negative
'have force cleanup all expired entries here because its possible that the expired entries will still counted cachesize taken from ',negative
'remove from all tables ',negative
'null just add ',negative
'bootstrap test ',negative
'the number nonnull keys they have associated hash codes and key data ',negative
'create temporary function using the jar ',negative
'query hooks that execute before compilation and after execution ',negative
'nonjavadoc see javasqldate javautilcalendar ',negative
'creates the commandline parameters for distcp ',negative
'this should not happen ',negative
'add shutdown hook cleanup the beeline for smooth exit ',negative
'insert row acid table ',negative
'the mapping that doesnt exist still shouldnt work ',negative
'have eager evaluators anywhere below then are eager too ',negative
'test select rootcol from ',negative
'return short string with the parameters the vector expression that will shown explain output etc ',negative
'find one session dir remove ',negative
'driverruninsert overwrite table orc select from inpy ',negative
'set progress bar completed when hive query execution has completed ',negative
'warn user could get stats for required columns ',negative
'skip the name and metadata ',negative
'expected exception remote metastore ',negative
'ownername ',negative
'',negative
'error out ',negative
'make sure row the output ',negative
'instance ',negative
'the cleaner will removed aborted txns datametadata but cannot remove aborted txn from txntowriteid there open txn aborted txn the aborted txn open txn and will removed also committed txn open txn retained ',negative
'cancelled the job request and return client ',negative
'the current element the current element ',negative
'loading the extra configuration options ',negative
'case the query served hiveserver dont pad with spaces hiveserver output consumed jdbcodbc clients ',negative
'non nulls ',negative
'noop ',negative
'nothing copy and cache ',negative
'restrictionh subquery predicates can appear only top level conjuncts ',negative
'the hive configs are received from with clause repl load repl status commands ',negative
'unknown unknown ',negative
'restore the old path information back this just prevent incompatibilities with previous versions hive ',negative
'set dynamic partitioning nonstrict that queries not need any partition references todo this may perf issue prevents the optimizer not ',negative
'level create gbr all keys vcol count for each ',negative
'the task the cache noop ',negative
'may null ',negative
'even there not data ',negative
'initialize the conversion related arrays assumes inittoplevelfield has already been called ',negative
'store char and varchar without pads write with string ',negative
'the fastbigintegerbytes method returns bit byte words and possible sign byte ',negative
'send state update for vertex completion this triggers status update sent out ',negative
'the join outputs concatenation all the inputs ',negative
'return positive modulo ',negative
'otherwise handle like normal generic udf ',negative
'regression test for defect reported hive ',negative
'assume the archive the original dir check exists ',negative
'',negative
'tests may leave this unitialized better set ',negative
'config name used find the maximum time job request can executed ',negative
'join ',negative
'cast string group string varchar string etc ',negative
'replace expression ',negative
'values ',negative
'replica memory ',negative
'',negative
'note there expectation that all fields will readthru ',negative
'the mapjoin has the same schema the join operator ',negative
'for thread safety allocate private writable objects for our use only ',negative
'return true the partition bucketedsorted the specified positions the number buckets the sort order should also match along with the ',negative
'double not between ',negative
'small table ',negative
'',negative
'minor comp ignore base files all deletes end first since ',negative
'optional string hivequeryid ',negative
'update expectedentries based factor and minentries configurations ',negative
'used convert decimal ',negative
'write the mutation ',negative
'preliminary checks the version the code these used done via another walk here done inline ',negative
'get the job request time out value this configuration value set then job request will wait until finishes ',negative
'since basex doesnt have suffix neither does pre acid write ',negative
'now that bootstrap has dumped all objects related have account for the changes that occurred while bootstrap was happening have look through all events during the bootstrap period and consolidate them with our dump ',negative
'populate the filters and filtermap structure needed the join descriptor ',negative
'template classname valuetype ifdefined ',negative
'user specified row limit set the query ',negative
'nobody can see this exception the threadpool just log ',negative
'reached endoffile ',negative
'the rpc library takes care timing out this ',negative
'add the queue only the first time this registered and ',negative
'link always used for vectorized reads acid tables some cases this cannot used from llap elevator because link not currently available there but required generate rowids for original files param hasdeletes there are any deletes that apply this split todo hive ',negative
'mark this small table being processed ',negative
'specified ',negative
'some columns from tables are not used ',negative
'the previous character isnt escape characters its the separator ',negative
'check that data has moved ',negative
'all children are base classes ',negative
'its either count count case ',negative
'assigning higher priority than filesystem shutdown hook that streaming connection gets closed first before filesystem close avoid ',negative
'break encountered union ',negative
'tostandardduration assumes days are always and hours are always minutes which may not always the case there are daylight saving changes ',negative
'only refresh once ',negative
'password must present ',negative
'create the nondeferred realargument ',negative
'the mapjoin has already been handled ',negative
'there should delta dir plus base dir the location ',negative
'set the table properties ',negative
'single long value hash map based the serialize the long key into binarysortable format into output buffer accepted ',negative
'newerclass ',negative
'check there already exists semijoin branch ',negative
'the row key column becomes string ',negative
'now try the table owner and see get better luck ',negative
'bail out ungracefully should never hit this here but would have hit semanticanalyzer ',negative
'already processed see backtracking ',negative
'privileges ',negative
'decrement batch size when this gets the batch will executed ',negative
'handle remaining middle long word digits ',negative
'test lazymap with bad entries empty key empty entries where and dont exist only for notation purpose stx with value entry separator etx with keyvalue separator ',negative
'toksubqueryexpr should have either children ',negative
'set memory threshold memory used after ',negative
'test with predicates such that partition pruning doesnt kick ',negative
'here means the last branch the multiinsert cardinality validation ',negative
'test set random subtracts high precision ',negative
'need keep predicate kind equal not equal that later while decorrelating logicalcorrelate appropriate join predicate generated ',negative
'infer pkfk relationship single attribute join case ',negative
'initialize pathtotableinfo ',negative
'add the property only exists table properties ',negative
'ambiguous call two methods with the same number implicit ',negative
'extract the information necessary create the predicate for the ',negative
'map tasks and reduce tasks are finishable state then priority given the task that has less number pending tasks shortest job ',negative
'dont push down any expressions that refer aliases that cant pushed down per getqualifiedaliases ',negative
'serde may not have this optional annotation defined which case conservative and say conversion needed ',negative
'all can handle limitoperator filteroperator selectoperator and final for nonaggressive mode minimal sampling not allowed for partitioned table all filters should targeted partition column ',negative
'backtrack sel columns prs ',negative
'remove the biggest key ',negative
'transient members initialized transientinit method temporary location for building number string ',negative
'free output columns inputs have nonleaf expression trees ',negative
'floor date operator need rewrite ',negative
'check null cols schemas for partition ',negative
'column the record identifier indicates record unique within transaction ',negative
'invalid paths ',negative
'load partition that doesnt exist ',negative
'the perbatch setup for inner bigonly join ',negative
'all partitions should miss target was marked virtually dropped ',negative
'need either move tmp files remove them ',negative
'todo nothing else should done for this task move ',negative
'drop connection without calling close hms thread deletecontext ',negative
'map key will list typeinfo isescaped escapechar ',negative
'note this could made more generic may common problem for the endpoints that can move around dynamically for now only handle this for the update ',negative
'convert from node ',negative
'since left integer always some products here are not included ',negative
'create hiveconf again run initialization code see value changes ',negative
'then continue use this perf logger ',negative
'process map joins with reducers pattern ',negative
'logj configuration file found successfully use hiveconf property value ',negative
'conflicts module configuration what will used weve already verified that includes and excludes are not present the same time for individual modules ',negative
'should not happen edit check false ',negative
'fail remove its probably internal error wed try handle the same way above restarting the session wed fail the caller avoid exceeding parallelism ',negative
'this point were dealing with all return types except ',negative
'ignore errors cleaning minimr ',negative
'dont create extra the metastore that has references ',negative
'entries should lru order the keyset iterator ',negative
'check mapreduce job needed merge the files the current size smaller than the target merge ',negative
'not much can about here ',negative
'generate enough delta files that initiator can trigger auto compaction ',negative
'append the separator needed ',negative
'test regular inputformat ',negative
'loginfoallocated alloccount size adebugdump ',negative
'now multiply and add sign bit ',negative
'struct ',negative
'shouldve done several heartbeats ',negative
'fall through ',negative
'the output column projection the vectorized row batch and the type infos the output ',negative
'the createtime will set the server side the comparison should skip ',negative
'txnhighwatermark ',negative
'keyseq ',negative
'specific test for hive tests timestamp assignment ',negative
'create only neededincluded columns data columns ',negative
'recurse over all the source tables ',negative
'there prefix then dont cut anything ',negative
'look for databases but not find any ',negative
'set codahale enabled cannot tag the values just prefix them for the jmx view ',negative
'cached buffer has the same lower offset the requested buffer ',negative
'each these should fail ',negative
'',negative
'now metastore connection should fail ',negative
'use the default separators ',negative
'later ',negative
'tablename either tablename dbnametablename given ',negative
'now have lock ',negative
'windowframe specifies the range which window function should applied for the current row its specified istarti and iendi boundary ',negative
'convert integer value representing timestamp nanoseconds one that represents timestamp seconds since the epoch ',negative
'pairwise columnhasnulls columnisrepeating ',negative
'major compact create base that has acid schema ',negative
'the seed port ',negative
'blank byte new tai lue letter low bytes ',negative
'the threshold percent then there throttling ',negative
'its not column table alias ',negative
'genericudf ',negative
'for some attempts check inheritance ',negative
'verify table for key long hash table hashmap ',negative
'assert that the table created still has hcat instrumentation ',negative
'todo call setremoteuser higher ',negative
'the data that need for this ',negative
'vectorized expression that selects rows ',negative
'check that the agg the entire input ',negative
'want abx come from finite field size where prime number prime for hence bitvectorsize has pick abx didnt come from finite field mod and mod will not pair wise independent consequence the hash values will not distribute uniformly from thus introducing errors the estimates ',negative
'noop this needed able instantiate the class from the name ',negative
'schemaname ',negative
'the corresponding reducesinkoperator ',negative
'find the highest failure count ',negative
'always use index the write methods dont write separator ',negative
'look top bits and return appropriate enum ',negative
'validate resultset columns ',negative
'init fails but the session also killed before that ',negative
'all the agg expressions are distinct and have the same ',negative
'the path the tracking root ',negative
'set the escape ',negative
'get the table names out ',negative
'look for under the old hive name ',negative
'left pad longer strings with multibyte characters ',negative
'find this parentcolname its parents ',negative
'all the vectors have the same length ',negative
'before making changes copypasting these ',negative
'second time will complete silently ',negative
'whether there are more than rows ',negative
'the partition doesnt qualify the global limit optimization for some reason ',negative
'fail early the columns specified for column statistics are not valid ',negative
'normalize the case for source replication parameter ',negative
'later ',negative
'clip off one byte and expect get eofexception the write field ',negative
'test second argument repeating ',negative
'end sync stuff ',negative
'get the new nextkvreader with lowest key ',negative
'from this point session creation will wait for the default pool sessions ',negative
'sort does not contain limit operation limit bail out ',negative
'test altering the table ',negative
'second branch should only have the ',negative
'total characters byte length ',negative
'this for transactional tables ',negative
'drop the partitions and get list locations which need deleted ',negative
'while are the process setting valid ',negative
'issue command with bad options ',negative
'given the data partition evaluate the result for the next row for streaming and batch mode ',negative
'interpolation needed because lower position and higher position has the same key ',negative
'comment for reviewers updatetabcols needed separate from tabcols because pass tabcols gethiveprivobjects for the output case will trip insertselects since the insert will get passed the columns from the select ',negative
'response true ',negative
'this the vectorized row batch description the output the native vectorized ptf operator based the incoming vectorization context its projection may include ',negative
'nothing updated yet ',negative
'clear the mask for array reuse this avoid masks array allocation inner loop ',negative
'ptf node form tokptblfunction name alias partitioningspec expression guaranteed have alias here check done processjoin ',negative
'that something didnt expect itd more likely fail ',negative
'run the operator pipeline ',negative
'optional bytes workspecsignature ',negative
'and metadata gets created ',negative
'all keys vcol ',negative
'todo longer term should pass this from client somehow this would optimization once that place make sure build and test writeset below using operationtype not locktype with static partitions assume that the query modifies exactly the partitions locked not entirely realistic since updatedelete may have some predicate that filters out all records out some partitions but plausible for acquire locks very wide all known partitions but for most queries only fraction will actually updated tells exactly which ones were written thus using this trick kill query early for queries may too restrictive ',negative
'not put the tab for the last column ',negative
'small value bytes ',negative
'between has args here but can vectorized like this ',negative
'filters are using index which should match rows ',negative
'this column not appearing keyexprs the ',negative
'flush the last record when reader out records ',negative
'rename based output schema join operator ',negative
'only dealing with special join types here ',negative
'the will look for there ',negative
'case dynamic partitioning lock the table ',negative
'update max executors now that cluster info definitely available ',negative
'current replication state must set the partition object only for bootstrap dump event replication state will null case bootstrap dump ',negative
'propagate nulls ',negative
'adjust the compression block position ',negative
'jobclose needs execute successfully otherwise fail task ',negative
'stats delete forgivable error ',negative
'step create temp table object ',negative
'execute optimization ',negative
'tests single threaded implementation checkmetastore ',negative
'rename the data directory ',negative
'owner like owner and owner like test ',negative
'there are union all operators means that the walking context contains union all operators please see more details gentezworkwalker ',negative
'base javaobject primitives javafieldref javaarray entry javaobject javafieldref primitives ',negative
'for now dont higher than the default batch size unless more work verify every vectorized operator downstream can handle larger batch size ',negative
'rewriting cannot performed ',negative
'this matches the list structure that hive writes ',negative
'this sets dependencies such that child task dependant the parent complete ',negative
'hashmap javafieldref primitives hashmapentry javafieldref ',negative
'are secure mode login using keytab ',negative
'join key exprs are represented terms the original table columns ',negative
'concatenate keeping the old logic for nonmm tables with temp directories and stuff ',negative
'infer uniquenes rowcountcol ndvcol tbd for numerics maxcol mincol rowcountcol why are intercepting project and not tablescan because have method for tablescan will not know which columns check for inferring uniqueness for all columns very expensive right now the flip side doing this only works post field trimming ',negative
'there nothing project are projecting everything then need introduce another relnode ',negative
'make sure has chance dump ',negative
'minimum value seen far ',negative
'vectorized because there inputfilename ',negative
'this point tablepath part hdfs and encrypted ',negative
'files size for splits ',negative
'total characters byte length ',negative
'for any exception conversion decimal produce null ',negative
'localjobrunner does not work with mapreduce outputcommitter need ',negative
'accumulo instance name with quorum ',negative
'pad with empty rows the number values group less than top num ',negative
'default all columns that are not metrics timestamp are treated dimensions ',negative
'calculate the length the utf strings input vector and place results output vector ',negative
'daemon ',negative
'analyze and process the position alias step out ',negative
'txnid ',negative
'check common conditions for both optimized and fast hash tables ',negative
'wants create new base for iow instead delta dir should specify here ',negative
'clear all memory partitions first ',negative
'check predicate needed predicate needed either input pruning not enough input pruning not possible ',negative
'now will return true ',negative
'exhausted reading all records close the reader ',negative
'pass the message the user essentially something about the table information passed hcatoutputformat was not right ',negative
'the delegation token not applicable the given deployment mode such hms not kerberos secured ',negative
'attach this sel the operator right before ',negative
'create backtrack selectop ',negative
'want send the heartbeat interval that less than the timeout ',negative
'for negative tests which succeeded need print the query string ',negative
'virtual columns start after the last partition column ',negative
'get the the spark job that was launched returns spark job was launched ',negative
'this check case the decrypted plaintext actually makes sense some way ',negative
'not map reduce task not conditional task just skip ',negative
'position not field name field names arent guaranteed the order fields recordidentifier writeid bucketid rowid ',negative
'check column defined not ',negative
'transition success state ',negative
'mystringset ',negative
'check filter condition type first extract the correlation out the filter ',negative
'test that existing sharedwrite with new sharedread coalesces ',negative
'payload ',negative
'create the index table does not exist ',negative
'netty defaults processors can changed via ',negative
'cannot just deallocate the buffer can hypothetically have users ',negative
'just give each table the same amount memory ',negative
'remove the lock specified ',negative
'set some values use for getting conf vars ',negative
'there should blocking operation recordprocessor creation otherwise the abort operation will not register since they are synchronized the same ',negative
'empty list ',negative
'partitions archived before introducing multiple archiving ',negative
'hivetxntimeout defined heartbeat interval will hivetxntimeout ',negative
'list maintain the incremental dumps for each operation ',negative
'the app state running get additional information from yarn service ',negative
'use lowercase table name prefix here statstask get table name from metastore fetch counter ',negative
'nothing got modified ',negative
'set the root the temporary path where dynamic partition columns will populate ',negative
'port ',negative
'can safely remove the condition replacing with true ',negative
'will increase the size the array demand ',negative
'bgenjjtree namespace ',negative
'any the table requests are null then need pull all the ',negative
'test string ',negative
'friday august ',negative
'serialize densesparse registers dense registers are bitpacked whereas ',negative
'short ',negative
'mergeisdirectflag need merge isdirect flag even newinput does not have parent ',negative
'reference the current row ',negative
'adding mssql jdbc driver exists ',negative
'files size for splits ',negative
'adjust all longs using power divisionremainder ',negative
'multikey hash multiset optimized for vector map join the key stored the provided bytes uninterpreted ',negative
'out ',negative
'buffer needed bridge ',negative
'this handles the common logic for destroy and return everything except the invalid combination destroy and return themselves well the actual statement that destroys returns ',negative
'see someone else evicted this parallel ',negative
'null ',negative
'verify vectorized expression ',negative
'should the cache size updated here after the result data has actually been deleted ',negative
'test that exclusive lock blocks shared reads ',negative
'bug the field has not sec override only set precisely sec ',negative
'now there are more than sources then have join case ',negative
'have use this ifelse since switchcase string supported java onwards ',negative
'for static partitions values would obtained from partitionkeyvalue clause ',negative
'ptf node form tokptblfunction name alias partitioningspec expression guranteed have lias here check done processjoin ',negative
'gce settings ',negative
'clear out the mapjoin set dont need anymore ',negative
'located the same position the input newproject ',negative
'primarily for debugging purposes atm since theres some unexplained tasktimeouts which are currently being observed ',negative
'make more explicit below that processhooks needs called last ',negative
'join positions for even index filter lengths for odd index ',negative
'fourth lower priority result canfinish being set false ',negative
'',negative
'fix the query for views ',negative
'dont use the username member may may not have been set get the value from conf which calls into getugi figure out who the process running ',negative
'use construct ',negative
'validate and setup resultexprstr ',negative
'change ',negative
'date str ',negative
'heartbeat indicates task has duck this must reverted ',negative
'have failed before building newcachedata deallocate other the allocated ',negative
'test that two different partitions dont collide their locks ',negative
'reset the previously stored rootnode string ',negative
'noscan uses hdfs apis retrieve such information from namenode ',negative
'create inline sql operator ',negative
'number columns the aliases does not match with number columns generated the lateral view ',negative
'add column expression for bloom filter ',negative
'this hashtable stores references array longs index the array hash the key these references point into infinite byte buffer see below this buffer contains records written one after another there are several simple record formats single record for the key key bytesvalue bytesvlong value lengthvlong key lengthpadding leave padding ensure have least bytes after key and value first multiple records for the key updated from single value for the key key bytesvalue bytesbyte long offset list start record list start record vlong value lengthvlong key lengthbyte long offset the list record lengths are preserved from the first record offset discussed above subsequent values the list value bytesvalue lengthvlong relative offset next record summary because have separate list record have very little list overhead for the typical case primary key join where theres list for any key large lists also dont have lot relative overhead also see the todo below the record looks follows for one value per key hash fixed bytes and stored expand rehashing and more efficiently deal with collision key hash refs offset wbs hash key val vlkl after that refs dont change they are not pictured when add the value rewrite lengths with relative offset the list start record that way the first record points the list record ref wbs hash key val offset vlkl after that refs dont change they are not pictured list record points the value ref wbs hash key val offset vlkloffset val add another value overwrite the list record dont need overwrite any vlongs and suffer because that ref wbs hash key val offset vlkloffset val val vloffset and another value for example vlkloffset val val vloffset val vloffset ',negative
'this loop fills the selected vector with all the index positions that are selected ',negative
'indicate that the query will use cached result ',negative
'hive try find the fake merge work for smb join that really another mapwork ',negative
'union ',negative
'shouldl return nulls end ',negative
'note not testing table rename because table rename replication not supported for tablelevel repl ',negative
'use separate method make easier create saslhandler without having ',negative
'aggregation buffer definition and manipulation methods ',negative
'verify number digits and each number has more digits ',negative
'current dest has distinct keys ',negative
'for tez used remember which position maps which logical input ',negative
'gather operators that belong root works works containing operators and share the same input operator these will the first target for extended shared work optimization ',negative
'include column names from serde the partition and virtual columns ',negative
'are checking the desired action ',negative
'verify the serialized format for dtype ',negative
'muxdesc only generated from corresponding reducesinkdesc ',negative
'cancel again current thread also interrupted ',negative
'set the parameters ',negative
'create drop priv requirement there the owner should have write permission ',negative
'the special case zero logic the beginning should have caught this ',negative
'inverse populated runtime ',negative
'nonjavadoc see ',negative
'update lru ',negative
'unionentry ',negative
'this helpful when sqoop installed each node the cluster make sure relevant jars jdbc particular are present the node running the command ',negative
'shared between threads including sessionstate ',negative
'used for rehashing get last set values current array size have minimum fill factor ',negative
'there should delta dir plus base dir the location steve ',negative
'privileges filter ',negative
'into originalversion ',negative
'check hiveserver config gets loaded when started ',negative
'single byte array value hash map optimized for vector map join ',negative
'normal case evict the items from the list ',negative
'special cases for avro with orc make table properties that avro interested available jobconf runtime ',negative
'esr lock are examining shared read ',negative
'write the output temporary directory and move the final location the end ',negative
'there hint but none the operators removed throw error ',negative
'nonacid case then all files would the base data path just return ',negative
'sort the queue may have put items here out order ',negative
'nonjavadoc see ',negative
'work can still null there merge work for this input ',negative
'this key evaluator translates from the vectorized format ',negative
'using parseinto avoids throwing exception when parsing ',negative
'iterate thru the file cache this besteffort ',negative
'get column names and sort order ',negative
'skip the column since wont have vector after reading the text source ',negative
'add ',negative
'decimal integer conversion ',negative
'getters ',negative
'nothing not mapreduce task ',negative
'reduce sink needed the query contains cluster distribute ',negative
'these are used cache previously created string ',negative
'optional processordescriptor ',negative
'cost transferring map outputs join operator ',negative
'the case tablesample the input paths are pointing files rather than directories need get the parent directory the filtering path that all files the same parent directory will grouped into one pool but not files from different parent directories this guarantees that split will combine all files the same partition but wont cross multiple partitions the user has asked path not directory ',negative
'skip command which readonly older postgres versions like see ',negative
'write string itself ',negative
'template classnameprefix returntype operandtype funcname operandcast ',negative
'now for each path that for the given versionnumber delete the znode from zookeeper ',negative
'for caching database objects key database name ',negative
'command should redacted avoid logging sensitive data ',negative
'replace all reducesinkoperators which are not the bottom ',negative
'tested ',negative
'user has told run local mode doesnt want autolocal mode ',negative
'set the new value the output string variable ',negative
'getallurls will parse zkjdbcurl and will plugin the active hss hostport ',negative
'now test tblproperties specified alter table compact statement ',negative
'must tezwork ',negative
'look for matches vertex specific counters ',negative
'stage ',negative
'need semishared ',negative
'create dummy mapreduce task ',negative
'singlecolumn string specific imports ',negative
'note that regular serde doesnt tolerate fewer columns ',negative
'build row type from field type name ',negative
'max number nodes when converting cnf ',negative
'check entries beyond first one ',negative
'need override this one too droptable breaks because doesnt find the table when checks ',negative
'hivedecimal number double ',negative
'note both fullacid and insertonly sinks ',negative
'trim off the ending any ',negative
'loginfogot ',negative
'test testing that the filters introduced eventutils are working correctly ',negative
'stage ',negative
'the startstop row conditions hbase ',negative
'using previously cached result ',negative
'size bigtable ',negative
'calculate typeinfo ',negative
'need not perform this optimization and should bail out ',negative
'for queries using script the optimization cannot applied without users confirmation ',negative
'the materialization was created otherwise query returns rows ',negative
'output from the script ',negative
'convert the mapjoin bucketized mapjoin ',negative
'check session files are removed ',negative
'check that hook disable transforms has been added ',negative
'and later way finding sasl property ',negative
'pass job initialize metastore conf overrides for embedded metastore case hivemetastoreuris ',negative
'table null either these then they are claiming lock the whole database and need check otherwise ',negative
'ignore all the other events logged above ',negative
'default handles this for both pool and nonpool session ',negative
'add the hadoop token the jobconf ',negative
'overlord and coordinator both run same jvm ',negative
'also dependent the udfexampleadd class within that jar ',negative
'child not rexcall instance can bail out ',negative
'xxx this could easily become hotspot ',negative
'nonempty java opts with xmx specified ',negative
'the following code should gone after hive using topological order ',negative
'set load server conf booleans false ',negative
'these represent the sorted columns ',negative
'presence grouping sets cant remove sqcountcheck ',negative
'expr built ddlsa should only contain part cols and simple ops ',negative
'now try find the file based sha and name currently require exact name match could also allow cutting off versions and other stuff provided that ',negative
'process hosts from which doas requests are authorized ',negative
'for now ',negative
'multithreaded file statuses and split strategy ',negative
'this should now fine since increased the configured header size ',negative
'bytes key hash map optimized for vector map join this the abstract base for the multikey and string bytes key hash map implementations ',negative
'only select operators among the allowed operators can cause changes the column names ',negative
'future this may examine writeentity andor config return appropriate hcatwriter ',negative
'get the next byte ',negative
'there integer room above ',negative
'load multiple random sets long values ',negative
'build reloptabstracttable ',negative
'mockresultset ',negative
'note that while this improvement over static initialization still not technically valid cause nothing prevents from connecting several metastores ',negative
'add this the list top operators always start from table ',negative
'',negative
'verify that drops were replicated this can either from tables ptns not existing and thus throwing returning nulls select returning empty depending what were testing ',negative
'mapside aggregation should reduce the entries atleast half ',negative
'this partitions set columns the same the parent tables use the parent tables not create duplicate column descriptor thereby saving space ',negative
'order forwarded ips per xforwardedfor http spec client proxy proxy ',negative
'special cases for orc need check table properties see couple parameters such compression parameters are defined they are then copy them job properties that will available jobconf runtime see hive for details ',negative
'construct using ',negative
'although instance objectstore accessed one thread there may many threads with objectstore instances the static variables pmf and prop need protected with locks ',negative
'set insiderview that can skip the column authorization for this ',negative
'check the location the result partition should located the destination table ',negative
'entries were added via conf initiate our defaults ',negative
'sql standard return null for zero one elements ',negative
'this method invoked for unqualified column references join conditions this passed the alias operator mapping the queryblock far try resolve the unqualified column against each the operator row resolvers the column present only one rowresolver treat this reference that operator the column resolves with more than one rowresolver treat ambiguous reference the column doesnt resolve with any rowresolver treat this invalid reference ',negative
'verify more invocations case success ',negative
'the total size local tables may not under the limit after merge mapjoinlocalwork and childlocalwork not merge ',negative
'this means that the derived colalias collides with existing ones ',negative
'add the left hand side the clause which contains the struct definition ',negative
'this can relaxed the future there requirement ',negative
'return true this custom udf custom genericudf ',negative
'scalarcolumn ',negative
'trigger transformation ',negative
'then nonfinishable must always precede finishable ',negative
'required build against reporter and statusreporter ',negative
'insert another row newlyconverted acid table ',negative
'lookup long the hash map param key the long key param hashmapresult the object receive small table values information match for spill has information where spill the big table row return whether the lookup was match match spilled the partition with the key currently spilled ',negative
'lets see this constant was folded because optimization ',negative
'now column can have alternate mapping this captures the alternate mapping the primaryfirst mapping still only held invrslvmap ',negative
'the input table bucketed choose the first bucket ',negative
'create empty database load ',negative
'validation has limited size ',negative
'entry and entries per partition ',negative
'since skew join optimization makes copy the tree above joins and there multiquery optimization place let not use skew join optimizations for now ',negative
'for now assume partition may have files perhaps better count them ',negative
'verify table for key byte hash table hashset ',negative
'handle default case for isrepeating setting for output this will set true later the special cases where that necessary ',negative
'used serialization only ',negative
'create and backtrack select operator top ',negative
'explicit pool specification invalid theres mapping that matches ',negative
'convert the lower case because events will have these names lower case ',negative
'part lowest word survives ',negative
'get the key and value ',negative
'test when second argument has nulls and repeats ',negative
'otherwise for each child run this method recursively ',negative
'drop table and check that trash works ',negative
'ordered columns are the output columns ',negative
'far have all the data from the beginning the part ',negative
'not all messages are parametrized even those that should have been link invalidtable invalidtable usually used with link getmsgstring this method can also used with invalidtable and the like and will match getmsgstring behavior another example link invalidpartition ideally you want the message have parameters one for partition name one for table name since this already defined any parameters one can still call code table name this way the message text will slightly different but least the errorcode will match note this should not abused adding anything other than what should have been parameter names keep msg text standardized ',negative
'there should residual since already negotiated that earlier however with pushes the original filter back down again since pusheddown filters are not omitted the higher levels and thus the contract negotiation ignored anyway just ignore the residuals reassess this when negotiation honored and the duplicate evaluation removed this ignores residual parsing from ',negative
'this the job tracker url ',negative
'finally write out the pieces sign power digits ',negative
'captured function ',negative
'parse the rewritten query string ',negative
'nonjavadoc this provides lazyboolean like class which can initialized from data stored binary format see int int ',negative
'only one bucket file ',negative
'important remove after unlock case fails ',negative
'theres buffer and another move reserving this ',negative
'skip the column that the root structure ',negative
'find out number partitions for each small table should same across tables ',negative
'double ',negative
'add the expressions that correspond the aggregation ',negative
'binary join nway join first biggest small table unconditionally create hashmap for the first hash partition ',negative
'sets the job state failed returns true failed status set otherwise returns false ',negative
'finally add project project out the column ',negative
'column stats hivecolstats might not the same order the columns reorder hivecolstats can build hivecolstatsmap using below ',negative
'required string fragmentid ',negative
'make sure the partitions directory not hdfs ',negative
'primary entry point factory method instead ctor ',negative
'lock states ',negative
'causing not sort the entire table due not knowing how selective the filter ',negative
'theres pending fragments queue some the cleanup for later point locks log rolling ',negative
'deep copy new mapred work ',negative
'sharedread ',negative
'add another via objectstore ',negative
'tinyint ',negative
'now try with hivehost principal ',negative
'primitive fields refs above with alignment ',negative
'projrel ',negative
'something happened and were not able rename the temp file attempt remove ',negative
'try fold constant expression ',negative
'one second before and after ',negative
'strings test ',negative
'add list running jobs kill case abnormal shutdown ',negative
'indicates this instance beeline running compatibility mode beeline mode ',negative
'dirs ',negative
'parse numrecords integer ',negative
'type timestamp minus type timestamp produces ',negative
'close the reader for this entry ',negative
'above returned null then the does not exist probably drop database exists clause dont try authorize then ',negative
'get rexnodes for original projections from below ',negative
'dropping expressions ',negative
'not call endgroup operators below because are batching rows output batch and the semantics will not work superendgroup ',negative
'nonjavadoc see ',negative
'column pruner ',negative
'system defaults usually replica disk ',negative
'the username not already available the url add the one provided ',negative
'throw away lowest word ',negative
'the accessed columns query ',negative
'try the repeating case ',negative
'save the original selected vector ',negative
'assume including everything means the vrb will have everything todo this rather brittle esp view schema evolution abstract not currently implemented hive the compile should supply the columns expects see which not all any schema vrb row cvs the right mechanism for that who knows perhaps resolve schema evolution ',negative
'queryid ',negative
'make sure can actually get session still parallelismetc should not affected ',negative
'test lastcolumntakesrest ',negative
'generate floatingpoint number that represents the exponent this processing the exponent one bit time combine many powers then combine the exponent with the fraction ',negative
'the poolsized list being fully drained ',negative
'remove the pwd from conf file that job tracker doesnt show this ',negative
'cant eliminate stripes there are deltas because the deltas may change the rows making them match the predicate todo see hive ',negative
'validate the first parameter which the expression compute over this should array strings type array arrays strings ',negative
'sort the files ',negative
'patch the optimized query back into original from clause ',negative
'uses default ',negative
'store the partitions that are currently being processed ',negative
'create initialize and test the serde ',negative
'allow latter ',negative
'map dbnametblname tsoperator ',negative
'union extra fields ',negative
'increased visibility this method only for providing better test coverage ',negative
'check that the directory created ',negative
'for the udtf operator ',negative
'create formatter that includes all the input patterns ',negative
'files size for splits ',negative
'additional bits pad long array block size ',negative
'setup list conf vars that are not allowed change runtime ',negative
'can continue ',negative
'actually available for execution and will not potentially result rejectedexecution ',negative
'for use wildcard pattern test like ',negative
'there materialized view update desc create introduce the end the tree ',negative
'add user metadata footer case any ',negative
'the path used above should not used second try each dump request written unique location however want keep the dump location clean might want delete the paths ',negative
'larger allocations will specialcased and will not use the normal buffer buffernextfree will set newly allocated array just for the current row ',negative
'used the joinas field access ',negative
'transform the operator tree ',negative
'for partial and complete objectinspectors for original data ',negative
'difference from standard file appender locking not supported and buffering cannot switched off ',negative
'check and transform group this will only happen for select distinct here the genselectplan being leveraged the main benefits are remove virtual columns that should not included the group add the fully qualified column names unparsetranslator that view supported the drawback that additional sel added not necessary will removed optimizer because will match ',negative
'remove all the entries from the parameters which are added repl tasks internally ',negative
'nothing process ',negative
'the heap over the keys storing indexes the array ',negative
'this before next update that deltedelta properly sorted ',negative
'set genericudfs which require need implicit type casting decimal parameters vectorization for mathmatical functions currently depends decimal params automatically being converted the return type see which not correct ',negative
'start overlapping txn ',negative
'child the optional constraint ',negative
'found files under subdirectory under table base path possible that the table empty and hence there are partition subdirectories created under base path ',negative
'check all elements are contained ',negative
'same but repeating input not null ',negative
'the vectorized input file format reader responsible for setting the partition column values resetting and filling the batch etc ',negative
'here then must compacting ',negative
'insert the records simulate hive table ',negative
'update the memory monitor info for llap ',negative
'looking for map reduce ',negative
'ascii string ',negative
'give evaluator chance setup for output execution setup output shape ',negative
'frac ',negative
'schema ',negative
'',negative
'attempts make connection using default connection config file available there connection not made return false ',negative
'tracks running fragments and completing fragments completing since have race the being notified and the task actually ',negative
'pass false for canretainbyteref since will not keeping byte references the input bytes with the method ',negative
'user specified perms invalid format ',negative
'configurations are always published servicerecord readapply configs jdbc connection params ',negative
'always remove the condition replacing with true ',negative
'couldnt find proper parent column expr ',negative
'returns the number children the stack the current node scope ',negative
'since local mode only runs with reducers make sure that the ',negative
'introduce and before the operator tree already contains ',negative
'short ',negative
'create rexsubquery node ',negative
'note the vectorgroupbydesc has already been allocated and will updated here ',negative
'execute the driver locally ',negative
'elemoi ',negative
'override ',negative
'dont combine inputformat ',negative
'tez merge file work will become tez merge file ',negative
'chain pattern ',negative
'make sure big table bytescolumnvectors have room for string values the overflow batch ',negative
'this necessary sometimes semantic analyzers mapping different than operators own alias ',negative
'for tokjoin and tokfullouterjoin ',negative
'keep record all the input path for this alias ',negative
'session vars ',negative
'whether any the node outputs unknown whether all the node outputs are divided ',negative
'member variables ',negative
'start exclusive infinity inclusive ',negative
'traverse the given node find all correlated variables note that correlated variables are supported filter only where having ',negative
'initialize the first value the delete reader ',negative
'the first split for base ',negative
'set the regular provided qualifier names ',negative
'this file unknown metastore ',negative
'create map join task and set big table ',negative
'filter out the deleted records ',negative
'infer column stats state ',negative
'look for functions with empty pattern ',negative
'whether any acid table insertonly table involved query ',negative
'lifted from but supports offset ',negative
'see marker class comment ',negative
'for these input with exponents have this point intermediate decimal exponent power and result intermediate input decimal exponent result scale scale scale scale scale scale scale scale scale scale scale scale ',negative
'sum all nonnull decimal column values maintain isgroupresultnull ',negative
'optional string astring ',negative
'any condition contains nonns nonns ',negative
'close will move the temp files into the right place for the fetch task the job has failed will clean the files ',negative
'set other parameters ',negative
'build the value reference word will return that will kept the caller ',negative
'reset defaults ',negative
'any the children contains null then return null ',negative
'note currently read variations only apply top level data types ',negative
'invalidwriteids ',negative
'firstname sue ',negative
'able acquire the lock and finish successfully ',negative
'add shutdown hook ',negative
'needs bit words ',negative
'table sampled some situation really can leverage row limit order safe not use now ',negative
'this transformation needs first because changes the work item itself ',negative
'this batch used vectorrow deserializer readers ',negative
'futureproofing the parser will actually not allow this ',negative
'cleanup just case something left over from previous run ',negative
'',negative
'range starts here ',negative
'case this has not been initialized elsewhere ',negative
'test for double type ',negative
'the aliases that are allowed map null scan ',negative
'only this table has spilled big table rows ',negative
'head ',negative
'following matchergroup would fail anyway and dont want skip files since that may data loss scenario ',negative
'run worker execute compaction ',negative
'may not have active resource plan the start ',negative
'restore the settings ',negative
'now suck the digits the mantissa use two integers collect digits each this faster than using floatingpoint the mantissa has more than digits ignore the extras since they cant affect the value anyway ',negative
'americalosangeles dst dates ',negative
'corresponding the input file stored name the output bucket file appropriately ',negative
'rows ',negative
'serialize some data the schema after altered ',negative
'theory the below call isnt needed non thriftmode but lets not ',negative
'fetchcolumns not called because had columns fetch ',negative
'note retries weve configured total attempts connect failure simulation count left after this ',negative
'delete dependency only other resource depends ',negative
'numbuckets ',negative
'close the pipedoutputstream before close the outermost outputstream ',negative
'the explain vectorization option was specified ',negative
'hive job credential provider set but not set use the jobconf ',negative
'',negative
'',negative
'set scratch dir permission ',negative
'test this supposed restrict events those that match the dbname and tblname provided the filter the tblname passed the filter null then restricts itself ',negative
'singlecolumn string hash table import ',negative
'there valid bucket pruning filter ',negative
'test invalid case with multiple versions ',negative
'predicate not deterministic ',negative
'could not parse the view ',negative
'need explicitly update proxyusers ',negative
'the begin the real elements ',negative
'look evaluator get output type info ',negative
'retry from same dump when the database empty also not allowed ',negative
'happens due side preemption the asking for task die theres hooks the moment get information over ',negative
'check that get the right files disk ',negative
'try put the most common first ',negative
'this code modified version that lets detect can return reference the bytes directly ',negative
'retry with same dump with which was already loaded also fails ',negative
'restart pool sessions ',negative
'should already true ',negative
'for these positions some variable primitive type string used size ',negative
'allow only keys that start with hive hdfs mapred for security dont allow access password ',negative
'hope this respected properly ',negative
'verify that cleans all old notifications ',negative
'when renaming partition should update partition location partition column stats there are any because partname field hms table partcolstats ',negative
'the nanosecond part fits bits ',negative
'not used yet since the writable rpc engine does not support this policy ',negative
'public long rdatetimeepoch the format hive understands default ',negative
'srswacquired lock are examining acquired can acquire because read can share with write and there must ',negative
'means that there stats ',negative
'blank byte ring above uda bytes ',negative
'reload related configuration changed ',negative
'tests whether the needs automatic setting parallelism ',negative
'save the info that required query time resolve dynamicruntime values ',negative
'add struct ',negative
'always need the base row ',negative
'sqlstate for cancel operation ',negative
'generate the aggregate see the reference example above ',negative
'union consisted bunch mapreduce jobs and has been split the union ',negative
'this either alter table add foreign key add primary key command ',negative
'for unregistered patterns fail ',negative
'class members for cookie based authentication ',negative
'verify that the sourcetable was created successfully ',negative
'not allow embedded metastore llap unless are test ',negative
'todo api changed ',negative
'havent processed all the parent sinks and need them done order compute the parallelism for this sink this case skip should visit this again from another path ',negative
'type when implemented ',negative
'jobconf will hold all the configuration for hadoop tez and hive ',negative
'multiple hash tables with hybrid grace partitioning ',negative
'has separate first step because dont set the default values the config object ',negative
'get the small table keyvalue container ',negative
'limit means that not need any task ',negative
'said before here use rewrite allcolref ',negative
'operator ',negative
'but there should more active calls ',negative
'run worker ',negative
'parsing elements one one ',negative
'set list bucketing context ',negative
'drop one database see what remains ',negative
'estimated hash table size ',negative
'clear the value ',negative
'per jdbc spec the request defines hint the driver enable database optimizations the readonly mode for this connection disabled and cannot enabled isreadonly always returns false ',negative
'set start which the timeout ',negative
'match ',negative
'though clone the tree were still using the same mapworkreducework ',negative
'generate that can make entry ',negative
'commaseparated intervals without brackets ',negative
'tcp server ',negative
'logic does not work unless the reducesink connected parent the mapjoin but this point have already removed all the parents from the mapjoin ',negative
'just return that case drop needed ',negative
'for serialization only ',negative
'now that let calcite process subqueries might have more than one ',negative
'genericudaf ',negative
'typical line length ',negative
'user grants ',negative
'todigitsonlybytes ',negative
'skewed value ',negative
'child tablescan there need push this predicate ',negative
'get the join keys from old parent reducesink operators ',negative
'create standard java object inspector ',negative
'test february nonleap year viewd due days diff ',negative
'its table alias will process that later dot ',negative
'flip the highestorder bit the sevenbyte representation seconds make negative values come before positive ones ',negative
'should have new root now ',negative
'the result the comparison the last row processed ',negative
'restore the original hdfs root ',negative
'this needed specifcally for hive tez addition ',negative
'end ',negative
'take integer fractional portion ',negative
'other writetypes related dmls ',negative
'the digits must fit without rounding ',negative
'nametotypeptr ',negative
'resfile ',negative
'call the method recursively over all the internal fields the given avro ',negative
'determine join type todo what about tokcrossjoin tokmapjoin ',negative
'after processing all the groups batches with evaluategroupbatch the nonstreaming ',negative
'with this map project the big table batch make look like output batch ',negative
'bgenjjtree constvalue ',negative
'decaying where the batchsize keeps reducing half ',negative
'both are non null first compare the table names ',negative
'required for hivereldecorrelator ',negative
'the default works bug ',negative
'propagate this change till the next ',negative
'todo this ',negative
'different instance same value ',negative
'used for logdir failure messages etc ',negative
'can merge ',negative
'should not materialized view ',negative
'path itself missing cannot recover from this error ',negative
'required optional optional ',negative
'store data ',negative
'this tablescandesc flag strictly set the vectorizer class for vectorized mapwork vertices ',negative
'the ref must table look for column name right child dot ',negative
'this map records such information ',negative
'',negative
'query should pass and create the table ',negative
'finish the last query ',negative
'streamingeval wrap regular eval getnext remove first row from values and return ',negative
'determine the length storage for value and key lengths the first record ',negative
'schemacounter name validation will done grammar part hive ',negative
'writable needed for this data type ',negative
'make sure all the numbers are converted long for size estimation ',negative
'',negative
'replication done need check the new property added ',negative
'only red qualifies and its entry ',negative
'start with the keywrapper itself ',negative
'eventscount ',negative
'offlineoffline ',negative
'compute knownpending tasks selfandupstream indicates task counts for current vertex and ',negative
'should allocate since not known ',negative
'maxevents ',negative
'only remove mvs first ',negative
'check there capacity dest pool move else kill the session ',negative
'table and all partitions have the same schema and serde need convert ',negative
'return session the pool can directly here ',negative
'defaultconstraints ',negative
'test deprecated ',negative
'some operations with new format ',negative
'lateral view ast has the following shape toklateralview tokselect tokselexpr tokfunction identifier params identifier tablealias ',negative
'knows where look compact ',negative
'reduce sink does not have any kids since the plan now has been broken into multiple tasks iterate over all tasks for each task over all operators recursively ',negative
'must different ',negative
'destpaths parent path doesnt exist should mkdir ',negative
'since remove reduce sink parents replace original expressions ',negative
'rejected ',negative
'the following two are public for any external users who wish use them ',negative
'are running tests ',negative
'rwx rwx rwx rwxrxrx rwxrwxrwx rxrxrx ',negative
'dont timeout because retry delay ',negative
'only support pattern matching via jdo since pattern matching java might different than the one used the metastore backends ',negative
'compare schemes ',negative
'patterns that match the middleend stack traces ',negative
'strict mode the presence order limit must specified ',negative
'deny this blacklist filter excludematches true and matched this whitelist filter and didnt match ',negative
'small table indices has priority over retain ',negative
'partspec doesnt exists return delegate one and the actual partition created movetask ',negative
'can the lazybinary format really tolerate writing fewer columns ',negative
'first try get from select ',negative
'return null ',negative
'the extra parameters will added server side check that the required ones are ',negative
'windowing specification get added child udaf invocation distinguish from similar udafs but different windows the udaf translated windowfunction invocation the ptftranslator here just return null for tokens that appear window specification when the traversal reaches the udaf invocation its exprnodedesc build using the columninfo the inputrr this similar how udafs are handled select lists the difference that there translation for window related tokens just return null ',negative
'collect grouping set info ',negative
'attempts execute will made using batchsizes throws retry exception ',negative
'create the layout for the queryid appender ',negative
'latin small letter turned bytes ',negative
'column reference will try resolve ',negative
'group them into the chunks want ',negative
'query using the hive command line ',negative
'expression for the operation ',negative
'each table creation itself takes more than one task give are giving max should hit multiple runs ',negative
'check case insensitive search ',negative
'reset this before calling positiontofirst ',negative
'powcol and powercol are special cases implemented separately from this template ',negative
'the caller responsible for destroying the session ',negative
'note previously did called oldhivedecimal ',negative
'execute init files always silent mode ',negative
'initialize source tablepartition ',negative
'for dynamic partitioned hash join run the logic for any ',negative
'the hive key and bytes writable value needed pass the key and value the collector ',negative
'nonjavadoc see ',negative
'populate with parameters defined ',negative
'this class annotates each operator with its traits the optraits class specifies the traits that are populated for each operator ',negative
'note multi insert not supported ',negative
'partname ',negative
'being acquired now ',negative
'themap might reused the protocol ',negative
'the char type info need set prior initialization and must preserved when the plan serialized other processes ',negative
'fall through doesnt map hivehcat type here for completeness ',negative
'union occurs before the sortmerge join not useful convert the the sortmerge join mapjoin the number inputs for the union more than would difficult figure out the big table for the mapjoin ',negative
'selectivityrs selectivityjoin ',negative
'see all the field expressions the left hand side are expressions containing constants only partition columns coming from same table ',negative
'all the tablespartitions columns should sorted the same order for example tables and are being joined columns and which are the sorted and bucketed columns the join would work long ',negative
'note needs called inside the txn otherwise could have deduplicated this with ',negative
'continue the next exprnode find match ',negative
'get our singlecolumn long hash map information for this specialized class ',negative
'mapping ',negative
'cap writebuffersize avoid large preallocations also want limit the size writebuffer because normally have partitions that ',negative
'divisionremainder get lower binary word quotient will either middle decimal both high and middle decimal that requires another divisionremainder ',negative
'return the current blocks compressed key length ',negative
'first rowcall new partition ',negative
'ignore the struct column and just copy all the following data columns ',negative
'counter may exceed limitation ',negative
'addtable ',negative
'add rounding and handle carry ',negative
'testing acid usage when maskingfiltering present ',negative
'multikey hash table import ',negative
'overflow ',negative
'compatible with the oldhivedecimal version zero has factor ',negative
'get the hive counters hive createdfiles deserializeerrors recordsinmap recordsoutreducer ',negative
'otherwise load lazily via storagehandler query time ',negative
'fixme ',negative
'basic algorithm scale away fractional digits present clear integer rounding portion ',negative
'lookup values needed for numeric arithmetic udfs ',negative
'union type currently not totally supported see hive ',negative
'max heap size ',negative
'set output vector ',negative
'get acl provider for most outer path that nonnull ',negative
'the number map reduce tasks executed the hiveserver since the last restart ',negative
'distinct processing the reducer query like select countdistinct key from transformed into ',negative
'get the memory hashmap ',negative
'compatibility mode enabled ',negative
'validate the second parameter which either solitary double array doubles ',negative
'this seems odd but wan make sure that run ',negative
'note this backward compat only should removed with createunallocated ',negative
'common base class not ',negative
'check split ',negative
'validatefillin scheme and authority this follows logic identical filesystemgeturi conf but doesnt actually obtain file system handle ',negative
'finally move recovered file actual file ',negative
'use the serialization option switch write primitive values either variable length utf string fixed width bytes serializing binary format ',negative
'things log the jobconf ',negative
'must set this key even differences empty otherwise client and will attempt ',negative
'first value for next column ',negative
'hive ',negative
'create new setop whose children are the filters created above ',negative
'killed failed state ',negative
'optional string value ',negative
'always round down the previous period for timestamps prior origin ',negative
'tests null value returned when file not present any the lookup locations ',negative
'can set the traits for this join operator ',negative
'precision ',negative
'not sel operator bail out ',negative
'optionally the next values small length could integer ',negative
'this pluggable policy chose the candidate mapjoin table for converting join sort merge join the largest table chosen based the size the tables ',negative
'initialize multikey members for this specialized class ',negative
'bgenjjtree fieldvalue ',negative
'per length means null map ',negative
'get fields out the lazy struct and check they match expected results ',negative
'want signal error the tableview doesnt exist and were configured not fail silently ',negative
'store column name maptargetwork ',negative
'precision ',negative
'populate the names and order columns for the first partition the first table ',negative
'able combine parent join and its left input join child the left keys over which the parent join executed need the same than those the child join thus iterate over the different inputs the child checking the keys the parent are the same ',negative
'get the tmp uri path will hdfs path not local mode ',negative
'cannot deserialize throw the specific exception ',negative
'get join condn ',negative
'nonjavadoc see ',negative
'',negative
'for null and true values return every partition ',negative
'maintain the stack operators encountered ',negative
'configuration conf credentials creds ',negative
'database ddl ',negative
'precision ',negative
'update primary and secondarykey ',negative
'required ',negative
'summary for top column values ',negative
'nonjavadoc see ',negative
'this should fail with given http response code error message since header more ',negative
'for now dont push anything into hbase nor store anything special hbase ',negative
'turn notification listener metastore ',negative
'minopentxn ',negative
'prepare blah lists for the following queries cut off the final ',negative
'blocking execute ',negative
'convert the filter one that references the child the project ',negative
'use the parser get the output operators ',negative
'parse the rewritten query string check need ',negative
'one fewer byte ',negative
'not supported cbo ',negative
'already have the tablescan for one side the join check this now ',negative
'the correlated variables ',negative
'create tablescans ',negative
'trimtrue ',negative
'metadataonly dump then the state the dump shouldnt the latest event the data not yet dumped and shall dumped future export ',negative
'possible overflow once ',negative
'create final loadmove work ',negative
'matching rows must the final block can end the binary search ',negative
'drain the first row which just contains column names ',negative
'not give out the capacity the initializing sessions the running ones expect init fast ',negative
'noop ',negative
'create batch with two string bytes columns ',negative
'udf ',negative
'close write ',negative
'assuming taskattemptid and are the same verify this ',negative
'the task done all operators are done well ',negative
'not delete the data ',negative
'add execute permission downloaded resource file needed when loading dll file ',negative
'standard objectinspector ',negative
'negative number flip all bits ',negative
'nonjavadoc see javalangstring javautilmap ',negative
'the process method was not called big table rows ',negative
'seconds wait until subsequent status minutes timeout for watch mode ',negative
'revert output cols sel exprnodecolumndesc ',negative
'estimate size aggregation buffer ',negative
'filter join condition filter see can pushed above the join filter cannot pushed join full outer join left outer and filter left alias join right outer and filter right alias ',negative
'need operate sorted data fully test binarysortable ',negative
'chosen noconditional task size will oversubscribed ',negative
'production ',negative
'offsets ',negative
'the user can specify the hadoop memory ',negative
'reducesinkoperators ',negative
'force the underlying initialize ',negative
'the issue with caching case bucket map join that different tasks process different buckets and the container reused join different bucket join results can incorrect the cache keyed operator and for bucket map join the operator does not change but data needed different for proper fix this requires changes the tez api with regard finding bucket and also ability schedule tasks reuse containers that have cached the specific bucket ',negative
'for cache ',negative
'nothing the moment ',negative
'replace filter current fil with new fil ',negative
'the result empty string negative start provided whose absolute value greater than the string length ',negative
'random sampling ',negative
'the scratch column information was collected the task get ',negative
'are going eliminate ',negative
'here rewrite the and also the masking table ',negative
'hide this doesnt look like simple property ',negative
'the parameters does not define any transactional properties return default type ',negative
'ccid monotonically increasing for any entity sorts order compaction history thus this query groups entity and withing group sorts most recent first ',negative
'multithreaded mode just once ',negative
'find the branch which this processor was invoked ',negative
'set custom prefix for hdfs scratch dir path ',negative
'get checksum file ',negative
'fetchworks sink used hold results each query needs separate copy fetchwork ',negative
'new attempt path crated add watch and scan for existing files ',negative
'there should still now directories the location ',negative
'sent over the wire from the and will take the place appiddagid queryidentifier ',negative
'file copied from path then need rename them using original source file name this needed avoid having duplicate files target same event applied twice ',negative
'partition column value null ',negative
'this lock used mutex commitabort and heartbeat calls ',negative
'hasmorerows ',negative
'verify that flattening and unflattening isrepeating works ',negative
'fall back regular api and create states without ',negative
'tests for gettable other catalogs are covered ',negative
'hive ',negative
'write basesplit into output ',negative
'set the default configs whitelist ',negative
'enabled get hosts property ',negative
'vectorized row batch ',negative
'the return value indicates success ',negative
'singlecolumn long specific declarations ',negative
'make sure run through the loop once before checking stop this makes testing much easier the stop value only for testing anyway and not used when called from hivemetastore ',negative
'the property set the metastore config the partition inherits the listed table parameters this property not set this test therefore the partition doesnt inherit the table parameters ',negative
'composite key class was provided but neither the types property was set and neither the getparts method hbasecompositekey was overidden the implementation flag exception ',negative
'first look for all the compactions that are waiting cleaned have not seen entry before look for all the locks held that table partition and record them will then only clean the partition once all those locks have been released this way avoid removing the files while they are use while the same time avoiding starving the cleaner new readers come along this works because know that any reader who comes along after the worker thread has done the compaction will read the more date version the data either newer delta newer base ',negative
'not need update column stats alter partition not for rename changing existing columns ',negative
'trigger query hooks after query completes its execution ',negative
'exceptions expected ',negative
'the raw data size ',negative
'simple implementation for now this will later expand dag evaluation ',negative
'associated with txn and handled performtimeouts ',negative
'initialize output ',negative
'todo poll periodically ',negative
'emulate biginteger deserialization used lazybinary and others ',negative
'not using fieldschemaequals comments can different ',negative
'walk through the operator tree ',negative
'adding column types used later ',negative
'remember mapping plan input ',negative
'owner testowner and lastaccesstime testparam ',negative
'attempt retrieving the schema from the data ',negative
'reorder fields record based the order columns the table ',negative
'existing ',negative
'generate the output column infos row resolver using internal names ',negative
'acid file would have schema like owid writerid rowid cwid could check this way onceif removed typedescription schema readergetschema liststring columns schemagetfieldnames ',negative
'this point have druid segments from reducers but need atomically rename and commit metadata moving druid segments and committing druid metadata one transaction ',negative
'create row file copy and row file copy ',negative
'weve eliminated the highest digit already with check above ',negative
'atomically move temp file the destination file ',negative
'required required required optional ',negative
'this primarily for testing avoid the host lookup ',negative
'equivalent algorithm exists ',negative
'roundcol special case and will implemented separately from this template ',negative
'targetpath table root unpartitioned table partition sourcepath result select part ctas statement ',negative
'direct handoff ',negative
'customized logj config log file ',negative
'test fields ',negative
'nonvectorized record reader created below ',negative
'are running tests are going verify that the contents the cache correspond with the contents the plan and otherwise fail this check always run when are running test mode independently whether ',negative
'vector expression doesnt support checked execution hold case there available checked variant ',negative
'dont through initiator for user initiated compactions ',negative
'last row group set true means the above row group spans compression buffer ',negative
'need instantiate the realinputformat ',negative
'todo parse make sure has alter table defaulttacid set tblproperties transactionaltrue alter table defaulttacidpart set tblproperties transactionaltrue ',negative
'repeated null fill down column ',negative
'either input input may have nulls ',negative
'compatibility ',negative
'create matchers for custom path string well actual dynamic partition path created ',negative
'walk operator tree create expression tree for list bucketing ',negative
'rewrite the agg calls each distinct agg becomes nondistinct call the corresponding field from the right for example countdistinct esal becomes countdistinctesal ',negative
'needed that the file actually loaded into configuration ',negative
'the name used the service registry may not match the host name were using try hostnamecanonical hostnamehost address ',negative
'locks ',negative
'remove viewbased rewriting rules from planner ',negative
'true return true ',negative
'ensure that have the correct identifier the column name ',negative
'useincludecolumns ',negative
'explicitly use link when getting the schema but store link this class serialized into the job conf ',negative
'wait for the initial resource plan applied ',negative
'mybitint ',negative
'check for two different classes below because initially the result but the future the system enhanced ',negative
'try converting the enum verify that this valid privilege type ',negative
'catch all errors throwable and execptions prevent subsequent tasks from being suppressed ',negative
'for join operators that can generate small table results reset their target scratch columns ',negative
'nonjavadoc see ',negative
'for everything exceptions are aborted ',negative
'last try get port just use default ',negative
'get the mwmtrigger object from ',negative
'quoted string ',negative
'the only concurrent change that can happen when hold the heap lock list removal ',negative
'nosuch ',negative
'check the grantor matches current user ',negative
'need provide different record reader for every type druid query the reason that druid results format different for each type ',negative
'for casting integral types boolean ',negative
'modify copy not the original ',negative
'for remote jdbc client try set the hive var using set hivevarkeyvalue ',negative
'trigger bugs anyone who uses this hostname ',negative
'table name pattern ',negative
'writeid for table way the future ',negative
'get the column path return column name exists column could dot separated example lintstringelemmyint ',negative
'the tablepartition exist and older than the event then just apply ',negative
'assumed that parent directory the destf should already exist when this method called when the replace value true this method works little different from command the destf directory replaces the destf instead moving under ',negative
'since left integer always some products here are not included ',negative
'nothing register return hosthostname ',negative
'dowritefewercolumns ',negative
'called during deserialization querydef during runtime ',negative
'this would the case for obscure tasks like truncate column unsupported for ',negative
'not matching the regex ',negative
'',negative
'drop any partitions ',negative
'are not starting zookeeper server here because qtestutil already starts ',negative
'push down projections ',negative
'add its locations the list paths delete ',negative
'assigning can subset columns this the projection the batch column numbers ',negative
'optional string queue ',negative
'verify privilege objects ',negative
'decimal scale updown ',negative
'the rankvalue the last row output ',negative
'authorization ddl ',negative
'populate inputinspectors for all children this demuxoperator those are stored ',negative
'this will call one the specific notifyevicted overloads ',negative
'collect aggrel output col names ',negative
'backtrack key columns crs prs ',negative
'note are called after doesnt have nearly much does every buffer already cachechunk ',negative
'set the fetchsize ',negative
'loginfoget input agetclass bgetclass ',negative
'remove from hadoopclientopts any debug related options ',negative
'system property that matches one our conf value names set then use the value ',negative
'column stats generates select computestats queries disable caching for these ',negative
'clean system properties that were set this test ',negative
'folding the constant ',negative
'partition smaller than the lagamt the entire partition lagvalues ',negative
'for the mapper processing the smj not initialized need close either ',negative
'note not cancel any user actions here user actions actually interact with kills ',negative
'there should delta dirs plus base dir the location ',negative
'the intermediate rename wouldve failed bootstrap dump progress ',negative
'any the elements was not referenced every operand bail out ',negative
'prefix workgetaggkey ',negative
'table scan operators dpp sources ',negative
'keys for all tables are the same only the first has deserialize them ',negative
'right border the min ',negative
'when generate results into the overflow batch may still end with fewer rows the big table batch nulsel and the batchs selected array will rebuilt with just the big table rows that need forwarded minus any rows processed with the ',negative
'wish add new line here ',negative
'adjustment only needed when exceed max precision ',negative
'nonjavadoc see int ',negative
'assume not streamjng default ',negative
'string including surrogate pair character ',negative
'the column names and order ascendingdescending matched the first sorted columns should the same the joincols where the size join columns for the table sorted abc convert the join any combination abc abc acb cab cba bca bac ',negative
'now check though the schemas are the same the operator changes the order columns the output ',negative
'converts the negative byte into positive index ',negative
'get the children the set clause each which should column assignment ',negative
'qbjointree from innermost outer ',negative
'prune any nulls present map values this the typical case ',negative
'set fetch size ',negative
'need for this all subqueries are maponly queries ',negative
'windowing handling ptfinvocationspec ptfdesc ',negative
'generate the output records ',negative
'order outputcolumn ',negative
'written major compaction ',negative
'and notification comes from the llaptaskreporter ',negative
'assume ',negative
'orc table restrict changing the serde can break schema evolution ',negative
'nonjavadoc see javasqltimestamp javautilcalendar ',negative
'first load the defaults from sparkdefaultsconf available ',negative
'optional bytes credentialsbinary ',negative
'part the partition specified create dummypartition this case since the metastore does not store partial partitions currently need store dummy partitions ',negative
'standardize lower case ',negative
'execute and exit ',negative
'the caller not within mapperreducer reading from the table via clidriver then may return null fall back using default constructor ',negative
'this test ',negative
'one vint with nanos ',negative
'this root task ',negative
'also note that any deletedeltas between given deltaxy range would made obsolete for example delta would make deletedelta obsolete this valid because minor compaction always compacts the normal deltas and the delete deltas for the same range that had directories delta deletedelta and delta then running minor compaction would produce delta and deletedelta ',negative
'assuming the objectinspector represents exactly the same type this struct this assumption should checked during query compile time ',negative
'since allow write operations cache while prewarm happening dont add tables that were deleted while were preparing list for prewarm ',negative
'modified ',negative
'smb join ',negative
'more complex methods which may wrap multiple operations ',negative
'the max for digits ',negative
'initialize the cookie based authentication related variables ',negative
'following parameters slated for removal prefer usage enum above that allows programmatic access ',negative
'this shouldnt happen ',negative
'end the record part the data ',negative
'must able take away the requisite number cant whered the ducks ',negative
'row specific recordwriters ',negative
'test perfectly divisible batchsize and decaying factor ',negative
'tells you what that mapping ',negative
'',negative
'initialize with estimated element size ',negative
'just return the object need further operation ',negative
'todo potential dfs call ',negative
'over all the children ',negative
'there overlap ranges create combined range ',negative
'the global limit optimization triggered will estimate input data actually needed based limit rows estimated input numlimit maxsizeperrow estimatedmap ',negative
'when neither nor are not set jobconf should contain only the credential provider path ',negative
'',negative
'pos change need since weve shrunk the string with same pos ',negative
'fair reentrant lock ',negative
'still equal compare lock ids ',negative
'not using saslkerberos use the password ',negative
'set out parameters ',negative
'contains nonequi join conditions bail out ',negative
'cars ',negative
'after using selected generate spills generate nonmatches any ',negative
'these parameters must match for all orc files involved merging does not merge the file will put into incompatible file set and will not merged ',negative
'get connection ',negative
'http transport mode ',negative
'joincond represents correlated predicate leftisrewritten rightisrewritten indicates either side has been replaced column alias side not rewritten get its text from the tokenstream for rewritten conditions form the text based the table and column reference ',negative
'see spark ',negative
'finally ',negative
'deserializes bit decimals the maximum bit precision decimal digits ',negative
'consider unwinding subclass ',negative
'partitionorder ',negative
'traling ',negative
'currently all spark work the cluster ',negative
'nonjavadoc see ',negative
'add new item the tez work ',negative
'run the script using sqlline ',negative
'check the partition spec valid ',negative
'note listpartition getpartitions removed with hive because will result oom errors with large addpartitions ',negative
'get privileges for this user and its roles this object ',negative
'cancel ',negative
'this operator allowed before mapjoin eventually mapjoin hint should done away with but since bucketized mapjoin and sortmerge join depend completely needed check the operators which are allowed before mapjoin ',negative
'give evaluator chance setup for rawinput execution setup rawinput shape ',negative
'otherwise return the arguments ',negative
'pop struct union ',negative
'try with with decimal input and desired output type ',negative
'now union the select operators selectop and selectopclone store the operator that follows the select after the join will ',negative
'rows will ',negative
'both the handlers should same ',negative
'compute the geometric average the ratios all the factors which are nonzero and finite ',negative
'supports acidinputformat which uses the key pass rowid info ',negative
'restart the sessions until one them refuses restart ',negative
'restore the original selected vector ',negative
'construct ptf operator ',negative
'test partition listing with partial spec ',negative
'need the where clause below ensure consistent results with readcommitted ',negative
'over all aggregation buffers and see they implement estimable ',negative
'check isrepeating propagation ',negative
'note only look the schema here deal with complex types somebody has set the reader with whatever ideas they had the schema and just trust the reader produce the cvbs that was asked for however only need look top level columns ',negative
'store path prefix this testfilter ',negative
'attempt dynamic partitioned hash join since dont have big table index yet must start with estimate numreducers ',negative
'all will filtered out ',negative
'serialize aggbuffer ',negative
'not supported for tables right now ',negative
'read basesplit from input ',negative
'check this table name qualified not ',negative
'short circuit and return the current number rows this ',negative
'failure here will cause this query not added the cache ',negative
'tables test ',negative
'increments file modification time ',negative
'exceeds max value ',negative
'show table properties populate the output stream ',negative
'well formed ',negative
'configure http client for ssl ',negative
'dummy for backtracking ',negative
'string between ',negative
'intentionally corrupt some files ',negative
'literal double ',negative
'try find valid entry but settle for pending entry that all have ',negative
'get set recordwriter based the column values ',negative
'add the queryid appender the queryid based route ',negative
'regular case accessing nested field column ',negative
'starttime ',negative
'bad files dont pollute the filesystem ',negative
'locked invalidated buffer was the list just drop will readded unlock ',negative
'remove all the candidate filter operators when get the ',negative
'enforce required retain the buffer sizes old files instead orc writer inferring the optimal buffer size ',negative
'repeated groupinputspecproto groupedinputspecs ',negative
'should not matter which txnmgr used here ',negative
'use url param indirectly the name env var that contains the url the urlparam default would look for beelineurldefault url ',negative
'nonjavadoc see ',negative
'testing multibyte string with reference set mid array ',negative
'dont need cancel this token the tokenrenewer for tokens takes care cancelling them ',negative
'position ',negative
'copy the join and the top sort operator ',negative
'test repeating null selection ',negative
'definitely long most longs fall here ',negative
'maponly subqueries can optimized future not write file ',negative
'the formulae jls section ',negative
'any table has bad size estimate need fall back sizing each table equally ',negative
'todo hive abort handling handling mergemapop ',negative
'now all the column values should always return null ',negative
'exception ',negative
'currently only column related changes can cascaded alter table ',negative
'when people forget quote string opop null for example select from sometable where ',negative
'gather statistics for the first time and the attach table scan operator ',negative
'new value the last isnt clobbered ',negative
'now ',negative
'the subquery predicate has been hoisted join the subquery predicate replaced true predicate the outer qbs wherehaving clause ',negative
'the current record needs returned based the filter conditions specified the user increment the counter ',negative
'bad luck handle the corner cases where bytes are multiple blocks ',negative
'careful maintenance the flag ',negative
'nonjavadoc see javaioinputstream long ',negative
'return loaned session goes back tez pool ',negative
'return true the remove was successful false otherwise ',negative
'distincts are not allowed with additional job ',negative
'this likely shutdown ',negative
'even though there are only two delta delta and one deletedelta ',negative
'type affinity does not help when type affinity does not match input args ',negative
'now ',negative
'tuesday january ',negative
'file named path doesnt exist nothing validate ',negative
'index into partitionspecs ',negative
'nnname ',negative
'number variables declared with the same data type and default ',negative
'issame ',negative
'number bytes for register ',negative
'everything should same before ',negative
'latin capital letter bytes blank byte ',negative
'should intercept here for possible decimal vector expression class ',negative
'delegation token passed from the client side not set the principal ',negative
'from parentrunner retried under exception notified only after exhausting retrycount ',negative
'clear the thread locals ',negative
'for these positions some variable primitive type string used for the ',negative
'iterate for the first time get all the names stages ',negative
'cancel the maintenance thread ',negative
'partial column statistics grouping attributes case column statistics grouping attribute missing then assume worst case gby rule will emit half the number rows ndvproduct ',negative
'todo add allocate overload with offset and length ',negative
'todo parts this should moved out tezsession reuse the clients but theres good place for that right now hive ',negative
'this null check specifically done the same class used handle both incremental and bootstrap replication scenarios for create function when doing bootstrap not have event for this event but rather when bootstrap started and hence pass null dmd for bootstrapthere should better way this but might required lot changes across different handlers unless this common pattern that seen leaving this here ',negative
'dynamic usecase called through ',negative
'clone rhssemijoin ',negative
'use the rowresolver from the input operator generate input objectinspector that can used initialize the udtf then the ',negative
'param conf throws see configuration ',negative
'give more ',negative
'string can not parsed converter will return null ',negative
'allow not but throw error for rest ',negative
'now ',negative
'inputformat ',negative
'get all items into array and sort them ',negative
'now prepare partnames with only partitions tabparttabpart ',negative
'use linear extrapolation more complicated one can added the ',negative
'given that not delete empty slot means match findreadslot key key slot slot pairindex pairindex empty slot ',negative
'will store all the new changed properties the job the udf context the the and setschema ',negative
'private methods ',negative
'the owner will also have select with grant privileges new view ',negative
'year granularity ',negative
'use only control chars that are very unlikely part the string the following mightlikely used text files for strings horizontal tab line feed form feed carriage return escape esc gcc only ',negative
'disable the expandevents for the purpose backward compatibility ',negative
'also make sure constant ',negative
'process minmax ',negative
'hash from the fetcher ',negative
'decompose and expression into its parts before checking the entire expression candidate because each part may candidate for replicating transitively over equijoin condition ',negative
'remove cast boolean not null boolean vice versa filter accepts nullable and notnullable conditions but cast might get the way ',negative
'key key key ',negative
'defines interface for reexecution logics fixme rethink methods ',negative
'exponent read from field ',negative
'input includes table columns partition columns ',negative
'check that have two deltas ',negative
'repeating else expression ',negative
'methods setreset getpartition modifier ',negative
'during the dfs traversal the ast descendant likely set error because subtree unlikely also group expression for example query such select concatkey from src group concatkey key will processed before concatkey and since key not group expression error will set ctx columnexprprocessor ',negative
'trigger ',negative
'the following information for creating vectorizedrowbatch and for helping with knowing how the table partitioned will stored mapwork and reducework ',negative
'get file metadata from cache create the reader and read ',negative
'maponly job can optimized instead converting mapreduce job can have another map job the same avoid the cost sorting the mapreduce phase better approach would write into local file and then have maponly job add the limit operator get the value fields ',negative
'stop wont able forward ',negative
'maintain count outstanding requests for tokenidentifier count goes safe remove the token ',negative
'reads all the delete events into memory during initialization and closes the delete event readers after exception gets thrown during initialization may have close any readers that are still left open ',negative
'note conditional vector expression not call ',negative
'calcite daytime interval millis value bigdecimal seconds converted millis ',negative
'weve seen both root and child can bail ',negative
'should have files ',negative
'remove default partition from partition names and get aggregate ',negative
'',negative
'otherwise tried ',negative
'anything remains this where starts ',negative
'iterating through the buffer should not cause the next buffer fetched ',negative
'now ',negative
'querytimeout ',negative
'check for dynamic partitions ',negative
'second check the tasks looked for must not have been accessed more than once result the traversal note that actually wind accessing times because each visit counts twice once check for existence and once visit ',negative
'add role privileges ',negative
'how much can the fraction moved ',negative
'check any the txns the list aborted ',negative
'add another via cachedstore ',negative
'tez internals may register the same task completing multiple times ',negative
'the conversion standard object inspector was necessitated hive the issue happens when select operator preceeds this operator the case subquery the select operator does not allocate new object hold the deserialized row this affects the operation the smb join which puts the object priority queue since all elements the priority queue point the same object the join was resulting incorrect results the fix make copy the object done the processop phase below this however necessitates change the object inspector that can used processing the row downstream ',negative
'provider setup ',negative
'make sure delta files are written with indexes compression and dictionary ',negative
'translate the positions correlated variables relative the join output leaving room for valuegenfieldoffset because valuegenerators are joined with the original left input the rel ',negative
'also test getting table from specific ',negative
'lets try parse timestamp with time zone and transform ',negative
'note that here ignore nanos part hive timestamp since nanos are dropped when reading hive from pig design ',negative
'this ptfdesc for mapside ptf operation ',negative
'test with predicates such that partition pruning works ',negative
'',negative
'offer accepted evicted ',negative
'pick unknown case ',negative
'these remaining combinations below definitely result overflow ',negative
'dont entirely believe that everything cleaned when close called based watching the trace logs ',negative
'replication dest will not external override set ',negative
'all join tables have keys doesnt matter what generate ',negative
'this will trigger the column pruner collect view column authorization info ',negative
'special handling for sql reload function ',negative
'why even compute syntheticprops isoriginal ',negative
'notifyreused implies that buffer already locked its also called once for new buffers that are not cached yet dont notify cache policy ',negative
'missing stripe stats old format numrows then its empty file and statistics present have differentiate stats empty file missing stats old format ',negative
'expected ',negative
'check the virtualcolumn directly ',negative
'have make sure theres slash after har otherwise resolve doesnt work ',negative
'attempting get the password should throw exception ',negative
'try outer row resolver ',negative
'disregard nulls for processing other words the arithmetic operation performed even one more inputs are null this improve speed avoiding conditional checks the inner loop ',negative
'first nonnull item ',negative
'call getsplits ',negative
'make sure got the right implementation columnmapping ',negative
'add the new entry ',negative
'return our mocked objects ',negative
'trigger update needed ',negative
'refactored out the equality case parsejoincondition that this can recursively called its left tree the case when only left sources are referenced predicate ',negative
'used for testing simulate method timeout ',negative
'newer versions and later support offsetfetch ',negative
'mainly for verification ',negative
'this object should the same cache otherwise must removed due init error ',negative
'build input output position map ',negative
'test rpcserveraddress configured ',negative
'modifier letter arrowhead bytes ',negative
'use the tablealias generate keybasealias ',negative
'replace sel selexprs ',negative
'tinsert and tinsert ',negative
'clone filtermap ',negative
'have plan prefer its logical result schema its available otherwise try digging out fetch task failing that give ',negative
'acid ',negative
'instantiate decimal vector expression the method sets the output column and type information ',negative
'nonspace character ',negative
'randomly pick the character corresponding the field and convert byte array ',negative
'this string reader should simply redirect its own seek what other types already ',negative
'null ',negative
'stddevpopx power sumx sumx sumx countx countx stddevsampx power sumx sumx sumx countx ',negative
'',negative
'optional bool default false ',negative
'flush any pending scheduler runs which may blocked wait seconds for the run complete ',negative
'for now dont support group decimal keys ',negative
'uppercase first letter ',negative
'above enforce certain order when the reutilization ',negative
'target table has cols ccc and partition col and had set currentdate want end with insert into target select currentdate where ',negative
'construct using ',negative
'make sure dont modify the config rpcconfiguration ',negative
'use check set columns are all partition columns true only all columns bitset are partition columns ',negative
'this will only send update its necessary ',negative
'disallow subqueries which hive doesnt currently support ',negative
'went through correctly then also hcatrecord and also equal the above and deepcopy and this holds through for multiple levels more serialization well ',negative
'shuffle write metrics ',negative
'txn write bucket txn write bucket bucket ',negative
'run this rule later stages since many calcite rules cant deal with semijoin ',negative
'take row from the current buffered batch ',negative
'the general idea here create created created created for each job submitted the node number generated and the payload the jobid basically this keeps track the order which jobs were submitted and zookeepercleanup uses this purge old job info since the jobsid node has createupdate timestamp this whole thing can removed ',negative
'logdebugclassname processop all keylength ',negative
'incase the join has extra keys other than bucketed columns partition keys need updated small tables ',negative
'values within the column type bounds ',negative
'init output object inspectors the return type for partial aggregation still list doubles but add the percentile values requested the end and handle before pass things the parent method the return type for final and complete full aggregation result which also ',negative
'all partitions and all but default ',negative
'resultsetnext blocks until the async query complete ',negative
'green and red qualify ',negative
'create map join task and set big table bigtableposition ',negative
'string variable ',negative
'and remind ourselves perform incremental normalization ',negative
'jaas login from ticket cache setup the client ',negative
'value therefore always logicalcorrelates left input which outer query ',negative
'createdirectories creates all nonexistent parent directories ',negative
'log warning hivedefaultxml found the classpath ',negative
'add post script ',negative
'number output columns array path expressions each which corresponds column array returned column values object pool nonnull text avoid creating objects all the time ',negative
'are now some intersecting buffer find the first intersecting buffer ',negative
'replace stdout and run command ',negative
'set query timeout second ',negative
'reject any clauses that are against column that isnt the rowid mapping indexed ',negative
'the perbatch setup for inner join ',negative
'swap column vectors simulate change data ',negative
'for negative number initializing bits ',negative
'put into the cache before initialized make sure can catch ',negative
'the fourth parameter precision factor has been specified make sure its ',negative
'splits are not shared across different partitions with different input formats ',negative
'the database newer than the drop event then noop ',negative
'test set and append methods ',negative
'get the positions for sorted and bucketed columns for sorted columns also get the order ascendingdescending that should also match for this converted maponly job get the positions for sorted and bucketed columns for sorted columns also get the order ascendingdescending that should ',negative
'done ',negative
'all other exceptions are considered emanating from unauthorized accesses ',negative
'expects both parameters local ',negative
'location specified ensure that full qualified name ',negative
'comment ',negative
'always allow the operation not replication scope ',negative
'need add this branch the key value info ',negative
'',negative
'implement vectorized function hexstring returning string ',negative
'move the next bit vector ',negative
'run without cascade ',negative
'bucketing isnt consistent there are bucket columns optimizer does not extract multiple column predicates for this ',negative
'debug display ',negative
'the number reducers should used for those bottom layer reducesinkoperators ',negative
'copy all the histogram bins from and other into overstuffed histogram ',negative
'view already exists thus should replacing ',negative
'larger than max compile duration used test ',negative
'delete any tables other than the source tables ',negative
'pass the row though the operator tree guaranteed that not more than row can produced from input row ',negative
'first write chunk length ',negative
'todo exprnodedesc expression tree could just use that and rid filterg ',negative
'null null ',negative
'the update has failed wed try with another candidate first but only the same priority ',negative
'metrics are first dumped temp file which then renamed the destination ',negative
'this mapping the values the small table hash table that will copied the small table result portion the output that mapping the lazybinary field order ',negative
'and only stmt implicit txn and uses acid resource ',negative
'new input column numbers ',negative
'check reduce vertex and its children more than check all the child ops are reduce output operators ',negative
'high word must zero check for overflow digits middle word ',negative
'returns job status for list input jobs list ',negative
'singlecolumn string outer null detection ',negative
'build rel for clause ',negative
'all rows from both side will present resultset ',negative
'the position the column keys the dummy grouping set column ',negative
'essentially partition values are represented strings but want the actual object type associated ',negative
'txn was committed but notification was not received was aborted either case can clean ',negative
'using metrics thus should always check whether this nonnull before using ',negative
'without hive with hive ',negative
'just move the marker according delta hit the limit the list was exhausted ',negative
'not present just unmanaged ',negative
'method that provides similar filter functionality filterholder above useful when ',negative
'otherwise subdirectories produced hive union operations wont readable ',negative
'the current members deserializeread have the field value ',negative
'not try finish and flush inprogress group because correct values require the last group batch ',negative
'set one null value for possible later use ',negative
'todo move into the below account for release call ',negative
'from tableacidtbl where inselect from tablenonacidorctbl ',negative
'use the move operation that created atomic ',negative
'this will work only the files are local files webhcat server which not very useful since users might not have access that file system this likely the hive issue ',negative
'truncate external table not allowed ',negative
'sleep twice the ttl interval things should have been cleaned then ',negative
'get the key column names for each side the join and check the keys are all constants ',negative
'set resolver and resolver context ',negative
'gss name for server ',negative
'see comments for next method ',negative
'can generate multiphase aggregates one distinct aggregate filter summinmaxcount nondistinct aggregate one more nondistinct aggregates ',negative
'password available url needs striped ',negative
'spot check correctness decimal column subtract decimal scalar the case for addition checks all the cases for the template dont that redundantly here ',negative
'need get connector look the users authorizations not otherwise specified ',negative
'otherwise find the row groups that were selected the client ',negative
'are not unique keys for ',negative
'enabled ',negative
'for expr aliases should iterated the order they are specified the query ',negative
'override this method forward its outputs ',negative
'implementation notes since only local file systems are supported there need use hadoop version path class javanio package provides modern implementation file and directory operations which better then the traditional javaio are using here particular supports atomic creation temporary files with specified permissions the specified directory this also avoids various attacks possible when temp file name generated first followed file creation see for the description nio api and for the description interoperability between legacy api nio api avoid race conditions with readers the metrics file the implementation dumps metrics temporary file the same directory the actual metrics file and then renames the destination since both are located the same filesystem this rename likely atomic long the underlying support atomic renames note this reporter very similar would good unify the two ',negative
'try the udf ',negative
'principalname ',negative
'disallow udfs that can potentially allow untrusted code execution ',negative
'nonnls ',negative
'task hash map ',negative
'perform any value expressions results will into scratch columns ',negative
'this akin cbo cumulative cardinality model ',negative
'child the boolean expression ',negative
'all columns got constant folded then disable this optimization ',negative
'the worker should remove the subdir for aborted transaction ',negative
'verify query result ',negative
'check the union fields length and offset ',negative
'test nonvectorized nonacid combine ',negative
'close abort ',negative
'tez blurs the boundary between map and reduce thus has its own config ',negative
'filesaddedchecksum ',negative
'number rows for the key the given table ',negative
'workerid ',negative
'for whatever failure reason including that trash has lower encryption zone retry with force delete ',negative
'acid write add plan output else dont bother ',negative
'bits minus byte ',negative
'verify that driver fails due older version schema ',negative
'count number false values seen far ',negative
'alter table with invalid column type ',negative
'get full objects for oracleetc batches ',negative
'cast int does not work expected still considered string order for some reason ',negative
'test that existing sharedread partition with new sharedread coalesces ',negative
'will start new key group can call flush the buffer children from lastchildindex inclusive the last child and propagate processgroup those children ',negative
'todo hive propagating abort dummyops ',negative
'maxed out capacity this move should fail the session ',negative
'hiveserver auth configuration ',negative
'for use columnscalar and scalarcolumn arithmetic for null propagation ',negative
'dbname ',negative
'mapping from constraint name list foreign keys ',negative
'remove any existing entries that are contained the new one ',negative
'orcsplitisacid returns true know for sure acid ',negative
'here ther were locks that blocked any locks locksbeingchecked acquire them all ',negative
'files size for splits ',negative
'there are inputs the execution engine skips the operator tree prevent from happening opaque zerorows input added here when needed ',negative
'column names column names default partition name case null empty value maximum dynamic partitions created per mapperreducer ',negative
'spark memory per task and total number cores ',negative
'will always release copies the end ',negative
'schema name ',negative
'specifies and pattern ',negative
'there should only directory left base ',negative
'create the row resolver for this operator from the output columns ',negative
'create the ',negative
'the selected array already filled want ',negative
'the sign encoded the least significant bit need adjust our decimal before conversion binary positive multiply negative logic does negate the biginteger not have since fasthivedecimal stores the numbers unsigned fast fast and fast need subtract one though and then multiply and add the sign bit consider this could combined ',negative
'metastore ssl settings ',negative
'optional group containing repeated anonymous group map containing ',negative
'set checkpoint task dependant create table task same dump retried for ',negative
'set reducer parallelism ',negative
'below are the properties that need set based what database this test going run ',negative
'keep track ',negative
'test that committing unlocks ',negative
'for backwards compatibility reasons honor the older set different from default ',negative
'tinyint smallint int bigint double float string string struct array map bool complex decimal char varchar date timestamp binary ',negative
'gets file data for particular offsets the range list modified place then returned since the list head could have changed ranges are replaced with cached ranges case partial overlap with cached data full cache blocks are always returned theres capacity for partial matches return type the rules are follows the requested range starts the middle cached range that cached range will not returned default and are cached the request for will only return from cache this may configurable impls this because assume wellknown range start offsets are used rgstripe offsets request from the middle the start doesnt make sense the requested range ends the middle cached range that entire cached range will returned and are cached the request for will return both ranges should really same however currently orc uses estimated end offsets fact know such cases that partiallymatched cached block can thrown away the reader will never touch but need code the reader handle such cases avoid disk reads for these tails real unmatched ranges some sort invalidcachechunk could placed avoid them todo param base base offset for the ranges stripestream offset case orc ',negative
'simple division yes should something like this but later anyway this will eventually replaced intrinsics java ',negative
'process singlecolumn string inner join vectorized row batch ',negative
'hashing the string potentially expensive better branch additionally not looking values for nulls allows not reset the values ',negative
'create estimators ',negative
'the stage that this vertex belongs ',negative
'reenable the node task succeeded slot may have become available also reset commfailures since task was able communicate back and indicate success ',negative
'dont use streaming for distinct cases ',negative
'only persist inputoutput format metadata when explicitly specified ',negative
'some mix tests string char varchar char varchar string ',negative
'need use cleanup interval which how often the cleanup thread will kick and check see any the connections can expired dont want this too often because itd like having minigc going off every often limit minimum the client has explicitly set larger timeout the cache though respect that and use that ',negative
'avoid adding group for correlated inexists queries since this rewritting into semijoin ',negative
'insert new filter between and parent ',negative
'unknown unknown ',negative
'not from this ',negative
'see comments for sqlprecision ',negative
'insert the select operator between ',negative
'make sure reuse changes the fifo order the session ',negative
'the first line check should handle the oob ',negative
'get the bucket ',negative
'set the last query time ',negative
'for output rows this operator ',negative
'already used update the port and retry ',negative
'delete all things ',negative
'the silly looking call builder below get the default value session timeout from curator which itself exposes system property ',negative
'allow cte definitions views can end with hierarchy cte definitions the top level query statement where view referenced views may refer other views the scoping rules use are search for cte from the current outwards order disambiguate between ctes are different levels qualifyprefix them with the the they appear when adding them the codealiastoctescode map ',negative
'keep partkeyvalmap synch well ',negative
'partspec mapping from partition column name its value ',negative
'validtxnlist ',negative
'testing substring index starting with and length equal array length ',negative
'init input object inspectors ',negative
'second check this work has multiple reducesinks split ',negative
'todo enable metadatafilter using configuration path file metadatafilter filter api ',negative
'nonjavadoc see ',negative
'return the settable equivalent object inspector for primitive categories for for table containing partitions and possibly different from the table return the settable inspector for the inspector for ',negative
'parsing the keys and values one one ',negative
'reduce side optimized plan ',negative
'set command currently not authorized through the api ',negative
'throw hiveexception the tablepartition bucketized ',negative
'files size for splits ',negative
'note after this the caller must send the downgrade message downgradedtask outside the writelock preferably before exiting ',negative
'errors ',negative
'were the only node just clear out the expression ',negative
'user explicitly specified queue name ',negative
'obtain inspector for schema ',negative
'get the size cache after ',negative
'restore ',negative
'byte values ',negative
'now test again with seed ',negative
'exprs ',negative
'skip the first which always required ',negative
'convert millis result back days ',negative
'indexcolumnvector includes the keys map ',negative
'javaobject primitives javaarray ',negative
'populate the names and order columns for the first table ',negative
'input files can pruned ',negative
'please take look the instructions hiveauthorizerjava before ',negative
'allowed operations intervalyearmonth intervalyearmonth intervalyearmonth intervalyearmonth date date operands reversible intervalyearmonth timestamp timestamp operands reversible intervaldaytime intervaldaytime intervaldaytime intervaldaytime date timestamp operands reversible intervaldaytime timestamp timestamp operands reversible ',negative
'try using permanent function and verify its only one that shows ',negative
'the join task converted mapjoin task this can only happen set true conditional task was ',negative
'todo hive report fatal error over the umbilical ',negative
'alias filter mapping ',negative
'inner map ',negative
'extract rest join predicate info infer the rest join condition that will added the filters join conditions that are not part ',negative
'the expiring session may may not the pool ',negative
'hive treats names that start with internalnames change the names dont run into this issue when converting back hive ast ',negative
'not proactively remove locked items from the heap and opportunistically try remove from the list since eviction mostly from the list eviction stumbles upon locked item either will remove from cache when unlock are going put back update depending whether this has happened this should cause most the expensive cache update work happen unlock not blocking processing ',negative
'for jackson ',negative
'dont remove the old entry still refers the old and need lookup later ',negative
'remove the pwd from conf file that job tracker doesnt show this logs ',negative
'add ngram estimation only the context matches ',negative
'routines cant used directly ',negative
'partition columns ',negative
'when type not set init hasnt been called yet ',negative
'dont wait for the thread ',negative
'create reduce ',negative
'can happen this operator does not carry forward the previous bucketing columns for another join operator which does not carry one the sides key columns ',negative
'read lock for get operation ',negative
'all the fields this event are final reason create new one for each listener ',negative
'the simd optimized form ',negative
'validations been done compile time validation needed here ',negative
'single double value ',negative
'tabledb ',negative
'check projrel only projects one expression check this project only projects one expression scalar ',negative
'our reading positioned the key ',negative
'get the mapredlocalwork ',negative
'restrictionm the case implied group correlated subquery the subquery always returns row exists subquery with implied gby will always return true whereas algebraically transforming join may not return true see specification doc for details similarly not exists subquery with implied gby will always return false ',negative
'columns where the length the bucketed columns ',negative
'config variable set via system property config variable set the cli config variable not the default list config variables and config variable the default list config variables but which has not been overridden ',negative
'this point are done processing the input close the record processor ',negative
'init output object inspectors the return type for partial aggregation still list doubles but add the percentile values requested the end and handle before pass things the parent method the return type for final and complete full aggregation result which ',negative
'list columns names and maps them indices ',negative
'perform major compaction nothing should change both deltas and base dirs should have the same name ',negative
'this the vectorized row batch description the output the native vectorized map join operator based the incoming vectorization context its projection may include ',negative
'having non null create table grants privileges causes problems the tests that compares underlying thrift table object created table with table object that was fetched from metastore this because the fetch does not populate the privileges field table ',negative
'todo the below assumes that all the arguments are the same type ',negative
'cancel the delegation token ',negative
'all drones where abandoned host try replacing them ',negative
'hbase api convoluted ',negative
'theres always just one file that have merged the uniondpetc should already account for the path ',negative
'tell our super class will evaluating our scratch bytescolumnvector ',negative
'verify that field changes are consistent with what hive does note could handle this ',negative
'finalize the previous record ',negative
'find the firing rule find the rule from the stack specified ',negative
'following the main loop which iterates through all the cookies send the client the generated cookies are the format hiveserverauthvalue cookie which identified hiveserver generated cookie validated calling the validation passes send the username for which the cookie validated the caller client side ',negative
'currently getimportedkeys always returns empty resultset for hive ',negative
'nonjavadoc see ',negative
'fallthrough ',negative
'special case for handling false constants ',negative
'undone until fast binarysortable supports complex types disable ',negative
'update nondistinct key aggregations keycolxtcoly ',negative
'finally exclude preds that are already the subtree given the metadata provider note this the last step trying avoid the expensive call the metadata provider ',negative
'lookup the state the map ',negative
'changing string value getcharacterlength should update accordingly ',negative
'createtable event partitioned table ',negative
'create file the folder mark ',negative
'nonjavadoc see ',negative
'remove the discardable operator ',negative
'for jackson instantiate ',negative
'need use the opparsecontext the child selectoperator replace the ',negative
'free any previously allocated buffers that are referenced vector ',negative
'first apply configuration applicable both hive cli and hiveserver not adding any authorization related restrictions hive cli grant all privileges for table its owner set this cli well that owner has permissions via hiveserver well ',negative
'generate ngrams wherever the context matches ',negative
'first just check that this translates ',negative
'this method contains various asserts warn environment variables are buggy state ',negative
'convert skewed table nonskewed table ',negative
'fall through ',negative
'aggr checks for sorted arglist ',negative
'executed relevant and used contain all the other details about the table not ',negative
'metastore and some partitions may have data based other filters ',negative
'create table select statement ',negative
'launch tez job ',negative
'noop ',negative
'mapper can span multiple filespartitions the serializers need reset the input file changed ',negative
'need test this with llap settings which requires some additional configurations set ',negative
'check single column for repeating ',negative
'this table not good big table ',negative
'individual column becomes string ',negative
'now load data into the tables and see incremental repl dropload can duplicate ',negative
'undone performance problem with conversion string then bytes ',negative
'',negative
'large fit into main memory ',negative
'after deserialization need recreate the txntowriteidlist its not under jsonproperty ',negative
'comment from booleanwritable date value boolean doesnt make any sense always set the output null ',negative
'dpp with the tree wont split and dont have remove ',negative
'dont call postread will have checked everything here ignore genericudfbridge its checked separately run posthook ',negative
'this will abort initializeop ',negative
'skip the first node which always required ',negative
'view ddl ',negative
'the length each field the value repeats for every entry then stored vector and isrepeating from the superclass set true ',negative
'following remote job may refer classes this jar and the remote job would executed different thread add this jar path jobcontext for further usage ',negative
'failed ',negative
'add new residual preds ',negative
'will extract the literals introduce the clause ',negative
'order likely currently running compactions will first lhs union ',negative
'read aggregated stats for one column ',negative
'new tai lue letter low bytes ',negative
'test basic truncate vector ',negative
'',negative
'start with empty wordlist and build ',negative
'key index nullsafe join flag ',negative
'adjustment array ',negative
'add constant object overhead for union ',negative
'gmt milliseconds after epoch ',negative
'check the source file unmodified even after copy see copied the right file ',negative
'metainfo ',negative
'run cleaner should remove the delta dirs ',negative
'have some disk data separate column chunk and put into cache ',negative
'create list top nodes ',negative
'there should rows the deletedelta because there have been delete events ',negative
'count ',negative
'test mode hive mode ',negative
'variable access should not done and use exportrootdir instead ',negative
'this means that the lock being committed this instant hence the cleaner should not remove even times out transaction ',negative
'fired only once the original node ',negative
'have the udf are the session registry both ',negative
'partition path can null the case new create partition this case try default checking the permissions the parent table partition itself can also null cases where this gets called generic ',negative
'not synchronizing creation mapop with invocation check immediately after creation case abort has been set relying the regular flow clean the actual operator exception thrown attempt will made cleanup the are here exit out via exception were the middle the opeartorinitialize ',negative
'this null for minor compaction may also null there base only deltas ',negative
'foreignschemaname ',negative
'replicate all the events happened far should fail the data files missing ',negative
'remove default partition from partition names and get aggregate stats again ',negative
'refs ',negative
'since the input ',negative
'are using oracle schema because simpler parse quotes backticks etc ',negative
'ckeys pkeys have constant node expressions avoid the merge ',negative
'topn query results records ',negative
'schedule major compaction all the partitionstable clean aborted data ',negative
'iterate over each group subqueries with the same group bydistinct keys ',negative
'take ones complement ',negative
'the driver context has been shutdown due query cancellation kill the spark job ',negative
'',negative
'tests wait queue behaviour for fragments which have reported the but have not given their executor slot ',negative
'xxx from oazktclientbase ',negative
'were updating add the rowid expression then make the following column accesses offset that dont try convert the rowid ',negative
'does the query have using clause ',negative
'now insert this record into the list ',negative
'serialize the rest the values the aggbuffer ',negative
'this block exists for debugging purposes want check whether the col stats cache working properly and are retrieving the ',negative
'means asc could really use enum here the thrift ',negative
'todo for now only support sampling two columns need change list columns ',negative
'errored ',negative
'error getting children node probably node has been deleted ',negative
'download resources dir ',negative
'token was not storage format token ',negative
'argh hcatrecord doesnt implement equals ',negative
'events insert last repl repldumpidx ',negative
'add the move task ',negative
'all small writes the first buffer should contiguous memory ',negative
'here start explicit txn ',negative
'output should have varchar type params ',negative
'table tbl liketbl listfieldschema cols tblgetsdgetcols assertequals colssize assertequalsnew fieldschemaa int null colsget mapstring string tblparams tblgetparameters tblparamsgethcatisd tblparamsgethcatosd hcatdriverrundrop table junitsemanalysis hcatdriverrundrop table liketbl ',negative
'the serdes for the partition columns ',negative
'for example select deptno countdistinct sal sumdistinct sal from emp group deptno becomes select deptno countdistinctsal sumdistinctsal from select distinct deptno sal distinctsal from emp group deptno group deptno ',negative
'unsupported conversion ',negative
'assign ids all vertices targets first then sources ',negative
'values needed for numeric arithmetic udfs ',negative
'size string buffer bytes ',negative
'gen ',negative
'load the column stats and table params with scale ',negative
'and retry ',negative
'return new hash multiset result implementation specific object the object can used access the count values when the key contained the multiset access spill information when the partition with the key currently spilled ',negative
'handle type casts that may contain type parameters ',negative
'instance shared utils ',negative
'integerbitmask ',negative
'specifications default utf string storage for backwards compatibility ',negative
'test using loadtablework ',negative
'condition occurs when the input has rows possible due filtering joins etc ',negative
'create column schema ',negative
'map column number type this always nonnull for useful vec context ',negative
'this not config that users set hivesite its only use share information between the java component the service driver and the python component ',negative
'breakup the original windowingspec into set windowingspecs each windowingspec executed instance ptfoperator preceded reducesink and extract the logic componentize straightforward distribute window specs from original window spec into set windowspecs based their partitioning group windowspecs subset the window invocations the queryblock that have the same order spec each group put new windowingspec and evaluated its own ptfoperator instance the order computation then inferred based the dependency between groups groups have the same dependency then the group with the function that earliest the selectlist executed first ',negative
'example handling dirs with shimshadoop ',negative
'used for the analyze command statistics used for the analyze command statistics noscan ',negative
'reset rowcontainers serde objectinspector and tabledesc ',negative
'find the closest pair bins terms coordinates break ties randomly ',negative
'reopening close open allowed opened state ',negative
'not keyword ',negative
'write file ',negative
'for local directory always write mapred intermediate store and then copy local ',negative
'the source table ',negative
'prepare future cache buffer ',negative
'since the user running the test belongs the group obtained above the call will succeed ',negative
'see setloadfiletype and setisacidiow calls elsewhere for example ',negative
'true this statement creates replaces materialized view ',negative
'single long key hash multiset optimized for vector map join ',negative
'concurrent forceeviction ignore ',negative
'lost the duck ',negative
'lastvalue when sort key specified then lastvalue should return the last value among rows with the same sort key value ',negative
'break iteration times out ',negative
'cleanup the scratch dir before starting ',negative
'size for each input alias ',negative
'this records how many rows have been inserted deleted separate from insertedrows ',negative
'readerspecific incompatibility like smb schema evolution ',negative
'this not used for delete commands partitioneval not set correctly needed for that ',negative
'theres lock dont need heartbeat start heartbeat for readonly queries which dont open transactions but requires locks for those that require transactions the heartbeat has already been started opentxn ',negative
'since there concept group dont invoke startgroupendgroup for mapper ',negative
'this implementation completely and exhaustively reverses the addgauge method above ',negative
'',negative
'row format serde ',negative
'smbjoin possible need correct bucketing ',negative
'revert ',negative
'nonjavadoc see ',negative
'that semanticanalyzer finds are use ',negative
'double scalarcolumn ',negative
'only create bucket files only dynamic partitions buckets dynamic partitions will created for each newly created partition todo iow integration full acid uses the else block create acids recordupdater hivefileformatutils and that will set ',negative
'drop the table exists ',negative
'note assume that workers run the same threads repeatedly can set the session here and will reused without explicitly storing the worker ',negative
'files size for splits ',negative
'metrics gathered default ',negative
'nonjavadoc see ',negative
'parameters used for jmx ',negative
'have create new object because the object belongs the code that creates and may get its value changed ',negative
'preserve operator before the gby well use resolve ',negative
'determine minimum all nonnull long column values maintain isgroupresultnull ',negative
'checks whether txn list has been invalidated while planning the query this would happen query requires exclusivesemishared lock and there has been committed transaction the table over which the lock ',negative
'hashmap javaobject ',negative
'isdynamicfunction used indicate the function not deterministic between queries ',negative
'date daytime interval timestamp ',negative
'this semijoin optimization should removed after were done iterating ',negative
'for now just small table values ',negative
'column statistics for column already found then merge the statistics ',negative
'here maybe compaction regular read delete event sorter the later cases should hive should compute sarg push down minmax key deletedelta ',negative
'commit the changes ',negative
'this the last keyvalue pair ',negative
'singlecolumn string specific members ',negative
'fmsketch treats the ndv all nulls but hll treates the ndv order get rid divide problem follow fmsketch ',negative
'allocate write ids for both tables and for all txns ',negative
'set the number instances which llap should run ',negative
'already registered ',negative
'clarification terms the originaldir directory represents the original directory the partitions files they now contain archived version those files the source directory the directory containing all the files that should the partitions note the har scheme ',negative
'nothing matches try creating custom counter ',negative
'when executes nonresultset query like create ',negative
'the following method introduces cast decimal and parent expression decimal ',negative
'set true ',negative
'create the serde ',negative
'check whether operation log file deleted ',negative
'this might seem counter intuitive but some queries like query select yearcalcsdate yrdateok from druidtableaucalcs calcs where yearcalcsdate null limit planed way where only push filter down and keep the project null hive project thus empty columns ',negative
'are getting the resources externally dont relocalize anything ',negative
'negative numbers flip all bits ',negative
'fallback for the case when orcsplit flags not contain hasbase and deltas ',negative
'tabletype ',negative
'small table file size ',negative
'optional int underscoreint ',negative
'make this method final improve performance ',negative
'analyze table partition compute statistics noscan ',negative
'subtree ',negative
'workaround for bug persisting nulls bytea column instead set empty bit vector with header ',negative
'pass marking invalid candidates checks based variance and ttl ',negative
'package visible that hmsmetricslistener can see them ',negative
'add granularity the row schema ',negative
'tolerance for long range bias and for short range bias ',negative
'long scalarscalar ',negative
'the case truncate table set the stats ',negative
'the dividedby logic consistent ',negative
'',negative
'hive normalizes partition spec for dates yyyymmdd format some versions java will accept other formats for datevalueof yyyymd and who knows what else the future some will not accept other formats cannot test normalization with them type check will fail before can ever happen thus test isolation ',negative
'end while ',negative
'trigger failover minihs and make sure the connections are closed ',negative
'uses deletedelegator kill job and ignores all exceptions ',negative
'console sql shell with command completion todo liuserfriendly connection promptsli lipage resultsli lihandle binary data blob fieldsli liimplement command aliasesli listored procedure executionli libinding parameters prepared statementsli liscripting languageli lixa transactionsli ',negative
'then this considered like the metastore server case ',negative
'production string ',negative
'check for two way join ',negative
'request intervalstt ',negative
'need keep the path part only because the hadoop will pass the path part only accept trailing the path with separator prevent partial matching ',negative
'into the rowmode mapjoinkey ',negative
'now add preconfigured users admin role ',negative
'stats setup ',negative
'delete the data the partitions which have other locations ',negative
'generate the groupbyoperator for the query block parseinfogetxxxdest the new groupbyoperator will child the param parseinfo param dest param param mode the mode the aggregation mergepartial partial param the mapping from aggregation stringtree the param groupingsets list grouping sets param groupingsetspresent whether grouping sets are present this query param whether grouping sets are consumed this group return the new groupbyoperator ',negative
'commit each partition gets moved out the job work dir ',negative
'patterns that are excluded verbose logging level filter out messages coming from log processing classes well run infinite loop ',negative
'set signum before result zero fastmultiply will set signum ',negative
'',negative
'obtain delegation tokens for the job ',negative
'case are stuck consume ',negative
'compare the column names and the order with the first tablepartition ',negative
'the time enter here the bytevalueslentgh and isnull must have already been compared ',negative
'enough compare the last level subdirectory which has the name event ',negative
'request llap splits for table ',negative
'set the catalog name hasnt been set the new table ',negative
'will fail ',negative
'pool ',negative
'construct list bucketing location mappings from subdirectory name ',negative
'generate resolved parse tree from syntax tree ',negative
'test the connection metastore using the config property ',negative
'else return ',negative
'run join releated optimizations ',negative
'build rel for having clause ',negative
'need initialize make sure nobody modified this table then current txn shouldnt read any data there conversion from nonacid acid table then default would assigned ',negative
'definite node constructed from specified number children that number nodes are popped from the stack and made the children the definite node then the definite node pushed the stack ',negative
'initialize the converter ',negative
'create filters top each setop child modifying the filter condition reference each setop child ',negative
'two vint without nanos ',negative
'should not occur since second parameter gettablewithqn false ',negative
'subsequent requests ',negative
'start the timer thread for cancelling the query when query timeout reached ',negative
'successfully scheduled ',negative
'the cumulative cardinality equal but the size bigger ',negative
'keep flushing until the memory under threshold ',negative
'since merge multiple operation paths assign new tags bottom layer reducesinkoperators this mapping used map new tags original tags associated ',negative
'found the proper columns ',negative
'add back probability qdigit ',negative
'',negative
'copy credentials ',negative
'write out integer part first then write out fractional part ',negative
'there are many ways have aux jars hive sigh ',negative
'make sure the new partition has the catalog value set ',negative
'verify that the schema now within the configuration the one passed ',negative
'write the current set valid transactions into the conf file ',negative
'readonly maps initialized once ',negative
'through hive ',negative
'dummy instance just trigger failover ',negative
'expected events not updated yet vertex success notification not received ',negative
'nothing needs done ',negative
'now vary isrepeating nulls possible only right ',negative
'get key element information ',negative
'check the affinity the argument passed with the accepted argument based the primitivegrouping ',negative
'are any the new transactions ones that care about ',negative
'backcheck force least output and this should have partial which empty ',negative
'all the tablespartitions columns should sorted the same order for example tables and are being joined columns and which are the sorted and bucketed columns the join would work long and are sorted the same order ',negative
'this trivial query block return ',negative
'existing ranges just accept the current ',negative
'ugi for the httphost spnego principal ',negative
'the real expression ',negative
'perform minor compaction since nothing was aborted subdirs should stay ',negative
'vector mode store char unpadded ',negative
'reduce only the parameters are significant ',negative
'tests that the absence stats for partitions andor absence columns ',negative
'only accept parse results parsed the entire string ',negative
'preevent listener ',negative
'must struct because list and map need parameterizedtype ',negative
'can boolean column which case return true count ',negative
'init output object inspectors the output partial aggregation list ',negative
'test retries when invocationexception wrapped metaexception wrapped jdoexception thrown ',negative
'initialize ',negative
'add spark job metrics ',negative
'only proceed the thenelse types were aligned ',negative
'this path points symlink path ',negative
'when have repeating values can unset the whole bitset once the repeating value not valid write ',negative
'grantee role check exists ',negative
'needed avoid file name conflict when big table partitioned ',negative
'same return one them ',negative
'every fields write null byte ',negative
'make sure the catalog name set the new partition ',negative
'note that the code removes the data from the field its passed the consumer expect have stuff remaining there only case errors ',negative
'allowrounding ',negative
'update the number entries that can fit the hash table ',negative
'further the dpp value needs generated from same subtree ',negative
'returns true thread pool created and can used for executing job request otherwise returns false ',negative
'change smallint and tinyint int ',negative
'single string key hash set optimized for vector map join the key will deserialized and just the bytes will stored ',negative
'initialize column buffers ',negative
'the dispatcher fires the processor corresponding the closest ',negative
'the boolean predicate ',negative
'validate false default disable the constraint ',negative
'create and add ast node with position grouping function input group clause ',negative
'simple case fits entirely the disk range ',negative
'cancel should noop either cases ',negative
'empty out side file ',negative
'',negative
'nonjavadoc see ',negative
'recursively create the exprnodedesc base cases when encounter column ref convert that into exprnodecolumndesc when encounter constant convert that into for others just ',negative
'need reconnect ',negative
'have valid batchiter and has more elements return them ',negative
'add sort clause that the row ids come out the correct order ',negative
'drop one table see what remains ',negative
'add the semijoin branch the map ',negative
'this version hadoop does not support hadoop introduces this new method ',negative
'constant node with value the agreed result ',negative
'key definitions related replication ',negative
'loginfoqueryplan ',negative
'disable web tests unless the test explicitly setting unique web port that dont mess ptests ',negative
'precision ',negative
'note hive ops not use the normal future failure path this will not happen case actual failure the future will just done the background operation thread was aborted ',negative
'parse the schema file determine the tables that are expected exist ',negative
'replace the path the file status the input path parquet may use the path the file status open the file ',negative
'the output position should not change since there are corvars ',negative
'close the connection soon the error message sent ',negative
'loginfofirst tail offset create list record firsttailoffset ',negative
'get the name file and look its properties see orccompresssize was respected ',negative
'copy nop skip itthis important for avoiding spurious overlap assertions ',negative
'enable disable test scheduling control ',negative
'add hiveconf variable with modes adaptor always use vectorudfadaptor for statements good vectorize but dont optimize conditional expressions better vectorize and optimize conditional expressions ',negative
'but should have already checked for that before this point ',negative
'eventtype ',negative
'update the target map work the second ',negative
'process failover ',negative
'the threshold where should use repeating vectorized row batch optimization for ',negative
'construct joinpredicateinfo ',negative
'precision ',negative
'table already there dont recreate ',negative
'rightinputrel ',negative
'never includes join condition the code was not modified for brevity ',negative
'desired schema does not include virtual columns partition columns ',negative
'create the dummy operators ',negative
'user not admin user allow the request only the user requesting for privileges for themselves role they belong ',negative
'two vint with nanos ',negative
'set the option for tolerating corruptions the read should succeed ',negative
'can null unit tests ',negative
'table newer leave ',negative
'unsecure case ',negative
'clean postconditions ',negative
'error out exit were not able parse the args successfully ',negative
'pig which uses hcat will pass this hcat that can find the metastore ',negative
'write some garbage the buffer that should erased ',negative
'when constructing the evaluator tree from exprnode tree look for any descendant leadlag function expressions they are found add them the llinfoleadlagexprs and add mapping from the expr tree root the llfunc expr ',negative
'precision ',negative
'suppress the stages vectorization off ',negative
'for types with parameters like varchar need determine the type parameters that should added this type based the original typeinfos ',negative
'validate all tasks ',negative
'always set the null flag false when there value ',negative
'update the queryid use the generated applicationid see comment below about why this done ',negative
'aid testing ',negative
'some value set ',negative
'first see gbkeys ',negative
'import table ',negative
'need not validate inppartspec here ',negative
'which will cause the scheduler loop continue ',negative
'use partition level authorization ',negative
'the magic bytes the beginning the rcfile ',negative
'check work has explain annotation ',negative
'print attr ',negative
'change these parameters ',negative
'this should database usage privilege once supported ',negative
'should honor the ordering records provided order select statement ',negative
'exhausted the batch longer have heartbeat ',negative
'lists clauses ',negative
'unlink connection between and its parent ',negative
'fill the child listcolumnvector with valuelist ',negative
'the line doesnt end with the delimiter the line empty add the cmd part ',negative
'grab sign bit and shift away ',negative
'the hivedecimalwritable set method will quickly copy the deserialized decimal writable fields ',negative
'this test calling ',negative
'padding leading zeroesones ',negative
'test stats deletion partition level ',negative
'create the list serialized splits for each bucket ',negative
'joins alias alias alias alias was not eligible for big pos must deterministic order map for consistent qtest output across java versions ',negative
'buffer one batch time for row retrieval ',negative
'test that existing sharedread with new sharedread coalesces ',negative
'first allocation write should add the table the nextwriteid meta table the initial value for write should and hence add with number write ids allocated here ',negative
'there are survivor spaces ',negative
'offset where this begins ',negative
'this method implementation preserved for backward compatibility ',negative
'offer accepted and gets evicted ',negative
'cant much expression not context filter cant treat null equivalent false here ',negative
'plus any correlated variables the input wants pass along ',negative
'',negative
'add struct field data the row ',negative
'priority ',negative
'short ',negative
'log error the acid table missing from the validwriteidlist conf ',negative
'are associated with transaction then heartbeat that well ',negative
'set list task ',negative
'call process once have full batch ',negative
'found unused port exit ',negative
'not put the unused duck back wed run the tasks below then assign priority ',negative
'merge filters from previous scan oring with filters from current scan ',negative
'ensures txn still there and expected state ',negative
'are only vectorizing reduce under tezspark ',negative
'partition specified get pruned partition list ',negative
'the constant value into and add the struct part exprnodestructs ',negative
'environment variables long values ',negative
'nodes need merged ',negative
'success ',negative
'remove all tok tokens ',negative
'there was preexisting work generated for the bigtable mapjoin side need hook the work generated for the associated with the rsmj pattern with the preexisting work otherwise need associate that the mapjoin linked the work associated with the rsmj pattern ',negative
'todobr could use combined instead list why not use expr ',negative
'error default value ',negative
'the size unavailable need assume size greater than this implies that merge cannot happen will return false ',negative
'the only case where duplicate elements matter the others are handled the below ',negative
'create scratch columns hold small table results ',negative
'the parsed statement contained dbtablename specification prefer that ',negative
'primitive writable class ',negative
'failed infer pkfk relationship for row count estimation fallback default logic compute denominator maxvry vsy maxvry vsy case multiattribute join ',negative
'mandatory property ',negative
'cbo did not optimize the query might need replace grouping function ',negative
'check multiset count ',negative
'secondary droptablecommand test for testing repldroptables effect partitions inside partitioned table when there exist partitions inside the table which are older than the drop event our goal this create table with repllastid say create partitions inside with repllastid and say now process drop table command with eventid should result the table and the partition with repllastid continuing exist but dropping the partition with repllastid ',negative
'nothing default ',negative
'doesnt seem like you should have this but java serialization wacky and doesnt call the default constructor ',negative
'the form partition ',negative
'stripe position within file ',negative
'server sets createtime ',negative
'create result cache object add one pair and retrieve them ',negative
'threshold percentage trigger the warning ',negative
'convert lowercase for the comparison ',negative
'use the keytab one was provided ',negative
'tokcreatefunction identifier stringliteral istempfunction toktemporary ',negative
'two events one for create and other for drop ',negative
'directory output results ',negative
'create groupingid column ',negative
'print job stages ',negative
'nulls and repeating ',negative
'complain about the deprecated syntax but still run ',negative
'get row size small table ',negative
'check elements the innermost array ',negative
'delete the bucket files now have empty delta dirs ',negative
'load next batch from disk ',negative
'pretend add trailing zeroes even when would exceed the ',negative
'right now the work graph pretty simple there preceding work have root and will generate map vertex there preceding work will generate ',negative
'create table database without location clause ',negative
'convert all nan and optionally infinity values vector null ',negative
'have already set the enabled conditions not met ',negative
'the table level and the storage handler level ',negative
'clear the username ',negative
'this not needed beyond compilation transient ',negative
'initialize the result ',negative
'not all argument elements need hold true ',negative
'used the framework runtime initialize the real initializer runtime ',negative
'setop rel ',negative
'commit the open txn which lets the cleanup txntowriteid ',negative
'close any open readers there was some exception during initialization rethrow the exception that the caller can handle ',negative
'nonjavadoc see ',negative
'create default route where events without queryid ',negative
'applicationspecific typecodes ',negative
'prepare stringbuilder for partid use future queries ',negative
'the reason why can get list split strategies here because for acid splitupdate case when have mix original base files insert deltas will produce two independent split strategies for them there global flag isoriginal that set ',negative
'this set drivercontext runtime ',negative
'optional string requestuser ',negative
'extract the values ',negative
'consider for large fill all isnull array and use the tighter else loop ',negative
'chose the table descriptor none the partitions present for consider the query select mapjoint count from join tkeytkey both and and partitioned tables but does not have any partitions fetchoperator invoked for and listparts empty that case use schema get the objectinspector ',negative
'happens case ',negative
'current number open txns ',negative
'adds the missing schemeauthority for the new partition location ',negative
'helper class submit fragments llap and retry rejected submissions ',negative
'this map which vectorized row batch columns are the key columns ',negative
'the mathmin and the fact that maxallocation int ensures dont overflow ',negative
'construct using ',negative
'validate closereopen ',negative
'metrics throws exception dont this when the key already exists ',negative
'easier read logs ',negative
'indicates whether cache statistics should collected ',negative
'exactly the same ',negative
'use magic length value indicate big ',negative
'init method hmshandler should not retried there are exceptions ',negative
'can save ourselves from spilling once have join emit interval worth rows ',negative
'one writable per row ',negative
'privileges obtained indirectly via roles ',negative
'override whats placed the configuration setup ',negative
'immutable maps ',negative
'generates plan for minmax when dynamic partition pruning ruled out ',negative
'the fastroundinteger methods remove all fractional digits set and ',negative
'reinstantiate the parent expression with new arguments ',negative
'smallint ',negative
'',negative
'regardless our matching result keep that information make multiple use for possible series equal keys ',negative
'future more sophisticated our conversion check ',negative
'for short range use linear counting ',negative
'release buffers are done with all the streams also see torelease comment ',negative
'heartbeat from task that are not currently tracking ',negative
'static partition created during setup dynamic partitions ',negative
'output top values ',negative
'stores datastore jpox properties ',negative
'lets not guess which one correct ',negative
'create table and pupulate with kvtxt ',negative
'lengths stream could empty stream already reached end stream before present stream this can happen all values stream are nulls last row group values are all null ',negative
'publish columns its subscribers ',negative
'responsible for capturing subquery rewrites and providing the rewritten query sql ',negative
'start offset compression buffer corresponding above row group ',negative
'dag scratch dir get session from the pool may different from tez one ',negative
'code true means current transaction started via start transaction which means cannot include any operations which cannot rolled back drop partition write nonacid table false its single statement transaction which can include any statement this not contradiction from the user point view who doesnt know anything about the implicit txn and cannot call rollback the statement course can fail which case there nothing rollback assuming the statement well implemented this done that all commands run transaction which simplifies implementation and allows simple implementation multistatement txns which dont require lock manager capable deadlock detection todo not fully implemented elaborate how this works also critically important ensuring that everything runs transaction assigns order all operations the system needed for replicationdr dont want allow nontransactional statements user demarcated txn because the effect such statement visible immediately statement completion but the user may issue rollback but the action the statement cant undone and has possibly already been seen another txn for example start transaction insert into transactionaltable values insert into select from transactionaltable rollback the user would for surprise especially they are not aware transactional properties the tables involved side note what should the lock manager with locks for nontransactional resources should release them the end the stmt txn some interesting thoughts ',negative
'guaranteed just because each dummystoreoperator can part only one work ',negative
'',negative
'filter does not change the input ordering filter rel does not permute the input all corvars produced filter will have the same output positions the ',negative
'increment cursor for perquery inclause list ',negative
'note edgecase here interaction with tablelevel repl load where that nukes out tablesupdated however explicitly dont support repl that sort and error out above ',negative
'the set operators the works that are merging need meet some requirements particular none the works that are merging can contain union operator this not supported yet might end with cycles the tez dag there cannot more than one dummystore operator the new resulting work when the operators are merged this due assumption mergejoinproc that needs further explored any these conditions are not met cannot merge ',negative
'used sortbased groupby mode complete partial partial ',negative
'set zookeeper dynamic service discovery configs ',negative
'the current record should not included the output detaillist ',negative
'set the values totalinputfilesize and totalinputnumfiles estimating them percentage block sampling being used ',negative
'total characters byte length ',negative
'flatten ',negative
'partition column ',negative
'',negative
'insert data legacy format ',negative
'example code ',negative
'this operator map join operator merge join operator ',negative
'first hash used locate start the block blockbaseoffset ',negative
'child operator object inspector converted its needed ',negative
'the tabledesc will null the case that all columns that table not used use dummy row denote all rows that table and the dummy row added caller ',negative
'this operator can removed ',negative
'accumuloinputformat complains you reset already set value just dont care ',negative
'for delete statements not null constraint need not checked ',negative
'bootstrap dump with open txn timeout ',negative
'write totalmonths dataoutput ',negative
'now change the resource plan change the mapping for the user ',negative
'only columns and constants can selected ',negative
'cascade ',negative
'determine the data and partition columns using the first partition descriptors partition count other words how split the schema columns the allcolumnnamelist and alltypeinfolist variables into the data and partition columns ',negative
'skip mapaggr gby ',negative
'ruleregexp rules are used match operators anywhere the tree ruleexactmatch rules are used specify exactly what the tree should look like particular this guarantees that the first operator the reducer and its parents are reducesinkoperators since begins walking the tree from ',negative
'check additional constraint ',negative
'stay with multikey ',negative
'encoded filenamechecksum files write into files ',negative
'the join the root but should always end with project operator ',negative
'state the rolling partition preceding numrowsrecived index represents the last output row preceding span rows before that are still held for subsequent rows processing the rows beyond numrowsprocessed followingspan ',negative
'hivedecimalwritable version ',negative
'functions ',negative
'set the configuration such that proxyuser can act behalf all users belonging the group foobargroup ',negative
'get the table from the client verify the name correct ',negative
'this relies findkeyreftoread doing key equality check and leaving read ptr where needed ',negative
'this used print the progress information pure text sample below map reducer map reducer map reducer map reducer ',negative
'limit can pushed down mapside all window functions need access rows before the current row this true for rank denserank and lead fns the window doesnt matter for lead the window for the function row based and the end boundary doesnt reference rows past the current row ',negative
'check any the roles this user owner ',negative
'the binarysortable serialization the saved key for possible series equal keys ',negative
'events insert last repl repldumpidx ',negative
'does require new job for grouping sets ',negative
'temporarily mark child all the ',negative
'should not affect schema conversion ',negative
'lookup long the hash multiset param key the long key param hashmultisetresult the object receive small table values information match for spill has information where spill the big table row return whether the lookup was match match spilled the partition with the key currently spilled ',negative
'nonjavadoc see ',negative
'ignore refcounts and errors for now ',negative
'surprise that midnight utc was local far are firm ground ',negative
'bootstrap repl ',negative
'cache indexlist ',negative
'use common conversion method share with ',negative
'for primitive object serialize plain string ',negative
'split starting row start will not read that row ',negative
'test single highprecision multiply random inputs arguments must integers with optional sign represented strings arguments must have digits and the number total digits ',negative
'handle decimal separately ',negative
'remove operator add ops new collection ',negative
'concurrent increase and termination increase fails ',negative
'exchange single partitions using complete partitionspec all partition columns ',negative
'column the target table that will pruned against ',negative
'need column expression other side ',negative
'call open mockmocktbl ',negative
'always collect input file formats ',negative
'not using position alias and number ',negative
'make new big table scratch column for the small table value ',negative
'nope look see hives conf dir has been explicitly set ',negative
'undone currently negative exponent not supported ',negative
'months produces type timestamp via calendar calculation ',negative
'optional long array length ',negative
'undone for now element list with null element null list ',negative
'use construct ',negative
'but for now just rely the cache put lock them before send them over ',negative
'references ',negative
'type timestamp plusminus type intervaldaytime ',negative
'see the execution thread has just completed operation and result available result available then return the result otherwise raise exception ',negative
'monotonic iff its first argument but not strict ',negative
'leap year ',negative
'globstatus api returns empty filestatus when the specified path does not exist but getfilestatus throw ioexception mimic the similar behavior will return empty array exception for external tables the path the table will not exists during table creation ',negative
'implicit assumption here that database level processed first before table level which will depend the iterator used since should provide the higher level directory listing before providing the lower level listing this also required such that the dbtracker tabletracker are setup correctly always ',negative
'any tablepartition directory not owned hive then assume table using storagebased auth set external transactional tables should still remain transactional ',negative
'already stopped else was never started another service failing canceled startup ',negative
'nothing start ',negative
'set temporary path communicate between the smallbig table ',negative
'destroy the sessions that dont need anymore ',negative
'filters are not over the rowid therefore scan everything ',negative
'switching tables between catalogs not allowed ',negative
'throw new not sort order and unique ',negative
'according the java documentation this does nothing but just case ',negative
'next call will eventually end which makes operation driver and starts its own clisessionstate and then closes which removes from threadloacal thus the session created this class gone after this fixed hiveendpoint ',negative
'see arraysbinarysearch javadoc ',negative
'',negative
'get nonssl socket transport ',negative
'compactor generated split for bucket that has data ',negative
'create row per database name ',negative
'add the last one ',negative
'the outputoi has all fields settable return ',negative
'case decided run everything local mode restore the ',negative
'before failover check are getting connection from minihs ',negative
'naked dot ',negative
'drop databaseschema exists databasename restrictcascade ',negative
'get credentials using the configuration instance which has hbase properties ',negative
'deserialize path offset length using filesplit ',negative
'test that adding multiple versions the same schema ',negative
'authenticate deny based its context completion ',negative
'operation emitter will used some internal druid classes ',negative
'set all properties specified the command line ',negative
'once enough rows have been output there need process input rows ',negative
'its key that this per hcatstorer instance object ',negative
'cant add null ',negative
'other columns provided nonnull values may able finish all rows with this input column ',negative
'http over thrift transport settings ',negative
'set child expressions ',negative
'make sure the bucketid max the numbuckets ',negative
'run major compaction ',negative
'need look further for checked variant this expression ',negative
'todo possible for heartbeats come from lost tasks those should told die which likely already happening ',negative
'just vint ',negative
'there anything wrong happen bail out ',negative
'select should return resultset ',negative
'and hash with mask out sign bit make sure its positive then know taking the result mod the range include salt argument this hash function can varied need rehash ',negative
'produce this sequence ',negative
'scsize ',negative
'working the assumption that single dag runs time per ',negative
'now create delete delta that has rowids divisible both and this will produce ',negative
'make sure the allocation transfered correctly return ',negative
'example file names are inputqoutmr inputqout ',negative
'jersey uses javautillogging bridge slf ',negative
'seek directly first record ',negative
'not group ',negative
'currently there way stop the metastore service will stopped when the test jvm exits this how other tests are also using metastore server ',negative
'note that shouldnt show from ',negative
'open session ',negative
'reference boolean column covert truth test ',negative
'set credentialprovider ',negative
'rows looked one repeated key need spill but filtered out rows need generated nonmatches too ',negative
'for operator the function name the operator text unless its our special dictionary ',negative
'iterate over the path partition descriptions find the partition that matches our input split ',negative
'the character set name starts with strip that ',negative
'check for escape character before the colon ',negative
'this colon escaped search again after ',negative
'joining condition ',negative
'processvectorgroup keybytes keylength keylength ',negative
'because have for true only sel and fil operators this rule will actually only match unionselfilfs the assumption here that are going have multiple operators multiple inserts current implementation does not support this more details please see hive ',negative
'http mode ',negative
'parse digits ',negative
'this will throw exception case the response from druid not array this case occurs for instance druid query execution returns exception instead array results ',negative
'remove all parts that are not partition columns see javadoc for details ',negative
'only keep the most significant decimaldigits digits ',negative
'copy all the values avoid creating more objects todo might cheaper always preserve data reset existing objects ',negative
'them ',negative
'prepare arguments for ',negative
'composite key types comma separated list different parts the composite keys order which they appear the key ',negative
'note this should never happen for tables ',negative
'data size still could not determined then fall back filesytem get file ',negative
'currently include all data partition and any vectorization available virtual columns the vrb ',negative
'for use ddl operations that only need shared lock such creating table for use ddl statements that not require lock ',negative
'set the rules for the graph walker for group and join operators ',negative
'were hijacking the big table evaluators and replacing them with our own custom ones ',negative
'there are filespartitions read need skip trying read ',negative
'expr alias parses but only allowed for udtfs this check not needed and invalid when there transform the ',negative
'add the additional postprocessing transformations needed ',negative
'uses the authorizer from sessionstate will need some more work get this run parallel however this should not bottle neck might not need parallelize this ',negative
'reducesink parents that missed ',negative
'note assume here that the data that was returned the caller from cache will not passed back via put right now its safe since dont anything but evict proactively will have compare objects all the way down ',negative
'wrong expression ',negative
'capacity exists ',negative
'allow implicit string double conversion ',negative
'couldnt determine common type dont cast ',negative
'when have yet another child beyond the current one save unselected ',negative
'object overhead bytes for int nanos bytes padding ',negative
'set ugi use kerberos have use the string constant support hadoop ',negative
'the creation time changed not check that ',negative
'ensure that get the right concrete columnmapping ',negative
'cast only needed for hadoop compatibility ',negative
'has limit use and not distribute the query ',negative
'typename datatype precision literalprefix literalsuffix createparams nullable casesensitive searchable unsignedattribute fixedprecscale autoincrement localtypename minimumscale maximumscale sqldatatype unused sqldatetimesub unused numprecradix ',negative
'optional string ',negative
'note support pruning the grouping set dummy key mergepartial case use the keycount passed the constructor and not ',negative
'can proceed with the conversion ',negative
'expect evicted but not failed ',negative
'boolean special case ',negative
'read each child node add results ',negative
'looks like some network outrage reset the file system object and retry ',negative
'hiveserver webui ',negative
'only the lightweight stuff ctor default llap coordinator created during ',negative
'current usage looks like its only for metadata columns but that changes then this method may need require type qualifiers aruments ',negative
'timeout ',negative
'boolean must come before the integer family its special case ',negative
'cascade only occurs with partitioned table ',negative
'the function should support both string and binary input types ',negative
'todo for llap assumption offheap cache ',negative
'match ',negative
'hash ineffective disable ',negative
'authorize drops there was drop privilege requirement and table not external external table data not dropped ',negative
'other rewrites ',negative
'',negative
'verify corrupted cache value gets replaced ',negative
'then remove the table ',negative
'colstatindex respond avglong avgdouble ',negative
'',negative
'number register bits ',negative
'preparation for hybrid grace hash join ',negative
'override the name provided repl load command ',negative
'olddbname and newdbname will the same were here ',negative
'this needs manually set under normal circumstances task does this ',negative
'for each file figure out which bucket ',negative
'should never happen ',negative
'discard context that cached for reuse per thread avoid allocating lots arrays and then resizing them down the line need bigger size ',negative
'all filters were executed during partition pruning ',negative
'nothing front this one prevent acquisition ',negative
'set the file owner hive the metastore run ',negative
'this method must return the decimal typeinfo for what getcasttodecimal will produce ',negative
'clean test space ',negative
'use boundarytype boundary amt sort key order behavior case preceding unb any any start current row any any scan backwards until row such rsk rsk start ridx ',negative
'this longer does expansions run commands the files used instead depends the developers have already unrolled those the files ',negative
'find out this synthetic predicate belongs the current cycle ',negative
'special entry for nondp case ',negative
'pruning sink operator with map join then pruning sink need not split separate tree add the pruning sink operator context and return ',negative
'there one and only one limit starting return the limit there limit return ',negative
'original files original directory base directory and delta directory ',negative
'first pass will drop the materialized views ',negative
'deserialize ',negative
'check null handling ',negative
'normalize name for mapping ',negative
'currently the long must fit markers setting these bit sizes determines the balance between max pool size allowed and max concurrency allowed this balance here not what want each while only objects limit but uses whole bytes and good for now delta and take the same number bits usually doesnt make sense have more delta ',negative
'cartesian product ',negative
'cache the hints before cbo runs and removes them ',negative
'error because thrift client have recreate base object ',negative
'lets see can convert aggregate into projects ',negative
'null null ',negative
'base since the presence base will make the originals obsolete ',negative
'this registration has done after knowntasks has been populated register for state change notifications that the waitqueue can reordered correctly the fragment moves out the finishable state ',negative
'require ownership there file require select insert and delete ',negative
'test for and precedence ',negative
'when splitupdate enabled for acid initialize separate deleteeventwriter that used write all the delete events case minor compaction only for major compaction history not required maintained hence the delete events are processed but not rewritten separately ',negative
'read the null byte again ',negative
'both cases this will the next day gmt ',negative
'figure out which factory were instantiating from hiveconf iff its not been set directly ',negative
'blank byte blank byte blank byte hyphenminus byte blank byte grave accent byte black sun with rays bytes ',negative
'javacc not edit this line ',negative
'check repeating ',negative
'command methods follow ',negative
'make sure that the wasnt escaped ',negative
'add any other header info ',negative
'table doesnt exist allow creating new one only the database state older than the update this inturn applicable for partitions creation well ',negative
'dpp work considered descendant because work needs finish for execute ',negative
'nonjavadoc see ',negative
'bytes the overhead for reference ',negative
'using targeta breaks this ',negative
'component ',negative
'update the parentop ',negative
'column not column reference bail out ',negative
'make sure will have less digits than ',negative
'this port already use try use another ',negative
'ensure size otherwise try again ',negative
'sleep half second ',negative
'table exists and older than the update now need ensure update allowed the partition ',negative
'handled via adminprivops see above ',negative
'correct testcase executed twice once with the typed setters once with the generic setobject ',negative
'the map vertex ',negative
'these are non standard version numbers cant perform the comparison these assume that they are incompatible ',negative
'test that locking table prevents locking partitions the table ',negative
'input and output are the same ',negative
'dont create splits for anything past logical eof ',negative
'pinged before log only occasionally seconds elapsed log again ',negative
'this implies that locks are needed for such command ',negative
'its value provided ',negative
'used load columns value into memory ',negative
'add would become recursive split ',negative
'call open read split mockmocktable ',negative
'note that not set location for repl load want that autocreated ',negative
'get our singlecolumn string hash set information for this specialized class ',negative
'the oldprunerpred and the newpprpred ',negative
'get delegation token for the given proxy user ',negative
'node disable timeout higher than locality delay ',negative
'the query contains windowing processing ',negative
'not will start transform the operator tree ',negative
'deep one bigger less the top ',negative
'allow create table only create should fail for rest the tables and hence constraints ',negative
'here want encode the error machine readable way json ideally errorcode would always set canonical error defined errormsg practice that rarely the case the messy logic below tries tease out canonical error code can exclude stack trace from output when the error specificexpected one its written stdout for backward compatibility webhcat consumes ',negative
'sum all nonnull double column values for avg maintain isgroupresultnull after last row last group batch compute the group avg when sum nonnull ',negative
'otherwise returns the expression that originated the column ',negative
'get task associated with this union ',negative
'verify integerdigitcount given fastscale ',negative
'return value modulo but always the positive range since prime this gives good spread for numbers that are multiples one billion which important since timestamps internally are stored number nanoseconds and the fractional seconds part often ',negative
'container reuse ',negative
'timestamp scalar case becomes use longdouble scalar class ',negative
'after classifying filters there more than joining predicate dont handle this return null ',negative
'creating new jars for classes that have already been packaged ',negative
'unnormalize divide get result ',negative
'find the new database ',negative
'get the internal map structure mapkeyvalue ',negative
'txnid was committed and thus not open ',negative
'unique the list ',negative
'get our own instance the transaction handler ',negative
'using such aggregate fileid cache not bulletproof and should disableable ',negative
'try with chunked stream here the chunked output didnt get chance write the endofdata ',negative
'the pruner should not have completed ',negative
'distkeylength doesnt include tag but includes bucknum cachedkeys ',negative
'perform decorrelation ',negative
'there mismatch bucketingversion then should set that way smb will disabled ',negative
'drop stats parameter which triggers recompute stats update automatically ',negative
'',negative
'select all with the null and not null child expressions and then expect the child not invoked ',negative
'optional string operationid ',negative
'current pos larger than shrinkedlength which calculated for each split table sampling stop fetching any more early exit ',negative
'write this out file and import into hive ',negative
'create rows file copy and rows file copy ',negative
'http path should begin with ',negative
'they arent the same but may able cast ',negative
'whether show link the most failed task debugging tips ',negative
'remember the condition variables for explain regardless whether specialize not ',negative
'reason compute interim row count where join type isnt considered because later ',negative
'some these tests require intercepting systemexit using the securitymanager safer registerunregister our securitymanager during setupteardown instead doing within the individual test cases ',negative
'throw file already exists that should never happen ',negative
'that the other locker hasnt checked yet and could lock well ',negative
'test longdouble version ',negative
'unsupported for the test case ',negative
'nonjavadoc see javautilmap ',negative
'clear the output ',negative
'enabling grouping the payload ',negative
'need handle offset with single digital hour see jdk ',negative
'arraynull compatible with any other arraytype ',negative
'and nodes need intersected ',negative
'all zeroes ',negative
'debug ',negative
'since dont have explicit end signal yet were going create and discard amnodeinfo instances per query ',negative
'the udf depends any external resources cant fold because the resources may not available compile time ',negative
'create view not use table alias ',negative
'tokens cannot used for the management protocol for now ',negative
'likely unsupported combination params not available yet skip benchmark cleanly ',negative
'only need execute get lock ',negative
'assume one request only for one file ',negative
'convert skewed table nonskewed table ',negative
'for calcite isdeterministic just matters for within the query ',negative
'nothing this eof ',negative
'the input already present make sure the new parent added the input ',negative
'this operator has not been removed include the list existing operators ',negative
'set http path ',negative
'submit the spark job ',negative
'now test with repeating flag ',negative
'based the paper daniel lemire streaming maxmin filter using more than comparisons per elem his algorithm works fixed size windows the current row for row and window computes the minmax for window the core idea keep queue max idx tuples tuple the queue represents the max value the range prev tupleidx idx using the queue data structure and following operations easy see that maxes can computed receiving the ith row drain the queue from the back any entries whose value less than the ith entry add the ith value tuple the queue ival the ith step check the element the front the queue has reached its max range influence fronttupleidx yes can remove from the queue the ith step the front the queue the max for the ith entry here modify the algorithm handle windows that are the form where numprecedingf numfollowing start outputing rows only after receiving rows the formula for influence range idx accounts for the following rows optimize for the case when numpreceding unbounded this case only max needs tarcked any given time ',negative
'row results should null ',negative
'only set the updater for insert for update and delete dont know unitl see the row ',negative
'simplified from position prefix synclength syncsize ',negative
'try instantiate the old replv task generation every event produced ',negative
'for each partition each table drop the partitions and get list ',negative
'value count rows ',negative
'loading metastore stats async execute simple ensure they are loaded ',negative
'since there was allocation failure dont try assigning tasks the next priority ',negative
'execute query ',negative
'permissions for the metrics file ',negative
'remove auth cookie ',negative
'prefix for top level properties ',negative
'otherwise this not sampling predicate and need ',negative
'cookies for requested uri authenticate user and add authentication header ',negative
'need use the expanded text for the materialized view will contain ',negative
'there are nulls our column ',negative
'the types for rows between range between windowing spec ',negative
'and expr ',negative
'start the split before this slice simple case will read cache from the split start offset ',negative
'someone else replacedremoved stale value try again ',negative
'the argumentcompletor allows match multiple tokens ',negative
'now create sparktasks from the sparkworks also set dependency ',negative
'determines should cache table its partitions stats etc ',negative
'the schema for intersect distinct like this all attributes countc cnt finally add project project out the last column ',negative
'otherwise fall through and proceed with nondecimal vector expression classes ',negative
'generate the serialized keys the batch ',negative
'indexes those equivalent columns ',negative
'nonjavadoc see langobject hand row each function provided there are enough rows for functions window call getnextobject each function output many rows possible based minimum output list ',negative
'relations involved join ',negative
'have not added this column before bail out ',negative
'precision char length ',negative
'returns single rowcolumn ',negative
'double ',negative
'even are just setting the scale when newscale greater than the current scale ',negative
'note hypothetically generic wmawaresession should not know about guaranteed tasks should have another subclass for however since this the only type for now this can live here ',negative
'principal name can user group role ',negative
'update clause ',negative
'fast check the next day directory exists return ',negative
'firstname john ',negative
'make compile happy ',negative
'check the function can short cut ',negative
'dont strip quotes ',negative
'start from end with ',negative
'reopen implies the use the reopened session for the same query that gave out for would have failed active query fail the user before its started ',negative
'set the session key token base encoded the headers ',negative
'not much can about honestly ',negative
'todo hive liststring materializedviews assertassertequals ',negative
'proper index dummy ',negative
'deal with dynamic partitions ',negative
'store away the keystore ',negative
'verify that the correct methods are invoked accumuloinputformat ',negative
'alter unpartitioned table set table property ',negative
'based securityutil ',negative
'setup inbloomfilter udf ',negative
'set transportmode ',negative
'thread spun start these other threads thats because cant start them until after the tserver has started but once tserverserve called arent given back control ',negative
'both delete events land corresponding buckets the original rowids ',negative
'first extract the information that the query provides ',negative
'',negative
'add these values mixedup green null char string with multbyte chars ',negative
'struct column can have null child column ',negative
'build with timestamp granularity column ',negative
'active passive enabled use default namespace ',negative
'need send the splits multiple buckets ',negative
'determine maximum all nonnull double column values maintain isgroupresultnull ',negative
'create queryid based route ',negative
'check that the bit the given index ',negative
'nonjavadoc see ',negative
'nodeofinterest the query ',negative
'ignore files length ',negative
'todo make add asynchronous add shouldnt block the higher level calls ',negative
'adding name the log file extra log line easier find the original there test error ',negative
'the schemadestf null means the destination not the local file system ',negative
'exit ',negative
'since may have both update and delete branches auth needs know ',negative
'verify that partition rename succeded ',negative
'match found use update the tables ',negative
'the constructor wasnt defined the implementation class flag error ',negative
'create the list record copy first record valuekey lengths there ',negative
'this number safety limit for writables ',negative
'get the process the field struct type ',negative
'asc desc ',negative
'multiline ',negative
'',negative
'add dependencies for the jars ',negative
'save the actual values from each row opposed the string representation ',negative
'note regardless what the input file format returns have determined with that only columns have values want any extra columns needed the table schema were set repeating null the batch ',negative
'make binary integer value the bytearray ',negative
'use tabledir rootdir ',negative
'further checks not file split return equality ',negative
'convert parent sel parent ',negative
'only left input repeating ',negative
'offset trims ',negative
'inject behaviour where repeats the insert event twice with different event ids ',negative
'returns whether record was forwarded ',negative
'ordered columns are the source columns ',negative
'note that theory are guaranteed have session waiting for here but the expiration failures etc may cause one missing pending restart ',negative
'myenumstructmap ',negative
'verify null output entry correct ',negative
'for given table and its bucket full file path list only keep the base file name remove file path etc and put the new list into the new mapping ',negative
'get the tblproperties value ',negative
'insert overwrite dynamic partition ',negative
'make sure get table with key returns empty list ',negative
'this operator allowed after mapjoin eventually mapjoin hint should done away with but since bucketized mapjoin and sortmerge join depend completely needed check the operators which are allowed after mapjoin ',negative
'tokens ',negative
'ids ',negative
'construct dummy null row indicating empty table and construct spill table serde which used input too ',negative
'the request has succeeded but failed add these partitions ',negative
'dont move this code the parent class theres binary incompatibility between hadoop and wrt minidfscluster and need have two different shim classes even though they are ',negative
'process sync entry minus syncescapes length read synccheck ',negative
'add jars that are already the tmpjars variable ',negative
'running ',negative
'doesnt have all skewed values within its data ',negative
'only accept struct which means that were already nested one level deep ',negative
'treat all inputs string the return value will converted the appropriate type ',negative
'',negative
'script operator blackbox hive optimization here assuming that nothing can pushed above the script same with limit create filter with all children predicates ',negative
'devenagari letter bytes ',negative
'handle null both sizes not repeating ',negative
'read paths from each symlink file ',negative
'change the current thread name include parent thread executed thread pool useful extract logs specific job request and helpful debug job issues ',negative
'get the total available memory from memory manager ',negative
'prefill the pool halfway ',negative
'ready start execution the cluster ',negative
'the dpp sink has target remove the subtree ',negative
'first hash used locate start the block blockbaseoffset subsequent hashes are used generate bits within block words avoid branches during probe separate masks array used for each longswords within block ',negative
'populate list exclusive splits for every sampled alias ',negative
'the expression can any one double long and integer try parse the expression that order ensure that the most specific type used for conversion ',negative
'close files ',negative
'update the topops appropriately ',negative
'retrieve generator ',negative
'make sure dbname and tblname are valid ',negative
'read not direct not need check its autho ',negative
'this means are hitting nested subquery dont need further ',negative
'that only allocate writeid only actually adding data adding partition data ',negative
'modify the options reflect the event instead the base row ',negative
'remember remove this when were out the loop cant the loop well get concurrent modification exception ',negative
'setup ',negative
'scalar query has aggregate and windowing and gby avoid adding sqcountcheck ',negative
'big value len big value bytes ',negative
'unprocessed role get its parents add processed and call this function recursively ',negative
'returns mapbucketnum listrecord ',negative
'swap get reference the left side ',negative
'theres point adding task with forcelocality set since that will never exit the queue add other tasks they are not already the queue ',negative
'used indicate the input sorted and shoudl used ',negative
'lastinputpath should changed the root the operator tree execmappermap ',negative
'there isnt already session name ahead and create ',negative
'asiaindia ',negative
'subqueries will need outer querys row resolver ',negative
'test that separate tables dont coalesce ',negative
'allowvoidprojection ',negative
'only release the related resources ctx drivercontext normal ',negative
'this can happen are querying the getfunctions before are getting the actual function between there can drop function user which case our call will fail ',negative
'iterate over partition columns figure out partition name ',negative
'set two dummy classes authorizatin managers two instances should get created ',negative
'invalid table partitioned but endpoints partitionvals empty ',negative
'reader schema was provided ',negative
'rename ',negative
'newline ',negative
'empty new database name ',negative
'the caller remembers the small value length ',negative
'set internal input format for all partition descriptors ',negative
'comparison null will always return false ',negative
'with multiple users concurrently issuing insert statements the same partition has side effect that some queries may not see partition the time when theyre issued but will realize the partition actually there when trying add such partition the metastore and thus get because some earlier query just created race condition for example imagine such table created create table name char partitioned string and the following two queries are launched the same time from different sessions insert into table partition values bob today creates the partition today insert into table partition values joe today will fail with that case want retry with alterpartition ',negative
'replication source file does not exist try cmroot ',negative
'now have complete statement process write the line buffer ',negative
'undone more ',negative
'create the mapjoinoperator ',negative
'conditional expression ',negative
'for better insertion performance values are added temporary unsorted list which will merged sparse map after threshold ',negative
'spot check ',negative
'jdbc says that means return all which the default ',negative
'validate the row values ',negative
'has been committed all others open ',negative
'drop table ignore error ',negative
'create mocked metastore client that returns table objects every time called will use same size for tableiterable batch fetch size ',negative
'rows forwarded will received listsinkoperator which replacing ',negative
'ensure columnnames are unique calcite ',negative
'unregister functions from local system registry that are not getallfunctions ',negative
'delete all the objects created ',negative
'todo side note the above lockrequestbuilder combines the both defaultacidtblpart entries ',negative
'enums are one two types fudge for hive enums strings come out ',negative
'struct ',negative
'get the sizes from the key buffer and aggregate ',negative
'use construct ',negative
'update buffer ',negative
'mapside join aliases ',negative
'copy the text ',negative
'some small alias not known too big ',negative
'adapts arrow batch reader row reader ',negative
'ptf invocation may entail multiple ptf operators ',negative
'option bypass task cleanup task was introduced hadoop mapreduce ',negative
'output positions ',negative
'this was the only predicate set filter expression null ',negative
'not done yet ',negative
'finally verify the key bytes match ',negative
'optional optional optional optional optional optional ',negative
'for binary join firstsmalltable the only small table has reference spilled big table rows for nway join since only spill once when processing the first small table only the firstsmalltable has reference the spilled big table rows ',negative
'partitionpruner may create more folding opportunities run constantpropagate again ',negative
'will swap the vectors from underlying row batch ',negative
'hive command operation types starts here ',negative
'need know cte not cte may have the same name table for example with select tab masking tab select from tab masking ',negative
'them they come back from restarts ',negative
'the abstract context for the kinds vectorized reading ',negative
'need the size above take effect ',negative
'the current table function has partition info specified inherit from the ptf the chain ',negative
'when splitupdate enabled not need account for buckets that arent covered this huge performance benefit splitupdate and the reason why are able because the deltas here are actually only the deletedeltas all the insertdeltas with valid user payload data has already been considered base for the covered buckets ',negative
'semishared can share with shared but not with itself ',negative
'allow anything ',negative
'attempt made save partition values nonpartitioned table throw error ',negative
'merge expressions ',negative
'the user explicitly importing new external table clear txn flags from the spec ',negative
'ceilnumsplits numpaths can get least numsplits splits ',negative
'not vectorize reduce ',negative
'apply the granularity function ',negative
'sourcetablename ',negative
'check number distinct keys greater than given max number entries ',negative
'can wrap inputs the execution vectorized use wrapper ',negative
'ascending ',negative
'maximum value seen far ',negative
'all external file systems ',negative
'analyze table ',negative
'rule cannot applied there are groupingsets ',negative
'original read block ',negative
'since set autocommit starts implicit txn close ',negative
'wipe out partition columns ',negative
'make sure get the right data back beforeafter compactions ',negative
'acid code path set rowid null ',negative
'inadequate total resources will never succeed wait for new executors become available ',negative
'does the same thing getfunctioninfo except for getting the function info ',negative
'filemetadata ',negative
'when the same node goes away and comes back the old entry will lost which means dont know how many fragments have actually scheduled this node ',negative
'bgenjjtree start ',negative
'fallback old mechanism which serves smb joins ',negative
'expected fail due old schema ',negative
'run hcat expression and return just the json outout ',negative
'null means the method does not accept number arguments passed ',negative
'miss locality request try picking consistent location with fallback random selection ',negative
'case success trigger scheduling run for pending tasks ',negative
'double the size the array needed ',negative
'combine splits only from same tables and same partitions not combine splits from multiple tables multiple partitions ',negative
'order which the results should output ',negative
'generic settings ',negative
'now flushforward all keysrows except the last current one ',negative
'returns true columns could inferred false otherwise ',negative
'slightly different depending where the test run specifically due file size estimation ',negative
'dynamic partitions ',negative
'merge the files the destination tablepartitions creating maponly merge job underlying data rcfile rcfileblockmerge task would created ',negative
'shutdown metrics ',negative
'flag for scan during analyze compute statistics ',negative
'llap not enabled noop ',negative
'this assumes that grouping will always used ',negative
'note that hive does not support udftodouble etc the query text ',negative
'decimal multiply ',negative
'hostname ',negative
'remove the resource plan disable all the queries die ',negative
'should have been able reach the union from only one side ',negative
'call super which calls super mapjoinoperator with ',negative
'set the semijoin hints parse context ',negative
'strip off leading blanks and check for sign ',negative
'hits misses tracked for candidate node ',negative
'best effort ',negative
'parse out the context and make sure isnt empty ',negative
'create table like tblname ',negative
'add the input newinput the set inputs for the query the input may may not already present the readentity also contains the parents from derived only populated case views the equals method for readentity does not compare the parents that the same input with different parents cannot added twice the input already present make sure the parents are added consider the query select from select from union all select from subq where both and depend select from select from addinput would called twice for one with parent and the other with parent when addinput called for the first time for parent added inputs when addinput called for the second time for the input from inputs picked and its parents are enhanced include and the inputs will contain parent parent parentsv the readentity already present and another readentity with same name added then the isdirect flag updated the values both ',negative
'bounded max executors ',negative
'for insert overwrite ',negative
'find first operator upstream with valid nonnull column expression map ',negative
'execute session hooks ',negative
'not empty least one the source tables being sampled and can not optimize ',negative
'for the children populate the newtooldexprmap keep track the original condition before rewriting for this operator ',negative
'cost must positive nudge ',negative
'bitset for flagging aborted write ids bit true aborted false open default value means there are open write ids the snapshot ',negative
'exercise broad range timestamps close the present ',negative
'this distinct udaf ',negative
'not analyze table column compute statistics statement dont any rewrites ',negative
'make sure that arglists size one ',negative
'convert mapjoin bucketed mapjoin the operator tree not changed but the mapjoin descriptor the big table enhanced keep the big table bucket small table buckets mapping ',negative
'factor includes scale ',negative
'roughly based biginteger code ',negative
'what the columnvector type the aggregation result ',negative
'need make sure that are using segment identifier ',negative
'not authorize temporary uris ',negative
'common generate join results from hash maps used inner and outer joins ',negative
'whose constructor would not have been called ',negative
'get our multikey hash set information for this specialized class ',negative
'create the select operator ',negative
'select countkey from far the reducer concerned ',negative
'the states where background operation progress wait for the callback also ignore any duplicate calls also dont kill failed ones handled elsewhere ',negative
'status change for active resource plan first activate another plan ',negative
'return false categories are not equal ',negative
'rcfile supports configurable serde ',negative
'this tests the case where older data has ambiguous list and not named indicating that the source considered the group significant ',negative
'the writes different directory ',negative
'numhashfunctions byte bitset array length bytes ',negative
'packing into vertex typically table scan union join ',negative
'will get regular single rows from the input file format reader that will need vectorrow deserialize ',negative
'parameters ',negative
'canevolve ',negative
'clear parameters lastinvoke ',negative
'this should fine since header should less than the configured header size ',negative
'since are walking backwards seek back buffer width that load the previous buffer rows ',negative
'test string ',negative
'batch already memory anyway will bypass the memory checks ',negative
'verify the content subdirs ',negative
'true are running test and the extra test file should used when the logs are ',negative
'and set the pattern and excludematches accordingly ',negative
'create orc file with small stripe size can write multiple stripes ',negative
'without data ',negative
'aborted ',negative
'this relies heavily what method determinesplits calls and doesnt ',negative
'add metastore ',negative
'got error might there anyway due permissions problem ',negative
'get the first live service instance ',negative
'formatteron ',negative
'convert input row standard objects ',negative
'they are both types strings that should fine ',negative
'consider generating column group equal value series ',negative
'equivalent works must have dpp lists same size ',negative
'only the threads need this thered most few dozen objects ',negative
'this should error analyze scope ',negative
'syllable mee bytes ',negative
'figure out there are any currently running compactions the same table partition ',negative
'nothing close here ',negative
'add the merge job ',negative
'create select operator ',negative
'nulls case not repeating ',negative
'the reason poll here that blocking queue causes the query thread spend nontrivial amount time signaling when element added wed rather that the time was wasted this background thread ',negative
'length each value the map ',negative
'writes the timestamptzs serialized value the internal byte array ',negative
'verify the config ',negative
'last singleton range ',negative
'this table has not been modified since materialization was created nothing ',negative
'have readentity defaultacidtblpart ',negative
'current buffer size should larger than initial size ',negative
'the group keys and distinct keys should the same for all dests using the first ',negative
'cant get any info without plan ',negative
'supposed dump path does not exist ',negative
'returns true passes the test false otherwise ',negative
'get cost the subset best rel may have been chosen not ',negative
'the txnid then there are transactions this heartbeat ',negative
'this the first keyvalueseparator this entry ',negative
'for fields descending order bit flip first ',negative
'returns nonnull fetchtask instance when succeeded ',negative
'reported once break ',negative
'make connection object that will throw exception ',negative
'bucket mapjoin llap ',negative
'invariant hllp ',negative
'sortmerge join ',negative
'have evicted the entire list ',negative
'nonjavadoc see javaioreader ',negative
'prepare data for client stat publishers any present and execute them ',negative
'extract the correlation out the filter ',negative
'multiple rules can matched with same cost last rule will choosen processor ',negative
'localize hiveexecjar well ',negative
'make sure for existing destination return false per filesystem api contract ',negative
'runtime that launches runnable tasks separate threads through taskrunners soon task isrunnable put queue any time most maxthreads tasks can running the main thread polls the taskrunners check they have finished ',negative
'for between clause ',negative
'read configuration parameters ',negative
'update sum the length the values seen far ',negative
'need add not null filter for constant ',negative
'step rename tmp output folder intermediate path after this point updates from speculative tasks still writing tmppath will not appear finalpath ',negative
'some existing chunk find max ',negative
'save some info for webui for use after plan freed ',negative
'now run its major compaction collapse events ',negative
'now duplicated field name should fail ',negative
'num distinct vals for col clause num distinct vals for col ',negative
'careful maintenance nulls ',negative
'grant option specified only update the privilege dont remove grant option has already been removed from the privileges the section above ',negative
'via hint ',negative
'maxparts ',negative
'after lead and lag call allow object associated with serde and writable associated with partition reset the value for the current index ',negative
'the element type not tuple subschema ',negative
'text ',negative
'first value repeated for all batches ',negative
'end digits ',negative
'jobclose has already been performed this operator ',negative
'simulate renaming via another metastore thrift server another hive cli instance ',negative
'data structures specific for vectorized operators ',negative
'call ',negative
'varchar between ',negative
'the client direct all calls catalog that does not yet exist ',negative
'copy into the current union task plan ',negative
'could wrapper with only size and get methods instead list sure ',negative
'create function desc ',negative
'for nonsingular args count can include null counted ',negative
'get buffer size and stripe size for base writer ',negative
'startdate sat letters day name ',negative
'for now keep the old logic for nonmm nondp union case should probably unified ',negative
'check for number created files ',negative
'these are from ',negative
'datanucleus wants autocreate but shall such thing ',negative
'caller will set signum ',negative
'reset the iter start ',negative
'columns and originalrr the original generated select ',negative
'todo should checked server side embedded metastore throws metaexception remote metastore throws ttransportexception ',negative
'max ndv across all column references from both sides table ',negative
'trailing spaces are not significant ',negative
'the leaves could shared the tree use set remove the duplicates ',negative
'source directory specified treat the target directory ',negative
'top the operator tree this could also reduce the amount data going the reducer ',negative
'the children are input ',negative
'done with this operator ',negative
'this also used for table conversion ',negative
'tests for the partition string partitionspecs string sourcedb string sourcetable string destdb string desttablename method ',negative
'set some conf vars ',negative
'mapping the fieldid the field ',negative
'for views the entities can nested default entities are the top level ',negative
'multigby singlers todo ',negative
'bogus encoding ',negative
'initialize with for nonacid and nonmm tables ',negative
'clone postjoinfilters ',negative
'shouldnt any others ',negative
'these are the filters which are common for every qtest ',negative
'tasks can exist the delayed queue even after they have been scheduled trigger scheduling only the task still pending state ',negative
'starting from the startnodes add the children whose parents have been included the list ',negative
'partsfound ',negative
'test adding constraint ',negative
'print the value ',negative
'execute set command and retrieve values for the conf vars specified above ',negative
'dont reset anything are reusing column vectors ',negative
'provide instance the code doesnt try make real instance just want test that fail before trying make connector with null username ',negative
'specify them the rules link ',negative
'part the big table portion the join output result ',negative
'passing query spec column names and column types used part hive physical execution ',negative
'process reduce sink added hiveenforcesorting ',negative
'resultschema will null cbo disabled cbo enabled with ast return path whether succeeded not resultschema will reinitialized will only not null cbo enabled with new return path and succeeds ',negative
'after reading the batch reset the pointer beginning ',negative
'preserve existing return type behavior for division nondecimal division should return double ',negative
'clusters buckets ',negative
'optional string amhost ',negative
'false and hence not found should error out ',negative
'dphj disabled only attempt bmj mapjoin ',negative
'test zerodivide show results null ',negative
'when dag specific cleanup happens itll better link this dag though ',negative
'update should take place such with replication ',negative
'check ifofserde ',negative
'note not alter the projectedcolumns projectionsize the batches just the included columns partition columns for now need model the object inspector rows because there are still several vectorized operators that use them need continue model the object having null objects for not included columns until the following has been fixed when have output struct for avg switch row groupby operators some variations vectormapoperator use the row super class process rows ',negative
'total size each hash entry ',negative
'requestline ',negative
'the current plan can thrown away after being merged with the original plan ',negative
'ensure this explicitly since versions before read doesnt ',negative
'external table and custom root specified update the parent path ',negative
'remove the ddl time that gets refreshed ',negative
'base name varchar fully qualified name such varchar ',negative
'are putting join keys last part the spilled table ',negative
'the table bucketed partition column not valid for bucketing ',negative
'granularity column ',negative
'hence need not allow same event applied twice ',negative
'validate the first parameter which the expression compute over ',negative
'decide default directory selection ',negative
'committed basetxnid ',negative
'close the underlying stream own ',negative
'prior singleton range ',negative
'instead multiple times ',negative
'data columns ',negative
'parameter was array arrays make sure that the inner arrays contain primitive strings ',negative
'bytemaxvalue ',negative
'have selected port client port update clientportlist necessary not the list add the port ',negative
'calendargetinstance calculates the currenttime needlessly cache instance ',negative
'position distinct column aggregator list map gby before rewrite ',negative
'this can happen for truncate table case for nonmm tables ',negative
'create barrier task for dependency collection import tasks ',negative
'gather big and small table output result information from the mapjoindesc ',negative
'bitpacking disabled all register values takes bits and hence can more flexible with the threshold for entries sparse map can allowed ',negative
'other references this can throw out ',negative
'copy the files out the archive into the temporary directory ',negative
'unsupported types error ',negative
'save connectionmap can closed users convenience ',negative
'tables ',negative
'the default was decided the serde ',negative
'here the layout export metadata warehouse acidtbl acidtblpart nonacidnonbucket nonacidorctbl nonacidorctbl delta orcacidversion bucket delta orcacidversion bucket delta orcacidversion bucket timport directories files ',negative
'this assumes all struct cols immediately follow struct ',negative
'analyze and create tbl properties object ',negative
'not needed without semijoin reduction ',negative
'know need bail out ',negative
'create http client with retry mechanism when the server returns status code ',negative
'only need promote comptype existingtype for efficiency check existing exclusive which case need never promote comp exclusive sharedwrite which case can promote even though they may both shared write comp sharedread theres never need promote ',negative
'since cannot know what columns will needed ptf chain not prune columns ptfoperator for ptf chains ',negative
'copy limit ',negative
'tolerance check double equality ',negative
'set where derby logs ',negative
'create new table similar previous one ',negative
'didnt find any lock with extlockid but readcommitted there possibility that existed when above delete ran but didnt have the expected state ',negative
'set ',negative
'nonjavadoc see javaioreader ',negative
'for cases where the table external ',negative
'long add them list order preserved from now ',negative
'oracle doesnt exhibit this problem ',negative
'compare timestamp timestamp ',negative
'the bucketingversion not relevant here never used for smb look the parent tables bucketing versions and for ',negative
'week granularity ',negative
'create the object inspector and the lazy binary struct object ',negative
'check oozie has set hcat deleg token use ',negative
'move past separator ',negative
'indicated selectedinuse and the sel array ',negative
'will trigger cleanup ',negative
'simplify vector bruteforce flattening nonulls and isrepeating this can used reduce combinatorial explosion code paths vectorexpressions ',negative
'are viewfs dont want use tmp tmp dir since rename from tmp final userhivewarehouse will fail later instead pick tmp dir same namespace tbl dir ',negative
'get the named url from user specific config file present ',negative
'only used for testing ',negative
'cast string ',negative
'not filter when ptf reducer ',negative
'collection separator ',negative
'case operator need rewrite ',negative
'execute query druid ',negative
'compute count only the register values are updated else return the cached count ',negative
'partitions ',negative
'create gss context ',negative
'',negative
'check table with the new name already exists ',negative
'invoked for test classes ',negative
'when adding support for new types should try use classes hive value system keep things more readable though functionally should not make difference ',negative
'maximum number decimal digits decimal long ',negative
'singlerow case theres next ',negative
'prefix for column names auto generated hive ',negative
'location not set utilize metastorewarehouse together with database name ',negative
'when negative when decimal zero when positive ',negative
'analyze table partition compute statistics the plan consists simple teztask followed statstask the tez task just simple tablescanoperator ',negative
'execute statement with the conf overlay ',negative
'that fractions query parallelism add etc ',negative
'whether have enforce sort anyway case deduplication ',negative
'more variations callbacks increase decrease and decrease increase the call coming before the message sent message should ever sent ',negative
'using metastore running existing cluster ',negative
'test normal drop should drop unconditionally ',negative
'then groups this arbitrary ',negative
'asserts the class invariant same types ',negative
'test andfilter operation ',negative
'where write our key and value pairs ',negative
'longmaxvalue ',negative
'will return back the original token which know insufficient ',negative
'covers both streaming api and post compaction style ',negative
'will unset and only these should updated and nothing for ',negative
'any destination partition present then throw semantic exception ',negative
'add only dynamic partition columns the temp table input data file ',negative
'this should fail ',negative
'sleep for ',negative
'set now can verify changed ',negative
'rare case buffer boundary unfortunately wed have copy some bytes ',negative
'this effectively the same the dense register impl ',negative
'sorts ',negative
'before the ownertype exists old hive schema user was the default type for owner lets set the default user keep backward compatibility ',negative
'test the inputformat execution path ',negative
'just fetch one blob weve serialized thrift objects final tasks ',negative
'llapcommon llaptez llapserver hiveexec hivecommon https deps jetty rewrite class registry ',negative
'select statement dynamic sql ',negative
'timeout minutes ',negative
'operator tree for processing row further optional ',negative
'test adding multiple partitions single partitionset atomically ',negative
'default the list empty operator wants add more counters should override this method and provide the new list counter names returned this method should wrapped counter names the strings should passed through ',negative
'add partitions the filesystem ',negative
'upload archive file hdfs ',negative
'otherwise wed create nullwritable and that isnt what want ',negative
'keep the hash multiset result for its spill information ',negative
'check giving invalid address causes retry connection attempt ',negative
'tag ',negative
'remain consistent need set input and output formats both ',negative
'start metrics for standalone remote mode ',negative
'success only all the commands were successful ',negative
'update max length new length greater than the ones seen far ',negative
'thread pool for actual execution work ',negative
'create join need check whether need update left case ',negative
'parition column case partition filter will evaluated partition pruner will not evaluate partition filter here ',negative
'give lot slack since low numbers consistent hashing very imprecise ',negative
'run aggregatejoin transpose cost based failed because missing stats continue with ',negative
'concurrent revocation and increase before the message sent ',negative
'localjobrunner does not work with mapreduce outputcommitter need use minimrcluster mapreduce ',negative
'distinct partitions created txnid because want txn used previous driverruninsert ',negative
'string range values empty strings ',negative
'call renamepartition without environment context ',negative
'run value expressions over original whole input batch ',negative
'create table scenarios ',negative
'nothing here ',negative
'remember the original parent list before start modifying ',negative
'initialize the execution engine based cluster type ',negative
'prefer numeric type arguments over other method signatures ',negative
'nearly from orcinputformat there are too many statics everywhere sort this out ',negative
'assume the index bad and full scan ',negative
'batch size from input and decaying factor ',negative
'indicates type was derived from the deserializer rather than hives metadata ',negative
'rxrx and and rand order limit ',negative
'find ',negative
'only last rows qualify ',negative
'delayed find local match ',negative
'this check isnt absolutely mandatory given the aborted check outside the ',negative
'run query loop that hit occasionally ',negative
'nonjavadoc see ',negative
'find the bucket and switch buckets need ',negative
'just return stats gathering should not block the main query ',negative
'create session test the hook got fired checking the expected property ',negative
'sets the env variable value defined ',negative
'for java serialization only ',negative
'undone null false ',negative
'special case handling for multior and multiand ',negative
'setup dynamic partition pruning where possible ',negative
'need add filter create tableop filterdesc and set child top ',negative
'throwing would more appropriate but not change the api ',negative
'nonjavadoc see int ',negative
'skip from block block since only need the header ',negative
'filter operator has the same output columns its parent ',negative
'calculate unselected ones last evaluate ',negative
'will fail ',negative
'neither open nor opening ',negative
'authorize the revoke and get the set privileges revoked ',negative
'nothing registration involved ',negative
'serialize the keys and append the tag ',negative
'add all udtf columns ',negative
'build the new list ',negative
'optimize plan ',negative
'used for create mapjoindesc should order ',negative
'handle null return the type pca ',negative
'since have base there must least delta which must result acid write must immediate child the partition ',negative
'',negative
'parent opsselgbrsgbrs ',negative
'undone dont reuse for now ',negative
'otherwise value may too long convert appropriate value based params ',negative
'reduce sink group operator ',negative
'add user privileges ',negative
'check julian days between jan and jan ',negative
'first row determines isgroupresultnull and long firstvalue stream fill result repeated ',negative
'make insert into nonacid take shared lock ',negative
'null null ',negative
'handle null return the type pcb ',negative
'create delete delta that has rowids divisible but not this will produce ',negative
'mergecount ',negative
'optional bytes ',negative
'props new properties ',negative
'direct encoding ',negative
'begin write abort ',negative
'join with groupby having orderby ',negative
'try deserialize using deserializeread our writable row objects created serializewrite ',negative
'must called last ',negative
'get the last valid row the batch still available ',negative
'typename used however now the value dbnametablename make backward compatible take the tablename part typename ',negative
'set the java key provider for encrypted hdfs cluster ',negative
'safeguard against potential issues cbo rowresolver construction disable cbo for now ',negative
'input type date epochdays ',negative
'execute ',negative
'asserttrueve instanceof ',negative
'note wmfragmentcounters are created before tez counters are created ',negative
'for import statement require uri rwxowner privileges input uri and necessary privileges the output table and database note privileges are only checked the object that type marked part readentity writeentity table present import will mark table writeentity and well authorize for that and not present ',negative
'rounding results ',negative
'should have all the necessary information vectorize ',negative
'has also been adjusted point these buffers instead compressed data for the ranges ',negative
'thread reading the ats guid ',negative
'need iterate detect original directories that are supported but not acid ',negative
'partial partition spec supplied make sure this allowed ',negative
'column name not contained needed column list then partition column not need evaluate partition columns ',negative
'isforcedeactivate ',negative
'now generate insert statement ',negative
'variablelength arguments ',negative
'inheritdoc ',negative
'all hadoop versions ',negative
'this error should really produced hive ddltask ',negative
'reposition the begining ',negative
'gets lock ',negative
'possible all children have same expressions but not likely ',negative
'use old timestamp writable hash code for backwards compatibility ',negative
'binary transport settings ',negative
'didnt set the last repl due some failure ',negative
'adding these job properties will make them available the outputformat checkoutputspecs ',negative
'pass null for aliases here because filters aliases the children node context and need filter them the current joinoperators context ',negative
'create table object ',negative
'todo constant constant ',negative
'the table not external and might not subdirectory the database ',negative
'looks for hivesitexml from the classpath found this class parses the hivesitexml return set connection properties which can used construct the connection url for beeline connection ',negative
'common case the segment one buffer ',negative
'here after commitabort but before next currenttxnindex still ',negative
'when supported read field field number random access currently only supports this return return true when the field was not null and data put the appropriate current member otherwise false when the field null ',negative
'return the column numbers the bucketed columns ',negative
'pools ',negative
'ignore ',negative
'inform the shuffle handler ',negative
'used cleanup cache ',negative
'past the timeout ',negative
'tracks requests executing per node ',negative
'nonjavadoc see javautillist ',negative
'process the actual join ',negative
'verify the configs are merged ',negative
'count characters forward ',negative
'referring job tracker and resource manager ',negative
'check select involves udtf ',negative
'overwrite the remote file already exists whether the file can added executor spark sparkfilesoverwrite ',negative
'global config vectorized input format enabled check these inputformats are excluded ',negative
'this input rel does produce the cor var referenced assume fieldaccess has the correct type info ',negative
'native vectorization supported ',negative
'make sure minihs the new leader ',negative
'allow implicit conversion from byte integer long float double ',negative
'startup may unable get number executors ',negative
'based actual timing ',negative
'reduce vertex ',negative
'checked semanticanalyzer should not happen ',negative
'write the first part the array ',negative
'test deprecated api ',negative
'all good ',negative
'analyzecreatetable uses thisast but dophase doesnt only reset here ',negative
'case were searching through especially large set data send heartbeat order avoid timeout ',negative
'case column stats hash aggregation grouping sets ',negative
'check removes the property ',negative
'the partition column were interested ',negative
'table non hive catalog ',negative
'run times present then set indicating values ',negative
'assume the existing vector always valid ',negative
'update update join key use ',negative
'first find first record for the key ',negative
'',negative
'the join keys cannot transformed the subquery currently will only return the base table scan the join keys are constants column even simple cast the join keys will result null table scan operator case constant join keys they would ',negative
'nondeterministic functions well runtime constants are not materializable ',negative
'specialized class for native vectorized reduce sink that reducing uniform hash single long key column ',negative
'all but decimal ',negative
'via the properties ',negative
'roles grants ',negative
'now only string text int long byte and boolean comparisons are treated special cases for other types reuse ',negative
'null value with different type need introduce cast keep ',negative
'note this method must call before exiting ',negative
'any other queries running the session ',negative
'all getxxx needs tolowercase because they are directly called from semanticanalyzer all setxxx does not need because they are called from which already lowercases the aliases ',negative
'decimal debugging ',negative
'make sure compiles with both hadoop and hadoop ',negative
'just created this directory its not case precreation nuke ',negative
'test this restricts events generated those that match provided message format ',negative
'istezorspark ',negative
'make this client wait job trcker not behaving well ',negative
'',negative
'start blocking run one the tasks the main thread ',negative
'these are the columns the big and small table that are bytecolumnvector columns ',negative
'get both tablealias and column name from columnorigin ',negative
'try with extra base ',negative
'replication case export table tbl location for replication ',negative
'filesinkoperator knows how properly write ',negative
'reason retry the challenge ticket not valid ',negative
'valid inputs possible case ',negative
'note can really slow ',negative
'alter table the wrong catalog ',negative
'cmapstringstring ctinyint csmallint cfloat cbigint ',negative
'construct sortrel ',negative
'first stripe will satisfy the predicate and will single split last stripe will ',negative
'initialize string table lazy fashion ',negative
'they cancel each other ',negative
'hive doesnt support currency type ',negative
'walk through the join condition building ndv for selectivity ',negative
'first through and set all our values for datanucleus and javaxjdo parameters this ',negative
'store the mapping path bucket number this needed since for the maponly job any mapper can process any file for mapper processing the file corresponding bucket should also output the file corresponding bucket the output ',negative
'left semi join hash set ',negative
'break the client ',negative
'verify ',negative
'dirname uniquely identifies destination directory filesinkoperator more than one filesinkoperator write the same partition this dirname should different ',negative
'ignored ',negative
'make sure credential provider path points ',negative
'this methods main use help unit testing this class ',negative
'common one time setup native vectorized map join operators processop ',negative
'test select root from ',negative
'transaction batch size case ',negative
'the underlying database field varchar need compare numbers ',negative
'this the bit where make sure dont group across partition schema boundaries ',negative
'the set pruning sinks ',negative
'node now has free capacity task should allocated ',negative
'user can override value hive config true ',negative
'use the existing tableinfo object else create new one ',negative
'immutable causes copyobj return the original object ',negative
'',negative
'for comma ',negative
'assume the high watermark can used maximum transaction ',negative
'this case possible path not enabled ',negative
'codes and messages this should fixed ',negative
'compile internal query capture underlying table partition dependencies ',negative
'the authenticated user ',negative
'nonjavadoc see javaioreader int ',negative
'run selftest query doesnt work will selfdisable what pita ',negative
'immutablelist ',negative
'selectively used fetch formatter ',negative
'revert configs not affect other tests ',negative
'subquery was just one conjunct ',negative
'test singlethreaded implementation checker throws hiveexception when the there dummy directory present the nested level ',negative
'',negative
'let transaction aborted ',negative
'cancel given delegation token ',negative
'capacity force should allocate otherwise ',negative
'done ',negative
'the callable shouldnt null execute the thread pool also should configured execute requests ',negative
'loginfowriting value valueoffset length tailoffset valueoffset ',negative
'sans header row ',negative
'check the output fixacidkeyindex should indicate the index was valid ',negative
'table partitions statistics and table partitions column statistics are accurate not ',negative
'cannot performed maponly job ',negative
'number digits mask from the end ',negative
'switch from based start offset the hive end user convention based start offset the internal convention ',negative
'try allocate from targetsized free list maybe well get lucky ',negative
'put shuffle version into http header ',negative
'vertex mergejoin ',negative
'this means the exception was caused something other than race condition creating the partition since the partition still doesnt exist ',negative
'find the skew information corresponding the table ',negative
'boundary length will span two compression buffers ',negative
'requesting for the stats source will implicitly initialize ',negative
'simulate partition update ',negative
'specialized class for doing vectorized map join that left semi join singlecolumn string using hash set ',negative
'mdonly table alter ',negative
'skip first child not involved the revert boolean the target type needs account for all operands ',negative
'need find the tables and data drop not part this dump ',negative
'handle the status change ',negative
'cause any problem the cleaner thread will remove this when this jar expires ',negative
'determine the the table from the job conf stored context the table properties are copied job conf therefore should able retrieve them here and determine appropriate behavior note that this will meaningless for nonacid tables will set null ',negative
'',negative
'return true retain item and false filter out ',negative
'thought creating template for each shims but couldnt generate proper mvn script ',negative
'',negative
'set the big table position both the reduce work and merge join operator should set with the same value ',negative
'create minimalistic table ',negative
'silent overflow ',negative
'boolean invert not expression left expression right expression ',negative
'add the new paths the znodes list well try for their removal well ',negative
'all the queries are maponly anyway the query most optimized ',negative
'inject properties from the main app that matches allowedprefix ',negative
'the name should not changed reload the with the original name ',negative
'end semijoinrulejava ',negative
'such base created compaction case nonacid acid table conversion definition there are open txns with ',negative
'verify that multi byte like expression doesnt match nonmatching string ',negative
'fail similarly when memory allocations fail ',negative
'this means have just created table and are specifying partition the load statement without precreating the partition which case lets use table input format class inherittablespecs defaults true when new partition created later will automatically inherit input format from table object ',negative
'remove countdistinct mapside gby ',negative
'create scratch dirs for this session ',negative
'depends this ',negative
'order sort clause ',negative
'extract the actual row from row batch ',negative
'nobucket table capture bucketid from streamedtable workaround hive bug that prevents joins two identically bucketed tables ',negative
'move past pair separator ',negative
'float ',negative
'bgenjjtree typestring ',negative
'todo clean exit ',negative
'check dead session get cleared ',negative
'see link nextnullwritable vectorizedrowbatch first and link when reading split original file and need decorate data with rowid this requires treating multiple files that are part the same bucket tranche for unbucketed tables single logical file number rowids consistently todo this logic executed per split every original file the computed result the same for every split form the same file this could optimized moving beforeduring split computation and passing the info the split hive ',negative
'not backward compatible ',negative
'handle remaining lower long word digits integer digits ',negative
'look for bean style accessors getfieldname and isfieldname ',negative
'https cannot done with zero copy ',negative
'subqueries ',negative
'and finally save the ',negative
'for now leave decimal precisionscale the name decimalcolumnvector scratch columns dont need their precisionscale adjusted ',negative
'task details fetch task tracker url ',negative
'this should eventually hang the delay code ',negative
'get http service port ',negative
'find the sourceinfo put values ',negative
'this case missing specification utf string storage ',negative
'getgenericudf actually clones the udf just call once and reuse ',negative
'new key ',negative
'return lowsurrogate ',negative
'user can override value for hive config false ',negative
'ignore the exception and fall through the default currentstateid ',negative
'empty string args ',negative
'note hadoop metric reporter does not support tags create single reporter for all metrics ',negative
'autosave then save ',negative
'cache rows guaranteed contain precedingspan rows before nextrowtoprocess ',negative
'expected ',negative
'stream variables ',negative
'calculation below consistent with also are capping the bloomfilter size below ',negative
'across process boundary normalized and stored type and not carried data for each row ',negative
'need generate limit ',negative
'leverage tez improve synchronization and the progress report behavior ',negative
'single threaded scheduler for tasks from wait queue executor threads ',negative
'the simd optimized form ',negative
'scale down does rounding ',negative
'with the sessions after thru all the concurrent user actions ',negative
'secure only set the registering service anyone can read the registrations ',negative
'replacing the right thing though since expect the kill all the fragments running the node via timeouts deallocate messages coming from the old node are sent the nodeinfo instance for the old node ',negative
'handled below ',negative
'use the serialization scale and create biginteger with trailing zeroes round the decimal necessary since are emulating old behavior and recommending the use instead just the slow way get the bigdecimalsetscale value and return the biginteger ',negative
'add the columns residual filters ',negative
'make copy since intend mutate sum ',negative
'construct using ',negative
'inherit the environment variables ',negative
'eventually enough small writes should result another buffer getting created ',negative
'get all locks ',negative
'admin check allows when set false when set true checks true and the logged user via pam spnego kerberos list ',negative
'where missing columns are nullfilled ',negative
'without extra structs ',negative
'parse out and havent already done and while were also parse out the precision factor the user has supplied one ',negative
'log not static make debugging easier being able identify which subclass ',negative
'null path unmanaged ',negative
'put accessed columns readentity ',negative
'requesting more partitions than allowed should throw exception ',negative
'helper setup default environment for task yarn ',negative
'add and verbose print verbose message ',negative
'timestamp ',negative
'querys session has compile lock timeout secs should ',negative
'first check will allow the user create table ',negative
'sortmerge join ',negative
'number variables and assignment expressions ',negative
'use session registry see ',negative
'are collapsing figure out this new row ',negative
'events are processed otherwise task metrics may get lost see hive ',negative
'spot check only nonstandard cases are checked for the same template another test ',negative
'are currently performing binary search the input dont forward the results currently this value set when query optimized using compact index the map reduce job responsible for scanning and filtering the index sets this value remains set throughout the binary search executed the until starting ',negative
'extracted from functionregistry ',negative
'partial partition spec has null parthandle ',negative
'use construct ',negative
'this means that the lock ready cleaned hence cannot ',negative
'cds are reused thry partition sds detach all cds from sds then remove unused cds ',negative
'this oid for kerberos gssapi mechanism ',negative
'advance the reader until reach the minimum key ',negative
'expect readreader return same key value objects common case ',negative
'well set for all others ensure determinism ',negative
'not sequential with previous ',negative
'add whether the row filtered not this value does not matter for the dummyobj because the join values are already null ',negative
'logger attempts ',negative
'doing select first less efficient but makes easier debug things ',negative
'uts there standalone hms running kick off compaction its done via runworker but normal usage concatenate blocking ',negative
'table properties ',negative
'metadatastore ',negative
'need input object inspector that for the row will extract out the ',negative
'noop for nontest mode for now ',negative
'flag that helps set the correct driver state finally block tracking the method has been returned error not ',negative
'rewrite the load launch insert job ',negative
'hive will always require user specify exact sizes for char varchar binary doesnt need any sizes decimal has the default ',negative
'reusing the tokenrewritestream map for views not overwrite the current tokenrewritestream ',negative
'most common scenario ',negative
'inside ',negative
'find which column contains the raw data size both partitioned and non partitioned ',negative
'varchar string length beyond max ',negative
'allocate new vectorization context reset the intermediate columns ',negative
'there should new directory base ',negative
'the parent same the then have cycle ',negative
'have get mtable again because datanucleus ',negative
'this for basic stats ',negative
'all key input columns are repeating generate key once lookup once ',negative
'dont need merge add the move job ',negative
'currently cannot handle nested complex types ',negative
'timestamp intervaldaytime intervaldaytime timestamp ',negative
'use construct ',negative
'hive removes liststatus from the code path there should only one read ops open after hive ',negative
'avro only allows maps with strings for keys only have worry about deserializing the values ',negative
'case need for the other case ',negative
'segment metadata query that retrieves all columns present the data source dimensions and metrics ',negative
'find all the indexes the sub byte ',negative
'undone for list and map yes for struct and union when field count different ',negative
'nonjavadoc see ',negative
'path for split unqualified the split from being sampled serves more than one alias the alias serves not sampled serves different alias than another path for the same split ',negative
'update the partition col stats for table cache ',negative
'single long value hash map based the serialize the long key into binarysortable format into output buffer accepted ',negative
'executeupdate prepared statement ',negative
'convert byteswritable byte ',negative
'oldpath destf its subdir its should definitely deleted otherwise its existing content might result incorrect extra data but not sure why changed not delete the oldpath hive not the destf its subdir ',negative
'set the context attribute true which will interpreted the request interceptor ',negative
'connections guard rails ',negative
'for now disable the test attempts ',negative
'gather output works operators ',negative
'not currently supported ',negative
'assign values from the row local variables ',negative
'expressions for project operator ',negative
'expressions these comparisons are anded together ',negative
'even large say varchar columns most strings are small ',negative
'optional submissionstate ',negative
'nonjavadoc see ',negative
'the last field the union field any ',negative
'the token file location comes after mainclass propval ',negative
'test integer ',negative
'outpath does not exist then means all paths within combine split are skipped they are incompatible for merge for example files without stripe stats those files will added incompatfileset ',negative
'stop nonexistent option ',negative
'first try temp table ',negative
'replication done need check new value set for existing property ',negative
'use hivevarchars internal text member read the value ',negative
'remove them ',negative
'add column info corresponding partition columns ',negative
'create map local operators ',negative
'counter for rows emitted ',negative
'share the code with recordreader ',negative
'for fullacid dont want delete any files even for overwrite see hivehive ',negative
'setup whitelist ',negative
'mapjoin and smbjoin not supported ',negative
'the plan consists statstask only ',negative
'may not own the table object create copy ',negative
'now get from cache ',negative
'find the table will working with ',negative
'wrap the current query string since can not add another inlist element value ',negative
'also minhistorylevel will have entry for the open txn ',negative
'proportion extra space provide when allocating more buffer space ',negative
'map from integer tag distinct aggrs ',negative
'initialize container use for storing tuples before emitting them ',negative
'otherwise create new condition ',negative
'source already path the checksum will always matching ',negative
'scale sumnulls based the number partitions ',negative
'check that partition keys have not changed except for virtual views ',negative
'use old value reference word expandandrehash key tablekey slot newslot newpairindex newpairindex empty slot ',negative
'compatibility mode need hook set and use ',negative
'first check all tables ',negative
'see serialization decimal for explanation below ',negative
'and any databases other than the default database ',negative
'update statistics based column statistics conditions keeps adding the stats independently this may result number rows getting more than the input rows ',negative
'create file system handle ',negative
'and load data into the same table which should now land delta ',negative
'remove any parents from mapjoin again ',negative
'determine the lock type acquire ',negative
'initial write small value ',negative
'input long set such without copying any modification the source will affect bloom filter ',negative
'value might have been changed because the normalization conversion ',negative
'set bootstrap dump location used ',negative
'already exists ',negative
'tests setting maxrows ',negative
'first calculate the length the output string ',negative
'since metavars are all different types use string for comparison ',negative
'count input and output are long just modes partial final ',negative
'check there are column stats available for these columns ',negative
'derived classes can set this different object needed ',negative
'the plan file should always local directory ',negative
'must iterate over all the delete records until find one record with ',negative
'get the valid write list for all the tables read the current txn ',negative
'dont user uber all mode everything can into llap which better than uber ',negative
'isoriginalmapjoin ',negative
'not analyze command and not column stats then not gatherstats ',negative
'converts negative byte positive index ',negative
'not possible expand since have more than one chunk with single segment this the case when user wants append segment with coarser granularity metadata storage already has segments for with granularity hour and segments append have day granularity druid shard specs does not support multiple partitions for same interval with different granularity ',negative
'this the last branch and always false assume alwaysfalse filter will get pushed down this branch wont read any data ',negative
'repeated iospecproto inputspecs ',negative
'get least splits ',negative
'integerminvalue ',negative
'finally make sure the file sink operators are set right ',negative
'now check that stats for partition didnt modify did not change ',negative
'need edit the configuration setup cmdline clone first ',negative
'note fix for sfnet bug working around issue jline see appending newline the end inputstream ',negative
'parameters the form keycolxtcoly ',negative
'need reset the monitor operation handle not available down stream ideally the monitor should associated with the operation handle ',negative
'usually this means weve already created the tables clean them and then try again ',negative
'use magic value indicating are writing the big value length ',negative
'there will data nodes there will task tracker nodes ',negative
'for ignore this for now but leave log message ',negative
'default implementation ',negative
'when fraction exactly and lowest new digit odd towards even ',negative
'get override compression properties via tblproperties clause set ',negative
'use construct ',negative
'not create predicate the leaf not the passed schema ',negative
'clear value arrays ',negative
'read all values ',negative
'map type contains schema the value element ',negative
'postgres specific parser ',negative
'push the context the end the serialized ngram estimation ',negative
'for this variation serialize the key without caring single long single string multikey etc ',negative
'that should ever change this will need reworking ',negative
'throw new type used without type params ',negative
'how many data columns the partition reader actually supplying ',negative
'checks for nonsecure hadoop installations ',negative
'set the jdbc connection pool ',negative
'assumed the caller have already allocated write for addingupdating data the acid tables however ddl operatons wont allocate write and hence this query may return empty result sets get the write allocated this txn for the given table writes ',negative
'report success for all other cases ',negative
'between and and firstname alan and substrxxxxx firstname and smith lastname and substrfirstname yyy ',negative
'intervaldaytime ',negative
'construct string column names based the number column types ',negative
'this actually alter table drop paritition statement ',negative
'serde null the input doesnt need spilled out ',negative
'ignore ',negative
'not able find thread execute the job request raise busy exception and client can retry the operation ',negative
'check the mere mortals ',negative
'can clear the global error when see that was set descendant node group expression because processgbyexpr returns exprnodedesc that effectively ignores its children although the error can set multiple times descendant nodes dfs traversal ensures that the error only needs cleared once also for case like select concatvalue concatvalue the logic still works the error only set with the first value all node processors quit early the global error set ',negative
'this does ',negative
'droptable event partitioned table ',negative
'reached the end the result file ',negative
'the data not escaped simply copy the data ',negative
'cache the values ',negative
'else the common code the end ',negative
'various final services configs etc ',negative
'previously assigned some rows with nonnull values the batch indices the unassigned row were tracked ',negative
'with hive hiverootlogger cannot have both logger name and log level still see split logger and level separately for hiverootlogger and hiveloglevel respectively ',negative
'failed dump the sidetable remove the partial file ',negative
'merge the two into the lateral view join the cols the merged result will the combination both the cols the udtf path and the cols the all path the internal names have changed avoid conflicts ',negative
'hadoopjobid ',negative
'used support where list struct that contains field called will return array that contains field all elements array ',negative
'rows match ',negative
'timeout for nodes larger than delay immediate allocation ',negative
'rename partition ',negative
'are working stripe over the min stripe size and crossed block boundary cut the input split here ',negative
'one the child conditions truefalse ',negative
'read the list ',negative
'initialize mapwork with smbmapjoin information ',negative
'nonjavadoc see javaioinputstream long ',negative
'pending update not done the task has terminated out date heartbeat ',negative
'setup our left semi join specific members ',negative
'methods summary ',negative
'current nodes the cache ',negative
'find all acid filesinkoperators ',negative
'union encountered for the first time ',negative
'there any partition column static partition dynamic ',negative
'this not new key well overwrite the key and hash bytes not needed anymore ',negative
'this should throw classcastexception ',negative
'need side file for this test create txn batch and test with only one ',negative
'returns set ',negative
'not support tables either this point could with some extra logic ',negative
'ensure that full qualified path most cases will since tblgetpath full qualified ',negative
'get locks that are relevant exclusive for insert overwrite ',negative
'all other primitive types are simple ',negative
'verify hwm properly set after repl load ',negative
'build tok from toksubquery the input subquery with correlation removed subqueryalias ',negative
'semantic analysis and plan generation ',negative
'standardlist uses arraylist store the row ',negative
'acid table this should fail ',negative
'row resolve once more because the columninfo row resolver already removed ',negative
'replicate only insert into operations ',negative
'files size for splits ',negative
'where listpartitions already provided where want fetch partitions lazily when theyre needed ',negative
'create source table ',negative
'get current mapred work and its local work ',negative
'test with remote metastore service ',negative
'note this tableexport actually never used other than for auth and another one ',negative
'not efficient but dont expect this called frequently ',negative
'propagate this value from dont allow users set initconf will set wont set otherwise noone calls setuppool ',negative
'are doing acid operation they will always all true recordupdaters always collect stats ',negative
'misc ddl ',negative
'are waiting for next block either will get told are done ',negative
'validate input reducework ',negative
'push not through between ',negative
'which not tracked directly but available jobsid node via mtime stat ',negative
'there extra dependency metricsregistry for snapshot ',negative
'there was parallel deallocate didnt account for the memory ',negative
'check tablepartition doesnt have ckpt property ',negative
'the alias ',negative
'set database specific parameters ',negative
'add the map ',negative
'irrelevant ',negative
'also populate with ',negative
'print out the location the log file for the user ',negative
'create the configuration hadoopsitexml file ',negative
'nonjavadoc see int ',negative
'wait for all invocations complete ',negative
'gobble the exception message delivery best effort ',negative
'alias ',negative
'now expand the view definition with extras such explicit column references this expanded form what well reparse when the view ',negative
'the split from something other than the file the logical bucket compute offset ',negative
'construct using ',negative
'perform data operation ',negative
'check has expected version marker ',negative
'there should really only one line with script failed ',negative
'unequal strings ',negative
'bootstrap dumpload ',negative
'select first and last rows ',negative
'reduce side work ',negative
'executed too small number reducers ',negative
'use dfs traverse all the branches until dpp hit ',negative
'first write table will allocate write and rest the writes should reuse ',negative
'',negative
'not need zookeeper the moment ',negative
'this has done synchronously avoid the caller getting this session again ideally wed get rid this threadlocal nonsense ',negative
'initialize stats publishing table ',negative
'last item ',negative
'operator ',negative
'for the case implicit type conversion varchar and varchar pick the common type for all the keys since during runtime same key type assumed ',negative
'lets say that passing null will not any filtering ',negative
'has the table changed since the query was cached for transactional tables can compare the table writeids the currentcached query ',negative
'assume not temp table try underlying client ',negative
'bloom filter uses binary ',negative
'should have some query and also its parent because supposition are subq ',negative
'skip for tests not present ',negative
'driverruninsert overwrite table select from inpy ',negative
'ensure have stripe metadata might have read before for filtering ',negative
'column family become map ',negative
'bgenjjtree xception ',negative
'should never come here ',negative
'plan using dummypartition can only lock the table unfortunately ',negative
'this catch all state when containers have not started yet llap has not started yet ',negative
'signed comparison longminvalue decimal ',negative
'cache columnlist from thissd ',negative
'the hostname doesnt contain port add the configured port hostname ',negative
'binary ',negative
'null out final members ',negative
'beware any implementation whose hashcode mutable reference inserting into map and then changing the hashcode can make disappear out the map during lookups ',negative
'this simulates the completion txnid ',negative
'constructor useful making projection vectorization context use with and addprojectioncolumn ',negative
'class iterator ',negative
'rootnotmodified false then startindx and endindx will stale ',negative
'missing class setting field ',negative
'the table should also considered part inputs even the table partitioned table and whether any partition selected not ',negative
'add fake partition dir ',negative
'rank functions type intdouble ',negative
'add the path alias mapping ',negative
'return array fields where the last field has the actual data ',negative
'output column the reducesink operator ',negative
'write totalseconds nanos dataoutput ',negative
'serialize context ',negative
'',negative
'requestmanager will catch this and handle like any other error ',negative
'client requesting fetchfromstart and its not the first time reading from this operation then reset the fetch position beginning ',negative
'necessary compare against hiveconf defaults hivesitexml not available task nodes like ',negative
'precision ',negative
'add needed columns ',negative
'kryo setter ',negative
'this dealing with tasks from different submission and cause the kill out before the previous submissions has completed handled the ',negative
'strict mode the presence order limit must specified ',negative
'extract join type ',negative
'single call get all column stats for all partitions ',negative
'ignore the predicate case not sampling predicate ',negative
'duplicates logic ',negative
'precision ',negative
'before any activity the table open ids ',negative
'nonjavadoc this provides lazylong like class which can initialized from data stored binary format see int int ',negative
'the first group ',negative
'start all the outputs ',negative
'open txn with writes ',negative
'set hivesitexml default hivesitexml that has embedded metastore ',negative
'note here should use the new partition predicate pushdown api get list pruned list ',negative
'the max size memory for buffering records before writes them out ',negative
'verify schema ',negative
'decimaltotimestamp should consistent with doubletotimestamp for this level precision ',negative
'has reached the end the current batch lets fetch the next batch ',negative
'sets the sticky bit stickybitdir now removing file kvtxt from stickybitdir unprivileged user will result dfs error ',negative
'acid tables have complex directory layout and require merging delta files read thus should not try read bucket files directly ',negative
'exprnodecolumndesc etc not have leadlag inside ',negative
'not insert not need anything ',negative
'capabilities ',negative
'existsordering and existspartitioning should false ',negative
'logdebugclassname logical logical batchindex batchindex null ',negative
'followed each key and then each value ',negative
'append task specific info stagingpathname instead creating subdirectory this way dont have worry about deleting the stagingpathname separately end query execution ',negative
'because divisor negative quotient most remainder must dividend itself quotient dividend divisor ',negative
'always ',negative
'unknown unknown ',negative
'empty value too ',negative
'',negative
'cache delegation tokens when link submits job that requires metastore access and this access should secure tcj will add delegation token the submitted job when the job completes need cancel the token since default the token lives for days and over time can cause oom not cancelled cancelling from mapper via custom outputcommitter for example requires the jar containing hivemetastoreclient and any dependent jars available the node running launchmapper specifying transitive closure the necessary jars headache for each release caching the token means cancellation done from webhcat server and thus has hive jars the classpath while its possible that webhcat crashes and looses this inmemory state but this would exceptional condition and since tokens will automatically cancelled after days the fact that this info not persisted persisting also complicates things because that needs done securely see ',negative
'nonjavadoc see javasqlnclob ',negative
'search for the key ',negative
'renametable event unpartitioned table ',negative
'bgenjjtree service ',negative
'first find out any the jobs needs run nonlocally ',negative
'sort for readability ',negative
'matters only for permanent functions ',negative
'keep mapping from tag the fetch operator alias ',negative
'colnames ',negative
'storage table could any storage system hbase cassandra etc ',negative
'the results this query execution might cacheable add placeholder entry the cache other queries know this result pending ',negative
'this task contains join can converted mapjoin task this operator present the mapper for sortmerge join operator present followed regular join cannot converted auto mapjoin ',negative
'and those from follow ',negative
'undone inner count ',negative
'multiparameter aggregations supported ',negative
'attempt delete temp file this fails not much can done about ',negative
'check this because will ',negative
'inner join hash map ',negative
'allocate the source conversion related arrays optional ',negative
'test regular outputformat ',negative
'create join rel ',negative
'merge should update registers and hence the count ',negative
'hive conf ',negative
'not check the state this coming from the updater under epic lock ',negative
'preallocated member for storing the physical batch index matching row single ',negative
'first determine whether rounding necessary based rounding point which inside integer part and get rid any fractional digits the result scale will ',negative
'now check more detail canhandleqbforcbo returns null query can ',negative
'error occurred retry ',negative
'todo the only reason this done this way because want unique subjects that the fsget gives different objects different fragments ',negative
'todo make these like operationtype and remove above char constatns ',negative
'recursively create the exprnodedesc base cases when encounter column ref convert that into exprnodecolumndesc when encounter constant convert that into for others just build the exprnodefuncdesc with recursively built children ',negative
'verify when second argument repeating ',negative
'exception should thrown ',negative
'provide faster way write hive interval year month without object ',negative
'strip off the stop marker which may left all the fields were the serialization ',negative
'have different settings from those hiveserver ',negative
'prepare prefix and suffix ',negative
'all table column names ',negative
'uri added later ',negative
'assume the caller will handle extra columns default with nulls etc ',negative
'this only useful for the daemons know themselves ',negative
'todo check defaults maxtimeout keepalive maxbodysize bodyrecieveduration etc ',negative
'currently support lazysimple deserialization and ',negative
'set permissions for current user dag ',negative
'encountering dot attempt resolve the leftmost name the parent query unqualified name assumed subquery reference dont attempt resolve this the parent because require all parent column references qualified all other expressions have type based their children expr children assumed refer neither ',negative
'',negative
'update table stats for partitioned table update stats alterpartition ',negative
'running queued running running running queued ',negative
'verify null output entry correct ',negative
'the jdbc spec says when you have duplicate column names the first one should returned ',negative
'leftfast leftfast ',negative
'try find the file the include path ',negative
'find the buddy the header list level dont know what list actually ',negative
'are stack trace ',negative
'move all the partition columns the end table columns ',negative
'errors are tolerated ',negative
'files size for splits ',negative
'theres delegation token available then use token based connection ',negative
'use that the current user ',negative
'start third batch aborttransaction everything dont properly close ',negative
'call copyfromlocal below basically assume src local file ',negative
'cannot push limit bail out ',negative
'',negative
'this tablescandesc flag strictly set the vectorizer class for vectorized mapwork vertices ',negative
'most likely this value should not exist ',negative
'clear away any residue from our optimizations ',negative
'inputs are not equal could zip till here ',negative
'void can anything ',negative
'utc epoch for instant ',negative
'configure getpassword fall back conf credential doesnt have entry ',negative
'batch full and have least more row ',negative
'change value metavar config param new hive conf ',negative
'else create new one ',negative
'read logs ',negative
'valid inputs ',negative
'start delegation token manager ',negative
'for reasons that are completely incomprehensible the semantic analyzers often ask for multiple locks the same entity for example sharedread and exlcusive lock the locking system gets confused this and dead locks resolve that well make sure the request that multiple locks are coalesced and promoted the higher level locking this put all locks components trie based dbname tablename partition name and handle the promotion new requests come this structure depends the fact that null valid key linkedhashmap database lock will map dbname null null ',negative
'nonempty java opts without xmx specified ',negative
'start exclusive infinity ',negative
'the clientugi ',negative
'this command has terminated ',negative
'iterate over all expression after select ',negative
'delete the data the table ',negative
'server thread pool start with minworkerthreads expand till maxworkerthreads and reject ',negative
'its the when matched ',negative
'tag union field the first byte parsed ',negative
'update our counts for the last key ',negative
'initialize the lazy object ',negative
'',negative
'that will propagate the inputs the join ',negative
'select from table situations nonmr can add things the job its safe add this the job since its not actually mapred job ',negative
'protect against bad location being requested ',negative
'new method that distributes the select query creating splits containing information about different druid nodes that have the data for the given query ',negative
'arrayentry ',negative
'step explain the query and provide the runtime rows collected ',negative
'now try find the file based sha and name currently require exact name match ',negative
'dummy ops need updated the cloned ones ',negative
'tablescan and join operators ',negative
'any more left ',negative
'mapentry ',negative
'when overwriting just start with empty timeline ',negative
'expand the array ',negative
'when set true use the overflow checked vector expressions ',negative
'doesnt support creating vrbs ',negative
'generate result within big table batch itself ',negative
'dummy alias just use the input path ',negative
'weve found something that matches what were trying lock ',negative
'numpartitions could not obtained from orm filters then get number partitions names and count them ',negative
'yyyyyyymm should more than enough ',negative
'gather physical pipeline based user config grping sets size ',negative
'read friendly string ',negative
'set hadoopusername env variable for child process that also runs with hadoop permissions for the user the job running ',negative
'add and dryrun generate list only ',negative
'nonjavadoc should ideally not modify the tree traverse however since need walk the tree any time when modify the operator might well here ',negative
'this function should overriden every sub class and the sub class should call superinitm parameters get mode set ',negative
'failed submit after retrying destroy session and bail ',negative
'negative power with range adjust the scale ',negative
'should copy properties first ',negative
'',negative
'converted sortmerge join ',negative
'rolegrantslist ',negative
'second incremental dump ',negative
'querydirectory should not null ',negative
'over all the destination structures and populate the related ',negative
'append prefix ',negative
'exhausted the batch longer have heartbeat for current txn batch ',negative
'involving constant truefalse values ',negative
'replace the commar finish clause string ',negative
'there are nulls ',negative
'any redirect handlers need added first ',negative
'avoid reading the footer twice will cache first and then read from cache parquet calls protobuf methods directly the stream and cant get bytes after the fact ',negative
'create failed compactions ',negative
'gby without distinct keys not prepared process distinct key structured rows ',negative
'column columnfamily with columnqualifier ',negative
'evaluate the keys ',negative
'replicate all the events except drop ',negative
'todo maybe throw ',negative
'',negative
'otherwise notify about spark jobs after the state notification ',negative
'grouping sets members ',negative
'nothing ',negative
'this code doesnt propagate msg cprgeterrorcode ',negative
'note its rather important that this and other methods catch exception not throwable combination with code perhaps unintentionally used also catch all errors and now allows ooms only propagate ',negative
'not event dump not table dump thus dump ',negative
'last try try parse date and transform ',negative
'walk ',negative
'resetting the queue config old hack that may remove future ',negative
'trigger vectorforward ',negative
'nonverbose pattern look for ',negative
'make sure result precisionscale matches the input precscale ',negative
'optimize this newwork given the big table position ',negative
'someone allocating this arena wait bit and recheck ',negative
'disk for the hybrid grace hash partitioning ',negative
'will trigger spills ',negative
'the child tasks may null case select ',negative
'going through file list and make the retry list ',negative
'verifying that method supported ',negative
'create the new filter that might pushed down ',negative
'dummy operator for not increasing seqid ',negative
'join keys dont match the bucketing keys ',negative
'get current input file name ',negative
'nothing needed here default ',negative
'set yarn queue name ',negative
'into the doas below ',negative
'conversion ',negative
'hivetmptablespace hivehdfssessionpath and are respectively saved hdfstmptablespace hdfssessionpath and localsessionpath saving them conf variables useful expose them end users but end users shouldnt change them ',negative
'test new api ',negative
'chooses representative alias and index use the string the first used because set the constructor ',negative
'this wont into checkandsend ',negative
'statement should open even after resultsetclose ',negative
'cpu cost sorting cost ',negative
'last batch can sometimes have less number elements ',negative
'spot check correctness decimal scalar multiply decimal column the case for addition checks all the cases for the template dont that redundantly here ',negative
'create conditional task and insert conditional task into task tree ',negative
'trimblanks ',negative
'add mapping from the table scan operator table ',negative
'relying watchserviceclose clean all pending watches ',negative
'bypass for explain queries for now ',negative
'exclude all standard table properties ',negative
'now the most important check when query this record for its schema ',negative
'these schemata are used other tests ',negative
'get not between filter expression this treated special case because the not actually specified the expression tree the first argument and dont want any runtime cost for that creating the vectorexpression needs done differently than the standard way where all arguments are passed the vectorexpression constructor ',negative
'when selectedinuse set false everything the batch selected ',negative
'this effectively dag completed and can used reset statistics being tracked ',negative
'get the object inspector for myrow ',negative
'note enhance showresourceplan display all the pools triggers and mappings ',negative
'not hiveinputformat custom vertexmanager will take care grouping splits ',negative
'nonjavadoc see ',negative
'the query was the result analyze table column compute statistics rewrite create column stats task instead fetch task persist stats the metastore per hive will also collect table stats when user computes column stats that means iscstats need collect table stats iscstats need include basic stats task else which should have move task with stats task already ',negative
'that follows this used for connecting them later ',negative
'allow ',negative
'verify cmrecycledb table part api moves file cmroot dir ',negative
'this means partition exists for the given partition key value pairs thrift cannot handle null return values hence getpartition throws indicate null partition ',negative
'try read the default named url from the connection configuration file ',negative
'already retrieved the incoming info check without ugi ',negative
'need track this some listeners pass through our config and need honor ',negative
'work ',negative
'notification generated for newly created partitions only the subset partitions ',negative
'dblevel repl loads testing done now moving table level repl loads each these cases the tablelevel repllastid must move forward but the dblevel lastreplid must not ',negative
'may flood the log ',negative
'hive should remove this code ',negative
'this creates orc data file with correct schema under table root ',negative
'end ',negative
'check aggoutputproject projects only one expression ',negative
'nonjavadoc see int int int ',negative
'wait for all threads ready ',negative
'log warning incase reporters were successfully added ',negative
'this true drop table ',negative
'the left was the left side right outer join ',negative
'metastore schema only allows maximum for constraint value column ',negative
'different duplicate some other function ',negative
'logicalproject maps set rows different set without knowledge the mapping functionwhether preserves uniqueness only safe derive uniqueness info from the child project when the mapping further more the unique bitset coming from the child needs mapped match the output the project ',negative
'test third argument repeating ',negative
'this point dont have anything special just run through the regular paces creating new task ',negative
'partition not found describe table partition ',negative
'customization this api done for most authorization implementations meant used for special cases apache sentry incubating null returned when customization needed for the translator see javadoc interface for details ',negative
'create operationlog object with above log file ',negative
'test longstring version ',negative
'this input rel does not produce the cor var needed ',negative
'inject behavior where repl load failed when try load table fails ',negative
'for set role all reset roles default roles ',negative
'num executors less than max executors per query which not expected case default executors will ',negative
'make sure that they have the same type ',negative
'when selectedinuse true start with every bit set false and selectively set certain bits true based the selected vector ',negative
'list ',negative
'fits one longword ',negative
'decimal maximum digits lower longs digits here ',negative
'islastgroupbatch ',negative
'precision ',negative
'are going serialize using the basic types ',negative
'the first shot fails then log the waiting messages ',negative
'now filter ',negative
'now serialize ',negative
'note that cache each slice separately could cache them together the end but then wont able pass them users without increfing explicitly ',negative
'walk through exprs and extract field collations and additional ',negative
'start only serializing primitives asis ',negative
'join then there should only either subquery ',negative
'various errors when creating spark client ',negative
'its childrens parents lists also see childoperatorstag operator here ',negative
'expecting not change the size internal structures ',negative
'initialize destination tablepartition ',negative
'set the bit key not null ',negative
'there another batch buffer ',negative
'checkconstraints ',negative
'the first child toktableorcol and nodeoutput null ',negative
'friday august ',negative
'this optimizer will serialize all filters that made the table scan operator avoid having multiple times the backend you have physical optimization that changes table scans filters you have invoke before this one ',negative
'does not end with then line continuation ',negative
'tbl location available use else derive the tbl location from database location ',negative
'precision ',negative
'mystringstringmap ',negative
'database name pattern ',negative
'this shouldnt happen the parser should have converted the union contained subquery just case keep the error fallback ',negative
'column list ',negative
'update cached aggregate stats for all partitions table and for all ',negative
'prefix for window functions discern leadlag udfs from window functions with the same name ',negative
'its either file glob ',negative
'wantwritable ',negative
'these anyway ',negative
'',negative
'exclude constants aggregatetrue occurs because aggregate would generate row even when applied empty table ',negative
'required required ',negative
'verify the writeid this committed txn should invalid for test txn ',negative
'data size still then get file size ',negative
'select from the new table should pass ',negative
'for pfile calculate the checksum for use testing ',negative
'optimize whole decimal fits two binary words ',negative
'semijoin keys and keys completely unrelated the cardinality both sets could obtained adding both cardinalities would there average case ',negative
'selpairgetkey the operator right before selpairgetvalue which only contains columns needed result set extra columns needed order will absent from ',negative
'precision ',negative
'validation ',negative
'remember which mapjoin operator links with which work ',negative
'deal with static partition columns ',negative
'check all the operators the stack currently only selects and filters ',negative
'examine all digits being thrown away determine result ',negative
'let cleaner delete obsolete filesdirs ',negative
'generate the local work for the big table alias ',negative
'the absence column statistics compute data size based based average row size ',negative
'project the columns the group plus the arguments the agg function ',negative
'calcite expects the grouping sets sorted and without duplicates ',negative
'group grants ',negative
'lfu extreme order accesses should ignored only frequency matters touch first elements later but less times they will evicted first ',negative
'since setstructfielddata and create return list getstructfielddata should able handle list data this required when table serde parquethiveserde and partition serde something else ',negative
'bootstrap repl ',negative
'handle three types scenarios with special case handling level metadata handling subsequent loadtask which will start running from the previous replicationstate other events these can only either table function metadata ',negative
'nway join has been calculated hashtableloader earlier just need retrieve that number ',negative
'partitioned input not sorted ',negative
'',negative
'assume millislocal midnight some date what are basically trying here from localmidnight utcmidnight whatever time that happens ',negative
'read the configuration parameters ',negative
'precision ',negative
'grant ',negative
'not operator bail out ',negative
'required required required required required required required required required required required required required required optional ',negative
'use construct ',negative
'remap arguments ',negative
'static partition specified ',negative
'the bottom aggregate has converted the distinct aggregate group clause ',negative
'mapping bucket number required tasks run ',negative
'using different code blocks that jdbc variables are not accidently reused between the actions different connectionstatement object should used for each action ',negative
'future only takes final seemingly final values make final copy taskid ',negative
'the equality implemented fully the implementation sorts the maps their keys provide transitive compare ',negative
'from the map jobs ',negative
'grandparent works need set these the parents the cloned works ',negative
'save last longword ',negative
'precision ',negative
'nothing update everything the same ',negative
'sequence tablescan operators walked ',negative
'resfile pctx roottasks fetchtask analyzer explainconfig cboinfo ',negative
'update table schema add the newly added columns ',negative
'',negative
'delete the data ',negative
'add the setrcols the input list ',negative
'dont eat and wrap runtimeexceptions because the objectbufferwrite handles specifically resizing the buffer ',negative
'that something blocking that would not block read ',negative
'rethrow without losing original stack trace ',negative
'create the root the operator tree ',negative
'flip the boolean variable ',negative
'copy the column values from the input row dpvals ',negative
'template expansion logic the same for both columnscalar and scalarcolumn cases ',negative
'precision ',negative
'sparkwork dependency graph from sparkwork with operators all ',negative
'dont bother cleaning from the txns table separate call will that dont know here which txns still have components from other tables partitions the table dont know which ones can and cannot clean ',negative
'record this change the metastore ',negative
'the testxml was not generated was corrupt let someone know ',negative
'assert false ',negative
'get delegation token for user from filesystem and write the token along with metastore tokens into file ',negative
'pass job initialize metastore conf overrides ',negative
'precision ',negative
'can process this batch immediately ',negative
'files size for splits ',negative
'these calls are see how much data there the setfrombytes call below will the same readvint reads but actually unpack the decimal ',negative
'const first argument then evaluate the result ',negative
'note could use lock allow concurrent calls for different sessions however all those calls add elements lists and maps and wed need sync those separately separately plus have object notify because lock does not support conditions ',negative
'not merge the mapredwork mapjoin has multiple input aliases ',negative
'the client requested that extra mapreduce step performed ',negative
'the code inside the attribute getter threw exception log and skip outputting the attribute ',negative
'get our multikey hash multiset information for this specialized class ',negative
'trigger lazy read metadata make sure serialized data not corrupted and readable ',negative
'blindly add this integer list should sufficient for the test case use the nonsettable list object inspector ',negative
'thread being interrupted ',negative
'add hbase properties ',negative
'want wait for the iteration finish and set the cluster fraction ',negative
'end the root object ',negative
'consistent with the behavior listpartitionnames the table does not exist return empty list ',negative
'char length available copy whole string value here ',negative
'after recovery there shouldntable any flushlength files ',negative
'captures how the input should ordered this captured list astnodes that are the expressions the sort clause ptf invocation ',negative
'operators with ',negative
'decimal ',negative
'initialize the array ',negative
'initialize aliastowork ',negative
'spot check decimal columncolumn subtract ',negative
'did not get token set oozie lets get them ourselves here essentially get token per unique output hcattableinfo this done because through pig setoutput method called multiple times want only get the token once per unique output hcattableinfo cannot just get one token since multiquery case store job the case when single pig script results jobs the single token will get cancelled the output committer and the subsequent stores will fail tying the token with the concatenation dbname tablename and partition keyvalues the output tableinfo can have many tokens there are stores and the tokenselector will correctly pick the right tokens which the committer will use and ',negative
'first check the table dir exists could have been deleted for some reason precommit tests ',negative
'nonjavadoc see javautillist ',negative
'look for interfaces both the class and all base classes ',negative
'table creation with long table name causes ',negative
'try recursive folding ',negative
'this method inserts the right profiles into profiles cbo depending the query characteristics ',negative
'scratch arrays used fastbigintegerbytes calls for better performance ',negative
'kerberos security enabled and doas enabled then additional params need set that the command run intended user ',negative
'rewrite value index for mapjoin ',negative
'initially all deltas and rcs are empty list starts there are objects take ',negative
'there are previous nodes then and the current node with the previous one ',negative
'smallint ',negative
'input key bigger than any keys hash ',negative
'run queries ',negative
'number reducers ',negative
'adjacencytype ',negative
'nonjavadoc see javalangobject ',negative
'',negative
'first see there direct match ',negative
'boundary specifies how many rows backforward windowframe extends from the current row boundary specified range boundary the number rows forward back from the current row current row which implies the boundary the current row value boundary which specified the amount the value expression must decreaseincrease ',negative
'the command has schema make sure nothing printed ',negative
'completion txnididtxnupdate ',negative
'return vector expression for custom not builtin udf ',negative
'strip trailing carriage return input ignore changes whose lines are all blank ',negative
'prspjoincrscgby ',negative
'dynamic partition pruning pipeline doesnt have multiple children ',negative
'rqst ',negative
'need copy standard object otherwise deserializer overwrites the values ',negative
'have storage specification for map column type ',negative
'because txn may include different partitionstables even auto commit mode ',negative
'optional int dagstarttime ',negative
'try resolve qualified name column reference the parent querys rowresolver apply this logic the leftmostfirst dot ast tree ',negative
'recall that the sequence must prsselcrs ',negative
'set the dynamic values the childwork ',negative
'key ',negative
'finalize the last record ',negative
'return proper response ',negative
'setup ',negative
'calculate the variance result when count public vectorization code can use etc ',negative
'min needed the case that entire string whitespace ',negative
'finally not reduce the input size bail out ',negative
'find operators with partition pruning enabled plan because these may potentially read different data for different pipeline these can with dpp with semijoin dpp ',negative
'expand all privileges any ',negative
'convert semijoin gby semijoin ',negative
'initialize the task and execute ',negative
'set the leftmost header the base and its buddy that are now being merged ',negative
'figure out there group ',negative
'find out cpu msecs the case that cant find out this number just skip the step print ',negative
'the hash table slots for bytes key hash table each slot longs and the array sized the slot triple nonzero reference word the key bytes the key hash code and nonzero reference word the first value bytes ',negative
'capacity check element needs evicted ',negative
'else this means pigs optimizer never invoked the pushprojection method need all fields and hence should not call the setoutputschema hcatinputformat ',negative
'external table should also check the underlying file size ',negative
'create the timeline for the existing and new segments ',negative
'completion txnididtxnupdate ',negative
'the pending query were waiting failed but there might still another pending completed entry the cache that can satisfy this query lookup again ',negative
'find the privrequirements that match iotype actiontype and add the privilege required reqprivs ',negative
'rowid ',negative
'todo either have kill the nonactor model would implicitly hope for the best and continue other threads the latter for now ',negative
'always bit insure the key reference nonzero ',negative
'exponent ',negative
'llapdaemonmxbean methods will exposed via jmx ',negative
'the size deserialized partition shouldnt exceed half memory limit ',negative
'binary tcp mode ',negative
'the reason this guard because when not have good way initializing the config the handlers thread local config until this call then once done though need not repeat this linking simply call setmetastorehandler and let the and what they want ',negative
'when using command always single line ',negative
'add the default sql completions ',negative
'configured warehouse local dont need bother checking ',negative
'must held same thread ',negative
'cascadetrue then need authorize the drop table action well ',negative
'ignore the exception because are not comparing long string here there should never exception ',negative
'introduce exchange operators below joinmultijoin operators ',negative
'start inclusive end inclusive ',negative
'trim off lower fractional digits but with rounding ',negative
'use tostring which will have exponents instead toplainstring ',negative
'same mapredtaskid figure ',negative
'test that when transaction aborted the heartbeat fails ',negative
'prevent hive configurations from being visible spark ',negative
'the uses the prior read type flush the prior deserializerbatch set here none ',negative
'its nondeterministic ',negative
'test that existing sharedwrite with new exclusive coalesces ',negative
'nonjavadoc see javaioreader ',negative
'make everything qualify and ensure selected not use ',negative
'commit has succeeded since exceptions have been thrown ',negative
'sparksubmit will take care that for ',negative
'get configuration parameters ',negative
'hence true only map side ',negative
'are columns used this select operator ',negative
'set client port have already had list valid ports use ',negative
'then null else null unusual case but possible ',negative
'since after various rules original relnode could have different corref might not have all need traverse the new node figure out new cor refs and put that into map ',negative
'test with table name which does not exists the given database ',negative
'will called before closing the orc file stop writing any additional information the acid key index ',negative
'turn the tree set into array can move back and forth easily ',negative
'for conditional expressions ',negative
'all partitions have been statically removed ',negative
'verify extension values the array ',negative
'special handling for set role statement ',negative
'make the table acid ',negative
'checking state per node for future failure handling scenarios where update ',negative
'create thread pool with poolsize threads threads terminate when they are idle for more than the keepalivetime bounded blocking queue used queue incoming operations operations poolsize ',negative
'try with decimal input and decimal output ',negative
'the row consists some string columns some arrayint columns ',negative
'analyze command ',negative
'for table only want delete delta dirs for aborted txns ',negative
'hiveserver metadata api types start here these corresponds various calls ',negative
'insert reduceside ',negative
'largest possible base exponent any exponent larger than this will already produce underflow overflow theres need worry about additional digits ',negative
'into llapnodeid get node info from registry that should can include ',negative
'float loses some precisions ',negative
'reuse old object prevent needless expr cloning ',negative
'support names like colxcoly ',negative
'the following tests spotcheck that vectorized functions with signature double funcdouble that came from template columnunaryfunctxt get the right result null propagation isrepeating propagation will checked once for single expansion the template for ',negative
'but count inner side before that make sure generates atmost row ',negative
'separate split ',negative
'reverse this ',negative
'basic algorithm determine rounding digit for rounding scale away fractional digits present rounding clear integer rounding portion and add ',negative
'txn there one started not finished ',negative
'',negative
'get tokens for default not all fss support delegation tokens wasb ',negative
'convert the group mapside group ',negative
'silly little pager ',negative
'test backward scan ',negative
'serialization the option selected ',negative
'create the thread pool for the web server handle http requests ',negative
'bootstrap case ',negative
'table inside view not care about its authorization ',negative
'druid json timestamp column name ',negative
'required required optional required required optional optional optional optional optional optional optional optional ',negative
'round without digits ',negative
'equal key series checking ',negative
'result privilege ',negative
'bgenjjtree fieldtype ',negative
'dynamic partition pruning enabled some all cases either true true ',negative
'auth specific confs ',negative
'test notequals operator for strings and integers ',negative
'get total number rows from all memory partitions ',negative
'nonjavadoc see javaioreader ',negative
'run the last combined strategy any ',negative
'',negative
'because use parentheses addition whitespace keyword delimiter need define new argumentdelimiter ',negative
'float ',negative
'slide the column names down for the name array ',negative
'just case deserialize decimal with trailing zeroes ',negative
'build map hive column names exprnodecolumndesc name the positions those projections the input ',negative
'',negative
'make sure struct record ',negative
'test for publish with invalid partition key name ',negative
'incomplete message buffer ',negative
'avoid double casting preserve original string representation constant ',negative
'nonjavadoc see ',negative
'should have printed out the header for the field schema ',negative
'for table explicitly try load partitions there separate partitions events ',negative
'handle countsumavg function for the case count and countdistinct ',negative
'calculate the function result for row the batch and set the output column vector entry the result ',negative
'but that set immutable ',negative
'note per our current constraints the behavior two parallel activates undefined although only one will succeed and the other will receive exception need proper semitransactional modifications support this without hacks ',negative
'are asked start from begining clear the current fetched resultset ',negative
'remove from all its parents child list ',negative
'break out and try executing ',negative
'each newinput ',negative
'null qualifier would mean all qualifiers that family want empty qualifier ',negative
'have use the length instead the actual prefix because the prefix location dependent byte hash for the path separator can less than due unicode characters ',negative
'now abort compact and ',negative
'need full scan ',negative
'join need update the state information accordingly ',negative
'row offsets will determined from the reader could set the first from last ',negative
'cababc ',negative
'check list element and value are same type ',negative
'should have been replaced ',negative
'connecting foo ',negative
'generate the columns according the column mapping provided ',negative
'this should never happen ',negative
'nonjavadoc see ',negative
'tablefunction may able accept its input stream this case the contract startpartition must invoked give the ptf chance initialize stream processing each input row passed via processrowor processrows invocation processrow can return more rows finishpartition invoked give the ptf chance finish processing and return any remaining rows ',negative
'cannot lock remove this from cache and continue ',negative
'look see how this filter created ',negative
'subscriber accept the feed and something depending the task type ',negative
'return remaining records any from last processed input record ',negative
'duration estimate the size the map changes can very different ',negative
'add all ',negative
'tailing zeroes difference ',negative
'connection however retry one more time ',negative
'process join ',negative
'root task cannot depend any other task therefore childtask cannot ',negative
'virtual columns ',negative
'select query ',negative
'',negative
'mysql returns the string not wellformed numeric value return longwritablevalueof but decided return null instead which more conservative ',negative
'initialize buffer read the entire stripe ',negative
'groupb user ',negative
'keep the parent correct ',negative
'grantrequest ',negative
'repeating expression ',negative
'remove all parameters that are tested the parameter tested part ',negative
'loop while you either have tasks running tasks queued ',negative
'multiply the same power ten shift the decimal point back the original place places the right the decimal will zero ',negative
'test string column string literal comparison ',negative
'base has rows splits delta has rows split and delta has splits ',negative
'get stored the udfcontext would have been stored there earlier call setpartitionfilter call setinput hcatinputformat only the frontend because internally makes calls the hcat server dont want these happen the backend the hadoop front end mapredtaskid property will not set ',negative
'compute product distinct values grouping columns ',negative
'someone else also trying append ',negative
'columns ',negative
'add this dummy the dummp operator list ',negative
'splice the section that have evicted out the list have already updated the state above need that again ',negative
'unregister the functions well ',negative
'after processing subqueries and source tables process partitioned table functions ',negative
'attempt cleanup stack trace elements that vary ',negative
'grape expects excludes key args map ',negative
'its imperative that code acquirelocks called for all commands that hivetxnmanager can transition its state machine correctly ',negative
'delegate updates over the source state tracker ',negative
'bootstrap load replica ',negative
'know merge will triggered execution time ',negative
'before each test ',negative
'create znode under the rootnamespace parent for this instance the server ',negative
'all rowids are unique read after conversion acid rowids are exactly the same before and after compaction also check the file name only after compaction for completeness note order rows file ends being the reverse order values clause why ',negative
'the url passed valid url with protocol use asis otherwise assume name parameter that have get the url from ',negative
'create our vectorized copy row and deserialize row helper objects ',negative
'specialized class for doing vectorized map join that inner join singlecolumn long and only big table columns appear the join result hash multiset used ',negative
'adjust counters and buffer limit ',negative
'type system for this ',negative
'adding the reducers run time statistics for the job the queryplan ',negative
'validate the operation renaming column name ',negative
'called the subquery agg and correlated doesnt have groupby added the ast ',negative
'insert event found then return null hence event dumped ',negative
'loop until the value correct run out tries ',negative
'outer joins with postfiltering conditions cannot merged ',negative
'load each incremental dump from the list each dump have only one operation ',negative
'coming from below ',negative
'create the table ',negative
'enable the hook check after the server startup ',negative
'from the above checks know fast zero ',negative
'get optional read variations for fields ',negative
'',negative
'',negative
'underflow ',negative
'change lock manager embedded mode ',negative
'uncompressedoffset middle integer encoding runs rle delta etc consume ',negative
'handle table schema ',negative
'this could expensive when there are lot compactions ',negative
'expressions ',negative
'the current split should use the preceding splits footerbuffer order skip footer correctly ',negative
'make one partitioned ',negative
'fastscale abspower hivedecimalmaxscale ',negative
'collect ',negative
'select query ',negative
'list overhead configured number element list size element ',negative
'short ',negative
'right full outer join need iterate through the row container that contains all the right records that did not produce results then for each those records replace the left side with null values and produce the records observe that only enter this block when have finished iterating through all the left and right records aliasnum numaliases and thus have tried evaluate the postfilter condition every possible combination note the left records that not produce results for left full outer join will always caught the genobject method ',negative
'can offer ecb even with some streams not discarded reset will clear the arrays ',negative
'note this work around hive calcite limitations wrt calcite can not accept expressions instead needs expressed input select hive can not preserve ordering through select boundaries this map used for outermost migrate the corresponding expressions from input select this used astconverter after are done with calcite planning ',negative
'scratch cols are ',negative
'',negative
'updates before its running ',negative
'make sure the referenced schema exists ',negative
'sort columns specified table ',negative
'construct new row resolver with everything from below ',negative
'byte bytes outputgetlength ',negative
'this create and publish the segment overwritten ',negative
'get the local path downloaded jars ',negative
'update key with assigned identifier ',negative
'useminmax minmaxenabled ',negative
'optional int dagindex ',negative
'timestamp strings should parse ',negative
'compactor should only schedule compaction for ttp deltanumthreshold not ttp ',negative
'additions the group clause ',negative
'adding columns and limited integer type promotion supported for orc schema evolution ',negative
'consider for now recompute integerdigitcount ',negative
'temp hdfs path for spark hashtable sink ',negative
'responsive again recovery ',negative
'somewhere like try randomize bit for now ',negative
'schedule task cleanup dangling scratch dir periodically initial wait for random time between min ',negative
'use table descriptor for columns ',negative
'same comment applies here ',negative
'and hash with mask out sign bit make sure its positive then know taking the result mod the range ',negative
'original tostring takes too much space ',negative
'create new mapwork ',negative
'test that the values added are there ',negative
'this global allows various validation methods set the not vectorized reason ',negative
'the table location already exists and may contain data ',negative
'search for any the sparktask ',negative
'',negative
'generate reducesinkoperator ',negative
'rewrite only analyze table column compute statistics dont rewrite analyze table command table stats are collected the table scan operator and not rewritten aggregation ',negative
'add the task the delayed task queue does not already exist ',negative
'check column type ',negative
'now prepare partnames with partitions tabparttabpart which are contained the ',negative
'tezllap requires rpc query plan ',negative
'waits for lock from fifer ',negative
'all children expression should resolved ',negative
'this call sets the default ssl params including the correct keystore the server config ',negative
'current key objectinspectors are standard objectinspectors ',negative
'check the file exists ',negative
'make sure the schema mapping right ',negative
'this will break the iterator however this the last task can add the way this currently runs only one duck distributed when failedupdate present that should ',negative
'files size for splits ',negative
'currently only functions columns and scalars supported ',negative
'pass the row rather than recordvalue ',negative
'prefer methods with closer signature based the primitive grouping each argument score each method based its similarity the passed argument types ',negative
'getset methods ',negative
'files size for splits ',negative
'test outputformat with compression ',negative
'this vectorized aware evaluator ',negative
'single column unnamed primary key default catalog and database ',negative
'check whether merging the works would cause the size the data memory grow too large ',negative
'may get treated base splitupdate enabled for acid see hive for details ',negative
'called transform tasks into local tasks where possibledesirable ',negative
'warning note not threadunique getconf ',negative
'test the validation incorrect null values the tables throws exception ',negative
'this method parses the custom dynamic path and replaces each occurrence ',negative
'could not find allowed path table scan operator hence are done ',negative
'create the operator tree ',negative
'must deterministic order map for consistent qtest output across java versions ',negative
'tests for droppartitionstring dbname string tblname string name boolean deletedata method ',negative
'validate the second parameter which should integer ',negative
'should never happen ',negative
'scaling definitely larger ',negative
'all the split strategies are done must safe access splitfutures ',negative
'not force script execution abort when failure occurs ',negative
'methods that create relational expressions ',negative
'input file name big bucket number ',negative
'use this constructor when there output column ',negative
'this value should get overwritten with correct value ditto ',negative
'handles the case like line show tables test comment ',negative
'there should only one parent ',negative
'make sure itereated through all possible connparams ',negative
'todo hive creationmetadata tablenames tablenames ',negative
'set remaining fractional portion nanos ',negative
'shall have enough time synchronize privileges during loading information schema ',negative
'create filesink operator ',negative
'return directly last value null ',negative
'must least able return back ',negative
'conversion ',negative
'helper methods ',negative
'are consuming too much memory ',negative
'backtrack can null when input script operator ',negative
'want use metricsdir the same directory the destination file support atomic move temp file the destination metrics file ',negative
'need compare partition name with requested name since some dbs like mysql derby considers whereas others like postgres ',negative
'copy fails fall through the retry logic ',negative
'lock the lowest priority buffer try evict well evict some other buffer ',negative
'logger the callstack from which the error has been set ',negative
'only ',negative
'generate mapredlocalworks for and hts ',negative
'hive pending rename afterclass ',negative
'conjunctive predicate elements are more than one then walk through them one one compute cross product ndv cross product computed multiplying the largest ndv all the conjunctive predicate elements with degraded ndv rest the conjunctive predicate elements ndv degraded using log functionfinally the ndvcrossproduct fenced the join cross product ensure that ndv can not exceed worst case join cardinalitybr ndv conjunctive predicate element the max ndv all arguments lhs rhs expressions ndvjoincondition min left cardinality right cardinality where pex the predicate element join condition with max ndv ndvpe maxndvleftexpr ndvrightexpr ',negative
'its not likely there bug but case happens must have found wrong filter operator skip the optimization then ',negative
'process join ',negative
'run major compaction ',negative
'constant add them coltoconstants halfdeterministic columns ',negative
'native vector map join hash table setup ',negative
'the cancel case where the driver state interrupted destroy will deferred the query process ',negative
'mapreduce api catch the error log debug message and just keep going ',negative
'todo rename files case ',negative
'reconnect only supported for and streaming jobs this time ',negative
'where the highest longword middle longword etc ',negative
'fill colstatus ',negative
'errors are handled the way over failsuccess informed via regular heartbeats killed via kill message when task kill requested the daemon ',negative
'now make sure delete deltas are present ',negative
'for demo purposes ',negative
'this hook verifies that the location every partition the inputs and outputs starts with the location the table very simple check make sure subdirectory ',negative
'there are modes reading for vectorization one for the vectorized input file format which returns vectorizedrowbatch the row one for using deserialize each row into the vectorizedrowbatch currently these input file formats textfile sequencefile and one using the regular partition deserializer get the row object and assigning the row object into the vectorizedrowbatch with vectorassignrow this picks input file format not supported the other two ',negative
'exponents into floatingpoint numbers ',negative
'import statement specified external ',negative
'state that the driver enters after close has been called clean the query results and release the resources after the query has been executed ',negative
'periodically report progress the context object prevent tasktracker from killing the templeton controller task ',negative
'llap cluster info does not need admin privilege since read only assigning privilege same ',negative
'get key columns ',negative
'convert byteswritable byte ',negative
'lazysimpleserde can convert any types string type using jsonformat however may add more operators thus still keep the conversion ',negative
'add interceptor add xsrf header ',negative
'come ride the api rollercoaster ',negative
'add map ',negative
'the source table now has partitions one textfile the other orc test adding these partitions the targettable without replicating the tablechange ',negative
'must deterministic order map for consistent test output across java versions ',negative
'first try split the task ',negative
'this should never happen provide good error message case theres bug ',negative
'select select transform ',negative
'there failure from here until when the metadata changed the partition will empty throw errors read ',negative
'expression splits each part the partition ',negative
'',negative
'make sure they are not public ',negative
'verify cleanup functionality open new session since this case needs close the session the end ',negative
'found some old value but couldnt incref remove ',negative
'location specified set partition ',negative
'partition cols just distribute the data uniformly provide better load balance the requirement have single reducer should set the number reducers use constant seed make the code deterministic ',negative
'link import tasks the barrier task which will inturn linked with repl state update tasks ',negative
'actualbatchsize ',negative
'sum and count are rolled sum hence sum represents both here ',negative
'use internal text member read value ',negative
'because tablenoautocompact was originally assumed noautocompact and then was moved ',negative
'join filter does not change the old input ordering all input fields from newleftinputie the original input the old ',negative
'the unit caching for orc column see orcbatchkey ',negative
'nulls not repeating ',negative
'check interrupt the last moment case get cancelled quickly ',negative
'jar file the hdfs should downloaded first ',negative
'reducers from the parent operators ',negative
'direct access interfaces ',negative
'the syntax should not allow these fields null but lets verify ',negative
'username ',negative
'first remove all the membership the membership that this role has been granted ',negative
'this returns the source corvar rel which produces cor var ',negative
'show tracking url for remotely running jobs ',negative
'make sure driver returns all results drop and recreate the necessary databases and tables ',negative
'doublestats ',negative
'inner join specific members ',negative
'find table which name contains tofind the dummy database ',negative
'the rowresolver setup for select drops table associations setup astnode unqualified name ',negative
'the spark job finishes before this listener called the queued status will not set ',negative
'accumulo like the above rangeinputsplit should have the table name ',negative
'static pattern regexrid patterncompilexidaf static simpledateformat dateparser new zzzzz ',negative
'set state close long all parents are closed ',negative
'try readlock the candidatelist timeout after maxreaderwaittime ',negative
'implies all properties needs inherited ',negative
'initialize schema ',negative
'argument descriptors ',negative
'the input sorted alias are already the last join operand can emit some results now note this has done before adding the current row the storage preserve the correctness for outer joins ',negative
'each items keyvalue format ',negative
'get column names ',negative
'get the rewritten ast ',negative
'test setup append next hivereplrootdir and use that the dump location ',negative
'there were exception batchsize doesnt change until there exception ',negative
'match unknown ',negative
'decimal string ',negative
'optional optional required required optional ',negative
'float types require conversion use noop ',negative
'build operator ',negative
'found files under currentpath add them the queue directory ',negative
'full test ',negative
'update location ',negative
'there could several big table input files mapping the same small input file find that one with the lowest bucket ',negative
'need evaluate just forward ',negative
'this class captures the information about conjunct the where clause the subquery for equality predicate capture for each side the ast the type expression basically what columns are referenced for expressions that refer the parent captures the parents columninfo case outer aggregation expressions need this introduce new mapping the outerquery rowresolver join condition must use qualified column references generate new name for the aggr expression and use the joining condition for having exists select from where minrz where the expression minrz from the outer query give this expression new name like rgbysqcol and use the join condition rgbysqcol ',negative
'format ',negative
'the form partition ',negative
'there are async requests satisfy them first ',negative
'order ',negative
'expect the correct ois ',negative
'assign repeated value index over and over ',negative
'hasresultset ',negative
'only copy data values entry not null the string value position undefined the position value null ',negative
'clear out any parents reducer the root ',negative
'turn off skew additional job required anyway for grouping sets ',negative
'with hive this should use static parts and thus not need ',negative
'the call ats appears block indefinitely blocking the ats thread while the hook continues submit work the executorservice with each query over time the queued items can cause oom the hookcontext seems contain some items which use lot memory prevent this situation creating executor with bounded capacity ',negative
'only ask for the views ',negative
'through each target column generate the lineage edges ',negative
'necessary copy the big table key into the overflow batchs small table result area ',negative
'always include headers since they contain nonvectorized objects too ',negative
'collect information about cte there any the base table cte should masked the cte itself should not masked the references the following main query ',negative
'set one the partitions skipped that command created for every other one ',negative
'may have missed the start the vertex due the seconds interval ',negative
'register not fire the rule again ',negative
'each qfile may include most one include exclude directive qfile contains include directive and hadoopver does not appear the list versions include then the qfile skipped qfile contains exclude directive and hadoopver listed the list versions exclude then the qfile skipped otherwise the qfile included ',negative
'stub actions ',negative
'the row matches skewed column names ',negative
'translate grouping set col bitset ',negative
'append the trailing path string any ',negative
'this function not deterministic function but runtime constant the return value constant within query but can different between queries ',negative
'the length tblprops only keep the rest ',negative
'aggregate operator ',negative
'windows that are unbounded following dont benefit from streaming ',negative
'',negative
'estimation larger than max ',negative
'lintstring ',negative
'all partitions with should have columns ',negative
'only contain multisourced because multisourced cannot hashed direct readable ',negative
'phaseresult false return ',negative
'the partition directory ',negative
'otherwise have wait until after the maskingfiltering step ',negative
'get map operator and initialize ',negative
'set appropriate acid readerswriters based the table properties ',negative
'add list ',negative
'not then create set hanging readers that sortmerge find the next smallest delete event ondemand caps the memory consumption someconst readers ',negative
'',negative
'triggers ',negative
'todo hive case abort request throw ',negative
'delete jar added using query ',negative
'aliases for java class names ',negative
'the union already initialized however the union walked from another input initunionplan idempotent ',negative
'future could check argcolvectornonulls and optimize these loops ',negative
'first violation for the session ',negative
'get custom path string ',negative
'value ',negative
'the basic idea similar unparsetranslator ',negative
'can ref cursor variable ',negative
'when txnid the lock ',negative
'the task cannot finish and slots are available then dont schedule also dont wait have task and just killed something schedule ',negative
'resolved task ',negative
'ignore safe does not exist ',negative
'check the character array has the character ',negative
'the head means the number bytes for register ',negative
'default executor when option specified ',negative
'step remove any tmp file doublecommitted output files ',negative
'event ',negative
'implement reloptrule ',negative
'invalidate cached aggregate stats ',negative
'currently partition spec can only static partition ',negative
'load using same dump with table should fail not empty ',negative
'case when need check that children not contain stateful functions they are not allowed ',negative
'regular insert export some deltas then import into new table ',negative
'note ptndesc can null empty for nonptn table ',negative
'construct sorted map partition dir partition descriptor ordering based patition dir map key assumption there mapping between partition dir and partition descriptor lists ',negative
'iterate thru the cols and load the batch ',negative
'there authorization anybody has administrator access ',negative
'this node was parsed while loading the definition another view being referenced the one being created and dont want track any expansions for the underlying view ',negative
'remove expression node descriptor and children for given predicate from mapping its already keys ',negative
'noop authentication ',negative
'now compact and see compaction still preserves the data correctness ',negative
'convert millis ',negative
'now look the current hive config value again avoiding getting defaults ',negative
'the new numreducer less than minreducer will not consider reducesinkoperator with this newnumreducer correlated reducesinkoperator ',negative
'add dummy aggregate stats object for the above parts partpart tab for col ',negative
'same also emit extra records from separate thread ',negative
'mode all just run ',negative
'case test with originals compactedbase insertdeltas deletedeltas exhaustive test ',negative
'check ',negative
'since were passing the object output the udtf directly the next ',negative
'which the minimum non value this case ',negative
'set pending false since scheduling about run any triggers this point will handled the next run new request may come right after this set false but before the actual scheduling this will handled this run but will cause immediate run after which harmless this mainly handle tryschedue request while the middle run since the event which triggered may not processed for all tasks the run ',negative
'handle sessions that are being destroyed users destroy implies return ',negative
'unless already installed all the cluster nodes well have ',negative
'repeated null permutations ',negative
'will also read the last row ',negative
'hivedefaultnull the system default value for null and empty string todo should allow user specify default partition hdfs file location ',negative
'wrap big catch throwable statement ',negative
'for inner joins push not null predicate the join sources for every non nullsafe predicate ',negative
'the default equals provided thrift compares the comments too for equality thus need compare the relevant fields here ',negative
'return node ',negative
'loginforead hash code for length ',negative
'need set the merge work that has been created part the dummy store walk merge work already exists for this merge join operator add the dummy store work the ',negative
'may produce filesplit that not orcsplit ',negative
'string can used write char and varchar when the caller takes responsibility for truncationpadding issues ',negative
'lead the whole partition not the iterator range ',negative
'for fullacid tables dont delete files for commands with overwrite create new basex there insert overwrite and load data overwrite ',negative
'add the privileges supported authorization mode ',negative
'construct the list columns that need projected ',negative
'this guaranteed positive because types only have children ids greater than their own ',negative
'properties file used configure logj ',negative
'srswwait lock are examining waiting this case keep looking its possible that something front blocking that the other locker hasnt checked yet and could lock well ',negative
'weve already locked the table the input dont relock the output ',negative
'the event already replayed then need replay again ',negative
'constant byte arrays ',negative
'the partition location already existed and may contain data lets try populate those statistics that dont require full scan the data ',negative
'earlier implementation have quoted boolean valuesso the new implementation should preserve this ',negative
'translated includes may superset writer includes due cache ',negative
'create table database specific location ',negative
'this the small table side ',negative
'verify that partitioned table partition property set worked ',negative
'check edge case where the does not allow one clause with single value ',negative
'this parent does not contain constant this position continue look other positions ',negative
'construct kerberostoken relies proxyuser configuration will the client making the request top the hss user accumulo will require proper proxyuser auth configs ',negative
'orc table restrict changing the file format can break schema evolution ',negative
'null last ',negative
'must the server uri added older version ',negative
'add columnstatistics for tbl metastore via objectstore ',negative
'add vector partition descriptor partition descriptor removing duplicate object the same vector partition descriptor has already been allocated share that object ',negative
'avoid processing the same config multiple times check marker ',negative
'for reads whatever sarg maybe applicable base its not applicable deletedelta since has user columns for compaction there never sarg ',negative
'nonjavadoc see javasqltime javautilcalendar ',negative
'singlecolumn long specific imports ',negative
'find the location the table ',negative
'sindouble ',negative
'pretend are ',negative
'logging disabled deny everything ',negative
'started with single drl assume there will consecutive missing blocks after the cache has inserted cache data also assume all the missing parts will represent one several column chunks since always cache column chunk boundaries ',negative
'order for this work hivesitexml must the classpath ',negative
'branch hit ',negative
'set ',negative
'over predicate only contains columns columnsmapped construct new predicate based mapping ',negative
'init keyfields ',negative
'will iterate through the children insert will traverse ',negative
'use byteswritable because supports comparable for our treemap ',negative
'renew the metastore since the cluster type unencrypted ',negative
'msb set then next qprime msb bits contains the value number zeroes msb set then number zeroes contained within pprime bits ',negative
'get the ndv ',negative
'the patterns orandeqop oreqop are not matched bail out ',negative
'todo shut down ',negative
'why cannot just use the exprnodeevaluator the column because the reduceside initialized based the rowoi the hivetable and not the the parent this operator the reduceside ',negative
'shut down all the servers ',negative
'multiple udfs with the same max type unless find lower one well give ',negative
'type intervalyearmonth longcolumnvector storing months ',negative
'create partitioned table event ',negative
'parse command line ',negative
'schema the mapreduce value object this heterogeneous ',negative
'that guaranteed fit any maximum allocation ',negative
'sort ascending resource nulls first ',negative
'ngram estimator object ',negative
'convert children aggparameters ',negative
'correlgetcondition was here however correlate was updated ',negative
'resourcetype ',negative
'lastanalyzed stored per column but thrift object has per multiple columns luckily nobody actually uses will set lowest value all columns for now ',negative
'copy clonetowork ensure rdd cache still works ',negative
'was false ',negative
'the output files filesink can merged they are either not being written table are being written table which not bucketed ',negative
'test need groupby shuffle ',negative
'pending task which not finishable ',negative
'dummy vertex treated branch join operator ',negative
'try parse where there millisecond part input expected return ',negative
'checks resource has uploaded hdfs for yarncluster mode ',negative
'case find rows which belong write ids that are not valid ',negative
'total size the inputs ',negative
'connection properties ',negative
'creating new connection expensive well reuse this object ',negative
'test both inputs repeating ',negative
'add back the nonexpired session need notify are the only ones waiting ',negative
'the state has changed between this and previous check within this method the failed update was rendered irrelevant just exit ',negative
'should ignore the failure ',negative
'selfdescribing input format will convert its data the table schema there will vectormapoperator conversion needed ',negative
'then scale back ',negative
'convert rexnode exprnodedesc ',negative
'use the object pool rather than creating new object ',negative
'the response will have one entry per table and hence get only one validwriteidlist ',negative
'can only happens wzcr for single input buffer ',negative
'these members have information for assigning row column objects into the vectorizedrowbatch columns say target because when there conversion the data type being converted the source ',negative
'construct using ',negative
'invalid inflation factor ',negative
'compare the field names using ignorecase semantics ',negative
'run with checkindex and save the output file can checked ',negative
'txntowriteids ',negative
'static class iterator ',negative
'for efficiency alpha multiplied ',negative
'there are more cores use the number cores ',negative
'data needs deletion check trash may skipped trash may skipped iff deletedata true obviously tbl external either user has specified purge from the commandline and not user has set the table autopurge ',negative
'determine who run ',negative
'upgrade from schema and revalidate ',negative
'value based compare remove first ',negative
'make sure both buckets are not empty ',negative
'field contains fieldtype which turn contains type ',negative
'bootstrap dump shouldnt fail the table droppedrenamed while dumping just log debug message and skip ',negative
'tables and tables inside view otherwise calcite will treat them the same ',negative
'output bucketed ',negative
'present only the qltest directory ',negative
'construct using ',negative
'sql indexed druid indexed ',negative
'use the tez grouper combine splits once per bucket ',negative
'matter statsgenerated user task all need recalculate the stats user alter table update statistics task from some sql operation which could collect and compute stats ',negative
'',negative
'missing fields ',negative
'pushes node the stack ',negative
'ensure metatore sitexml does not get override this ',negative
'create delta directory ',negative
'heartbeats can only sent for open transactions there race between committingaborting transaction and heartbeat example heartbeat sent for committed txn exception will thrown similarly dont send heartbeat metastore server might abort txn for missed heartbeat right before commit txn call ',negative
'string representation folding constant ',negative
'the length the scratch byte array that needs passed bigintegerbytes etc ',negative
'nothing its not partitioned table ',negative
'division ',negative
'changes the owner user and verify the change ',negative
'startrowoffset ',negative
'the conf does not define any transactional properties the parseint should receive value which will set default type and return that ',negative
'construct object above type ',negative
'create the httphttps url jdbc driver will set https url ssl enabled otherwise http ',negative
'the split ends within and would read the last row this slice exact match ',negative
'spilled batchindex batchindex length length ',negative
'for whatever reason reserve failed ',negative
'error couldnt find the task lastsetguaranteed does not change the logic here does not account for one special case have updated the task but the response was lost and have received network error the state could inconsistent making ',negative
'output format string not supported anymore warn user deprecation ',negative
'this test the parameter value denotes the method which needs throw error ',negative
'one call init one called here ',negative
'cint cboolean cdouble cstring carrayint ',negative
'nothing handle rud add another status ',negative
'try fixing this should result new fixed file ',negative
'for permanent functions check for any resources from local filesystem ',negative
'check ckpt property set tablepartition after bootstrap load ',negative
'call the different round flavor ',negative
'test the idempotent behavior create function ',negative
'setting success false make sure that the listener fails rollback happens ',negative
'only create mapjoin the user explicitly gave join without mapjoin hint ',negative
'context for list bucketing ',negative
'nonjavadoc see javaioreader int ',negative
'this only responds ',negative
'verify the column name ',negative
'plumb the kryomessagecodec instance through the constructors ',negative
'range starts here ',negative
'the lateral view forward operator has children select and selectcols for the udtf operator the child index the select because thats the way that the dag was constructed only want get the predicates from the select ',negative
'materializationtime ',negative
'nonjavadoc see ',negative
'throw hiveexception for other than rcfile and orcfile ',negative
'unparsetranslator ',negative
'and return the ones which have marked column ',negative
'this project will what the old input maps ',negative
'return defaultname selexpr not simple xxyyzz ',negative
'table cache not yet prewarmed add this set which the prewarm thread can check that the prewarm thread does not add back ',negative
'joda parsing only supports millisecond precision ',negative
'use exact byte array which might generate array out bounds ',negative
'total characters byte length ',negative
'check the other side the join using the dynamiclistcontext ',negative
'assume this hashtable loaded only when tez enabled ',negative
'first comparison unsigned ',negative
'then master commits everything goes well ',negative
'join groupby distinct lateral view subq ctas insert not analyze command and single sourced ',negative
'dont cache the filesystem object for now tez closes and cache will fix all that ',negative
'check mapred ',negative
'test basic case ',negative
'make sure the functioninfo listed persistent rather than temporary ',negative
'started initialize context for new batch ',negative
'operation ',negative
'dont need add this new entry since theres already overlapping one ',negative
'there are several hive relnode types which not have their own visit method defined the hiverelshuttle interface which need handled appropriately here per jcamachorodriguez should not encounter during these checks need add those here ',negative
'todo make expr traversal recursive extend traverse inside elements dnfcnf extract more deterministic pieces out ',negative
'utility methods used store pairs ints long ',negative
'testspecific ',negative
'instantiate the valueprocessor based the input type ',negative
'myenumset ',negative
'test dependent getting new buffer within ',negative
'key full table name string format dbnametablename ',negative
'the rest optimizations ',negative
'only used for materialized views only used for materialized views only used for materialized views only used for materialized views only used for materialized views ',negative
'with bucketed target table union all not removed ekoifman tree ext orcacidversion delta bucket orcacidversion delta bucket directories files ',negative
'interfere with the view creation skip the rest this method ',negative
'check all the arguments ',negative
'field find record identifier field bucket record for inspecting record for inspecting bucket ',negative
'only first second operator contains dpp pruning ',negative
'closing the chunked output stream early gives error ',negative
'since dont clone jobconf per alias ',negative
'todo allocate work remove the temporary files and make that dependent the redtask ',negative
'evaluate ',negative
'cannot have scalar scalar ',negative
'common repeated join result processing ',negative
'may have been created ',negative
'check for this pattern the pattern matching could simplified rules can applied during decorrelation correlaterelleft correlation condition true leftinputrel aggregate groupby singlevalue projecta may reference covar ',negative
'not test mode then create the appender ',negative
'always validate acls ',negative
'nonjavadoc see ',negative
'for backward compatibility fieldnames can also integer strings ',negative
'replace void type with string when the output temp table local files void type can generated under the query select null from insert overwrite local directory abc select null from where there column type which the null value should converted ',negative
'string pathnames the path separator the file separator directory ',negative
'test null propagation ',negative
'droptabledesc todo this currently used for both drop table and drop partitions ',negative
'make sure reduce task environment points ',negative
'need some initial values case user dont call initialize ',negative
'config settings ',negative
'such that the aggregation expressions need handled the windowing ptf invoke this function clear the aggexprs the dest ',negative
'add the path the list input paths ',negative
'get all the stuff for dont emptylist check expect partitions have sds ',negative
'see need fetch default constraints from metastore ',negative
'the alias modified subqa and subqa from identify the right subquery ',negative
'scale downup the column statistics based the changes number rows from each parent for there are parents for join operator with parent having rows and parent having rows now the new number rows after applying join rule then the column stats for columns from parent should scaled down and stats for columns from parent should scaled down ',negative
'check the stats ',negative
'any tablepartition updated then update repl state object ',negative
'make sure the schemas both sides are the same ',negative
'verify create table only table should created retry ',negative
'signature for wrapped loader see comments ',negative
'propertiesfile contains the spark keys that are meant for sparkconf object ',negative
'check this user has necessary privileges reqprivs this object ',negative
'will requeue and not kill the queries that are not running yet ',negative
'update the create table descriptor with the resulting schema ',negative
'optimize inner join keys small table results ',negative
'the middle and lowest longwords highest digit number ',negative
'note this code would invalid for transactional tables any kind ',negative
'otherwise fall through and process the what saw before possible trailing blanks ',negative
'use hive type name ',negative
'whether session running silent mode not ',negative
'optional int purgedmemorybytes ',negative
'partition and order ',negative
'mixed source all types ',negative
'not boolean column return half the number rows ',negative
'fill coltype ',negative
'the column positions the operator should like this nonpartition columnsstatic partition columnsdynamic partition columns exprnodecolumndesc exprnodecolumndesc from input generate itself from input ',negative
'keep alive enabled not close the connection ',negative
'this our row test expressions ',negative
'buffer the top the heap ',negative
'make the new join rel ',negative
'annotate the aggregate operator with this info ',negative
'this table needs converted crud acid ',negative
'should make inf ',negative
'get create context object create have clean later well ',negative
'disable feature ',negative
'set true default only actively set the multiple key case support outer join ',negative
'add new entry for this table ',negative
'flush current group batch last batch group ',negative
'infer any column can primary key based column statistics ',negative
'value ',negative
'first the cross join ',negative
'keys ',negative
'partition cant have this name ',negative
'this covers backward compat cases where this prop may have been set already ',negative
'this the first time the table being initialized transactionaltrue any valid value can set for the ',negative
'expand all supported privileges ',negative
'normal column also string ',negative
'charxvarcharx types ',negative
'merge and sort result ',negative
'check isshutdown opportunistically its never unset ',negative
'query per mbean attribute ',negative
'doesnt make sense have both and make sure one matches the other ',negative
'build regular expression for operator rule ',negative
'first check should not have repeats results ',negative
'finally connect the union work with work ',negative
'get compatible taskid for bucketname ',negative
'have the optional fourth parameter make sure its also integer ',negative
'empty value ',negative
'base this hiveoperation instead this and ddlnolock peppered all over the code seems much cleaner each stmt identified particular hiveoperation which think makes sense everywhere this however would problematic for merge ',negative
'not flattened struct need unflatten ',negative
'redo createtableview analysis because its not part dophase ',negative
'explain type ',negative
'want make sure this runs low priority the background ',negative
'set input the join that should omitted the output ',negative
'for the tab case add all the columns the fieldlist from the input schema ',negative
'note hadoop onwards includes class that usable asis however since have work multitude hadoop versions including very old ones either duplicate their code here not support xsrffilter older versions hadoop duplicate minimize evilugh see hadoop for details what this doing this method should never called hadoop available ',negative
'more rows ',negative
'found dynamic partition pruning operator ',negative
'light results from union queries need aware that subdirectories can exist the partition directory want ignore these subdirectories and promote merged files the partition directory ',negative
'production bool ',negative
'template classname valuetype varianceformula descriptionname ',negative
'time part the timestamp should not skipped ',negative
'orcinputformat will get mock from filesystemget add global files ',negative
'',negative
'the options principalkeypad not work with proxyuser sparksubmitsh see hive spark spark hive could only support doas delegation token renewal but not both since doas more common case both are needed choose favor doas when doas enabled use kinit command otherwise pass the principalkeypad spark support the token renewal for ',negative
'form truncated boolean include array for our vectorrow deserializers ',negative
'this not hash groupby return ',negative
'the parents have already been created create the last child only ',negative
'check that the hcat result valid and has valid json ',negative
'lock response acquired can create the heartbeater ',negative
'has parents guaranteed ',negative
'randomuuid slow since its cryptographically secure only first query will take time ',negative
'drop table after dump ',negative
'get the sorted children expr strings ',negative
'disallow update and delete nonacid tables ',negative
'rest the data serialized long values for the bitset which are supposed bitwiseored ',negative
'for vertex group all outputs use the same keyclass valclass and partitioner pick any one source vertex figure out the edge configuration ',negative
'allow use external byte for efficiency ',negative
'the one join column for this specialized class ',negative
'use fsshell change group permissions and extended acls recursively ',negative
'need convert the jobcontext into jobconf jobconf hive jobcontext hcat ',negative
'next partition next partition nothing ',negative
'optimize the query select countdistinct keys from where bucketized and sorted partial aggregation can done the mappers this scenario ',negative
'check the properties expected hive client without metastore ',negative
'test with empty array ',negative
'unknown ',negative
'for now use calcite default formulas for propagating ndvs the query tree ',negative
'the output stream serialized objects ',negative
'this avoid getting notified low memory too often and flushing too often ',negative
'store the byte every eight elements ',negative
'create data buffers for these columns can copy strings into those columns value ',negative
'make sure pad the right amount spaces vallength terms code points while stringutilsrpad based the number java chars ',negative
'nothing there user connection configuration file beelinesitexml the path ',negative
'also convert tofrom binarysortable representation ',negative
'there are nulls either input vector ',negative
'note udaf not included exprcolmap ',negative
'pairwise columnhasnulls columnisrepeating columnhasnulls columnisrepeating ',negative
'this can happen for numbers less than for bdprecision bdscale this case well set the type have the same precision the scale ',negative
'same query within dag priority lower values indicate higher priority ',negative
'update twice accurately detect cache dirty not ',negative
'add constant struct field names references overhead ',negative
'divide that can have more reducers ',negative
'parse until key separator currentlevel ',negative
'rethrow the sqlexception ',negative
'create nonexistent path for row results ',negative
'introduce and before ',negative
'sanity check make sure there alias conflict after merge ',negative
'quote the database requires ',negative
'left side ',negative
'fail heartbeater that can get runtimeexception from the query more specifically its the original ioexception thrown either mrs tezs progress monitoring loop ',negative
'print only the errors the operation log and the query results ',negative
'use the input tablescanoperator case there mapside reshape input the parent reducesinkoperator ptfoperator use its output ',negative
'hive ',negative
'the threshold should less than bytes for the reason divide sparse mode after serialization the entriesin sparse map are compressed and delta encoded varints the worst case size varints are bytes hence entries ',negative
'this called from replication task the user the user who has fired the repl command this required for standalone metastore authentication ',negative
'columnscalar ',negative
'mapping from column name default value ',negative
'only expect here because well get whichever the partitions published its stats last ',negative
'each validwriteidlist separated with and each one maps one table ',negative
'first set basic stats true ',negative
'memory pressure ',negative
'with uncompressed streams know are done earlier ',negative
'return the current blocks key length ',negative
'always set the semijoin optimization victim ',negative
'locations for each the storage types ',negative
'case when there are changes multiple table properties ',negative
'the default pool not disabled override the size with the specified parallelism ',negative
'always keep transactional tables managed tables ',negative
'basic algorithm determine rounding part meets bankers rounding rules for rounding scale away fractional digits present rounding clear integer rounding portion and add ',negative
'test http mode ',negative
'add self ',negative
'write out the buffer into file add beeline commands for autocommit and close ',negative
'',negative
'set bitvectorindex ',negative
'reset the driver ',negative
'use the defalut methods for next the child class ',negative
'creationmetadata ',negative
'use its conversion ability ',negative
'nonjavadoc see javasqlsqlxml ',negative
'add new token shared store need persist expiration along with password ',negative
'now generate the matchs single small table values will put into the big table batch and come back matchs any multiple small table value results will into ',negative
'the only time this condition should false the case dynamic partitioning ',negative
'test null left ',negative
'sparsesparse overload dense ',negative
'',negative
'verify batch size ',negative
'the new table rhs table ',negative
'restrict instantiation ',negative
'the task hasnt started inform about fragment completion immediately its possible for the callable never run ',negative
'this column that dont want not included are done ',negative
'hivehistory object ',negative
'dont support using multiple chars delimiters within complex types ',negative
'the ctor should throw the error ',negative
'find the positionsorder the sorted columns the table corresponding ',negative
'iterate through the line and invoke the addcmdpart method whenever the delimiter seen that not inside ',negative
'the number integer digits the decimal when the integer portion zero this ',negative
'cancel existing watches ',negative
'check compaction set for this table ',negative
'add the value the arraylist ',negative
'include state for cached columns ',negative
'note keep the typeinfo and arrays ',negative
'database url ',negative
'but note that any sql error will also result return false ',negative
'the most correct behavior throw only the request tries enable the readonly mode ',negative
'start third batch but dont close ',negative
'should generate inf ',negative
'indicate whether the pages should thrown away not ',negative
'blobstore section ',negative
'passing charvarchar arguments should prefer the version evaluate with text args ',negative
'for predicate check candidate for pushing down limit optimization the expression must the form rankfn constant ',negative
'when doing updates and deletes always want sort the rowid because the acid reader will expect this sort order when doing reads ignore whatever comes from the table and enforce this sort order instead ',negative
'stats from the record writer and store the previous fsp that cached ',negative
'keep track subqueries which are scalar correlated and contains aggregate subquery expression this will later special cased subquery remove rule for correlated scalar queries with aggregate have take care the case where inner aggregate happens empty result ',negative
'not need evaluate the input row for this parent can just forward the child this muxoperator ',negative
'general case can have unlimited branches currently only handle either branch ',negative
'check column order and types ',negative
'inputgeti since inputget the schema ',negative
'dynamicserde always writes out byteswritable ',negative
'nonjavadoc see int ',negative
'use destination tables location ',negative
'other format pattern should also work ',negative
'pass null complete batch ',negative
'partcol and nonpartcol replaced with partcol and true which will folded partcol this cannot done also for ',negative
'maybe someone removed the field probably ignore ',negative
'nonjavadoc see javasqltimestamp ',negative
'optional parameter ',negative
'read the template into string ',negative
'optional int timestamp ',negative
'get the input expression ',negative
'write with additional uncommitted ids should match ',negative
'join keys have difference sizes ',negative
'setup vectorized deserialization for the key and value ',negative
'fourth group ',negative
'partitioned table ',negative
'the deadline exception needs retry and thrown immediately ',negative
'check path conforms hives file name convention hive expects filenames specific format like but load data commands can let you add any files any partitionstables without renaming this can cause movetask remove files some cases where movetask assumes the files are are generated speculatively executed tasks example movetask thinks the following files are same partm partm assumes taskid and retains only large file supposedly generated speculative execution this can result data loss case concatenatemerging filter out files that does not match hives filename convention ',negative
'serde info ',negative
'was merge task local map reduce task nothing can inferred ',negative
'hang onto byte array for holding smaller byte values ',negative
'this particular test doesnt care which the lower pri tasks gets the duck ',negative
'have this reverse order that drop the materialized view first ',negative
'hivespark keys are passed down the remotedriver via conf ',negative
'private methods should never catch sqlexception and then throw metaexception the public methods depend sqlexception coming back they can detect and handle deadlocks private methods should only throw metaexception when they explicitly know theres logic error and they want throw past the public methods all public methods that write the database have check for deadlocks when sqlexception comes back and handle they see one this has done with the connection pooling mind this they should call checkretryable after rolling back the transaction and then they should catch retryexception and call themselves recursively see committxn for example ',negative
'override this for using extended fieldobject ',negative
'run given query and validate expecated result ',negative
'only update someone waiting for info have the info ',negative
'were going wait for the session abandoned ',negative
'inform the scheduler that this fragment has been killed the kill failed that means the task has already hit final condition ',negative
'sdsetbucketcolsnew arrayliststring ',negative
'found ',negative
'tokdropfunction identifier ifexists temp ',negative
'check have ckpt property set bootstrap dump location used and missing for tablepartition ',negative
'set values reference copy the data out and verify equality ',negative
'find free port ',negative
'move the query results the query cache directory ',negative
'partitioned table need get the old stats the partition and update the table stats based the old and new stats ',negative
'query column stats for column whose stats were updated the previous call ',negative
'move past key separator ',negative
'split for delta which filtered out entirely txns ',negative
'jump the end current line when multiple line query executed with parameter passed one line string separated with ',negative
'nothing done ',negative
'case analyze command ',negative
'this should never happen however want make sure propagate the exception ',negative
'verify table for key row and byte hash table hashmap ',negative
'list partitions that are required populated from processing each event ',negative
'buffer since that what used vectorserializerow serializewrite periodically flush this buffer disk ',negative
'lastevent ',negative
'udf which sleeps for some number simulate long running query ',negative
'there are paths from the operator previous lvj operator the same selectoperator selectoperator gets cols for udtf udtfoperator ',negative
'source file system list source paths ',negative
'nothing prune for this mapwork ',negative
'escaping happened are already done ',negative
'small table values are set null ',negative
'verify table for key long hash table hashset ',negative
'error the script itself likely caused incompatible change new functionality states added ',negative
'',negative
'not setting payload since the mrinput payload the same and can accessed ',negative
'low memory canary set and records after set canary exceeds threshold trigger flush ',negative
'nonjavadoc see ',negative
'initialize all the dummy ops ',negative
'used datumwriter applications should not call ',negative
'case udtf selectoutputrr may null ',negative
'constructors ',negative
'tiemestamp ',negative
'maxtables ',negative
'keep track the principals which privileges have been checked for this object ',negative
'countdistinct countdistinct find the correct mapping ',negative
'query per mbean ',negative
'metalisteners ',negative
'not mess with table instance ',negative
'retrieve the tables from the metastore batches some databases like ',negative
'not assign the input value object the timestampvalues array element always copy value using set methods ',negative
'reduced ',negative
'map precision the number bytes needed for binary conversion ',negative
'hive cant handle select rank overorder sumcsumc from group but can handle select rank over order from select sumcsumc from group introduce project top this gby ',negative
'get the input table and make sure the keys match ',negative
'harprocessor regular operation ',negative
'creates job request object and sets execution environment creates thread pool execute job requests param requesttype job request type param config name used extract number concurrent requests serviced param config name used extract maximum time task can execute request param enablecanceltask flag indicate whether cancel the task when exception timeoutexception raised ',negative
'llapinputformat needs know the file schema decide schema evolution supported ',negative
'this required ',negative
'specific the multi group optimization ',negative
'not retrying when user explicitly stops the test ',negative
'event loads will behave similar table loads with one crucial difference precursor order strict and each event must processed after the previous one the way handle this strict order follows first start with taskchaintail which dummy noop task the head our event chain for each event process tell analyzetableload create tasks that use the taskchaintail dependency then collect all those tasks and introduce new barrier taskalso which depends all these tasks then this barrier task becomes our new taskchaintail thus get set tasks follows evtask evtask evtaskrootevtask evtask once this entire chain generated add evtaskroot roottasks execute the entire chain ',negative
'for tostring the time does not matter ',negative
'this only used the error code path ',negative
'open txn doesnt allocate writeid the entries for aborted and committed should retained ',negative
'repair metadata hms ',negative
'the time currentcompactions generated and now ',negative
'throw new ',negative
'instantiate the underlying output format since there way parametrize instance class ',negative
'writing delta dirs need make clone original options avoid polluting for ',negative
'latin small letter turned bytes ',negative
'connect using the jdbc url and userpass from conf ',negative
'convert logs rowset ',negative
'trigger scheduling run case theres some task which was waiting for this node ',negative
'the cookie based authentication not enabled the request does not have valid cookie use the kerberos password based authentication ',negative
'excluded overrides included ',negative
'the selected vector represents selected rows clone the selected vector ',negative
'not final since may change later due decimal ',negative
'reserialize the splits after grouping ',negative
'get all data out ',negative
'concern currently differentiate decimal columns their precision and scale ',negative
'first use tgetparameters prune the stats ',negative
'that case just pick one and spill ',negative
'this work now moved the parentwork thus should ',negative
'rrr ',negative
'add udfdata udf necessary ',negative
'kids reduce sink operator mapjoin operators merged into root task ',negative
'set filter expression ',negative
'catch throwables besteffort report job status back the client its rethrown that the executor can destroy the affected thread the jvm can die whatever would happen the throwable bubbled ',negative
'ignore this particular error expected ',negative
'null null ',negative
'merge with children predicates ',negative
'opereations none them are enforced hive right now ',negative
'statistics track allocations ',negative
'trigger bootstrap replication ',negative
'keytypeptr ',negative
'high word multiplier digit commad ',negative
'normally import trying create table partition that does not yet exist error condition however the case repl load possible that are trying create tasks create table inside that asofnow does not exist but there precursor task waiting that will create before this encountered thus instantiate defaults and not error out that case the above will change now since are going split replication load multiple execution tasks and hence could have created the database earlier which case the waitonprecursor will ',negative
'encode ',negative
'fix the nonprintable chars ',negative
'put the filter skewed column skewed keys ',negative
'pull the output schema out the taskattemptcontext ',negative
'generate backtrack select operator ',negative
'verify the filter correct and returns rows and contains columns and the contents match ',negative
'does the move task involve moving local file system ',negative
'llap only mode grace hash join will disabled later the llapdispatcher anyway since the presence grace hash join disables some native vectorization optimizations will disable the grace hash join now before vectorization done ',negative
'compile being called multiple times clear the old shutdownhook ',negative
'terminate ',negative
'attach the original predicate the table scan operator for index optimizations that require the pushed predicate before pcr later optimizations are applied ',negative
'retrieve all partitions from the table ',negative
'recordreader readerrows ',negative
'reassign new port just case one the services grabbed the last one ',negative
'test select rootcola from ',negative
'should have more rows ',negative
'not call startgroup operators below because are batching rows output batch and the semantics will not work superstartgroup ',negative
'listofneedfetchnext contains all tables that have joined data their candidatestorage and need clear candidate storage and promote their nextgroupstorage candidatestorage and fetch data until reach new group ',negative
'set the newlyconstructed ranges the current state ',negative
'make sure dont get stuck time however unlikely that ',negative
'however the above query contains dynamic partitions subq and subq have write directories and respectively the movetask that follows subq and subq tasks still moves the directory parent ',negative
'now update this record being worked this worker ',negative
'the numerator the tablesample clause ',negative
'also dynamic partitioning being used want set appropriate list columns for the columns dynamically specified these would partition keys too would also need removed from output schema and partcols ',negative
'newfunc ',negative
'check batch ',negative
'are unique keys for left ',negative
'acid off post upgrade you cant make any tables acid will throw ',negative
'may processed thread which ends executing before task ',negative
'replicate the incremental drops ',negative
'copy string value asis ',negative
'kerberos not enabled ',negative
'constant char projection ',negative
'only selects and filters are allowed ',negative
'are skipping calling checkoutputspecs for each partition can throw when more than one mapper writing partition see hcatalog also avoid contacting the namenode for each new fileoutputformat instance general this should for most fileoutputformat implementations but may become issue for cases when the method used perform other setup tasks ',negative
'use large capacity that doesnt require expansion yet ',negative
'destf existing directory replace true delete followed renamemv equivalent replace replace false rename actually move the src under dest dir destf existing file rename actually replace and not need delete the file first ',negative
'classloader parent can pollute the session see hive ',negative
'try reading table user should succeed ',negative
'files size for splits ',negative
'columnstatsaccurate params set correct value ',negative
'exceeds maxvalue ',negative
'ignore and use uuid instead ',negative
'its nondeterministic udf set unknown true ',negative
'ignore this expected ',negative
'need further down for select with all file sink script child since all cols are needed for these ops however one the children not file sink script still down ',negative
'need run cbo table ref virtual table not supported ',negative
'ignore the exception ',negative
'revert the projected columns back because batch can reused our parent operators ',negative
'start the protocol server after properly authenticating with daemon keytab ',negative
'create empty priv set ',negative
'make sure dont give out more than allowed due doublerounding artifacts ',negative
'the small table hash table for the native vectorized map join operator ',negative
'common leftsemi join result processing ',negative
'dont need this anymore ',negative
'modifier letter small bytes ',negative
'retries will done till decaying factor reduces decaying factor log base batchsize plus most significant bit batchsize plus will give the number expected calls ',negative
'nonjavadoc see int javalangstring ',negative
'matter loc the table location part location must directory ',negative
'traverse path filter projects get the tablescan case unique keys stop you reach project will handled the invocation the project case getting the base rowcount path keep going past project ',negative
'for partial and final objectinspectors for partial aggregations ',negative
'whole value copied including spaces ',negative
'the table not yet loaded cache ',negative
'this insert originalwriteid should set this transaction not will reset the following anyway ',negative
'varchar tests ',negative
'memoize decorator for ',negative
'traverse the aborted txns list starting first aborted txn index ',negative
'create thread pool with queue wait time execute the operation this will ensure that job requests are rejected there are already maximum number threads busy ',negative
'some parts the system cant handle rows with zero fields ',negative
'template classnameprefix returntype operandtype funcname operandcast resultcast ',negative
'this delimiter directive reset our delimiter ',negative
'preventing empty ctor from being callable ',negative
'may caused for other different errors but take our path readonly ',negative
'all qfiles handled via this ',negative
'wed like move this hivemetastore for any nonnative table but first need support storing null for location table ',negative
'initialize the cache ',negative
'nosasl ',negative
'fastislong returns false ',negative
'nothing here will throw exception the following block ',negative
'with many arguments ',negative
'try getobjectinspector ',negative
'the curraliasid and currtopop not valid any more ',negative
'see the structexp exp has atleast one expression with partition column with single table alias not bail out might have expressions containing only partitioning columns say where and are both partitioning columns ',negative
'with filesinks and buckets should see since data sorted rowid where tnxid the first component should see ',negative
'string classname thisgetclassgetname ',negative
'binary search find the closest bucket that should into bin should interpreted the bin shift right order accomodate result bin the range where means that the value greater than all the bins currently the histogram also possible that ',negative
'generate dummy preupgrade scripts with valid sql ',negative
'groupingsets are known mapreducer side but have real processing ',negative
'report size the extent possible ',negative
'comparing time not sufficient since two may created the same time which case inserting into treesetmap would break ',negative
'determine class ',negative
'descending sort based split size followed file name followed startposition ',negative
'optional string connectedvertexname ',negative
'the following data should changed ',negative
'ship additional artifacts for example for tez ',negative
'query has run capture the time ',negative
'hive pending rename after ',negative
'create fetch operator ',negative
'this value should into smallbuffer ',negative
'taskinfo instances for two different tasks will not the same only single instance should ',negative
'test that write can acquire after read ',negative
'make both scaling follows unscaledvalue significand scale scaletwoscaledown ',negative
'task requested host got host ',negative
'check data distribution buckets ',negative
'this class provides and implementation for case insensitive token checker for the lexical analysis part antlr converting the token stream into upper case the time when lexical rules are checked this class ensures that the lexical rules need just match the token with upper case letters opposed combination upper case and lower case characteres this purely used for matching lexical rules the actual token text stored the same way the user input without actually converting into upper case the token values are generated the consume function the super class antlrstringstream the function the lookahead funtion and purely used for matching lexical rules this also means that the grammar will only accept capitalized tokens case run from other tools like antlrworks which not have the implementation ',negative
'tablespec got from the query user specified which means the user didnt specify partitions their query ',negative
'this map should map columninfo ',negative
'couldnt find asterisk its not prefix ',negative
'state byte the first record ',negative
'encode schema with write out ',negative
'dont remove the operator for distincts ',negative
'and where the transaction has already been committed per snapshot taken ',negative
'now create delete delta that has rowids divisible but not this will produce ',negative
'write the null bytes ',negative
'for all other data types use int conversion some point should have all conversions make sure the value fits ',negative
'conversions ',negative
'remember the dummy ops created ',negative
'this indicates logged inconsistency from our pointofview and will not make this ',negative
'optional userpayloadproto userpayload ',negative
'createtime ',negative
'alias stage local task ',negative
'nothing ',negative
'should allocate ',negative
'note that this method assumes you have already decided this acid table cannot ',negative
'due reflection the jdo exception wrapped ',negative
'test dryrun schema upgrade ',negative
'handled ',negative
'this could due either uri syntax error file constructor illegal arg dont really care which one ',negative
'this fast path for query optimizations can find this info quickly using directsql point failing back slow path here ',negative
'currently evaluate the constants special ways ',negative
'note the row mapping not relevant when ',negative
'compare they are the same constant ',negative
'groupby query results records types defined metastore ',negative
'the driver registered the driver manager get the connection via the driver manager ',negative
'prevent equals from being overridden subclasses always use you need any other equality than objectequals ',negative
'check empty ',negative
'get the string value and convert date value ',negative
'verify that multi byte like expression matches matching string ',negative
'even the data not deleted ',negative
'start index means give the last characters the string ',negative
'primarykeys ',negative
'elapsedms ',negative
'this pluggable policy chose the candidate mapjoin table for converting join sort merge join the leftmost table chosen the join table ',negative
'create structs ',negative
'can get the number rows from the first vector ',negative
'timestampdate higher precedence than stringgroup ',negative
'clauses need combined keeping all elements ',negative
'bgenjjtree senumdeflist ',negative
'have fired already once return and the test will fail ',negative
'workgettablespecs null means not analyze command and then not followed column stats should clean column stats ',negative
'nice message ',negative
'not making this configurable best effort ',negative
'',negative
'delete the data the tables which have other locations ',negative
'for some reason even with mbeanexception available them runtime exceptions can still find their way through treat them the same mbeanexception ',negative
'reopen essentially just destroy get new session for session use ',negative
'were only considering the first element the list for the type ',negative
'use our helper from byte string ',negative
'some places fileinputformat this blocklocation used figure out sizesoffsets and completely blank one will not work ',negative
'dont fetch the footer ppd happens ',negative
'nonjavadoc see ',negative
'now only string text int long byte and boolean comparisons are treated special cases for other types reuse ',negative
'writing instrumentation agent for object size estimation ',negative
'nothing fall through and verify the data ',negative
'exclusives can never pass ',negative
'this denotes listing any directories where during replication want take care level operations first namely our case its only during creation the replica warehouse ',negative
'char binary conversion include trailing spaces ',negative
'add the column only the family has not already been added ',negative
'make new client since transport was closed for the last one ',negative
'create some acid tables unpartitioned table partitioned table ',negative
'test various scenarios ',negative
'ignoring and continuing watch for additional elements the dir ',negative
'leading spaces are significant ',negative
'recreate table external drop partition and should ',negative
'test hiveconf property variable substitution hivesitexml ',negative
'computes the temporal run time statistics the reducers for specific jobid ',negative
'stores the temporal statistics milliseconds for reducers specific job ',negative
'complete txn ',negative
'bail mux operator because currently the mux operator masks the emit keys the constituent reduce sinks ',negative
'initqueryfile ',negative
'tbl ',negative
'initialize using target data type names projection the column range typessize ',negative
'check there data the resultset ',negative
'duplicate function with possibly replaced children ',negative
'find context for current input file ',negative
'yay shortcircuited skip everything remaining the batch and return ',negative
'fetch the bucketing version from table scan operator ',negative
'track the input columninfos that are added the output columninfo has multiple mappings then add the column only once but carry the mappings forward ',negative
'there should original bucket files and plus new delta directory ',negative
'for dynamic partitioned hash join rule may not get run for all the reducesink parents because the parents the mapjoin operator get removed later this method keep track the parent mapjoin mapping ',negative
'return immediately batch empty ',negative
'insert event unpartitioned table ',negative
'are here means user requesting role doesnt belong ',negative
'nonjavadoc see ',negative
'move data file backup path ',negative
'the error codes are hivespecific and partitioned into the following ranges errors occurring during semantic analysis and compilation the query runtime errors where hive believes that retries are unlikely succeed runtime errors which hive thinks may transient and retrying may succeed errors where hive unable advise about retries addition the error code errormsg also has sqlstate field sqlstates are taken from section iso see most will just rollup the generic syntax error state but specific errors can override the that state see this page for how mysql uses sqlstate codes ',negative
'when start minihs will leader sequential start ',negative
'repeated nonnull permutations ',negative
'accumuloinputformat expects ',negative
'anything else indicates failure ',negative
'dont close the socket the stream already does that needed ',negative
'load all the default config values from hiveconf ',negative
'finder finds the first index its pattern given byte array its threadsafety depends its implementation ',negative
'mapping from constraint name list not null columns ',negative
'outputformat ',negative
'executionresult will errnook only all initfiles execute successfully ',negative
'find the base files original new style ',negative
'get the all path making select ',negative
'extrapolation not needed for this column extrapolation not possible for this column countpartitionname ',negative
'note not actually used for pool sessions verify some things like doas are not set ',negative
'create dummy select this select needed the walker split the mapjoin later ',negative
'partitions are not added write entries drop partitions hive ',negative
'ignore all other chars outside the enclosure ',negative
'lazyobject can only binary when its not string well return ',negative
'build the supported formats list ',negative
'mostly dup genincludedcolumns ',negative
'use bigint ',negative
'require all the directories present with some values ',negative
'the order processing follows wed reclaim kill all the sessions that can reclaim from various user actions and errors then apply the new plan any then give out all can give out restart get and reopen callers and rebalance the resource allocations all the affected pools for every session wed check all the concurrent things happening ',negative
'return all ',negative
'validate the value ',negative
'',negative
'todo fucntioncache ',negative
'then all output will null ',negative
'cant much outputtypeinfo not set ',negative
'let driver strip comments using sql parser ',negative
'working the assumption that the user here will the hive user doas false well make past this false check ',negative
'this method ',negative
'first column exists ',negative
'next user did specify perms ',negative
'transport mode ',negative
'returns list the distinct exprs without duplicates for given clause name ',negative
'remove the znodes weve already tried from this list ',negative
'utf continuation bytes have high bits equal ',negative
'nonjavadoc see ',negative
'pause dont get banned ',negative
'lets use timeout more than the socket lifetime simulate reconnect ',negative
'delete any stale log files left around from previous failed tests ',negative
'process singlecolumn long inner bigonly join vectorized row batch ',negative
'get the query string from the conf file the compileinternal method might hide sensitive information during query redaction ',negative
'check any the txns the list committed ',negative
'whether characters such and are interpreted valid boolean literals ',negative
'test the environment variables can set this test fails all the other tests will also fail because environment not getting setup ',negative
'copy default aux classes jsonhbase ',negative
'call the tests ',negative
'format column for create statement ',negative
'with trimtrue parsing can handle spaces ',negative
'orcrecordupdater inconsistent about when creates empty files and when does not this creates empty bucket hive ',negative
'not run cbo build error message ',negative
'remove the branches ',negative
'normally trailing fractional digits are removed but emulate the oldhivedecimal setscale and internalstorage need trailing zeroes here note this can cause decimal that has too many decimal digits because trailing zeroes for represent that case punt and convert with biginteger alternate code ',negative
'count only the keys ',negative
'just examine the middle and lower words ',negative
'get available map memory ',negative
'can fail with ',negative
'not create counter does not exist ',negative
'add list bucketing location mappings ',negative
'streaming ingest dir cannot have updatedelete events ',negative
'test that two different databases dont collide their locks ',negative
'counter name starts with vertex then just return max value across all vertex since trigger validation only interested violation that are greater than limit any vertex violation use case shufflebytes for any single vertex limit perform action ',negative
'get token info check renew date ',negative
'cannot combine any the children bail out ',negative
'keep the hash set result for its spill information ',negative
'case test with originals and base single split strategy with two splits compacted ',negative
'the result should have project top otherwise ',negative
'set watch the znode ',negative
'droppartition event partitioned table ',negative
'batchindex taskname getoperatorid candidate classname batch ',negative
'this point may have parsed integer ',negative
'for partition key type this will primitive typeinfo ',negative
'codes ',negative
'data needs deletion check trash may skipped ',negative
'the rule has been applied bail out ',negative
'using old table object hence reset the owner current user for new table ',negative
'still broken for partitions ',negative
'the createtable above does not update the location the tbl object when the client thrift client and the code below relies the location being present the tbl object get the table from the metastore ',negative
'num total and completed tasks ',negative
'check that the added partitions are expected ',negative
'alter table partition column column newtype only takes one column time ',negative
'hadoop conf var names ',negative
'add countkeycol replace distinct ',negative
'gbyrsgby top bottom ',negative
'supports random access ',negative
'get aggregate stats for all partitions table and for all but default ',negative
'configuration and things set from ',negative
'look comments dummystoreoperator for additional explanation ',negative
'bypass the clause and select the second disjunct ',negative
'force local cache have deltas ',negative
'the child operators cleanup input file has changed ',negative
'attach the resources the session cleanup ',negative
'special case for because java doesnt strip zeros correctly that number ',negative
'generate groupbyoperator ',negative
'nonnull only for writing serverside ',negative
'set the umask conf such that filesdirs get created with tabledir permissions following three assumptions are made actual filesdirs creation done recordwriter underlying output format assumed that they use default permissions while creation default permissions umask honored underlying filesystem ',negative
'registry for system functions ',negative
'tracks new additions via add while the loop processing existing ones ',negative
'required required optional ',negative
'apply prejoin order optimizations ',negative
'try forceevict the fragments the requisite size ',negative
'register this comparator ',negative
'files size for splits ',negative
'total entries valid fake ',negative
'such dropping table partition ',negative
'tracks number exprs from correlated predicates added select list ',negative
'for left semi joins may apply the filters now ',negative
'compute stats ',negative
'group deptno aggregate ',negative
'checkmetastore call will fill result with partitions that are present filesystem and missing metastore accessed through and partitions that are not present filesystem and metadata exists metastore accessed through getpartitionnotonfs ',negative
'metadata ppd ',negative
'datanucleus throws npe when try serialize table object retrieved from metastore workaround that reset following objects ',negative
'struct column such root ',negative
'catalogs are actually not supported ',negative
'merge statement ',negative
'each listener called above might set different parameter the event this write permission allowed the listener side avoid breaking compatibility change the api method calls ',negative
'fill array with pattern that will never match sync ',negative
'now that uri and times are set correctly set the original tables uri and times ',negative
'',negative
'dont exceed the range have one ',negative
'get the count txns from the given list are open state the returned count same the input number txns then means all are open state ',negative
'see hive ',negative
'then castnull ',negative
'negate ',negative
'',negative
'some other process probably writing the file just sleep ',negative
'union all removal kicks and get subdirs ekoifman tree ext hiveunionsubdir orcacidversion delta hiveunionsubdir orcacidversion delta hiveunionsubdir orcacidversion delta ',negative
'double scalarscalar ',negative
'initialize mapredwork ',negative
'groupsetposition includes all the positions ',negative
'main path created new file cache someone created one parallel someone created one parallel and then went stale ',negative
'test minor compaction ',negative
'reached end file ',negative
'unique rows ',negative
'does this hashcode belong this reducer ',negative
'task failed fetch its error code available also keep track the total number failures for that ',negative
'merge should convert hll dense ',negative
'verify the data the same ',negative
'merge only useful for extended merging not have inputs ',negative
'obtained from the hiveexception thrown ',negative
'this number carefully chosen minimize overhead and typically allows one vectorizedrowbatch fit cache ',negative
'merge remove and reduce project possible ',negative
'avoid concurrent modify the hashmap ',negative
'this gives easy way get compaction can only wait for those this utility started ',negative
'check all record writers implement statistics atleast one doesnt implement stats interface will fallback conventional way ',negative
'print primary key containing parents ',negative
'taskdisplay doesnt have tostring using json ',negative
'needed virtual columns are those used the query ',negative
'other alter operations are already supported hive ',negative
'static partition ',negative
'token available replace the placeholder ',negative
'get semi join here this mapside groupbyoperator needs removed ',negative
'corresponding writable classes for the following hadoop ',negative
'partitions ',negative
'rowfilterexpression applied the whole table dbnameobjectname for example rowfilterexpression can key and key and ',negative
'nonjavadoc see ',negative
'operand not ',negative
'have when matched and boolean expr then delete ',negative
'prec scale ',negative
'',negative
'update the tables cache ',negative
'rwxrwxrwx ',negative
'try again with left input also having nulls ',negative
'computes the skew for all the mapreduce irrespective success failure ',negative
'check theres least some degree stats available ',negative
'spill the current block tmp file ',negative
'sort order ',negative
'convert the ngram list format suitable for hive ',negative
'get our singlecolumn string hash multiset information for this specialized class ',negative
'the rule for headers header and buffer array element for some freelist can only modified the corresponding freelist lock held ',negative
'compare register values and store the max register value ',negative
'load the schema version stored metastore ',negative
'compile and execute can get called from different threads case ',negative
'role ',negative
'',negative
'queryid for the query current transaction ',negative
'generic http server error ',negative
'now check that stats were updated ',negative
'replace existing blanks with new blanks ',negative
'this value always seconds and includes suffix ',negative
'create the group operator ',negative
'create the corresponding client ',negative
'contains explicit field and partition ',negative
'validate the ddl valid operation the table ',negative
'update the childop selectop ',negative
'weve read bits roll them into the result ',negative
'stream data into streaming table with buckets then copy the data into another bucketed table ',negative
'keep connection open hang associated resources temp tables locks ',negative
'string including style literal characters cat kanji ',negative
'case jackson able deserializeit may use different implementation for the map which may not preserve order ',negative
'retrieve the additional httpheaders ',negative
'least one partition per expression not ifexists ',negative
'expr translation helper methods ',negative
'deserialize and test ',negative
'details pig added support for boolean fields which shipped this version pig depends antlr hcatalog depends heavily hive which this time uses antlr antlr and are incompatible pig and hive cannot depended the same project pig did not use antlr for its parser and can coexist with hive that pig version depended hcatalog this time ',negative
'rcfile ',negative
'repeat again ',negative
'descriptionvalue ',negative
'map this key back the confvars associated with ',negative
'minihs will become leader ',negative
'new array cannot contain the records wthe same key just advance dont check ',negative
'try out some metastore operations ',negative
'now oldordinal relative oldinput ',negative
'oldhivedecimal returns the hash code its internal bigdecimal our testhivedecimal verifies the matches new ',negative
'not fold namedstruct only struct ',negative
'columnscalar ',negative
'three different cases ',negative
'count query ',negative
'taskkill should also received during rejected submission will let that logic handle retries ',negative
'cannot ',negative
'create schema with serde then map ',negative
'construct ',negative
'would only apply after the callback for the current message ',negative
'set scratch directory ',negative
'partitioned table delete specific partitions ',negative
'call open read split mockmocktable ',negative
'create delta file bucket ',negative
'note the metadata cache may deallocate additional buffers but not this one ',negative
'get the path corresponding the dynamic partition columns ',negative
'validate the column names that are present are the same missing columns will implicitly defaulted null ',negative
'convert and then try instantiate replicationtask ',negative
'into same work ',negative
'any join participants from other has alias like posintname which size should caculated for each resolver ',negative
'the decendants contains limitoperatorreturn false ',negative
'create the folder and its parents not there ',negative
'tablefunction may able provide its output iterator case can then for mapside processing and for the last ptf reduceside chain can forward rows one one this will save the timespace populate and read output partition ',negative
'create the create table grants with new config ',negative
'parse the message field ',negative
'for ppd need column expression map that during the walk the processor knows how transform the internal col names following steps are dependant the fact that called ',negative
'create dir for mpart ',negative
'init objectinspectors ',negative
'executor create the ats events ',negative
'swallow exception ',negative
'simulate emitting records processnextrecord with small memory usage limit ',negative
'return partition metadata ',negative
'any repeated null key column match for whole batch ',negative
'updated the count will computed again ',negative
'remaining batches ',negative
'map from each basework its cloned jobconf ',negative
'here already saw current file and now found another file for the same bucket the current file not the last file the logical bucket ',negative
'tag want remain trash after deletion multiple files share the same content then ',negative
'get list events matching dbpattern tblpattern ',negative
'there should delta dir per partition location ',negative
'all chunks have been processed nothing more ',negative
'copied the entire buffer else theres more data process will handled next call ',negative
'delete gby gby sel from the pipeline ',negative
'release the lock ',negative
'tests with queries which can pushed down and executed with directsql ',negative
'this select for update query which takes lock the table entry already there nextwriteid ',negative
'inversion escaping happened are can reference directly ',negative
'singlecolumn string check for repeating ',negative
'use sampled partitioning ',negative
'record what type write this default nonacid old style ',negative
'setting file length longmaxvalue will let orc reader read file length from file system ',negative
'drop table already enforced hive only check for table level location even the table partitioned ',negative
'max txn does not change for transaction batch ',negative
'todo add tez session reconnect after tez ',negative
'alias ',negative
'set explicit session name control the download directory name ',negative
'check table exists ',negative
'set the values they are different the new stats ',negative
'set foreign key name null before sending listener ',negative
'for intermediate sum field ',negative
'create ',negative
'create the objectinspectors for the fields note currently columnarobject uses same objectinpector lazystruct ',negative
'were loading into instead into the warehouse then the olddbname and newdbname must the same ',negative
'collist will null for operators ',negative
'have filled digits and have more room our limit precision fast decimal ',negative
'set list work ',negative
'adjust groupingset position gbkeys for groupingset position needed note groupingid added map side only dont grpset ',negative
'calculate the variance sample result when count public vectorization code can use etc ',negative
'are using shared database then remove not known databases tables views ',negative
'empty list case ',negative
'test timestamp string ',negative
'move past parent field separator ',negative
'add the testnullappender the default route ',negative
'converted the expression search condition ',negative
'native not would decided annotation need evaluate that first ',negative
'not strictly necessary the whole queue check again ',negative
'dont create new object are already out memory ',negative
'now add all the default handlers ',negative
'skipping through comments ',negative
'extract order for each column from collation ',negative
'for each subquery also these different filesinks need linked each other ',negative
'skewed column names ',negative
'need look token info ',negative
'ifnotexists ',negative
'neither input has nulls verify that this propagates output ',negative
'check cases for arrif and mapkeyv for these should not generate paths like arrf mapv otherwise would have mismatch between type info and path ',negative
'make sure check format this right ',negative
'schema validation enforces that the key string ',negative
'ignored the attribute was not found which should never happen because the bean just told that has this attribute but this happens just dont output the attribute ',negative
'sort both the lists ',negative
'default column storage specification inherits from table level default ',negative
'allocate overflow batch columns hand ',negative
'arithmetic operations rely getting conf from sessionstate need initialize here ',negative
'spin until resolves extremely rare ',negative
'this may happen acid state absent from config ',negative
'must manually set with setmaxlength ',negative
'create nullappender ',negative
'only columns can sortedbucketed particular applying function column voids any assumptions ',negative
'user unsets queue name will fallback default session queue ',negative
'reset the pointer ',negative
'valid generated cookies found return null ',negative
'the set object containing the list ',negative
'process hints ',negative
'sintstring ',negative
'nonjavadoc this processor addresses the rsmj case that occurs tez the smallhash table side things the work that will part must connected the work via broadcast edge should not walk down the tree when encounter this pattern because the type work map work reduce work needs determined the basis the big table side because may mapwork need for shuffle reduce work ',negative
'bail here make the operation idempotent ',negative
'lets check that side files exist etc ',negative
'update ndv joined columns minvry vsy ',negative
'add alternative alias for the column this instance represents and its index the ',negative
'first batch always based batch size ',negative
'alphamm value for bits hash seems perform better for default hash bits ',negative
'wont updated ',negative
'compare ports ',negative
'lower this for big value testing ',negative
'estimated number reducers ',negative
'since did not remove reduce sink parents keep the original value expressions ',negative
'for join sel for lateral view sel for union does not count should copied both sides ',negative
'this should ideally happen separate thread ',negative
'test the need explicitly set the constant smaller value ',negative
'false return false ',negative
'currop now points the topmost tablescan operator ',negative
'automatic conversion double done here ',negative
'data with the separator bytes before creating put object ',negative
'since the file read pig need make sure the values are format that pig understands otherwise will turn the value null read ',negative
'get the bucket positions for the table ',negative
'add insert event twice with different event allow apply both events ',negative
'not match copy again from ',negative
'convert any nulls present map values empty strings this done the case backing dbs like oracle which persist empty strings nulls ',negative
'only one table lets check all partitions ',negative
'use construct ',negative
'setop ',negative
'that appear the small table portion the join output for outer joins ',negative
'there are deletes and reading original file must produce synthetic rowids order see any deletes apply ',negative
'noop default ',negative
'sign ',negative
'additional work for union operator see unionq ',negative
'this next section repeats the tests with maxlength parameter that less than the number current characters the string and thus affects the trim ',negative
'mostly this indicates that the initiator paying attention some table even though ',negative
'prior none singleton range ',negative
'batchsize when isrepeating ',negative
'combine equivalent work into single one sparkworks work graph ',negative
'not included equals ',negative
'create selectdesc ',negative
'try allocate using brute force approach from each arena ',negative
'user ',negative
'evaluate subsequent child expression over unselected ones only ',negative
'authorization setup using sessionstate should revisited eventually authorization and authentication are not session specific settings ',negative
'grouping reference ',negative
'verify the number buckets equals the number files this will not hold for dynamic partitions where not every reducer produced file for those partitions this case the table not bucketed hive requires files for ',negative
'will override and ignore isset fields ',negative
'all rows are filtered repeating null otherwise rows are filtered ',negative
'implementation verify oninit called when hmshandler initialized ',negative
'validate ',negative
'the isoriginal file the root the partition table thus from preacid conversion write and belongs primordial writeid ',negative
'this not cast process the function ',negative
'fallback picking the value from environment ',negative
'move the record from txncomponents into that the compactor ',negative
'errorcode ',negative
'data created export command ',negative
'this should also trigger meta listener notification via ',negative
'cache the client asks for else just return the value ',negative
'this assume round ',negative
'allocfraction ',negative
'todo this should making use confdir load configs setup for tez etc ',negative
'optional group containing repeated anonymous group bag containing ',negative
'use hiveinputformat that can control the number map tasks ',negative
'otherwise continue get the group type until ',negative
'gby ',negative
'mapping from constraint name list check constraints ',negative
'the metadata level there are restrictions column names ',negative
'there was authorization issue ',negative
'subscriber can get notification about drop table hcat listening topic named hcat and message selector string hcatevent hcatdroptable ',negative
'this read entity direct read entity and not indirect read that when ',negative
'show table information ',negative
'first try without qualifiers would resolve builtintemp functions ',negative
'hiveconf has not changed same object should returned ',negative
'this method called the rcfilereader constructor overwritten can access the opened file ',negative
'finds all contextual ngrams sequence words and passes the ngrams the ',negative
'slight hack communicate dynamicserde that the field ids are not being set but things are ordered ',negative
'and submit method when the job config created ',negative
'good ',negative
'create methods ',negative
'database ',negative
'lowvalue ',negative
'division inverse multiplication ',negative
'none the group expressions are constant nothing ',negative
'typespecific implementations ',negative
'output types they will the concatenation the input refs types and ',negative
'raise error user has specified partition column for stats ',negative
'create planner and copy context ',negative
'enable ssl support for hms ',negative
'connect all small dir map work the big dir map work ',negative
'add all the dependencies list ',negative
'find min ndv for joining columns ',negative
'add filter countc branches ',negative
'case exception assume unknown type bytes ',negative
'cant access metadata carry ',negative
'blindly add this integer list should sufficient for the test case ',negative
'batch statements ',negative
'also not loaded ',negative
'look everything front this lock see should block ',negative
'add original entries ',negative
'then merge the operators the works are going merge ',negative
'the can closed from under the task interrupted release cache buffers are assuming here that torelease will not present such cases ',negative
'null any the args are nulls ',negative
'least checkers always ',negative
'todo even out the batch sizes should replaced ',negative
'nothing ',negative
'the original partition files are deleted after the metadata change because the presence those files are used indicate whether the original partition directory contains archived unarchived files ',negative
'need set output name for reduce sink now that know the name the downstream work ',negative
'allocate the target related arrays ',negative
'make sure currently running txn considered aborted housekeeper ',negative
'still the same column ',negative
'each column has height ',negative
'add list bucketing predicate the table scan operator ',negative
'create fetch work ',negative
'the file name bucket number mapping maintained store the bucket number the execution context this needed for the following scenario insert overwrite table select from where and are sortedbucketed the same keys into the same number buckets although one mapper per file used possible that any mapper can pick any file depending the size the files the bucket number corresponding the input file stored name the output bucket file appropriately ',negative
'provide with new url access the datastore ',negative
'and terminate ',negative
'sampling filter then ignore the current filter ',negative
'create database with table ',negative
'set default location not specified and this physical table partition not view ',negative
'parse the struct using multichar delimiter ',negative
'drop destsequencefile ',negative
'timestamp format specified just use default lazy inspector ',negative
'files found for example empty tablepartition ',negative
'verify that the table was created successfully ',negative
'update the null counter ',negative
'all the partitions need updated single command can used ',negative
'connect using principal via beeline with inputstream ',negative
'comment column empty ',negative
'remove any parallel edge between semijoin and mapjoin ',negative
'unlike not append log when stopped ',negative
'will properly set string binary serialization via createlazyfield ',negative
'send some status periodically ',negative
'stick back into result variables ',negative
'partition spec was already validated caller when create tablespec object ',negative
'this source table not copy out ',negative
'set job name ',negative
'java class ',negative
'max rows can put into one block ',negative
'string types get converted double ',negative
'the skew keys match the join keys then add the list ',negative
'either tablehandle isnt partitioned null replexport after becomes null null this noopreplication export can skip looking ptns ',negative
'max length for the charvarchar then the return type reverts string ',negative
'the opparsecontext the parent selectoperator ',negative
'key ',negative
'indicator the chunked input does not know stop reading ',negative
'compute the size query when the nextvalue added the current query ',negative
'test bad args getxxx throws sqlexception ',negative
'check whether the materialized view invalidated ',negative
'compare the value each element array until match found ',negative
'set task ',negative
'expectation here not run into timeout ',negative
'any name does not matter ',negative
'information field made class allow readfield agnostic whether top level field within complex type being read ',negative
'unfortunately seem get instances varchar object inspectors without params when oldstyle udf has evaluate method with varchar arguments disallow varchar oldstyle udfs and only allow genericudfs defined with varchar arguments then might able enforce this properly ',negative
'note all maps and lists have absolutely sorted otherwise well produce different results for hashes based the jvm being used ',negative
'the interface for single byte array key hash multiset contains method ',negative
'guava versions have stats collection enabled default and not expose recordstats method check for newer versions the library and ensure that stats collection enabled default ',negative
'update previous comment there does seem one place that uses this and that authorize show databases hcat commandline which used webhcat and userlevel auth seems reasonable default this case the now deprecated hcatalog approached this another way and that was see the user had said above appropriate requested privileges for the hive root warehouse directory that seems the best mapping for user level privileges storage using that strategy here ',negative
'create map and fetch operators ',negative
'trim down the total number ngrams weve exceeded the maximum amount memory allowed note although kpf specifies the size the estimation buffer dont want keep performing nlogn trim operations each time the maximum hashmap size exceeded handle this actually maintain estimation buffer size kpf and trim down kpf whenever the hashmap size exceeds kpf this really has ',negative
'the bucket the valid range mark covered wish hive actually enforced bucketing all the time ',negative
'used half the mem for small joins now lets scale the rest ',negative
'',negative
'single string key hash map optimized for vector map join the key will deserialized and just the bytes will stored ',negative
'use the current directory not specified ',negative
'this consistently works locally but never ptest ',negative
'test that exclusive table locks coalesce one ',negative
'get tables make sure the locations are correct ',negative
'build via hive typeinfo and parquet schema ',negative
'get fieldschema stuff any ',negative
'set provider options ',negative
'test with and without specifying schema randomly ',negative
'convert the types needed for plugin api ',negative
'lowest middle high ',negative
'log summary every seconds ',negative
'picks topn pairs from input ',negative
'initialize one columns source deserializtion information ',negative
'wed need sleep once per round instead ',negative
'the directory this move task moving ',negative
'other operators functions ',negative
'join fil for the above complex operator tree selectivityjoin selectivityrs selectivityrs and ',negative
'read the keys before the delta flushed ',negative
'there are elements the map ',negative
'return the variable length from config ',negative
'pattern look for the hive query and whether matched ',negative
'construct using ',negative
'perform major compaction ',negative
'ensure hiveserver sitexml does not get override this ',negative
'need the base operatorjava implementation startendgroup the parent class has functionality those that map join cant use note the mapjoin can run the reducer only tez ',negative
'evaluate batch that temporary arrays the expression have residual values interfere later computation ',negative
'byte string maybe replace with localdirid and construct the fly longs reference overheads ',negative
'union ',negative
'authorize individually ',negative
'fail the update too get rid the duck for the next test ',negative
'hash aggregation not behaving properly disable ',negative
'stripe offset outside the split boundary then ignore the current stripe will handled some other mapper ',negative
'still files remains copied due failurechecksum mismatch after several attempts then throw error ',negative
'directory does not exist create ',negative
'find any constraints and drop them ',negative
'verify that have got correct set deletedeltas also ',negative
'first generate all the opinfos for the elements the from clause ',negative
'fakepart path partition added since the defined partition keys are valid ',negative
'operationhandle ',negative
'this replication spec then replacemode semantics might apply ',negative
'internal usage only length variable length data type cannot determined this length will used ',negative
'create the column descriptors ',negative
'return null only the file system schema not recognized ',negative
'createtable truncate insert ',negative
'have setup this again the underlying pmf keeps getting reinitialized with original reference closed ',negative
'now set other column nullable too ',negative
'tezjsonparser ',negative
'now the relevant tablescanoperators are known find there exists semijoin filter any them remove ',negative
'right now not handle the case that either them bucketed should relax this constraint with followup jira ',negative
'the hashcode and equals methods the key class ',negative
'location should allocate force capacity otherwise ',negative
'this the case when weve encountered decimal separator the fractional part will not change the number but will verify that the fractional part ',negative
'dryrun checks are meaningless for mutable table should always succeed unless there runtime ioexception ',negative
'whether theres spilled data processed used hold restored ',negative
'find dynamic partition columns relies consistent order via linkedhashmap ',negative
'sessionstate not available runtime and hivegetgetconf not safe call ',negative
'these are all values that put here just for testing ',negative
'types get initialized case they need setup any internal data structures ',negative
'keep track colnametoposmap for new select ',negative
'can get them from hdfs add group and permission ',negative
'split leaf join predicate expressions from left right ',negative
'project has any correlated reference make sure they are also provided the current correlate they will projected out the lhs ',negative
'special property starting with mapreduce that would also like effect changes ',negative
'that case will fail ',negative
'reprocess spilled data ',negative
'add spark job handle the hive history ',negative
'prefix operator ',negative
'ignore ',negative
'the user asked for stats collected some stats like number rows require scan the data however some other stats like number files not require complete scan ',negative
'input paths ',negative
'this ensures dont create skew with ducks and queries with simple rounding wed produce round whereas adding the last delta the next query wed round and thus give out intended note that fractions dont have all the same like this example ',negative
'expect fail since the time component not ',negative
'verify syntax error ',negative
'fastisint returns false ',negative
'old partition does not exist ',negative
'use kryo serialize hashmap ',negative
'caller responsible for setting children and input type information ',negative
'cause root cause and efirst useless exception its wrapped ',negative
'multikey specific lookup key ',negative
'storagehandler passed table params ',negative
'cast the input decimal casttype decimal try not lose precision for numeric types ',negative
'hive ',negative
'this only for the purpose authorization only the name matters ',negative
'specify orcsplit that starts beyond the offset the last stripe ',negative
'properties ',negative
'transformation left join for correlated predicates and inner join otherwise ',negative
'vertexs parent connections ',negative
'should generate ',negative
'out range probes ',negative
'set the key ',negative
'schema the mapreduce key object this homogeneous ',negative
'crucial here that dont reset the overflow batch will loose the small table ',negative
'convert tasks involving join into mapjoin hiveautoconvertjoin true the tasks involving join are converted consider the query select from join tkey tkey join tkey tkey there mapreduce task which performs way join the task would converted conditional task which would have children mapjoin considering the big table mapjoin considering the big table mapjoin considering the big table mapreduce join the original task note that the sizes all the inputs may not available compile time runtime determined which branch want pick from the above however set true and the sum any tables smaller than then mapjoin created instead the conditional task for the above the size less than the threshold then the task converted mapjoin task with the big table this case further optimization performed merging consecutive maponly jobs consider the query select from join tkey tkey join tkey tkey initially the plan would consist mapreduce jobs perform join for and followed another mapreduce job perform join the result with after the optimization both these tasks would converted maponly tasks these maponly jobs are then merged into single maponly job followup hive would possible merge maponly task with mapreduce task consider the query select tkey count from join tkey tkey group tkey initially the plan would consist mapreduce jobs perform join for and followed another mapreduce job perform groupby the result after the optimization the join task would converted maponly tasks after hive the maponly task would merged with the mapreduce task create single mapreduce task ',negative
'formatteron ',negative
'then add any cor var from the left input not need change ',negative
'missing one partition ',negative
'hash map which stores job credentials the key signature passed pig which ',negative
'create selection operator ',negative
'restore the reducer ',negative
'negative unix time ',negative
'this should removed eventually hive gives more detail explanation whats happening and hive why this done briefly for replication the graph huge and memory pressure going huge keep lot references around ',negative
'will cause underflow for result position must yield null ',negative
'can add verboselogging cause mockito log invocations ',negative
'fast ',negative
'scale updown ',negative
'note this may use additional inputs from the caller maximum query parallelism the cluster based physical constraints ',negative
'run compaction ',negative
'then need set the graph connection especially need connect this cloned parent work with all the grandparent works ',negative
'orc writer reuses streams need clean them here and extract data ',negative
'todo this makes many assumptions how generic args are done ',negative
'nuke trailing ',negative
'top level view should care about its access info ',negative
'fetch table ablias ',negative
'build tokselect tokselectexpr ast tree for count ',negative
'trigger and action expressions are not validated here since counters are not ',negative
'this might little bit too muchbut most cases this should true ',negative
'the first time ',negative
'use textfile default ',negative
'will rewrite include the filters transaction list can produce partial rewritings ',negative
'invalid character new database name ',negative
'confirm the batch sizes were the two calls create partitions ',negative
'single predicate condition ',negative
'the current table function has order info specified ',negative
'minihs will leader ',negative
'will try merge this clause into one the previously added ones ',negative
'have released the session trying reuse and going back into queue can start ',negative
'this bit should not for valid value references use for value marker ',negative
'logrefresherror always throws ',negative
'nothing but count the batch size ',negative
'just remember for later processing ',negative
'',negative
'check that the mapping schema right check that the columnfamily mapped mapkey where key extends lazyprimitive and thus has type categoryprimitive ',negative
'replace original stddevsampx with sqrt sumx sumx sumx countx case countx when then null else countx end ',negative
'selsel rule ',negative
'signature string associate with hcattableinfo essentially ',negative
'extended desc table then show the complete details the table ',negative
'that produces multiple queries for that need least ',negative
'insert overwrite existing partition ',negative
'retvalue transient store this separately ',negative
'sort only references field positions collations field the collations field the newrel now need refer the new output positions its input its output does not change the input ordering theres need call propagateexpr ',negative
'create the appender ',negative
'lock correctly see the comment the lock field the locking needs reworked ',negative
'the small table result portion the output for outer join ',negative
'prompt ',negative
'not supported all ',negative
'deserialize the hcatpartitionspec using the target hcatclient instance ',negative
'otherwise queue the session and make sure update this pool ',negative
'scan through any remaining digits ',negative
'the original location exists here then must the extracted files because the previous step moved the previous original location ',negative
'mapping bucket size all splits bucket bytes ',negative
'tez requires use rpc for the query plan ',negative
'compactor types ',negative
'when inserting into new partition the add partition event takes care insert event ',negative
'any events were queued the responder give them the record reader now ',negative
'the end update free list head ',negative
'configure export work ',negative
'the server sends very frequently ',negative
'read one field one field ',negative
'this should removed when authenticator and the username mess cleaned ',negative
'release the unreleased buffers see class comment about refcounts ',negative
'expiration queue synchronized and notified upon when adding elements without jitter wouldnt need this and could simple look the first element and sleep for the wait time however when many things are added once may happen that will see the one that expires later first and will sleep past the earlier expiration times when wake may kill many sessions once avoid this will add queue under lock and recheck time before wait dont have worry about removals worst wed wake vain example expirations are added this order due jitter the expiration threads sees that first will sleep for then wake and kill all sessions once because they all have expired removing any effect from jitter instead expiration thread rechecks the first queue item and waits the queue nothing added the queue the item examined still the earliest expired someone adds the queue while waiting will notify the thread and would wake and recheck the queue ',negative
'replication destination will not external override set ',negative
'create rows file ',negative
'there should only single split line ',negative
'filter files starts with note hadoop consider files starts with hidden file however need replicate files starts with find least use cases for har files index and masterindex required files ',negative
'nanos ',negative
'retrieve the for the table initialized hiveinputformat ',negative
'dont request any locks here the table has already been locked ',negative
'insert table select should not return resultset ',negative
'generate plan param param srcrel return todo grouping sets roll throws semanticexception ',negative
'all keywrappers must ',negative
'nothing hanlde future rud where may want add new state types ',negative
'required required required optional optional optional optional optional optional ',negative
'structs recursively compare the fields ',negative
'only altering the database property and owner currently supported ',negative
'prscgbymcrscgbyr map aggregation prscgbyrcomplete copies desc cgbym cgbyr and remove cgbym and crs ',negative
'not use byteswritable here avoid the bytecopy from ',negative
'firstname john greg firstname alan firstname and firstname owen ',negative
'list bucketed table cannot converted transactional ',negative
'create keyvalue structs and add the respective fields each one ',negative
'now apply resource plan any this expected pretty rare ',negative
'verify that udf not whitelist fails ',negative
'this isnt equal then bogus key values have been inserted error out ',negative
'for getpos ',negative
'schema size cannot matched then could because constant folding converting partition column expression constant expression the constant expression will then get pruned column pruner since will not reference any columns ',negative
'the situation here and other readers currently such setbuffers never called serde reader case and serde reader case the only one that uses vectors when the readers are created with vectors streams are actually not created all could have set vectors then set buffers wed trouble here may need implement that this scenario ever supported ',negative
'expected exception ',negative
'column type ',negative
'for any exception conversion integer produce null ',negative
'set the for future reference other listeners ',negative
'identd ',negative
'cant assume jdk implementing this explicitly return longcomparex longminvalue longminvalue ',negative
'that could especially valuable given that this almost always the same set ',negative
'tokquery above insert ',negative
'for each alias add object inspector for short the last element ',negative
'need send the state update again the state has changed since the last one ',negative
'might not exist ',negative
'ordinals for various reasons why error this type can thrown ',negative
'when columns the union operator empty ',negative
'check elements the innermost struct ',negative
'param did not parse url not url ',negative
'extension acidoutputformat that allows users add additional options todo since this only used for testing could not control the writer some other way simplify link ',negative
'have initialize the thread pool before start this one uses ',negative
'there are couple possibilities consider here see should recurse not path regex and may match multiple entries this likely load and should liststatus for all relevant matches and recursecheck each those simply passing the filestatus recursetrue makes sense for this path singular directoryfile and exists recursetrue check all its children applicable path singular entity that does not exist recursefalse check its parent this likely case needing create dir that does not exist yet ',negative
'output type boolean ',negative
'the record writer provides stats get from there instead the serde ',negative
'return join collations ',negative
'only one send can active the same time ',negative
'grouping sets expressions ',negative
'for all the other aggregations set the mode partial ',negative
'walk the other part ast ',negative
'wait for the first item arrive the queue and process ',negative
'abandon the reuse attempt ',negative
'assertassertequals ',negative
'the filesplit constructor hadoop and package private cant use this constructor used create the object and then call readfields just pass nulls this super constructor ',negative
'see for some examples the merge ast for example given merge into acidtbl using nonacidpart source acidtbla sourcea when matched then update set sourceb when not matched then insert valuessourcea sourceb get ast like this tokmerge toktabname acidtbl toktabref toktabname nonacidpart source toktableorcol acidtbl toktableorcol source tokmatched tokupdate toksetcolumnsclause toktableorcol toktableorcol source toknotmatched tokinsert tokvaluerow toktableorcol source toktableorcol source and need produce multiinsert like this execute from acidtbl right outer join nonacidpart acidtbla sourcea insert into table acidtbl select nonacidparta nonacidpartb where acidtbla null insert into table acidtbl select targetrowid nonacidparta nonacidpartb where sort acidtblrowid ',negative
'create table with unique name testdb ',negative
'the field corresponds column family hbase ',negative
'use case amt unbounded caught during translation ',negative
'year month ',negative
'newsortcollist had null value means that least one the input sort columns did not have representative found the output columns assume the data longer sorted ',negative
'return the new join replacement ',negative
'not bothering with removing the entry theres limited number hosts and good chance that the entry will make back when the used for long duration ',negative
'',negative
'push the node the stack ',negative
'',negative
'query parallelism might fubar ',negative
'create testnullappender drop events without queryid ',negative
'set dest name mapping new context chid tokfrom ',negative
'with aim consolidate the join algorithms either hash based joins mapjoinoperator sortmerge based joins this operator being introduced this operator executes sortmerge based algorithm replaces both the joinoperator and the smbmapjoinoperator for the tez side things works either the map phase reduce phase the basic algorithm follows the processop receives row from big table order process the operator does fetch for rows from the other tables once have set rows from the other tables till hit new key more rows are brought from the big table and join performed ',negative
'get the final state the spark job and parses its job info ',negative
'alias only can selected ',negative
'here need see remaining columns are dynamic partition columns ',negative
'convert text and write ',negative
'joinkey null process each row different group ',negative
'third value ',negative
'build column names ',negative
'finally write the hash code ',negative
'threads are not configured then they will executed current thread itself ',negative
'',negative
'through all bytes the byte ',negative
'create the lazyobject for storing the rows ',negative
'overflow ',negative
'for now check only table ',negative
'since for left join are only interested rows from left can get rid right side ',negative
'column stats will inaccurate ',negative
'move the hfiles files from the task output directory the location specified the user ',negative
'all nulls are now explicit ',negative
'mysql postgres sql server ',negative
'nonjavadoc see parsecontext ',negative
'might just the default which case can drop that one its empty ',negative
'set the arguments for ',negative
'set the base class ',negative
'process the bytes that can escaped the last one cant ',negative
'create dummy select select all columns ',negative
'intentionally using the deprecated method make sure returns correct results ',negative
'all fields have been parsed bytes have been parsed need set the startpositions fieldslength ensure can use the same formula calculate the length each field for missing fields their starting positions will all the same which will make their lengths and uncheckedgetfield will return these fields nulls ',negative
'run with recover and save the output file can checked ',negative
'this scales down because possibility rounding ',negative
'change the children the original join operator point the map ',negative
'wait for all threads ready release them the same time ',negative
'nonacidnonbucket copy copy base bucket deletedelta bucket deletedelta bucket delta bucket delta bucket delta bucket directories files ',negative
'this only needed new grouping set key being created ',negative
'check for the escaped colon remove before doing the expensive regex replace ',negative
'make one materialized view ',negative
'the scale the decimal ',negative
'the offset the destination array for the beginning this missing range ',negative
'test this supposed only allow events ',negative
'nonjavadoc see ',negative
'the for loop below ',negative
'note that for temp tables there need rename directories ',negative
'they dont seem work the ipc timeout needs set instead ',negative
'source hiveeventsproto ',negative
'have cases where are running query like countkey count such cases the readcolids either emptyfor count has just the key column either case nothing gets added the scan readallcolumns true are going add all columns else are just going add key filter run ',negative
'objecttypeptr ',negative
'generate data file for test ',negative
'input output row resolvers ',negative
'generate the vertexsubmit information for all events ',negative
'partitioned table expect partition values convert user specified map have lower case key names ',negative
'fallback integer parsing ',negative
'save can verify end stream need mark support wrap with bufferedinputstream ',negative
'dont expect corr vars withing join union for now only expect cor vars top level filter ',negative
'get the maximum the number tasks the stages the job and cancel the job goes beyond the limit ',negative
'bail having clause uses select expression aliases for aggregation expressions could what hive does but this non standard behavior making sure this doesnt cause issues when translating through calcite not worth ',negative
'dont care about these ',negative
'the user hasnt been reading row use the fast path ',negative
'assert only expected assert with this call ',negative
'update the list byte size ',negative
'close the writer finalize the metadata ',negative
'since may need split the task lets walk the graph bottomup ',negative
'grouping happens execution phase the input payload should not enable grouping here ',negative
'note would cause too many hash collisions ',negative
'dont throw exception the target location only contains the stagingdirs ',negative
'suffix first ',negative
'with split update new version the row new insert ',negative
'junk after trailing blank padding ',negative
'production thisfieldid requiredness fieldtype thisname fieldvalue commaorsemicolon ',negative
'enable zero copy record reader ',negative
'consider query like select countdistinct from group with rollup assume that hivemapaggr set true and false which case the group would execute single mapreduce job for the groupby the group keys should abgroupingsetfor rollup ',negative
'the form dim loj fact roj dim dim semij fact then return null ',negative
'the job configuration passed the configuration will cloned from the pig job configuration this necessary for overriding metastore configuration arguments like the metastore jdbc connection string and password the case embedded metastore which you get when hivemetastoreuris ',negative
'turn off metastoreside authorization ',negative
'masking and filtering should created here ',negative
'copy info that may required the new copy the settableudf calls below could replaced using this mechanism well ',negative
'the task generated the first pass ',negative
'this only required support the deprecated methods ',negative
'',negative
'hiveintest turn notification listener meta store ',negative
'remove entries from well dont start looking there again but only the highest write include this compaction job highestwriteid will null upgrade scenarios ',negative
'todo add interval and complex types ',negative
'join eligible for sortmerge join only eligible for bucketized map join dont need check for bucketized map join here are guaranteed that the join keys contain all the ',negative
'still null ',negative
'type with padded spaces ',negative
'from expressions ',negative
'the columns correspond ',negative
'case the query served hiveserver dont pad with spaces ',negative
'set the security key provider that the minidfs cluster initialized with encryption ',negative
'this ends being set test mvn for instance ',negative
'lets not use them anywhere unless absolutely necessary ',negative
'parser has done some verification the order tokens doesnt need verified here ',negative
'find the new delta file and make sure has the right contents ',negative
'overflow use slower alternate ',negative
'consistent with other apis like makeexpressiontree null returned indicate that the filter could not pushed down due parsing issue etc ',negative
'expr ',negative
'test both repeating ',negative
'can have long messages and should trimmable when published the jira via the jiraservice ',negative
'append not supported the cluster try use create ',negative
'bgenjjtree constlistcontents ',negative
'use our specialized hash table loader ',negative
'update config hive thread local well and init the metastore client ',negative
'for last stripe need get the last from the last row ',negative
'through the set partition columns and find their representatives the values ',negative
'table partitioned single key ',negative
'double columnscalar ',negative
'expected row count the join query well run ',negative
'finish the current innot clause and start new clause replace the commar ',negative
'sha matches ',negative
'first ',negative
'try with extra delta ',negative
'histogram bins use the percentile approximation ',negative
'reorder the wait queue note assume that noone will take our capacity based the fact that are doing this under the epic lock the epic lock removed wed need the steps under the queue lock could pass update state ',negative
'try load the composite factory one was provided ',negative
'check for the operators who will process rows coming this map operator ',negative
'call the real methods for these ',negative
'this can happen for data stream when all the values are null ',negative
'complex type support for now ',negative
'will make sure the ephemeral node created server will present even under connection session interruption will automatically handle retries ',negative
'this must called after all the explicit register calls ',negative
'try again with value that wont fit digits make ',negative
'nonjavadoc see javalangstring javautilmap ',negative
'find out the operator invoked mapside reduceside get the deserialized querydef reconstruct the transient variables querydef create input partition store rows coming from previous operator ',negative
'verify both get node heartbeat ',negative
'test hcatcontext ',negative
'verify individual arguments ',negative
'otherwise throw which means the file owner dead ',negative
'read the items from the input stream and confirm they match ',negative
'bucket map join only split goes memory ',negative
'semijoin dpp work considered child because work needs ',negative
'the dimension columns ',negative
'push each aggregate function down each side that contains all its arguments note that count because has arguments can ',negative
'this for updatedelete incl merge because introduce this column into the query part rewrite ',negative
'few situations where need the default table path without object ',negative
'repeating non null ',negative
'end else could not allocate ',negative
'the clause contains any expression ',negative
'make sure actually the table name ',negative
'this method will only return full resource plan when activating one give the caller the result atomically with the activation ',negative
'wait for the server bootup ',negative
'create hadoop configuration without inheriting default settings ',negative
'need set the global null that this reuse may pointless ',negative
'noinspection constantconditions ',negative
'this because function returns null the mapping for key removed the table mutated ',negative
'note this implements rather than extending because that class authspecific class and refactoring would kludge too many things that are potentially public api the base though what very simple thing protect against csrf attacks and that simply add another header running with xsrf filter enabled then will reject all requests that not contain this thus add this here the clientside this simple check prevents random other websites from redirecting browser that has login credentials from making request their behalf ',negative
'scopes execution code blocks with own local variables parameters and exception handlers ',negative
'bitvectorsize can use bits bytes represent ',negative
'selnocomputesel never seen this condition and also removing parent not safe current graph walker ',negative
'cant handle distinct ',negative
'otherwise the case analyze table compute statistics for columns ',negative
'create tablescan operator ',negative
'msb bits ',negative
'serialize path offset length using filesplit ',negative
'connect using token via beeline using script ',negative
'determine the keys for the current clause ',negative
'nonjavadoc see ',negative
'enablecstr ',negative
'getwritableobject will convert lazyprimitive actual primitive writable objects ',negative
'pack the output into the scratch longs ',negative
'run cleaner ',negative
'the dispatcher fires the processor corresponding the closest matching rule and passes ',negative
'need reset during reopen when needed ',negative
'might generate other types that are not recognized field reference nested field but since this just additional optimization bail out without introducing the select groupby below the right input the left semijoin ',negative
'make sure nanos are preserved ',negative
'off dont sit and hammer the metastore tight loop ',negative
'finish the scheduled compaction for ttp and manually compact ttp make them comparable again ',negative
'the current subexpression precalculated groupby etc ',negative
'parsing statement now done logic ',negative
'until change made use the admin option default false with authorization ',negative
'corresponds semanalyzer gengroupbyplanmr corresponds semanalyzer gengroupbyplanmr corresponds semanalyzer ',negative
'table doesnt exist allow creating new one only the database state older than the update ',negative
'new batch has been fetched its not empty have more elements process ',negative
'maximum number paritions aggregated per cache node ',negative
'bottom operator does not contain offsetfetch ',negative
'heartbeat only for active tasks errors etc will reported directly ',negative
'tests for droppartitionstring dbname string tblname liststring partvals options method ',negative
'and mutable read position for thread safety when sharing hash map ',negative
'result could cached this object were made immutable ',negative
'the has decimal second vint flag not set ',negative
'append the stripe buffer the new orc file ',negative
'for singular arg count should not include null countcase when and departmentid not null then else null end ',negative
'put the mapping from part prunerpred ',negative
'hashmap ',negative
'parent table the same database change the actual destination otherwise keep name ',negative
'used for buffering appends before flush them out ',negative
'isinunmanaged ',negative
'the index the child which the last row was forwarded key group ',negative
'this means the reader has already been closed ',negative
'added for rounding off ',negative
'ceil integer argument noop but less code handle this way ',negative
'boolean double atomiclong ',negative
'get rid tokselexpr ',negative
'use different batch for vectorized input file format readers they can their work overlapped with work the row collection that vectorrow deserialization does this allows the partitions mix modes for flush the previously batched rows file change ',negative
'',negative
'this call accessed from server side ',negative
'set these streams only the stripe different ',negative
'there cte walk the whole ast ',negative
'this optimizer for replacing tempfetching from temp with single direct fetching which means not needed any more when conversion completed ',negative
'selectivity key cardinality semijoin domain cardinality ',negative
'locations origloc becomes important ',negative
'cancel the previous expansion any ',negative
'dont read enough data for the first message decoded ',negative
'test params ',negative
'for functions that dont support window this provides the rows remaining added output functions that return window can throw this method shouldnt called for ranking fns return leadlag fns return the leadlag amt ',negative
'retrieve creation metadata needed ',negative
'there can only one instance per path ',negative
'build rel for select clause ',negative
'doas user different than logged user need check that that logged user authorized run doas ',negative
'task requested unknown host got host since host full and only host left random pool ',negative
'replacing inactive plan ',negative
'update cacheusage reference the pending entry ',negative
'remember for additional processing later ',negative
'are not pulling constants are pulling constants but this not constant ',negative
'only change txnmgr the setting has changed ',negative
'verify the fetched log from the beginning log file ',negative
'since interval types not currently supported table columns need create them expressions ',negative
'set the move task dependent the current task ',negative
'try read the given named url from the connection configuration file ',negative
'fieldcount get types for key value ',negative
'setup the completer for the database ',negative
'lets try populate those stats that dont require full scan ',negative
'the future can add checking for username groupname etc based for example ',negative
'this tests the case where older data has ambiguous structure but the correct interpretation can determined from the repeated name array ',negative
'likely found table scan operator ',negative
'mybyte ',negative
'exit the jvm ctrlc received and current statement executing ',negative
'finally for all the pools that have changes promote queued queries and rebalance ',negative
'interpret ctrlc request cancel the currently executing query ',negative
'following sel will for columns from udtf not adding sel here ',negative
'sort operator get deterministic results ',negative
'replace any that appear the prefix with regular ',negative
'inverse please see comments for multiply divide digit commad scale down fraction digits negative exponent number zeros after dot down shift ',negative
'timestamp not between ',negative
'number columns vector for each column number rows that qualify havent been filtered out array positions selected values ',negative
'test one random hiprecision decimal add ',negative
'first aggregation calculation for group ',negative
'write base file partition ',negative
'offset some offset middle the slice but see todo for firststart ',negative
'relocate all assigned slots from the old hash table ',negative
'count the number digits the mantissa including the decimal point and also locate the decimal point ',negative
'deleteupdate generates delete event for the original row ',negative
'was using hadoopinternal api get tasklogs disable until mapreduce fixed ',negative
'rewrite logic the original left input will joined with the new right input that has generated correlated variables propagated for any generated cor vars that are not used the join key pass them along joined later with the correlatorrels that produce them ',negative
'keylength hashcode slot ',negative
'run with each split strategy make sure there are differences ',negative
'default many existing explain classesmethods are nonvectorized vectorized methodsclasses have detail levels summary operator expression detail you the right you get more detail and the information for the previous levels included the default summary the path enumerations are used mark methodsclasses that lead vectorization specific ones can avoid displaying headers for things that have vectorization information below for example the tezwork class marked summarypath because leads both summary and operator methodsclasses and marked operatorpath because only display operator information for operator expression and detail typically live inside summary operator classes ',negative
'hash bits ref dont match ',negative
'specialized class for doing vectorized map join that inner join singlecolumn string using hash map ',negative
'gets lock ',negative
'drop part listener events fired for public listeners historically for drop table case limiting internal listeners for now avoid unexpected calls for public listeners ',negative
'shuffle read metrics ',negative
'setup resolve make connections ',negative
'timeout msec ',negative
'there are cases where increment counters insert overwrite all partitions are newly created insert into table which creates new partitions some new partitions ',negative
'ascii are http control characters that need escaped and are and respectively ',negative
'thrift cannot return null result ',negative
'set the bucketing version ',negative
'the multiple parts partition predicate are joined using and ',negative
'used and not readfield ',negative
'get script failed with code some number ',negative
'restart asynchronously dont block the caller ',negative
'easier case take quick path ',negative
'authorization checks passed ',negative
'default false ',negative
'lru could also implement lru doubly linked list cacheentry keeps its node ',negative
'replacing getaliastowork should use that information instead ',negative
'shortmaxvalue ',negative
'gets lock ',negative
'returns true the join conditions execute over the same keys ',negative
'get configuration parameters ',negative
'next arena being allocated ',negative
'put the mapping from table scan operator prunerpred ',negative
'add path components explicitly because simply concatenating two path string not safe for example foo yields foo which will parsed authority path ',negative
'the default ones are created case null tests override this ',negative
'not make remote call under any circumstances this supposed async ',negative
'should now only have the unexpected folders left ',negative
'flush the metastore cache this assures that dont pick objects from previous query running this same thread this has done after get our semantic analyzer this when the connection the metastore made but before analyze ',negative
'note this includes any outer join keys that need into the small table area ',negative
'firstname owen ',negative
'longstats ',negative
'end ',negative
'need set type name should always timestamplocaltz ',negative
'update the credential provider location the jobconf ',negative
'external table ',negative
'the simd optimized form ',negative
'keys ',negative
'map join work ',negative
'set back that column pruner the optimizer will not the ',negative
'support for statistics that seems popular answer ',negative
'give ',negative
'verify that have got correct set deltas ',negative
'test gtltltegte for numbers ',negative
'verify that flattening and unflattenting nonulls works ',negative
'current txn aborted this wont read any data from other txns safe unregister the minopentxnid from minhistorylevel for the aborted txns even the txns the list are partially aborted safe delete from minhistorylevel the remaining txns are either ',negative
'reverse the value ',negative
'amnodeinfo will only cleared when querycomplete received for this query when detect failure the side failure heartbeat single queuelookupcallable added here have make sure one instance stays the queue till the query completes ',negative
'offer accepted and gets evicted ',negative
'the root path not useful anymore ',negative
'',negative
'the partitions that are not required ',negative
'count returns true since count produces empty result set ',negative
'base env impl simply defers systemgetenv ',negative
'all bit numbers qualify multiply get nanoseconds ',negative
'carefully handle nulls ',negative
'populate map task ',negative
'the rhs references table sources and this qbjointree has lefttree hand the lefttree and let recursively handle there are cases passing condition down the leftside rightside dont contains references the lefttrees rightalias pass the lists down the leftside contains refs the lefttrees rightalias the rightside doesnt switch the leftcondal and leftconal lists and pass down the rightside contains refs the lefttrees rightalias the leftside doesnt switch the rightcondal and rightconal lists and pass down case both contain references the lefttrees rightalias cannot push the condition down either contain references both left right cannot push forward ',negative
'',negative
'there are two cases arraytype and arraystruct either case the element type the array represented tuple field schema the bags field schema the second case struct more naturally translates the tuple the first case arraytype simulate the tuple putting the single field tuple ',negative
'returning null from this can serve err condition ',negative
'crossing reduce sink file sink means the pruning isnt for this parent ',negative
'aggregation for semijoin ',negative
'druid returns bad request when not found ',negative
'generate groupbyoperator for partial aggregation ',negative
'merge only the register length matches ',negative
'not currently handled ',negative
'fall through miss locality localityrequested ',negative
'assume might bad will not try kill the query here just scrap the ',negative
'bloom known have higher fpp make tests pass give room for another ',negative
'calculate tags individually since the schema can evolve and can have different tags worst case both schemas are same and would end doing calculations twice get the same tag determine index value from fileschema determine index value from recordschema ',negative
'',negative
'its essentially mapjoindesc ',negative
'return the url based the aggregated connection properties ',negative
'fill down null flags undone ',negative
'walk through leaf predicates building ',negative
'address ',negative
'return ',negative
'scaledown ',negative
'ensure associated master key available ',negative
'undone unknown option ',negative
'release memory simple deallocation ',negative
'that can read the input format ',negative
'check privileges input and output objects ',negative
'the external client handling umbilical responses and the connection read the incoming data are not coupled calling close here make sure error one will cause the other closed well ',negative
'sum all nonnull long column values maintain isgroupresultnull ',negative
'data expected series data chunks the form chunk sizechunk byteschunk sizechunk bytes the final data chunk should length chunk which will indicate end input ',negative
'from the jar file the parent lib folder ',negative
'verify the data are intact even after applying applied event once again missing objects ',negative
'select all locks for this ext and see which ones are missing ',negative
'dont write rowseparatorbyte because that should handled file format ',negative
'set cache directory ',negative
'there are inputs rel does not need changed ',negative
'fail transactional property specified ',negative
'there are different cases for group depending mapreduce side hash aggregation grouping sets and column stats dont have column stats just assume hash aggregation disabled following are the possible cases and rule for cardinality estimation ',negative
'',negative
'doas set true for hiveserver will create proxy object for the session impl ',negative
'stop kafka ingestion first ',negative
'real grant time added metastore ',negative
'mix mix bit values reversibly this reversible any information abc before mix still abc after mix four pairs abc inputs are run through mix through mix reverse there are least bits the output that are sometimes the same for one pair and different for another pair this was tested for pairs that differed one bit two bits any combination top bits abc any combination bottom bits abc differ defined for and transformed the output delta gray code string commonly produced subtraction look like single bit difference the base values were pseudorandom all zero but one bit set all zero plus counter that starts zero some values for arotck arrangement that satisfy this are well didnt quite get bits diffing for differ defined with onebit base and twobit delta used choose the operations constants and arrangements the variables this does not achieve avalanche there are input bits abc that fail affect some output bits abc especially the most thoroughly mixed value but doesnt really even achieve avalanche this allows some parallelism readafterwrites are good doubling the number bits affected the goal mixing pulls the opposite direction the goal parallelism did what could rotates seem cost much shifts every machine could lay hands and rotates are much kinder the top and bottom bits used rotates define mixabc rotc rota rotb rotc rota rotb mixabc ',negative
'have found reduce sink ',negative
'replace trailing ',negative
'parse integer portion ',negative
'actually only most one loop ',negative
'previous call updated ',negative
'multiply self ',negative
'all key input columns are repeating generate key once lookup once since the key repeated must use entry regardless selectedinuse ',negative
'plan make sure need locks its possible theres nothing lock ',negative
'test the group needs partition level sort use the style shuffle shufflesort shouldnt used for this purpose see hive ',negative
'nothing else updated after the first update ',negative
'filter expression since will taken care partition pruner ',negative
'currently are not handling dynamic sized windows implied range based windows ',negative
'create union above all the branches ',negative
'dates are also valid timestamps dates are within ',negative
'should called after session registry checked ',negative
'test null values ',negative
'try deserialize using deserializeread our writable row objects created serde ',negative
'for canceling the query should bound session ',negative
'could get either query hint select expr ',negative
'get old table ',negative
'every row qualified newsizen then can ignore the sel vector streamline future operations selectedinuse will remain false ',negative
'initialized which may cause drop events ',negative
'check whether monotonic preserving cast otherwise cannot push ',negative
'sub fields ',negative
'load hiveserversitexml this hiveserver and file exists metastore can embedded within hiveserver such cases the conf params hiveserversitexml will override whats defined ',negative
'call setugi only unsecure mode ',negative
'the aggregation type minmax extrapolate from the leftright borders ',negative
'output columns ',negative
'make list before opening the rpc attack surface ',negative
'first adjust count expression any ',negative
'most the stuff can handle are generic function descriptions handle the special cases ',negative
'context for reading vectorized input file format ',negative
'exponent part ',negative
'must deterministic order map for consistent qtest output across java versions see hive ',negative
'could not transform anything bail out ',negative
'names have the name and names have the view name ',negative
'this check case the ciphertext actually makes sense some way ',negative
'txn ',negative
'for tokfunction the function name stored the first child unless its our special dictionary ',negative
'arithmetic with type timestamp and type intervalyearmonth longcolumnvector storing ',negative
'compute keys and values standardobjects ',negative
'the table was already marked transactionaltrue then the new value must match the old value any attempt alter the previous value will throw error exception will still thrown the previous value was null and attempt made set this behaviour can changed the future ',negative
'the reduce sink has not been introduced due bucketingsorting ignore ',negative
'once nodeid includes fragmentid this becomes lot more reliable ',negative
'need fill information about the key and value the reducer ',negative
'accumulo token information ',negative
'',negative
'create vectorized expr ',negative
'compute the total size per bucket ',negative
'changing this file make sure make corresponding changes ',negative
'repeating non null ',negative
'this constant boolean expression return the value ',negative
'this offer will rejected ',negative
'create reducesink operator followed another limit ',negative
'make special case for and ',negative
'create new map join operator ',negative
'end astnodeoriginjava ',negative
'only check one file exit the loop when have least one ',negative
'running state ',negative
'lookup cache entries table used the query for cache invalidation ',negative
'create test table with autopurge true ',negative
'print message reached least rows for join operand wont print message for the last join operand since the size will never goes joinemitinterval ',negative
'',negative
'europeuk ',negative
'show partition information ',negative
'group value ',negative
'nothing further test ',negative
'initialize the results ',negative
'build the expression based the partition predicate ',negative
'also try other patterns ',negative
'create list bucketing subdirectory only storedasdirectories ',negative
'fill the column vector with nulls ',negative
'events insert last repl repldumpidx ',negative
'check that start with default ',negative
'failure the sasl handler will throw exception indicating that the sasl negotiation failed ',negative
'the fact that stdev doesnt increase with increasing misscount captured outside ',negative
'allow null values for map ',negative
'statoptimization not applied for any reason the fetchtask should still not have been set ',negative
'this the option which will make delete writer ',negative
'need new hashmap since stats object reused across calls ',negative
'iterate backwards from the destination table the top the tree based the output column names get the new columns ',negative
'maxdecimal with round ',negative
'add data files the partitioned table ',negative
'',negative
'iterate through the list ',negative
'newproject plus any aggregates that the oldagg produces ',negative
'assumes deserializer not buffering itself position over uncompressed stream not sure what effect this has stats about job ',negative
'check elements the innermost union ',negative
'when not yet seen number lines fetch batch from remote hive server ',negative
'child process add here explicitly ',negative
'create the project before because need new project with extra column ',negative
'its hadoop ',negative
'columns statistics for complex datatypes are not supported yet ',negative
'triggerexpression ',negative
'store ugi transport the rpc setugi ',negative
'firstname lastname ',negative
'this yields empty because starting idx out bounds ',negative
'for debug tracing the name the map reduce task ',negative
'need differentiate between unmatched pattern and nonexistent database ',negative
'downstream code expects set valid value ',negative
'materialized ',negative
'cpartcols ppartcols have constant node expressions avoid the merge ',negative
'once called first will never able write again ',negative
'base case are eager child stateful ',negative
'match the given bytes with the like pattern ',negative
'for some reason even with mbeanexception available them runtime exceptions can still find their way through treat them the same mbeanexception ',negative
'settable ',negative
'instead retrying with this task will try pick different suitable task ',negative
'sharedwrite ',negative
'hive behavior where double decimal decimal gone ',negative
'check that the union has come out unscathed scathing unions allowed ',negative
'todo should checked server side embedded metastore throws metaexception remote metastore throws tprotocolexception ',negative
'map that says which mapjoin belongs which work item ',negative
'can later run the same logic that run ',negative
'same union order reveresed ',negative
'swe lock are examining exclusive ',negative
'check the character numbers with the length ',negative
'user wants file store based configuration ',negative
'partition columns will always the last ',negative
'drop partition will clean the partition entry from the compaction queue and hence cleaner have effect ',negative
'sequence file write ',negative
'list input expressions particular aggregate needs more will add expression the end and will create extra ',negative
'precompute groupby keys and store reducekeys ',negative
'throw error the user asked for sort merge bucketed mapjoin enforced and sort merge bucketed mapjoin cannot performed ',negative
'type mismatch when string col filtered string that looks like date ',negative
'show locks filter ',negative
'',negative
'attempt extended acl operations only its enabled but dont fail the operation regardless ',negative
'additional argument needed which the outputcolumn ',negative
'this delta directory can considered base ',negative
'setbyvalue ',negative
'figure out which stripes need read ',negative
'skip padded values ',negative
'add projection column projection vectorization context ',negative
'argumentcompletor always adds space after matched token this undesirable for function names because space after the opening parenthesis unnecessary and uncommon hive stack custom completor top our argumentcompletor ',negative
'column type char and constant type string then convert the constant char ',negative
'get result vector ',negative
'operator top ',negative
'implicitconvertable ',negative
'verify all scopes were recorded ',negative
'already existing table ',negative
'fetch name and type ',negative
'todo could generate vector row batches that vectorized execution may get triggered ',negative
'iterate for the second time get all the dependency ',negative
'are guaranteed that can get data here since size not zero ',negative
'post pass ',negative
'date highlow value stored long stats but allow users set highlow value using either date format yyyymmdd numeric format days since epoch ',negative
'lockid ',negative
'the hcatalog ',negative
'transformation sqcountcheckcount true filter generated top subquery which then joined left inner with outer query this transformation done add run time check using sqcountcheck throw error subquery producing zero row since with aggregate this will produce wrong results because further rewrite such queries into join ',negative
'the first child should the table are deleting from ',negative
'the session active but not found the pool internal error ',negative
'calcite ',negative
'complete path futures and schedule split generation ',negative
'want handle counters ',negative
'could muxdemux operators currently not supported ',negative
'need insert null before processing first row for the case preceding and preceding ',negative
'for analyze repl load walk through the dir structure available the path looking each and then each table and then setting the appropriate import job its place ',negative
'dont insist nullstructserde produce correct column names ',negative
'this sets the map operator contexts correctly ',negative
'length compression buffer compressed uncompressed length ',negative
'verify that udf black list fails even though its included whitelist ',negative
'cpu cost hashtable construction cost ',negative
'not fill tokenizer until user requests since filling could read data not meant for this instantiation ',negative
'case order only reducer used need another shuffle ',negative
'number output columns array pathnames each which corresponds column mapping from pathnames enum partname array returned column values object pool nonnull text avoid creating objects all the time array null column values input objectinspectors ',negative
'iso timestamps ',negative
'populated column map type deprecated slated for removal with ',negative
'assume default that would find everything ',negative
'net transfer cost ',negative
'remove parent reducesink operators ',negative
'execute query ',negative
'want try these whether they succeed fail ',negative
'ignore exception simply dont add this attribute back the resultant set ',negative
'initialize workload management ',negative
'entire batch filtered out ',negative
'generate umbilical token applies all splits ',negative
'array string ',negative
'smile mapper used read query results that are serialized binary instead json ',negative
'parts ',negative
'this the central piece for bucket map join and smb join has the following responsibilities group incoming splits based bucketing generate new serialized events for the grouped splits create routing table for the bucket map join and send serialized version payload for the edgemanager for smb join generate grouping according bucketing for the small table side ',negative
'case test with originals and deltas two split strategies with two splits for each ',negative
'there are nulls the inputcolvector ',negative
'null object not serialize ',negative
'original scale less than use original scale value otherwise preserve least fractional digits ',negative
'project the subset fields ',negative
'anything else preserve original call ',negative
'hive servers session input stream not used ',negative
'first stripes will satisfy the predicate and merged single split last stripe will ',negative
'skip walking the children ',negative
'register comes before the unregister for the previous dag ',negative
'insert query move itself noop ',negative
'return the mocked storagedescriptor ',negative
'fractional part ',negative
'find the base created for iow ',negative
'otherwise build timeline existing segments metadata storage ',negative
'ptf functions ',negative
'need notify any queries waiting the change from pending status ',negative
'try these best effort ',negative
'append regexes that user wanted add ',negative
'hostname ',negative
'specify the columns deserialize into range starting column number ',negative
'only need check conformance alter table enabled acid insertonly tables dont have conform acid requirement like orc bucketing ',negative
'calculate collection ',negative
'totalsize and the numfiles are set ',negative
'bucket columns are empty then numbuckets must set ',negative
'tests with queries which cannot executed with directsql because the contain like after falling back orm the number partitions cannot fetched the method they are fetched the method ',negative
'nonjavadoc see javasqlnclob ',negative
'retrieve hivetxntimeout milliseconds its defined seconds then divide give safety factor ',negative
'add maintenance thread that will attempt trigger cache clean continuously ',negative
'didnt try launch job either means there was work got here the result communication failure with the either way want wait ',negative
'merge with the downstream col list ',negative
'store varchar type stripped pads ',negative
'then actually the compaction ',negative
'sanity check should not receive keys with tags ',negative
'for unpartitioned table partition values are specified ',negative
'this parameter constant ',negative
'files size for splits ',negative
'need add nonmatch row with nulls for small table values ',negative
'try obtaining udaf evaluators determine the ret type ',negative
'signed comparisons ',negative
'not creating view need track view expansions ',negative
'deserialize just the columns buffered batch which has only the nonkey inputs and streamed column outputs ',negative
'operands converted timestamp result interval daytime ',negative
'bgenjjtree field ',negative
'clone the token wed need set the service the one are talking ',negative
'alternate connect string specification configuration ',negative
'concurrent increase and revocation before the message sent ',negative
'add field separator ',negative
'are using test specific database then just drop the database ',negative
'stats again ',negative
'for mapside invocation ptfs cannot utilize the currentkeys null check decide invoking startpartition streaming mode hence this extra flag ',negative
'reserve bytes for writevaluerecord fill there might junk there null them ',negative
'enforce bucketingsorting disabled numbuckets will not set ',negative
'copy over configs touched above method ',negative
'skewed info ',negative
'set the configuration such that proxyuser can act behalf all users belonging the real groups that the ',negative
'skip the for loop will skip its value ',negative
'',negative
'are closing file without writing any data ',negative
'replicate the drop events and check tables are getting dropped target well ',negative
'alan firstname ',negative
'big table alias ',negative
'test repeating nonnull noselection ',negative
'hive does not support resultsetmetadata preparedstatement and hive describe does not support queries have execute the query with limit ',negative
'nonjavadoc see ',negative
'retrieve from side ',negative
'since serde reuses memory will need make copy ',negative
'done with this branch ',negative
'assert that has got added table schema ',negative
'select operator project these two columns ',negative
'repeated string lstring ',negative
'the fixed size for the aggregation class already known get the variable portion the size every numrowsestimatesize rows ',negative
'all the txns the list have preallocated write ids for the given table then just return this for idempotent case ',negative
'find all configurations where the key contains any string from hiddenset ',negative
'provide this public method help explain vectorization show the evaluator classes ',negative
'privileges required dont need check this object privileges ',negative
'make sure the key mismatch causes error ',negative
'definitely not long ',negative
'add all columns from lateral view ',negative
'constructing the row object etc which will reused for all rows ',negative
'general filter cannot pushed below windowing calculation applying the filter before the aggregation function changes the results the windowing invocation when the filter the partition expression the over clause can pushed down for now dont support this ',negative
'add list bucketing pruner ',negative
'',negative
'there are nulls the inputcolvector ',negative
'walking through all active nodes they dont have potential capacity ',negative
'dont invoke from within scheduler lock ',negative
'instantiate bloomfiltercheck based input column type ',negative
'',negative
'since may calculation and produce scratch column need map the right column ',negative
'adds null element ',negative
'the conversion going through bit integer ',negative
'assume the char maximum length was enforced when the object was created ',negative
'job callable task for job submit operation overrides behavior execute submit job also overrides the behavior cleanup kill the job case job submission request timed out interrupted ',negative
'before the drop ',negative
'cannot scaled decimals that cannot represented instead use biginteger instead ',negative
'for each source read get shared lock ',negative
'deletedata ignoreunknowntable ifpurge ',negative
'its not worth adding the extra state ',negative
'set the escaping related properties ',negative
'exception happens during docopyonce then need call getfilestoretry with copy error true retry ',negative
'validate the second parameter which should array strings ',negative
'finally add project project out the last columns ',negative
'handle implementation instance and invoke appropriate inputformat method ',negative
'addpartitionsempty list normal operation ',negative
'dont want the table dir ',negative
'used for lazyfetchpartitions cases ',negative
'definitely not short ',negative
'first incremental dump ',negative
'bgenjjtree typelist ',negative
'rolename ',negative
'child jvm wont need change debug parameters when creating its own children ',negative
'all partitions with blurbhasnewcolumn were added after the table schema changed ',negative
'bad input ',negative
'remove the primitive types ',negative
'cast decimal input returntype ',negative
'keep cause the original exception ',negative
'reached the end field ',negative
'now start with low message size limit this should prevent any connections ',negative
'start the job ',negative
'user ',negative
'use tblproperties ',negative
'remember original string representation constant ',negative
'bail out ',negative
'reduce might end creating expression with null type conditionnull null reduced condition null with null type since this condition which will always boolean type cast boolean type ',negative
'open session and set the test data ',negative
'add default size for columns for which stats were not available ',negative
'add constant object overhead for struct ',negative
'decimal binary conversion ',negative
'txn not empty txn get better msg ',negative
'transient members initialized transientinit method ',negative
'does not sort memory footprint zero ',negative
'they are not equal could zip till here ',negative
'remove the container mapping ',negative
'dont need track anything for this task new notifications etc ',negative
'overflow this not expected ',negative
'with copy copy ',negative
'check whether the specified baseworks operator tree contains operator ',negative
'disable json file writing ',negative
'create proper tablecolumn desc for spilled tables ',negative
'check the partitions partspec the same defined table schema ',negative
'todo hive add additional information such executors container size etc ',negative
'normal close when there are inserts ',negative
'overrides values from the hivetezsite ',negative
'build hive table scan rel ',negative
'get old stats object present ',negative
'check they are operating the same table not move ',negative
'bootstrap done now incremental first test dblevel repl loads both dblevel and tablelevel repllastid must updated ',negative
'the partition did not change ',negative
'recreate the remote client not active any more ',negative
'optional int attemptnumber ',negative
'verify that there are missing privileges ',negative
'',negative
'rowid ',negative
'always localize files from conf duplicates are handled level todo could the same thing below and only localize missing ',negative
'get ssl socket ',negative
'reset buffers store filter push down columns ',negative
'close after expiry time and cache access should have tore down the client ',negative
'should set false when using this plugin avoid getting serialized event runtime ',negative
'the reader doesnt support offsets adjust offsets match future splits cached split was starting row start that row would skipped byte ',negative
'this needed for serde pagingspec uses jacksoninject for injecting selectqueryconfig ',negative
'now create three types delete deltas first has rowids divisible but not second has rowids divisible but not and the third has rowids divisible both and this should produce delete deltas that will thoroughly test the sortmerge logic when the delete events the delete delta files interleave the sort order ',negative
'complex tree with multiple parents ',negative
'constant for now will make configurable later ',negative
'remaining fields are cells addressed column name within row ',negative
'could not abort all txns this batch this may happen because parallel with this operation there was activity one the txns this batch this not likely but may happen client experiences long pause between heartbeats unusually longextreme pauses between heartbeat calls and other logic checklock lock etc ',negative
'return the multiset count for the lookup key ',negative
'then find the leftmost logical sibling select because thats what hive uses for aliases ',negative
'hiveconf getconf and setconf are this class because alterhandler extends configurable always use the configuration from hms handler making alterhandler not extend configurable not the scope the fix for hive ',negative
'significant effect when kpf very high ',negative
'test min max generates each stripe ',negative
'not change the location tested that the location will changed even the location not set null just remain the same ',negative
'top operator not pure limit bail out ',negative
'use zero copy record reader ',negative
'queryinfo will only exist more work came after this was scheduled ',negative
'move over the separator for next search ',negative
'mergepartial ',negative
'deserialize the bloom filter ',negative
'may drive this via configuration well ',negative
'close the connection ',negative
'nonjavadoc see ',negative
'set the signature for the view materialized view ',negative
'replsrctxnids ',negative
'set the thread name with the logging prefix ',negative
'derived ',negative
'for negative testing purpose ',negative
'helper object that efficiently copies the big table columns that are for the big table ',negative
'order which the results should ',negative
'update tags reduce sinks ',negative
'maximum number open transactions thats allowed ',negative
'optional int myint ',negative
'operationtype ',negative
'object hash ',negative
'make the new projrel provide null indicator ',negative
'marker track there starting double quote without ending double quote ',negative
'because that monotonically increasing give new unique row ids ',negative
'double ',negative
'not external table ',negative
'optional vectorized value expressions that need run each batch ',negative
'mapreduce job create temporary file ',negative
'make sure all the null entries this long column output vector have their data vector element set the correct value per the specification prevent later arithmetic errors zerodivide ',negative
'stageid ',negative
'aux values ',negative
'executionexception raised job execution gets exception return client with the exception ',negative
'died for any reason lets get new set hosts ',negative
'see the comment ',negative
'batch processop ',negative
'not repeating ',negative
'should noop for sparse ',negative
'nonjavadoc see javasqltimestamp javautilcalendar ',negative
'neginfinity start inclusive ',negative
'oracle requires special treatment usual ',negative
'scalarcolumn ',negative
'hide constructor ',negative
'when reading the hashtable mapjoinobjectvalue calculates alias filter and provide join ',negative
'need set this because with and client side split generation end not finding the map work this because thread local madness tez split generation multithreaded plan cache uses thread locals setting causes the split gen code use the conf instead the map work ',negative
'the list servers the can locate ',negative
'write the results into the file ',negative
'read from the given accumulo table ',negative
'server ',negative
'make multiple ',negative
'the keys used store info into the job configuration ',negative
'import tasks generated the event table updated for table level load then need update the repl state any object ',negative
'helper determine the size the container requested from yarn falls back mapreduces map size tez container size isnt set ',negative
'clear everything ',negative
'parse fraction portion ',negative
'this based vectorizer code minus the validation ',negative
'helpers ',negative
'some selected binary operators null etc one the expressions are ',negative
'return path may have aggrf aggrf gby and then select aggrf aggrf sel thus need use colexp find out which position corresponding which position ',negative
'look for reducesinkoperator ',negative
'see the comment inside ',negative
'the child single range ',negative
'rename needs change the data location and move the data the new location corresponding the new name the table not virtual view and the table not external table and the user didnt change the default location new location empty and ',negative
'basic adding and removing operations called only while holding lock ',negative
'first delete the materialized views ',negative
'null direction ',negative
'reuse existing perf logger ',negative
'check the new entry contains the existing ',negative
'merge the sidefile into the newly created hash table ',negative
'column value lengths for each the selected columns ',negative
'finally open the store ',negative
'inline map join operator ',negative
'smbjoin not supported ',negative
'the ast has children the second has partition spec ',negative
'ctas case the file output format and serde are defined the create table command rather than taking the default value ',negative
'ival ',negative
'this position parent constant reverse look colexprmap find the childcolname ',negative
'the utility this method not certain ',negative
'setting the hidden list ',negative
'assume that ams and run under the same user ',negative
'check for this pattern the pattern matching could simplified rules can applied during decorrelation correlaterelleft correlation condition true leftinputrel projecta rexnode aggregate groupby agg agg projectb references covar rightinputrel ',negative
'inserts are done ',negative
'transactions should committed ',negative
'code copied over from udfweekofyear implementation ',negative
'subscriber can get notification newly add partition particular table listening topic named dbnametablename and message selector string hcatevent hcataddpartition ',negative
'returns name hashfile made hashtablesink which read mapjoin ',negative
'wed increase the base limit and adjust dynamically based and processing perf delays ',negative
'skip the potential big table identified above ',negative
'load this metastore and file exists ',negative
'this tabledesc does not contain the partitioning columns ',negative
'the order which the two paths are added important the lateral view join operator depends having the select operator give the row first ',negative
'owid with the given value found searching now for rowid retrieve the actual compressedowid that matched check rowid outside the range all rowids present for this owid ',negative
'first use getparameters prune the stats ',negative
'the following actions are authorized through ',negative
'grouping should pruned which the last key columns see ',negative
'default just the hive catalog should cached ',negative
'partition spec string input file names big ',negative
'are using plain sasl connection with userpassword ',negative
'can get the table definition from tbl ',negative
'happening via statstask via user ',negative
'right trim slice byte array and return the new byte length ',negative
'overriden copy start index end index that needed through optimization for maskingfiltering ',negative
'nothing special just use the children methods ',negative
'error close the channel ',negative
'cant subtract null ',negative
'for subclasses ',negative
'the expression not null nonnullable column then can either remove the filter replace with empty ',negative
'submit job request maximum concurrent job submit requests are configured then submit request will executed thread from thread pool job submit request time out configured then request execution thread will interrupted thread times out also does best efforts identify job submitted and kill quietly ',negative
'input ',negative
'this query create pending cache entry but was never saved with real results cleanup this step required there may queries waiting this pending cache entry this entry will notify the waiters that this entry cannot used ',negative
'alright fine well use our defaults ',negative
'day hour minute second ',negative
'can this task merged with the child task this can happen big table being ',negative
'impossible throw any json exceptions ',negative
'copy nulls from the nonrepeating side ',negative
'doing singlevalue the entire input ',negative
'this code doesnt propagate ',negative
'check any the partitions already exists desttable ',negative
'test that existing sharedread table with new sharedwrite coalesces ',negative
'nonjavadoc see ',negative
'comment ',negative
'combine credentials and credentials from job takes precedence for freshness ',negative
'fetch same schema ',negative
'set service and client ',negative
'semijoin dpp work considered descendant because work needs ',negative
'inner bigtable only join specific ',negative
'check the stats ',negative
'objectinspectoroi return ',negative
'finally add the partitioning columns ',negative
'needresult ',negative
'nonjavadoc see ',negative
'physical dir gets created ',negative
'create two bucketed tables ',negative
'used the join input too large fit memory ',negative
'this get should fail because its variance way past maxvariance ',negative
'files size for splits ',negative
'bgenjjtree typedef ',negative
'assert bucket listing expected ',negative
'set the transactionlocking the derby metastore ',negative
'finally write out the pieces sign scale digits ',negative
'only increase the targets here ',negative
'calling from code that does not use filters asis ',negative
'make some noisy avoid disk caches data ',negative
'required required optional optional optional ',negative
'multiple clients may initialize the hook the same time ',negative
'and insert overwrite insert into table ',negative
'distinct ',negative
'send actual heartbeat only the task count ',negative
'tests whether there another list element another map keyvalue pair ',negative
'ensure that throw out any exceptions above highwatermark make link iswriteidvalidlong faster ',negative
'white flag with horizontal middle black stripe uff bytes ',negative
'with data ',negative
'mark txn aborted mark txn aborted ',negative
'todo mssplit certain dont need this already adds this resource static ',negative
'logger the string appender ',negative
'its not clear how filtering for stringcol should work which side coerced let the expression evaluation sort this one out not metastore ',negative
'try dropping table user should fail ',negative
'seems reasonable upper limit for this ',negative
'need make sure that the key type and the value types are settable ',negative
'create the ptfdesc from the qspec attached this ',negative
'arent building split start new one ',negative
'the following datetimeinterval arithmetic operations can done using the vectorized values ',negative
'unpartitioned table ',negative
'write committed txn should valid ',negative
'try invalid state transition the handle this ensures that the actual state change were interested actually happened since internally the handle serializes state changes ',negative
'are overwriting segments with new versions ',negative
'undone multiple types ',negative
'create some data ',negative
'insert having plan here ',negative
'when qbjointree merged into this one its leftpos filters can refer any the srces this qbjointree particular filterforpushing refers multiple srces this qbjointree collect them into postjoinfilters then add filter operator after the join operator for this qbjointree ',negative
'blank byte latin letter small capital bytes ',negative
'tests for int partitions method ',negative
'not special cased later subquery remove rule ',negative
'cluster info changes qam should called with the same fractions ',negative
'preempt specific host ',negative
'override this for concrete initialization ',negative
'not used ',negative
'this map which vectorized row batch columns are the big table key columns since may have key expressions that produce new scratch columns need mapping ',negative
'',negative
'final row computation will consider join type ',negative
'tablename ',negative
'here there only split since only have data for bucket ',negative
'the following data might changed ',negative
'index disabled ',negative
'drop partition will clean the partition entry from the compaction queue and hence worker have effect ',negative
'nonjavadoc see javaioreader ',negative
'',negative
'generate lineage info for create view statements lineagelogger hook configured ',negative
'always have find least one location otw the test useless ',negative
'the future support arbitrary characters identifiers then well need escape any backticks identifier doubling them ',negative
'add materializations planner ',negative
'there could multiple keyvaluepairs separated comma ',negative
'singlecolumn string specific lookup key ',negative
'decrease refcount ',negative
'for now bail out decimal constants with larger scale than column scale ',negative
'the thread should stop after this ',negative
'create read intermediate data ',negative
'need the mapping and type information ',negative
'pick the maximum reducers across all parents the reduce tasks ',negative
'use this constructor when only ascending sort order used ',negative
'default prefix deltaprefix ',negative
'testonly counter ',negative
'dynamic partition keys should added field schemas ',negative
'invariant that avros tag ordering must match hives ',negative
'the buffer has lived the heap all along restore heap property ',negative
'configure preexechooks with disallow transform queries ',negative
'the main thread throws exception for some reason propagate the exception the client and initiate safe shutdown ',negative
'columns starts with tablefieldssize ',negative
'any child null set unknown true ',negative
'get the children expr strings ',negative
'precisionscale enforcement methods ',negative
'cannot handle null scalar parameter ',negative
'only support decimal columns when the have the same scale ',negative
'add the remaining fields ',negative
'unknown unknown unknown ',negative
'nonjavadoc serializes this value into the format used link javamathbiginteger this used for fast assignment decimal hivedecimalwritable internal storage see openjdk for reference implementation param scratch param signum return ',negative
'struct ',negative
'theres failure from here when the metadata updated there will data the partition error while trying read the partition the archive files have been moved the original partition directory but rerunning the archive command will allow recovery ',negative
'since this dynamic partitioned hash join the work for this join should reducework ',negative
'ignoreprotection ',negative
'test getter for configuration object ',negative
'note that not need lock for this entity this used operations like alter table partition where its actually the partition that needs locked even though the table ',negative
'replace reducesinkop with hashtablesinkop for the rsops which are parents mjop ',negative
'this the object hash class variation ',negative
'augment ',negative
'boolean values are stores and convert and compare ',negative
'this column not included ',negative
'add initial column vectorization context when ',negative
'getmacroname null always treat different from others ',negative
'update the seed ',negative
'transactional found but the value not expected range ',negative
'use construct ',negative
'have data until the end current block had until the beginning ',negative
'record disk more data write buffer ',negative
'path has writing permissions ',negative
'singleton ',negative
'format list columns for create statement ',negative
'set the skipped field null ',negative
'check whether hiveconf initialize logj correctly ',negative
'partition name value ',negative
'events ',negative
'task has been queued for execution the driver ',negative
'thrown for example there such user the system ',negative
'this method tries merge the join with its left child the left ',negative
'nonqualified types should simply return the typeinfo associated with that type ',negative
'output will also repeating ',negative
'confirm the batch sizes were the two calls create partitions ',negative
'avoid npe below for some reason argument has multiline command ',negative
'check partition exists exists skip the overwrite ',negative
'check the query results were cacheable and created pending cache entry successfully saved the results the usage would have changed queryusingcache ',negative
'can prevent updates from being sent out the new node ',negative
'finally write out the pieces sign power digits ',negative
'different paths ',negative
'make sure that the port unused ',negative
'now that have the big table index get real numreducers value based big table ',negative
'then uks ',negative
'with the releases probably running the other closing thread ',negative
'not check for acid does not create new parts and this expensive hell todo add api get table name list for archived parts with single call nobody uses this could skip the whole thing ',negative
'number dynamic partition columns involved partitions ',negative
'put the beginning relies the knowledge internal implementation pave ',negative
'this would usually block boundary this would usually block boundary ',negative
'nonjavadoc see javaioinputstream ',negative
'this must hadoop version earlier resorting the earlier method getting the properties which uses saslprops field ',negative
'source exists rename otherwise create empty directory ',negative
'more original files read ',negative
'reverify directory layout and query result using the same logic above ',negative
'matches filprojts ',negative
'there are columns projection only join just assume weight ',negative
'use construct ',negative
'read with partition filter ',negative
'fraction below implicitly ',negative
'filter the name list removing elements one one can slow arraylist ',negative
'set check before checking ',negative
'assert ',negative
'not cleaning the interrupt status ',negative
'rcfile specific parameter ',negative
'this list type ',negative
'handle overloaded methods first ',negative
'hide constructor for make benefit glorious singleton ',negative
'nothing trim ascii ',negative
'check configuration for any userprovided authorization definition ',negative
'loginfowriting value valueoffset length valuelength unlikely case length key and value for the very first entry want tell this apart from empty value well just advance one byte this byte will lost ',negative
'may need strip away the stop marker when thrift mode ',negative
'public static getinstanceint fields return ',negative
'are currently searching the data for place begin not return data yet ',negative
'map cvalue map rowvalues assertequals cvaluesize ',negative
'',negative
'start from clean slate metastore ',negative
'reducesink only available during compile ',negative
'get partition this partition should not have the newly added column since cascade option ',negative
'remove from current root task and add conditional task root tasks ',negative
'derby script format run file ',negative
'without data ',negative
'get the avglen ',negative
'nonjavadoc see ',negative
'try deserialize ',negative
'have least nonblank skip trailing blank characters ',negative
'this check for controlling the correctness the current state ',negative
'could changed local execution optimization ',negative
'set other table properties ',negative
'small optimization delete delta cant raw format ',negative
'http mode ',negative
'retrieve the user name the final validation step ',negative
'not see the mesg follows type format which typically the case ',negative
'and have their type infos ',negative
'generate the map join operator ',negative
'verify escaped partition names dont return partitions ',negative
'udaf assumed deterministic ',negative
'verify added entry for each output ',negative
'convert the bucket mapjoin operator sortmerge map join operator ',negative
'not supported for tables sampler breaks separate dirs into splits resulting mismatch when the downstream task looks them again assuming they are table roots could somehow unset the flag for the main job when the sampler succeeds since the sampler will limit the input the the correct directories but dont care about ',negative
'the context table name can null repl load done full but need table name for alloc write and that received from source ',negative
'the result the last character which occupies bytes ',negative
'update based the final value the counters ',negative
'simply dispatch the call the right method for the actual sub type basework ',negative
'create mapping from the group columns the table columns ',negative
'generate the statement analyze table tablename compute statistics for columns nonpartitioned table case will generate tsselgbyrsgbyselfs operator staticpartitioned table case will generate operator dynamicpartitioned table case will generate operator however not need specify the partitionspec because the data going inserted that specific partition can compose the staticdynamic partition using select operator ',negative
'timestamp scalarcolumn ',negative
'just had leading zeroes and possibly dot and trailing blanks value ',negative
'get the next inlist value element needed ',negative
'formal conversion ',negative
'create new union operator ',negative
'undone also remember virtualcolumncount ',negative
'the input timestamps are stored long values ',negative
'partitioned insert ',negative
'write has changed not valid anymore need recompile ',negative
'not remove the parameter yet because have separate initialization routine that will use down below ',negative
'extract join key expressions from hivesortexchange ',negative
'requested ',negative
'',negative
'check that dont find unexpected columns ',negative
'jdbc url each list keyvalkeyval and sessvarlist sessconfmap hiveconflist hiveconfmap hivevarlist hivevarmap ',negative
'check the partition exists shouldnt ',negative
'catalog ',negative
'this simulates the completion delete from tab txn ',negative
'copied the entire buffer ',negative
'will using this for each while also sending rgs processing avoid buffers being unlocked run refcount one ahead each ',negative
'check that all the outputs have been processed not insert them into queue before the current vertex and try again its possible structure like this where may added the queue before ',negative
'otherwise can reuse the session either the kill has failed but the user managed return early fact can fail because the query has completed earlier the user ',negative
'files all ',negative
'catname ',negative
'build map column name col index original schema assumption hive table can not contain duplicate column names ',negative
'the map were building ',negative
'determine maximum all nonnull decimal column values maintain isgroupresultnull ',negative
'ignore errors ssl tests where the connection misconfigured ',negative
'add filters that apply more than one input ',negative
'new metadata always have two parameters ',negative
'the bucket set component this data structure proves too large there the option moving trove hppc effort reduce size ',negative
'copy the padding ',negative
'the caller could recheck the location but would probably find locked ',negative
'for selselcompute case move column exprsnames child parent ',negative
'found but there exists subpartition ',negative
'timer will null arent using the metrics ',negative
'found child mapjoin operator its size should already reflect any mapjoins connected stop processing ',negative
'todo implement remove all watches for the specified pathstring and its subtree ',negative
'nonjavadoc see long ',negative
'input data ',negative
'default serialization format ',negative
'which case user the latter match ',negative
'fill host with tasks leave host empty try running task host should preempt ',negative
'aggregates groupby ',negative
'unpartitioned table writing the scratch dir directly good enough ',negative
'now try pick another task update potentially the same task ',negative
'convert exprnode rexnode ',negative
'since conflicting txn rolled back commit succeeds ',negative
'the directory needs changed send the new directory ',negative
'sizes ',negative
'the table not partitioned return empty list ',negative
'turn mocked authorization ',negative
'loginfoargs har arg ',negative
'return for xxzz and xxyyzz ',negative
'get calcite return type for agg ',negative
'handle next round ',negative
'the sorting columns the child are more specific than those the parent assign sorting columns the child the parent ',negative
'this relates level event tracked via ',negative
'all done parsing lets run stuff ',negative
'build the versions list ',negative
'file successfully copied just skip this file from retry ',negative
'all must selected otherwise size would zero repeating property will not change ',negative
'will make sure the tasks place the wait queue held until gets scheduled ',negative
'just spot check because already checked the logic for long the code from the same template file ',negative
'copy remainder digits which start the top remainder ',negative
'want resolve the leftmost name the parent querys hence left walk down the ast until reach the bottom most dot ',negative
'begin write ',negative
'repeated int lint ',negative
'since there txn open are heartbeating the txn not individual locks ',negative
'for persistent function the function dropped all functions registered sessions are needed reloaded ',negative
'nonjavadoc see javaioreader long ',negative
'need shut down background thread gracefully driverclose will inform background thread cancel request sent ',negative
'note calcite considers tbls equal their names are the same hence need provide calcite the fully qualified table name dbnametblname and not the user provided aliases however hive name can not appear select list case join where table names differ only name hive would require user ',negative
'retest the locks ',negative
'configure http client for cookie based authentication ',negative
'build the resultset from response ',negative
'worker identity ',negative
'input rel ',negative
'this case need get the working directory and this requires filesystem handle revert original method ',negative
'verify table not fuond error ',negative
'the given task sparktask then search its work dag for ',negative
'try underlying client ',negative
'obtain col stats for non partition cols ',negative
'either got the tablename from the import statement first priority from the export dump ',negative
'add the regex match anything else afterwards the partial spec ',negative
'spark configurations are updated close the existing session case async queries confoverlay not empty sessionconf and conf are different objects ',negative
'maintain join keys child join schema update join key map with keys ',negative
'find our bearings the stream ',negative
'nonjavadoc this processor addresses the rsmj case that occurs spark the smallhash table side things the work that will part must connected the work via broadcast edge should not walk down the tree when encounter this pattern because the type work map work reduce work needs determined the basis the big table side because may mapwork need for shuffle reduce work ',negative
'todo handle replication changes tablestats ',negative
'check llapaware split orcsplit make sure its compatible ',negative
'nway all later small tables ',negative
'this doesnt create key index presumably because writeroptions are not set options ',negative
'get column ',negative
'are the last the concurrent operations finish commit ',negative
'consolidation since all leaves are required ',negative
'should set foo bar should set blah should ignored ',negative
'subclass must provide the link instance ',negative
'add partition keys table schema note this assumes that not ever have ptn keys columns inside the table schema well ',negative
'unixtimestamp polymorphic ignore class annotations ',negative
'',negative
'temporary until native vector map join with hybrid passes tests false ',negative
'get the databases ',negative
'write three files partition ',negative
'columns null then need create the leaf ',negative
'functions like nvl coalesce case can change null introduced nonpart column removal into nonnull and cause overaggressive prunning missing data incorrect result ',negative
'input pruning enough add the filter for the optimizer use later ',negative
'for smb join replaced with number part taskid making output file name big alias not partitioned table its bucket number ',negative
'hadoopproxyuser set env property ',negative
'worth waiting for the timeout ',negative
'versionversion ',negative
'submit accept dag session closed this will include reopening session time ',negative
'realativeoffsetword last value this was the first value written ',negative
'should final but writable ',negative
'cololdname colnewname columntype comment colcomment firstafter columnname cascaderestrict ',negative
'',negative
'count the tasks intermediate state waiting ',negative
'case column stats hash aggregation grouping sets ',negative
'rewriteenabled ',negative
'check that the data still exist ',negative
'meh ',negative
'lefttree null ',negative
'partition and bucket columns are sorted ascending order default ',negative
'generate reducesinkoperator ',negative
'submit spark job through local spark context while spark master local mode otherwise submit spark job through remote spark context ',negative
'connect this tablescanoperator child ',negative
'dataoutputstream byteswritable ',negative
'test with doasfalse ',negative
'firstname john sue ',negative
'weight ',negative
'this allows doas proxy user passed along across process boundary where delegation tokens are not supported for example ddl stmt via webhcat with doas parameter forks hcat which needs start session that proxies the end user ',negative
'otherwise just leave tez decide how much memory allocate ',negative
'clone readeroptions for deleteevents ',negative
'for submit operation tasks are not cancelled verify that new job request should fail with ',negative
'partitions match the specified partition filter ',negative
'common inner join result processing ',negative
'nothing here ',negative
'versioning addingdeleting fields ',negative
'remove nested dpps ',negative
'the dispatcher fires the processor corresponding the closest matching rule and passes the context along ',negative
'object inspectors corresponding the struct returned terminatepartial and the fields within the struct maxlength sumlength count countnulls ndv ',negative
'compute groupby columns from groupby keys ',negative
'helper function allow setcollection operations with exprnodedesc ',negative
'make sure that file does not exist ',negative
'the retainall method does set intersection ',negative
'test for user neo ',negative
'add constraints necessary ',negative
'initialize footer buffer ',negative
'output ',negative
'this method check the new column list includes all the old columns with same name and type the column comment does not count ',negative
'case are partitioning the segments based time and max row per segment maxpartitionsize ',negative
'pull out the first table from the show extended json ',negative
'for all the other column groups generate new values down ',negative
'setup tablescan desc ',negative
'but assume its extremely rare for individual partitions ',negative
'while and should done when start ',negative
'get the single tablescanoperator vectorization only supports one input tree ',negative
'passing creds prevents duplicate tokens from being added ',negative
'make sure dont compact dont need compact but ',negative
'fulltablename ',negative
'due hive define our own constant ',negative
'sessionopen will unset the queue name from conf but mockito intercepts the open call ',negative
'longdoubledecimal ',negative
'row null means there are more rows closeop another case can that the buffer full ',negative
'give the outthread chance finish before marking the operator done ',negative
'tests whether credential provider updated when set and when hiveconf sets jobconf should contain the mapred env variable equal and the property should equal value ',negative
'feed current full batch operator tree ',negative
'any tablepartition updated then update repl state table object ',negative
'fetch remaining logs ',negative
'after the join only selects and filters are allowed ',negative
'this might deadlock lets retry ',negative
'delete jars added using query ',negative
'setup client side split generation ',negative
'type for column and constant are different currently not support pushing them ',negative
'now add the key wrapper arrays ',negative
'have nonnull value hand ',negative
'lock should freed now ',negative
'run compaction worker compaction but not compact table but only transit the compaction request ',negative
'for list the value and key lengths record were overwritten with the relative offset new list record ',negative
'not selectedinuse ',negative
'fromeventid ',negative
'weve switched jodajava calendar which has more limited time range ',negative
'this assumes the distribution variable size keysaggregates the input the same the distribution variable sizes the hash entries ',negative
'nonjavadoc see ',negative
'sets taskspec which has vertex its input and tasks belonging vertex ',negative
'another loop table bucketed ',negative
'succeeded adding all the values ',negative
'return type should have same length the input ',negative
'only for testing ',negative
'this our use case not having passwords stored the clear hive conf files ',negative
'priority ',negative
'for hive udtf operator ',negative
'were going kill some queries and reuse the sessions maybe restart and put the new ones back into the pool however the pool has shrunk will close them instead ',negative
'will test the reconfiguration the header size changing the password length ',negative
'the lower bits are the absolute value offset ',negative
'all are null none are selected ',negative
'need remove hive also not change default see smb operator ',negative
'the system properties ',negative
'loginfonuking dir ',negative
'compute the values ',negative
'original bucket files delta directory and deletedelta should have been cleaned ',negative
'modify partition column type and comment ',negative
'what want order ccend desc ccstart asc but derby has bug sort that currently running jobs are the end the list bottom screen and currently running ones are sorted start time ',negative
'hive doesnt have the concept notnull ',negative
'check start forward rows new child the current key group rows will not forwarded those children which have index less than the currentchildindex can call flush the buffer children from lastchildindex inclusive currentchildindex exclusive and propagate processgroup those children ',negative
'split into digit middle and lowest longwords remainder division ',negative
'scrutinize escape pair specifically replace ',negative
'would useful have enum for type insertdeleteload data ',negative
'densecolix index orc writer with includes skip the root column get the original text file index then add the root column again this makes many assumptions also this only works for primitive types vectordeserializer only supports these anyway the mapping for complex types with subcols orc would much more difficult build ',negative
'restrictionm allow only subquery expression per query ',negative
'outside need create new limit ',negative
'set generic options ',negative
'hive syntax allows define case expressions two ways case when then when then else end translated into the case function else clause optional case when then when then else end translated into the when function else clause optional however calcite only has the equivalent the when hive function thus need transform the case function into when further else clause not optional calcite example consider the following statement case when then fee when then fie end will transformed into case when then fee when then fie else null end ',negative
'multikey specific repeated lookup ',negative
'shamelessly copied from path hadoop ',negative
'ambiguous case which should assumed level according spec ',negative
'scalarscalar ',negative
'pretend that one field used ',negative
'buffer the leaf node ',negative
'just drop transactionalfalse for backward compatibility case someone has scripts with transactionalfalse ',negative
'create the join aux structures ',negative
'operator below ',negative
'some hiveexceptions semanticexception dont set canonical errormsg explicitly but there logic compile find appropriate canonical error and return its code error code this case want preserve for downstream code interpret ',negative
'the update statement remember splitupdate udi ',negative
'free the htable connections ',negative
'only single double was passed parameter single quantile being requested ',negative
'col ',negative
'build tokwhere null check for joining column ',negative
'type date longcolumnvector storing epoch days minus type date produces type intervaldaytime storing nanosecond interval longs ',negative
'initialize using objectinspector array and column projection array ',negative
'all columns data partition and virtual are added ',negative
'unlimited lifetime ',negative
'parse out the kerberos principal host realm ',negative
'copy credentials and any new config added back jobcontext ',negative
'this type can only created when splitupdate ',negative
'obtain delegation token for the give user from metastore ',negative
'list paths that dont need merge but need move the dest location ',negative
'exception expected ',negative
'negative tests ',negative
'sort itself should not reference cor vars ',negative
'additional bookkeeping info for the stored stats ',negative
'call open read split mockmocktable ',negative
'return result ',negative
'add the bits the bottom the current word ',negative
'then remove all the grants ',negative
'fractional part has starting with zeros ',negative
'create table select ',negative
'ensure the correct task was preempted ',negative
'note the reason this method exists outside the noarg getdeserializer method case there userimplemented messagefactory thats used and some the messages are older format and the rest another then what messagefactory default irrelevant should always use the one that was used create deserialize there exist only implementations this json and jms additional note rather than config parameter does make sense have this use jdbclike semantics that each messagefactory made available register itself for discoverability might worth pursuing ',negative
'set bit null byte when field not null ',negative
'tolerance for long range bias when bias enabled and when bias disabled and for short range bias ',negative
'extract the raw data size and update the stats for the current partition ',negative
'dont evaluate nondeterministic function since the value can only calculate during runtime ',negative
'since the output the udtf struct can just forward that ',negative
'get the standard objectinspector the row ',negative
'test single highprecision divide random inputs ',negative
'add result order are processing ',negative
'get col object out ',negative
'checkcorrect codec checkcorrect codec ',negative
'the child also decimal cast needed hope can target type narrower ',negative
'replaceoverwrite introduces new files ',negative
'have readentity ',negative
'check the sample columns are the same the table bucket columns ',negative
'throw new not sort order and unique ',negative
'inform the routing purgepolicy send out fake log message the error level with the mdc for this query setup with llap custom appender this message will not logged ',negative
'desired parallelism the reduce task ',negative
'nonjavadoc see windowtablefunction supports streaming all functions meet one these conditions the function implements returns non null object for the that implements invocation fixed window unbounded preceding following ',negative
'test that write blocks write but read can still acquire ',negative
'singlecolumn long outer get key ',negative
'since right has longer digit tail and doesnt move will shift the left digits our addition into the result ',negative
'old table not the cache but the new table can cached ',negative
'interval types can use long version ',negative
'convert nonacidorctbl acid table ',negative
'dont set dagclient null here execute will only clean operators its set ',negative
'should only called for testing ',negative
'make one entry produce false result ',negative
'add int values ',negative
'empty string ',negative
'timestamps are stored long convert and compare ',negative
'get past this then the column name did match the hive pattern for internal column name such col etc must match the schema for the appropriate column this means people cant use arbitrary column names such col and expect ignore ',negative
'return worst case unknown ',negative
'internal names ',negative
'remove unnecessary information from target ',negative
'autodetermine local mode allowed ',negative
'try the basic test with nonchunked stream ',negative
'repl dump ',negative
'done output does not need committed hive does not use outputcommitter ',negative
'transaction states ',negative
'list the current connections ',negative
'this map used for set the stats flag for the cloned filesinkoperators later process ',negative
'convert the partition filter expression into string expected hcat and pass setlocation ',negative
'type doesnt require any qualifiers ',negative
'start reading role names from next position ',negative
'not know what bail out for safety ',negative
'not column ',negative
'make sure can add back ',negative
'disable avoid verbose app state report yarncluster mode ',negative
'case test with originals and deltas but now with only one bucket covered will have originals insertdeltas for only one bucket but the deletedeltas will for two buckets two strategies with one split for each when splitupdate enabled not need account for buckets that arent covered the reason why are able because the valid user data has already been considered base for the covered buckets hence the uncovered buckets not have any relevant ',negative
'check first otherwise webhcatlog full stack traces from filesystem when clients check for status exitvalue completed etc ',negative
'remove the previously peeked element ',negative
'ignore the parent already exists ',negative
'construct valuetabledescs and ',negative
'offset before which this guaranteed end can only estimated ',negative
'return greater than because lefts digits below rights scale ',negative
'creation time will set server and not ',negative
'this flow usually taken for import command ',negative
'consider cleaning this code and eliminating the arrays vectorization only handles one operator tree ',negative
'ignore starting quote ',negative
'mysql returns the string not wellformed numeric value return intwritablevalueof but decided return null instead which more conservative ',negative
'convert the work containing sortmerge join into work had regular join note that the operator tree not changed still contains the smb join but the plan changed aliastowork etc contain all the paths was regular join this used convert the plan mapjoin and then the original smb join plan used ',negative
'singleton ',negative
'rerun ppd through project column pruning would have introduced above scans pushing filter just above hive can push into storage incase there are filters non partition cols this only ',negative
'entry null due zerodivide ',negative
'for sparktez rely the generated selectoperator the type casting consider sel int sel int sel double first merge sel and sel into union and then merge union with sel get union then selectoperator will inserted hence error will happen afterwards the solution here insert one after union which ',negative
'elements queue integer are index fetchoperator segments ',negative
'create partition specs ',negative
'check all the input txns are open state write should allocated only for open transactions ',negative
'not exceed the configured max reducers ',negative
'dependency check depth ',negative
'all ast nodes must implement this interface provides basic machinery for constructing the parent and child relationships between nodes ',negative
'check that the files are removed ',negative
'write lock for add evict and clean operation ',negative
'files size for splits ',negative
'',negative
'strip and ',negative
'row resolver the subquery set the semanticanalyzer after the plan for the subquery genned this needed case the subquery select list contains tokallcolref ',negative
'worker stays initiated state and wait the above alter table retunrs almost immediately the test here check that seconds pass that the command driver actually blocks before cancel fired ',negative
'the column number and type information for this one column long reduce key ',negative
'lock required here ',negative
'nonjavadoc see javaioinputstream int ',negative
'unspecified default one will used for line break ',negative
'prefixed with avoid finding entry cmpath ',negative
'indicates that the recorded value null ',negative
'entire query can run locally save the current tracker value and restore when done ',negative
'shallow copy astnode ',negative
'recursive types ',negative
'this computes stats and should stats deduplicated too ',negative
'operationstate ',negative
'most general case where the left and right keys might have nulls and caller requires valued logic return select edeptno edeptno select deptno from emp becomes select edeptno case when ctc then false when dti not null then true when edeptno null then null when ctck ctc then null else false end from left join select count countdeptno from emp cross join select distinct deptno true from emp edeptno dtdeptno keys are not null can remove and simplify select edeptno case when dti not null then true else false end from left join select distinct deptno true from emp edeptno dtdeptno could further simplify select edeptno dti not null from left join select distinct deptno true from emp edeptno dtdeptno but have not yet the logic true can just kill the record the condition evaluates false unknown thus the query simplifies inner join select edeptno true from inner join select distinct deptno from emp edeptno dtdeptno ',negative
'encapsulates statistics about the duration all reduce tasks corresponding specific jobid the stats are computed the hadoopjobexechelper when the job completes and then populated inside the queryplan for each job from where can later accessed the reducer statistics consist the run times all the reduce tasks for job all the run times are milliseconds ',negative
'values ',negative
'get the last colname for the reduce key ',negative
'some parts session state like mrstats and vars need proper synchronization ',negative
'first drop any databases catalog ',negative
'input data col col blue red green green red blue null red col data empty string unset null property ',negative
'both inputs nonnegative ',negative
'the real workhorse spend time and energy this method there need keep hcatstorer lean and fast ',negative
'expected error ',negative
'nonjavadoc see javasqlsqlxml ',negative
'get next batch ',negative
'note print systemerr instead sserr because cant parse our commandline havent even begun and therefore cannot expected have reasonably constructed started the sessionstate ',negative
'new true this ',negative
'item for new partition queued now ',negative
'conversionhelper can called without method parameter length checkings for terminatepartial and merge calls ',negative
'first known state was completed wait for the app launch start ',negative
'mapping from the regex lines the log file where find true ',negative
'copy insert branch and duplicate the first branch will the update for the merge statement while the new branch will the insert for the ',negative
'isnull copying necessary ',negative
'not use map join case cross product ',negative
'have reached running state now check running nodes threshold met ',negative
'will wait for seconds for the task cancelled its still not cancelled unlikely will just move ',negative
'sit the cache while not use ',negative
'bgenjjtree include ',negative
'set timeout undo everything ',negative
'create table ',negative
'dont register with deleteonexit ',negative
'nonempty java opts with xmx specified ',negative
'add partition column stats ',negative
'this can use significant resources and should not done the main query thread ',negative
'this corr scalar subquery with agg expect one aggregate ',negative
'sparkllap always wraps query under subquery until that removed from sparkllap ',negative
'make sure the default value expression type exactly same columns type ',negative
'dynamic partition columns ',negative
'gbinputrels schema like this ',negative
'the connection should fail since the dry run ',negative
'always set these explain can see ',negative
'table partitioned and immutable then the presence the partition alone enough throw error not need check for emptiness decide throw error ',negative
'test for boolean type ',negative
'marker annotations for functions that reflector should ignore pretend does not exist ',negative
'drop occured part replicating drop but the destination table was newer than the event being replicated ignore but drop any partitions inside that are older ',negative
'column type ',negative
'can pretty sure that entire line can processed single command since always add line separator the end while calling ',negative
'the input the gby has tab alias for the column then add entry based that tabalias for this query select count from group needs tabaliasb colaliasx the gby tabaliasb comes from looking the rowresolver that the ancestor before any gbyreducesinks added for the gby operation ',negative
'inputs ',negative
'good performance for common case where small table has complex objects ',negative
'specialized class for doing vectorized map join that inner join multikey using hash map ',negative
'create postfiltering evaluators needed ',negative
'want lock here the database lock will cover the tables and putting lock will actually cause deadlock ourselves ',negative
'before readbatch initial the size offsets lengths the default value ',negative
'clientprotocol ',negative
'write the value out ',negative
'handler multiline sql ',negative
'test setting fetch size below max ',negative
'candidate for preemption ',negative
'remove this task from its children tasks ',negative
'nulls possible left right ',negative
'integerflag ',negative
'used memory totalmemory freememory ',negative
'start would throw already existed here ',negative
'the tabletracker here should new instance and not existing one this can only happen when break between loading partitions ',negative
'then load the sparkclientimpl config ',negative
'peel off the levels get the underlying array ',negative
'this task the callback instead ',negative
'find functions which name contains tofind the default database ',negative
'min ',negative
'isset assignments ',negative
'for integers have optional minmax filtering ',negative
'compare timestamp against timestamp long seconds and double seconds with fractional nanoseconds timestampcol timestampcolumn timestampcol longdoublecolumn longdoublecol timestampcolumn timestampcol timestampscalar timestampcol longdoublescalar longdoublecol timestampscalar timestampscalar timestampcolumn timestampscalar longdoublecolumn longdoublescalar timestampcolumn ',negative
'specifies ',negative
'tracks the number elements with the same rank the current time ',negative
'table partitionedadd partdir and partitiondesc ',negative
'this point have number save fastresult round have exponent will power operation fastresult ',negative
'set true for columns that needed skip loading into memory ',negative
'simulate the unknown source ',negative
'blank byte blank byte blank byte blank byte blank byte white start bytes ',negative
'the only way get the return object inspector and its return type initialize ',negative
'from the moment that have two destination clauses know that this multiinsert query thus set property right value using would equivalent but use avoid setting the property multiple times ',negative
'expressions ',negative
'max value stored registered cached determine the bit width for ',negative
'common code ',negative
'val ',negative
'workerspath the directory path where all the worker znodes are located ',negative
'the variable was the right need swap things around ',negative
'test third argument with nulls ',negative
'entry either expired was invalidated due table updates ',negative
'because the length cant known now ',negative
'static partition spec ends with ',negative
'use the earlier match ',negative
'return true the table bucketedsorted the specified positions the number buckets the sort order should also match along with the ',negative
'skip empty lines ',negative
'base with delete deltas ',negative
'remember this rel dont fire rule again review jhyde oct rules should not save state rule should recognize patterns where does does not need ',negative
'possible that the background init thread has finished parallel queued the message for but also returned the session the user ',negative
'lets use the table from the cache ',negative
'create the list children ',negative
'warning note this point createdbexportdump lives only world where replicationspec replication scope later make this work for nonrepl cases analysis this logic might become necessary also this using replv semantics with listfiles laziness copy export time ',negative
'would nice there was way determine quotes are needed ',negative
'one input table ',negative
'get left table alias ',negative
'get udaf evaluator ',negative
'find the last matching xmx following word boundaries format xmxsizeggmmkk ',negative
'add custom cookies passed the jdbc driver ',negative
'are doing work here wed normally constructor later decide not specialize well just waste any scratch columns allocated ',negative
'set some info for the query ',negative
'remember min value and ignore from the denominator ',negative
'interrupt all thread and verify get and expected message ',negative
'returns false index already exists map ',negative
'update condition ',negative
'reads the the next field afterwards reading positioned the next field return return true when the field was not null and data put the appropriate current member otherwise false when the field null ',negative
'otherwise discard the escape char ',negative
'due data freshness ',negative
'create top project fixing nullability fields ',negative
'test failing due guava dependency druid should have less dependency guava ',negative
'allow operation txn ',negative
'have tez installed ',negative
'extract the bits num into value from right left ',negative
'need deep copy here since doing something like lastfrom from instead will make ',negative
'serialize the row into bytestream param obj the object for the current field param objinspector the objectinspector for the current object param level the current level separator param writebinary whether write primitive object utf variable length string fixed width byte array onto the byte stream throws ioexception error writing the serialization stream return true serializing nonnull object otherwise false ',negative
'expect json file updated ',negative
'replace unparsable synonyms ',negative
'examine the buddy block and its subblocks detail ',negative
'test droppartitionbyname ',negative
'hive describe outputs emptystring null row before partition information ',negative
'this keep track subquery correlated and contains aggregate ',negative
'flip the bits because calcite considers that means that the column participates the groupby and does not opposed groupingid ',negative
'must obtain vectorized equivalents for filter and value expressions ',negative
'create currently replicated noop ',negative
'will ask for preliminary mapping this allows escape the unmanaged path quickly the common case its still possible that resource plan will updated and ',negative
'there are actual accumulo index columns defined then build the comma separated list accumulo columns ',negative
'note all dist cols have single output col name ',negative
'should prepare the valid write ids list based validtxnlist current txn txn exists the caller then they would pass null for validtxnlist and required get the current state txns make validtxnlist ',negative
'extra fields but dont ',negative
'create base outputformat ',negative
'partitionvals ',negative
'gives progress over uncompressed stream assumes deserializer not buffering itself ',negative
'create and drop some additional metadata test drop counts ',negative
'stringvalue ',negative
'accumulo rangeinputsplit doesnt preserve usesasl the ',negative
'send the client ',negative
'see for why recheck the queue ',negative
'rounding setscale methods ',negative
'case compaction this the file the current bucket ',negative
'column expression the table being filtered the semijoin optimization ',negative
'this method overridden each task todo execute should return taskhandle return status executing the task ',negative
'double columncolumn ',negative
'dummy value for use tests ',negative
'called after the tasks have been generated run another round optimization ',negative
'maponly job the task needs processed ',negative
'get metastorethrift privilege object for this principal and object not looking ',negative
'null should smaller than any other value put null the front end ',negative
'need shift everything bits left and then shift back populate the sign field ',negative
'expression either the leftright side equality predicate the subquery where clause the entire conjunct for the where clause for subquery where and then the expressions analyzed are the left and right sides the equality predicate and the exprtype tracks whether the expr has reference subquery table source has reference outerparent query table source ',negative
'role names are caseinsensitive ',negative
'partitionkeys ',negative
'initialize the users process only when you receive the first row ',negative
'mapjoin should not affected join reordering ',negative
'unexpected ',negative
'file handle ',negative
'specify the external warehouse root ',negative
'rsgbrs ',negative
'indicates whether node disabled for whatever reason commfailure busy etc ',negative
'note the following code removing folded constants exprs deeply coupled with columnpruner optimizer assuming columnprunner will remove constant columns dont deal with output columns except one case that the join operator followed redistribution operator ',negative
'make sure skip backwardcompat checking for those tests that dont generate events ',negative
'convert hcatschema and pass hcatinputformat ',negative
'add those that are not part the final set residual ',negative
'parse value ',negative
'options for the python script that are here because our option parser cannot ignore the unknown ones ',negative
'handle structs composed partition columns ',negative
'read and decode dictionary ids ',negative
'srsr lock are examining shared read ',negative
'thats what this this alter and swallow ',negative
'the output the key expression the input column ',negative
'unregister may come after the new dag has started running the methods are expected synchronized hence the following check sufficient ',negative
'used capture view conversions this used check for recursive cte invocations ',negative
'calculate the variance family variance variancesample standarddeviation result when count public vectorization code can use etc ',negative
'optimize for most common case primitive ',negative
'check the pipeout files are removed ',negative
'volatile ensures that static access returns metrics instance fullyinitialized state alternative synchronize static access which has performance penalties ',negative
'fields but for uniformity ',negative
'eventtime ',negative
'the sortmerge join creates the output sorted and bucketized the same columns ',negative
'none ',negative
'have able peek ahead one byte ',negative
'note not currently part the hiverelnode interface ',negative
'add one more record and close ',negative
'add any redirects ',negative
'get from conf pick changes make sure not set too low and kill the metastore maxsleep the max time each backoff will wait for thus the total time wait for successful lock acquisition approximately see backoff maxnumwaits maxsleep ',negative
'case schema not file system ',negative
'wait before launching the next round connection retries ',negative
'create new view ',negative
'whether need transformation for each parent ',negative
'found udf metastore now add the function registry ',negative
'remove operator ',negative
'all currently open txns any have txnid than commithighwatermark ',negative
'move works following the current reduce work into new spark work ',negative
'add subdirectory the work queue maxdepth not yet reached ',negative
'protection against construction ',negative
'update the existing row and insert another row newlyconverted acid table ',negative
'these parameters controls the maximum time job submitstatuslist operation executed templeton service time out the execution interrupted and timeoutexception returned client time out for list and status operation there action needed they are read requests for submit operation best effort kill the job its generated enabling this parameter may have following side effects there possibility for having active job for some time when the client gets response for submit operation and list operation from client could potential show the newly created job which may eventually killed with guarantees submit operation retried client then there possibility duplicate jobs triggered time out configs should configured seconds ',negative
'',negative
'xmx specified ',negative
'test various combinations ',negative
'for nonviews need some extra fixes ',negative
'test owner ',negative
'total characters byte length ',negative
'add owner privilege user owner the object ',negative
'remove writeset info for current txn since its about abort ',negative
'there backup servers ',negative
'spot check correctness decimal column multiply decimal scalar the case for addition checks all the cases for the template dont that redundantly here ',negative
'invariant rightlength leftlength rightoffset within the buffers ',negative
'get names these roles and its ancestors ',negative
'dynamic partition pruning enabled some all cases ',negative
'get the skewed values all the tables ',negative
'skip the test java cryptography extension jce unlimited strength jurisdiction policy files not installed ',negative
'should return tbl and tbl ',negative
'',negative
'grouping sets are not allowed ',negative
'',negative
'this hook used for verifying the column access information that generated and maintained the queryplan object the all the hook does print out the columns accessed from each table recorded the columnaccessinfo the queryplan ',negative
'statsdata ',negative
'get the exprnodedesc corresponding the first start node ',negative
'this the first time have realized are stack trace this case the previous line was the error message add that the stack trace well ',negative
'map built during translation ',negative
'process the input columns find nonnull value for each row track the unassigned batchindex the rows that have not received nonnull value yet similar selected array ',negative
'genericudtf stateful too copy ',negative
'could not get stats cannot convert ',negative
'put into the map that this task was before decided update ',negative
'dont currently support the between ends being columns they must scalars ',negative
'verify that the new version added schema ',negative
'the same object ',negative
'for testing ',negative
'turn escape off ',negative
'drop table overk ',negative
'this rebuild theres nothing here ',negative
'default value true however optimization deems this edge important should set this false this does not guarantee that the edge will stay however increases the chances ',negative
'and test dropping this specific table ',negative
'joinkeysjoinkeysoi are initialized after making merge queue setup lazily runtime ',negative
'tries get lock and gets waiting state ',negative
'write intermediate file the specified path the format the path ',negative
'that its easy find reason for local mode execution failures ',negative
'delete from tab ',negative
'sets taskspec with inputs and tasks belonging vertex ',negative
'verify was preempted also verify that finished single executor otherwise could have run anyway ',negative
'refer groupingid column ',negative
'hive the following code change only needed for hbase due hbase and will not required once hive bumps its hbase version that time will only need here ',negative
'this constructor used momentarily create the object match can called ',negative
'caller will return the batch ',negative
'check for required fields check for substruct validity ',negative
'test ',negative
'use the same logic ',negative
'serde properties ',negative
'parameter added the restricted list add test ',negative
'insert data ',negative
'thread pool for callbacks completion execution work unit ',negative
'only the big table input source should vectorized applicable ',negative
'key used save the partition dropped partspecs ',negative
'plain acid table acid table with customized tblproperties ',negative
'optional vectorized key expressions that need run each batch ',negative
'initialize with estimated element size record initial buffer size ',negative
'jar spec there manifest must the first entry the zip ',negative
'exclude the newlygenerated select columns from etc resolution ',negative
'the foo bar and blah params order not guaranteed ',negative
'this should cost based decision but till enable the extended cost model will use the given value for the variable ',negative
'todo authorization ',negative
'files size for splits ',negative
'returns the root node the ast only makes sense call this after successful parse ',negative
'input and output serdes ',negative
'noncbo path retries execute create view and believe will throw the same error message ',negative
'apply druid transformation rules ',negative
'here know represents group expression ',negative
'seconds ',negative
'add identity ',negative
'there one repeated field for mapcol the field name map and its original type mapkeyvalue ',negative
'pause for just bit for retrying avoid immediately jumping back into the deadlock ',negative
'tries get the job result job request completed otherwise sets job status failed such that execute thread can necessary clean based failed state ',negative
'use synchronized map since even read actions cause the lru get updated ',negative
'the columns the old column descriptor the columns the new one then change the old storage descriptors column descriptor convert the mfieldschemas their thrift object counterparts because maintain datastore identity identity the model objects are managed jdo not the application ',negative
'logger can resource stream real file cannot use copy ',negative
'replace filter ',negative
'the base writer ',negative
'can directly add positions into cordefoutputs since join ',negative
'create column info with new tablealias ',negative
'lrr case just store boundaries which could split boundaries reader positions wouldnt able account for torn rows correctly because the semantics our exact reader positions and inexact split boundaries are different cannot even tell lrr use exact boundaries there can mismatch original midfile split wrt first row when caching may produce incorrect result adjust the split boundary and also dont adjust depending where falls best wed end with spurious disk reads cache row boundaries but splits include torn rows this structure implies that when reading split skip the first torn row but fully read the last torn row linerecordreader does want support different scheme wed need store more offsets and make logic account for that ',negative
'nonjavadoc see javaioreader ',negative
'has been replaced hivesmbjoincacherow ',negative
'small value bytes use small length from valuewordref ',negative
'hint true shouldremove redundant anyway ',negative
'table ',negative
'rowsetlog should contain execution well performance logs ',negative
'there are paths under which the instances get registered standard path used zkregistrybase where all instances register themselves also stores metadata secure unsecure leader latch path used for activepassive configuration where all instances register under leader path but only one among them the leader secure unsecure ',negative
'the tokens have distinction between identifiers and quotedidentifiers ugly solution just surround all identifiers with quotes ',negative
'there any aggregate function this group not unnecessary ',negative
'the condition fetched here can reference udf that not deterministic but defined part the select list when view play but the condition after the pushdown will resolve using the udf from select list the check here for deterministic filters should based the resolved expression refer test case ',negative
'and must have columns also the partition locations must lie within the table directory ',negative
'',negative
'unable get the database set the dbname empty ',negative
'udftostring need the following mappings ',negative
'bytes key hash multiset optimized for vector map join this the abstract base for the multikey and string bytes key hash multiset implementations ',negative
'need update roottoworkmap case the key since even ',negative
'also compute the correct cfcq pairs can assert the right argument was passed ',negative
'value needs converted match the type params length etc ',negative
'serialize ',negative
'then need truncate the exceptions list accordingly ',negative
'what can about ',negative
'build new ',negative
'verify the data are intact even after applying applied event once again existing objects ',negative
'alter partitioned table set table property ',negative
'character then reverse the whole string ',negative
'dbtable return table ',negative
'convert integer string ',negative
'test that underflow produces null ',negative
'index set child ',negative
'map from integer tag nondistinct aggrs with key parameters ',negative
'copy columnvectors overflowbatch remember buffered columns compactly the buffered vrbs without other columns scratch columns ',negative
'instantiate default values not specified ',negative
'function calls from the query plan ',negative
'add the partition again that drop table with partition can ',negative
'getters ',negative
'attribute methods ',negative
'url host port ',negative
'any input has not been rewritten not rewrite this rel ',negative
'mock beeline ',negative
'set conf use llap user rather than current user for llap registry ',negative
'currently only print the first port consistent with old behavior ',negative
'',negative
'project everything from the lhs and then those from the original ',negative
'not need update stats alter tablepartition operations ',negative
'number rows that match the regex but have missing groups ',negative
'create partial select query ',negative
'and preemption should attempted host despite host having available capacity ',negative
'session has expired and will returned later ',negative
'generate the cmd line run the child jvm ',negative
'apply sarg needed and otherwise determine what rgs read ',negative
'found best match during this processing use ',negative
'compare with old cacheend ',negative
'can mux operator ',negative
'null for tables virtualview for views materializedview for mvs ',negative
'now that the properties are can instantiate sessionstate ',negative
'',negative
'repeated string tableswritten ',negative
'generic udtfs ',negative
'output will also repeating and null ',negative
'add the table spec for the destination table ',negative
'',negative
'same session object expected ',negative
'partitioning columns the parent are not assigned assign partitioning columns the child the parent ',negative
'create dbs ',negative
'matches ',negative
'this dependency removed for hbase ',negative
'ensures that object cached that everyone uses the same instance ',negative
'try create rcfilereader ',negative
'and their types ',negative
'the tests here are heavily based some timing there some chance fail ',negative
'perform incremental normalization ',negative
'set the inferred sort columns for the file this filesink produces ',negative
'round ',negative
'diskdata ',negative
'',negative
'mock beelineopts ',negative
'intentionally set this high that will not trigger major compaction for ttp ',negative
'make sure check for side file case streaming ingest died ',negative
'rewrite the delete update into insert crazy but works deletes and update actually are inserts into the delta file hive delete delete from tablename where will rewritten insert into table tablename partition partcols select rowid partcols from tablename sort rowid update update tablename set expr where will rewritten insert into table tablename partition partcols select all partcolsfrom tablename sort rowid where all all the nonpartition columns the expressions from the set clause will reattached later the where clause will also reattached later the sort clause put there that records come out the right order enable merge read ',negative
'consider this enable issue not not vectorized issue ',negative
'our outputs are the transitive outputs our inputs ',negative
'need know aggregate count since corr subq with count aggregate ',negative
'this only when not initialized but may need find way tell the caller how initialize the valid size ',negative
'pass unparsed name here ',negative
'add expr the list predicates rejected from further pushing that know add createfilter ',negative
'this partition ',negative
'exception expected only filter enabled and injection disabled ',negative
'',negative
'catch with the big table ',negative
'get service ticket from the authorization header ',negative
'call this method may called after all the all fields have been read check for unread fields note that when optimizing reading stop reading unneeded include columns worrying about whether all data consumed not appropriate often arent reading all design since parses the line through the last desired column does support this function ',negative
'bad format ',negative
'process join filters ',negative
'event ',negative
'compacts ',negative
'test with batch size and decaying factor ',negative
'load set ',negative
'position beginning ',negative
'input the script ',negative
'set global member indicating which virtual columns are possible used ',negative
'removes the threadlocal variables closes underlying hms connection ',negative
'tests the case when tblpathpapbpcfile for table with partition does not throw hiveexception ',negative
'exceeds this value ',negative
'set the table write all the acid file sinks ',negative
'the getsplits call should have resulted lock acidtbl ',negative
'nonjavadoc see ',negative
'this for all complex types and binary ',negative
'case keepalive ensure that timeout handler does not close connection until entire ',negative
'some tests expected pass invalid schema ',negative
'max ',negative
'replace the reducer with our fully vectorized reduce operator tree ',negative
'longer relevant for ',negative
'set functions create the null check query for notin subquery predicates for subquery predicate like not select from where the not null check query select count from where and null this subquery joined with the outer query plan the join condition the join condition ensures that case there are null values the joining column the query returns rows the ast tree for this tokquery tok from toksubquery the input subquery with correlation removed subqueryalias tokinsert tokdestination tokselect tokselectexpr ast tree for count tokwhere null check for joining column ',negative
'supports keeping object without having import that definition ',negative
'preven subsequent runs until new trigger set ',negative
'since hive here then its not acid table there should never any deltas ',negative
'then try the brute force search for something throw away ',negative
'',negative
'get the job handle associated with the spark job ',negative
'just safe check ensure that are not reading empty delete files ',negative
'since are creating with scale fraction digits zero trim ',negative
'loginfogetting parent ptnrootgetname ',negative
'synthetic predicate with dynamic values ',negative
'get around hbase failure single node see bug ',negative
'terminate the old task and make current task dependent ',negative
'after concatenating them with and operator ',negative
'start the shuffle service before the listener until its service well ',negative
'map zcolumninfo for ',negative
'replicate table definition ',negative
'make sure that the number column aliases the clause matches ',negative
'contains reducer the optimization always since there exists reducer the sortingbucketing properties due the sortmerge join operator are lost anyway the plan cannot wrong ',negative
'runs the templeton controller job with args utilizes toolrunner run the actual job ',negative
'intentional fall through ',negative
'hiveconf hivestatsndverror default produces vectors ',negative
'raise custom exception like ioexception and verify expected message this should not invoke cancel operation ',negative
'were performing binary search need restart ',negative
'close the client connection with zookeeper ',negative
'loop once again with the new cause current ',negative
'why this checking for deltasisempty hive ',negative
'retrieve attempt log into logdir ',negative
'verify data ',negative
'type promotion everything goes decimal ',negative
'the field bits which fields include for each grouping set ',negative
'the size the biggest small table ',negative
'test forward scan ',negative
'small case just write the value bytes only ',negative
'generate the kerberos ticket under the following scenarios cookie authentication disabled the first time when the request sent the server returns which sometimes means the cookie has expired ',negative
'tried ',negative
'cte referenced queryblock add subquery for now sqalias the alias used alias specified used the cte name works just like table references adding done copying ast cte setting astorigin cloned ast trigger phase new qbexpr update data structs remove this table reference move invocation ',negative
'number rows means that statistics from metastore not reliable ',negative
'repeated nonnull fill down column ',negative
'handle the case for unpartitioned table ',negative
'try zerodivide show repeating null produced ',negative
'end pattern ',negative
'setup hashcode ',negative
'test setter for map object ',negative
'choose array size have two hash tables hold entries the sum the two should have bit more than twice much space the ',negative
'added conf member set the repl command specific config entries without affecting the configs ',negative
'the number reducers the child more specific than that the parent assign the number reducers the child the parent ',negative
'wont happen ',negative
'wrap the transport exception rte since subjectdoas then goes and unwraps this for out the doas block then unwrap one more time our catch clause get back the tte ugh ',negative
'propagate ',negative
'nothing case proc null ',negative
'the output precision greater than the input which should cover least rows the scale the same the input ',negative
'convert inputs ',negative
'tables that were serialized with columnsetserde doesnt have metadata this hack applies all such tables ',negative
'cancel currently executing tasks ',negative
'the list element object inspector ',negative
'skip rest checks user admin ',negative
'standard error allowed for ndv estimates for fmsketch lower value indicates higher accuracy and ',negative
'replace existing view ',negative
'nonjavadoc see javasqlarray ',negative
'generate reducesinkoperator there special case when want the rows randomly distributed reducers for load balancing problem that happens when there distinct operator set the numpartitioncolumns for this purpose this ',negative
'drop any left over catalogs ',negative
'treeset anyway uses treemap use plain treemap able get value collisions ',negative
'this idempotent ',negative
'adding column names used later ',negative
'expect cache requests from the middle here ',negative
'count any input except null which for count and output long just modes partial complete ',negative
'some filters may have been specified the show locks statement add them the query ',negative
'gen gbrsgbrsgb pipeline ',negative
'will considered enable and novalidate and relyfalse ',negative
'the driver not already available the url add the one provided ',negative
'check that src and dest are the same file system ',negative
'dynamic partition list the statstask ',negative
'this method ends with anything except retry signal the caller should fail the operation and propagate the error the its caller metastore client thus must reset retry counters ',negative
'job request type ',negative
'start the scheduled poll task ',negative
'load data ',negative
'all the vertices belong the same dag just use numbers ',negative
'make sure the table the target database didnt get clobbered ',negative
'fall through ',negative
'this input rel not rewritten ',negative
'add single child and restart the loop ',negative
'write credential with token file ',negative
'use construct ',negative
'read the schema version stored metastore ',negative
'possible that read and write entities contain old version the object before was modified statstask get the latest versions the object ',negative
'the buffer not the full heap demote the top item the heap into the list ',negative
'make sure all leaves are visited least once ',negative
'successfully convert bucket map join ',negative
'evaluate the hashcode ',negative
'represents select expression the context windowing these can refer the output windowing functions and can navigate the partition using leadlag functions ',negative
'filter condition true ignore ',negative
'actually run which different under doasfalse this seems intended ',negative
'reducers dont produce enough files well the same for tables for now ',negative
'jar then this would needed ',negative
'dont retry immediately use delay with exponential backoff ',negative
'lrfu cache policy doesnt store locked blocks when cache the block locked simply nothing here the fact that was never updated will allow add properly the first notifyunlock well set priority account for the inbound one lock not heap ',negative
'extract information ',negative
'fold after replacing possible ',negative
'should have also been thrown out ',negative
'add the newly generated clause subexpr ',negative
'',negative
'test that dont drop the unnecessary tuple the table has the corresponding struct ',negative
'get children key node ',negative
'this wasnt empty txn wed get better msg ',negative
'will set the larger the parents ',negative
'mark that branch the big table branch ',negative
'this will create project which will project out the column positions ',negative
'return the next back servers port ',negative
'merge should convert hll dense ',negative
'case the expression tablecol col can regex this can only happen without clause dont allow this for exprresolver the group case ',negative
'schema only ',negative
'calculate filter propagation directions for each alias for innersemi join for left outer join for right outer ',negative
'nonnative and nonmanaged tables are not supported movetask requires filenames specific format ',negative
'extract each entry from the pathenv ',negative
'return true ',negative
'possible ',negative
'value needs converted match type params ',negative
'dump metrics string json ',negative
'dump all the events except drop ',negative
'gained again ',negative
'not using here because forces connection the ',negative
'default aggregate counters across the entire dag example shufflebytes would mean shufflebytes each vertex aggregated together create dag level shufflebytes use case shufflebytes across the entire dag limit perform action ',negative
'skewed info ',negative
'make sure the updates are not sent out order compared how apply them ',negative
'number mantissa digits before decimal point ',negative
'',negative
'let cleaner delete obsolete filesdirs ',negative
'release all the previous buffers that may not have been able release due reuse ',negative
'get the actual length first ',negative
'ensure this set the config that the can read ',negative
'drop test and its tables and views ',negative
'change the engine tez ',negative
'test for only colnames being empty ',negative
'this guard for special druid types hyperunique currently not support doing anything special with them hive however those columns are there and they can actually read normal dimensions with select query thus print the warning and just read them string ',negative
'operations that have objects type commandparams function are authorized solely the type ',negative
'note this doesnt maintain proper newstream semantics any could either clone this instead enforce that this only called once ',negative
'str ',negative
'move logic that can reused ',negative
'separator for open write ids separator for aborted write ids ',negative
'process the position alias groupby and orderby ',negative
'there are none theyre not readable ',negative
'get the characterbyte the offset the string equal the fieldid ',negative
'default column name ',negative
'definitely int most ints fall here ',negative
'right trim slice byte array and place the result into element vector ',negative
'though given short hcat the map will emit ',negative
'null ',negative
'columns have been added ',negative
'needed initargs for certain execution paths ',negative
'singlecolumn string specific variables ',negative
'multiple concurrent local mode job submissions can cause collisions working dirs and system dirs workaround rename map red working dir temp dir such cases ',negative
'',negative
'get forwarded hosts address ',negative
'validate and setup patternstr ',negative
'duplicate column names currently simple algorithm this can optimized later need but should not major bottleneck the number columns are anyway not big ',negative
'should only need insert the token the first time ',negative
'deserialize key into vector row columns ',negative
'that uniont null converted just ',negative
'bitset for flagging aborted transactions bit true aborted false open default value means there are open txn the snapshot ',negative
'test databasemetadata queries which not have parent statement ',negative
'looks like some pools were removed kill running queries requeue the queued ones ',negative
'top ',negative
'hive probably wont support really care ',negative
'copy log file ',negative
'for fully specified ptn follow strict checks for existence partitions metadata for unpartitioned tables follow filechecks for partially specified tables this would then need filechecks the start ptn write doing metadata checks can get potentially very expensive fat conf there are large number partitions that match the partial specifications ',negative
'get partitions name ascending descending ',negative
'start the process add the cache ',negative
'make sure all partitioning columns referenced actually exist and are the correct order the end the list columns produced the view also move the field schema descriptors from derivedschema the partitioning key ',negative
'assume each has unique serde ',negative
'set the key check this new group same group ',negative
'short running updated nothing expect rows writeset ',negative
'partvalues ',negative
'the interface for single long key hash map lookup method ',negative
'the thread still active and needs cancelled then cancel this may happen case task got interrupted timed out ',negative
'replace the crs sel operator ',negative
'lets remember the join operators have processed ',negative
'transaction and locking methods ',negative
'remove the semijoin optimization branch along with all the mappings the parent has all the branches collect them and remove them ',negative
'race with querycomplete ',negative
'updated when add this the queue ',negative
'wait for the current future ',negative
'stages elapsed time ',negative
'time after which metastore cache updated from metastore the background update thread ',negative
'dont push sampling predicate since createfilter always creates filter with issamplepred false also the filterop with sampling pred always ',negative
'bunch things get setup the context based conf but need only the tmp directory ',negative
'web port ',negative
'did not add any factor there are common factors can ',negative
'will succeed and transition for cleaning ',negative
'overlay the values any system properties whose names appear the list confvars ',negative
'this conditions need pushed into semijoin since this condition corresponds ',negative
'create dbtpart part part test recycle single file part recycle table ',negative
'create the temp directories ',negative
'remove the paths which are not part ',negative
'both neededcolumnids and neededcolumns should never null when neededcolumnids empty list means needed column not need any column evaluate ',negative
'store types and tables separately because one cannot use table servicemethod struct ',negative
'used struct and union complex type readers indicate the final field has been fully read and the current complex type finished ',negative
'optimization the conditionaltask avoids linking movetask that are expensive blobstorage systems instead linking creates one movetask where the source the first movetask source and target the second movetask target ',negative
'needs major compaction ',negative
'only can have single partition spec ',negative
'all others from the remote service cause the task fail ',negative
'this file something dont hold locks for ',negative
'since left integer always some products here are not included ',negative
'have field and are positioned read ',negative
'default for all other objects this false ',negative
'string char varchar and binary for char and varchar when the caller takes responsibility for truncationpadding issues when true conversion needed into external buffer least bytes use get the result otherwise currentbytes currentbytesstart and currentbyteslength are the result ',negative
'group path alias according work ',negative
'there cannot exist any sampling predicate ',negative
'all rows from right side will present resultset ',negative
'this will work with the new support rewriting load into ias ',negative
'assumption acid columns are currently always the beginning the arrays ',negative
'authorization errors ',negative
'nonjavadoc see ',negative
'combinesort temp and normal table results ',negative
'have estimation lowerbound and higherbound use estimation between lowerbound and higherbound ',negative
'only split pruning hive has been fixed the writer ',negative
'set temp file containing error output sent client ',negative
'nonjavadoc see ',negative
'nonjavadoc see javalangstring javalangstring javalangstring javalangstring ',negative
'create tmp dir for mergefilework ',negative
'the small key length the key big length allbitson then the key length stored the writebuffers ',negative
'nonjavadoc see javalangstring javalangstring ',negative
'datanucleus objects get detached all over the place for real reason ',negative
'now lets take look input sizes ',negative
'extract possible candidates pushed down ',negative
'remove restrictions the variables that can set using set command ',negative
'can generate ranges from rowid ',negative
'arraylistmultimap important here retain the ordering for the splits ',negative
'lookup ndvs side ',negative
'the has other siblings will add considered next iteration ',negative
'cannot wrap reader for nonvectorized pipeline ',negative
'example ',negative
'this the small table side case smb join need send each split the corresponding bucketbased task the other side case split needs multiple downstream tasks need clone the event and send the right destination ',negative
'keep order name consistent with jdo ',negative
'delta may present from previous failed task attempt ',negative
'minhistorylevel will have entry for the open txn ',negative
'nested complex types cannot folded cleanly ',negative
'the same process this will still only get the functions from the first metastore ',negative
'add only the corresponding family has not already been added ',negative
'write the byte stream every keyvalue pairs ',negative
'base javaobject primitives javafieldref entry javaobject javafieldref ',negative
'start writing array contents ',negative
'skip past blank characters ',negative
'vectorized row batch not for example original inspector for orc table etc ',negative
'stats part ',negative
'all them were false return false ',negative
'hours ',negative
'get the sort aliases these are aliased the entries the select list ',negative
'grab the oldest inmemory buffered batch and dump disk ',negative
'statics for when the mock created via filesystemget ',negative
'this must hadoop where was protected ',negative
'map table name the correct columnstatstask ',negative
'use system zone when converting from timestamp timestamptz ',negative
'this tests checks that appropriate delta and deletedeltas are included when minor compactions specifies valid open txn range ',negative
'only case full outer join with smb enabled which not possible convert regular join ',negative
'print next vertex ',negative
'dbnamematching alone ',negative
'attributes ',negative
'todo even listener for default new true this ',negative
'join current union task old task ',negative
'verify the eventid was passed the nontransactional listener ',negative
'check whether there column needed the windowing operation that missing the project expressions for instance the windowing operation over aggregation column hive expects that column the select clause the query the idea that there column missing will replace the old project operator two new project operators project operator containing the original columns the project operator plus all the columns that were missing project top the previous one that will take out the columns that were missing and were added the previous project ',negative
'insert the current constant value into exprnodestructs list there struct corresponding the current element create new one insert ',negative
'notify listeners ',negative
'comparing paths multiple times creates lots objects creates pressure for tables having large number partitions such cases use precomputed paths for comparison ',negative
'skip duplicated grouping keys happens when define column alias ',negative
'link sel ',negative
'try repeating both sides ',negative
'minihs will become leader ',negative
'update the last access time for this node ',negative
'check input objects length doesnt match then output new writable with correct params ',negative
'have ioexception other than ',negative
'',negative
'vectorizer does not vectorize row deserialize mode the input format has input formats will clear the isvectorized flag they are doing vrb work ',negative
'byte ',negative
'values outside the column type bounds will fail runtime ',negative
'process join keys ',negative
'must have column name followed with type ',negative
'subtraction with type date longcolumnvector storing days and type timestamp produces ',negative
'process the combine splits ',negative
'spilled small tables ',negative
'the first directory becomes the base for combining ',negative
'use the tez hash table loader ',negative
'field expression should resolved ',negative
'this sql standard maxn zero items should null ',negative
'look for functions without pattern ',negative
'default list bucketing directory name internal use only not for client ',negative
'test acid with vectorization combine ',negative
'node already exists ',negative
'get all items into array and sort them ',negative
'wait for the child process finish ',negative
'create new union and sort ',negative
'finally can create the grouped edge ',negative
'not need anything ',negative
'',negative
'decimal addition subtraction ',negative
'setup expr col map ',negative
'since raw data was possibly escaped make split work now need remove escape chars they dont interfere with downstream processing ',negative
'get the map for postovertex ',negative
'compactor states should really enum ',negative
'check privileges ',negative
'copy files with retry logic failure source file dropped changed ',negative
'only consider tables for which hold either exclusive shared write lock ',negative
'create new sparkwork for all the small tables this work ',negative
'leadership status change happens inside synchronized methods also use single threaded executor service for handling notifications which guarantees ordering for notification handling leadership status change happens when tez sessions are getting created the notleader notification will get queued executor service ',negative
'the next row will require another call increasebufferspace since this new buffer should used ',negative
'only one them ',negative
'iterate over the selects search for aggregation trees use string keys eliminate duplicate trees ',negative
'the denominator the tablesample clause ',negative
'unpartitioned table ',negative
'now that have exited read lock safe remove any invalid entries ',negative
'authentication only authentication and integrity checking using signatures authentication integrity and confidentiality checking ',negative
'following special cases for different type subqueries which have aggregate and implicit group and are correlatd existsnot exists not allowed throw error for now plan allow this later scalar this should return true since later subquery remove rule need know about this case always allowed but returns true for cases with aggregate other than count since later subquery remove rule need know about this case ',negative
'initialize common server configs needed both binary http modes ',negative
'partitioned table delete all ',negative
'required required required required optional optional optional optional optional ',negative
'test that existing sharedwrite table with new sharedread coalesces ',negative
'the serialized all null key and its hash code ',negative
'the evaluate yields true then pass all rows else pass rows ',negative
'configuration for the application master ',negative
'hdfs scratch dir ',negative
'event ',negative
'move the specified work from the sparkwork the targetwork note that order not break the graph since need for the edges ',negative
'print current state before exiting ',negative
'retrieve results ',negative
'other integer types not supported yet ',negative
'file locations searched the correct order ',negative
'add tracking information check source state already known and send out update ',negative
'finishable state checked the task via explicit query the taskrunnercallable ',negative
'with the parent based its position the list parents ',negative
'the objects that have been printed ',negative
'have add this one manually for tests the initialized via the metastorediretsql and dont run the schema creation sql that includes the insert for which can locked the entry happens via notificationevent insertion ',negative
'error storage specification ',negative
'add input path ',negative
'',negative
'construct the edgemanager descriptor used all edges which need the routing table ',negative
'parser only allows fooab not foofooa foob ',negative
'assign row from list standard objects count ',negative
'nonjavadoc see javasqldate javautilcalendar ',negative
'when there are live nodes the cluster and this timeout elapses the query failed ',negative
'required optional optional optional optional optional optional optional optional optional ',negative
'gce firewall set through instance tags ',negative
'first handle special cases one the special case methods cannot handle returns null ',negative
'this case happens only when prs key empty which case can use number distribution keys and key serialization info from crs ',negative
'nothing this case ',negative
'path the filesinkoperator table blobstore path ',negative
'aggregate size from aggregation buffers ',negative
'the union operators from the operator tree later ',negative
'since sql case insenstive just make sure that the comparison column names and check expressions column reference work convert the key lower case ',negative
'can get away with the use varname here because varname hivename for pwd ',negative
'the assumption here the path file only case this different acid deltas the isfile check avoided here for performance reasons ',negative
'integermaxvalue ',negative
'these delims passed serde params comment passed table params ',negative
'the values from ',negative
'max rows rows from left side ',negative
'there should only column sourceoperator ',negative
'avoid denominator getting larger and aggressively reducing number rows will ease out denominator ',negative
'process singlecolumn string leftsemi join vectorized row batch ',negative
'get the table from the client again verify the name has been updated ',negative
'txnmanagerfactory singleton the default true has already been created and wont throw ',negative
'hint provided use that size ',negative
'case the dynamic value resolves null value ',negative
'clean ',negative
'will transform using clause and make look like onclause lets generate valid onclause ast from using ',negative
'serialize the union tagvalue ',negative
'for shell commands use unstripped command ',negative
'used alias ',negative
'try nulls both sides ',negative
'know the job has finished check the futures here ourselves ',negative
'alterpartition only for changing the partition location the table rename ',negative
'create the final group operator ',negative
'check whether the username the token what expect ',negative
'the task longer required and asks for deallocation ',negative
'check the last node ',negative
'the conf string for columnsbuffersize ',negative
'keep this within chars width more columns needs added then update min terminal width requirement and separator width accordingly ',negative
'should only managed tables passed here check table the default table location based the old warehouse root then change the table location the default based the current warehouse root the existing table directory will also moved the new default database directory ',negative
'track which small tables havent been processed yet ',negative
'',negative
'hive hiveserver the jars for this udf may have been loaded different thread and the current thread may not able resolve the udf test for this condition ',negative
'addpartitions normal operation ',negative
'create type with nestinglevel levels nesting ',negative
'write final length chunk ',negative
'might have multiple ranges coming from children ',negative
'serialize the row component using the rowidfactory the normal case this will just ',negative
'build map which tracks the name column inputs signature corresponding table column name this will used replace column references check expression ast with corresponding column name input ',negative
'specifying the right type info length tells which the last column ',negative
'pull out deterministic exprs from nondeterministic and push down deterministic expressions separate filter ',negative
'convert the metastore thrift objects result objects ',negative
'running the movetask and task parallel may cause the mvtask write and task write for the same partition ',negative
'currently not used hive codebase but intended authorize actions that are directly userlevel theres storage based aspect this can follow one two routes can allow default that way this call stays out the way can deny default that way privileges are authorized that not understood and explicitly allowed both approaches have merit but given that things like grants and revokes that are userlevel not make sense from the context storagepermission based auth denying seems more canonical here ',negative
'now the correct way through objectinspectors ',negative
'yarn service has started llap application now for some reason state changes complete then fail fast ',negative
'the parent ops for hashtablesinkop ',negative
'simply need remember that weve seen union ',negative
'there are skewed values nothing needs done ',negative
'different locks from same txn should not conflict with each other ',negative
'temporary selected vector ',negative
'transform case when with just thenelse into statement ',negative
'currently metastore does not store column stats for partition column calculate the ndv from partition list ',negative
'sleep until all threads with clean tasks are completed ',negative
'gettable checks whether database exists should moved here ',negative
'return the serialized bytes ',negative
'read the keys and values ',negative
'check list elements are primitive objects ',negative
'create three catalogs ',negative
'accurate short value cannot obtained ',negative
'export command uses metadata ',negative
'dont call tez doesnt sign fragments ',negative
'because the implementation the jsonparserfactory are sure that can get tezjsonparser ',negative
'algorithm convert decimal three bit words three enough for the decimal since represent the decimal with trailing zeroes trimmed skip leading zeroes the words once find real data nonzero byte add sign byte buffer necessary add bytes from the rest bit words return byte count ',negative
'create temp table directory ',negative
'timestamp ',negative
'try one sorted ',negative
'input row resolver ',negative
'and pass settaskplan the last parameter ',negative
'child need for pruning ',negative
'validate location string ',negative
'ignore this exception there problem itll fail when trying read write ',negative
'the drop has fail nonexistent partitions cannot batch expressions that because actually have check each separate expression for existence could small optimization for the case where expr has all columns and all operators are equality assume those would always match one partition which may not true with legacy nonnormalized column values this probably popular case but thats kinda hacky lets not for now ',negative
'tokensig could null ',negative
'return true this data type handled the output vector integer ',negative
'otherwise dont know what make maybe ',negative
'serdes here ',negative
'this method used validate check expression since check expression isnt allowed have subquery ',negative
'try running priority task ',negative
'length files cannot orc files not valid for ',negative
'default can always use the multikey class ',negative
'there was parallel cache eviction the evictor accounting for the memory ',negative
'partition mixed case ',negative
'but for now will just retry will evict more each time ',negative
'test default table types returned ',negative
'set metastoreoverlay parameters ',negative
'retrieve the stats obj that was just written ',negative
'start the split falls somewhere within before this slice note the linerecordreader will skip the first row even start directly its start because cannot know its the start not unless its note that give special treatment here unlike the eof below ',negative
'nonjavadoc see ',negative
'for the virtual columns the internalname upper case and the alias lower case since put them the fieldnames became lower cased look the inputrr for the fieldname alias ',negative
'multiple move happens only first move will chosen ',negative
'making this public note that its ordering undefined ',negative
'add any partition key values provided part job info ',negative
'for remote jdbc client try set the conf var using set foobar ',negative
'wait for all the events written off the order service important ',negative
'test for duplicate publish ',negative
'also initialize paritioned table test against ',negative
'data should get sorted your etlmerge process here group the data partitionvalues rowidbucketid order the groups rowidwriteid rowidrowid ',negative
'throw the wasnt rolled back ',negative
'hour granularity ',negative
'map cvalue map rowvalues assertequals cvaluesize ',negative
'else expression ',negative
'tstage just simple way generate test data ',negative
'use list for easy cumtomize ',negative
'pairwise columnhasnulls columnisrepeating ',negative
'then try get from all ',negative
'conditions for being partition column ',negative
'this method could beyond the integer ranges until scale back need twice more variables ',negative
'now release single session from ',negative
'could generate different error messages ',negative
'check the record record already encoded once does reuse the encoder ',negative
'this method takes object accepts whatever types that are passed ',negative
'call this first then send interrupt the thread ',negative
'deleterule ',negative
'rows ',negative
'finally remove the rest the expression from the tree ',negative
'note necessary merge task the parent the move task and not the other way around for the proper execution the execute method ',negative
'verify the union has been hidden and just the main type has been returned ',negative
'build new environmentcontext with ifpurge ',negative
'not antisymmetric ',negative
'ensure that are consistent when comparing the base class ',negative
'has its reducesink parent removed ',negative
'deleted then good the last parameter ifexists set true ',negative
'test repeating case ',negative
'retrieve delegation token for the given user ',negative
'check see that the vertices are correct ',negative
'escape the escape and escape the asterisk ',negative
'add the tables well outputs ',negative
'case grouping sets groupby will output values for every setgroup this the index the column that information will sent ',negative
'set parameter false connection int smallint allowed ',negative
'all iface apis throw texception ',negative
'seemed not close files properly error situation ',negative
'legacy handling ',negative
'release all locks including persistent locks ',negative
'should make ',negative
'within range specified ',negative
'check highest digit for rounding ',negative
'start removing lru nodes ',negative
'suppress here real issue will get caught where clause handling ',negative
'the included columns the reader file schema that include acid columns present ',negative
'logdebugclassname logical logical batchindex batchindex key continues savekey savejoinresultname ',negative
'should lockid ',negative
'query will get cancelled before creating partitions ',negative
'copy order ',negative
'some nonzero offsets ',negative
'',negative
'add shutdown hook flush the history history file ',negative
'store partition key expr maptargetwork ',negative
'cancel hcat and jobtracker tokens ',negative
'have estimation lowerbound and higherbound use estimation between lowerbound and higherbound ',negative
'aggregate mode should followed union that need analyze ',negative
'need preserve currentdate ',negative
'newer overrides the older ',negative
'continue analyzing from the child astnode ',negative
'cannot create nullwritable instances ',negative
'warm couple times ',negative
'todo simple wrap rethrow for now clean with error ',negative
'explain analyze composed two steps step analyzestaterunning run the query and collect the runtime rows ',negative
'the the validate input method ',negative
'shouldnt happen getall ',negative
'end string ',negative
'read all the fields and create partitions sds and serdes ',negative
'merge histograms ',negative
'not allow users override zerocopy setting the rest can taken from user config ',negative
'local temp dir specific this driver ',negative
'print inline vertex ',negative
'overflow not error here just means this smaller ',negative
'allow undecorated char and varchar support scratch column type names ',negative
'with all input columns repeating ',negative
'make sure this has enough integer room accomodate others integer digits ',negative
'datewritablev mutable datestatsagg needs its own copy ',negative
'print information about calls that took longer time info level ',negative
'validate input both new and old uri should contain valid host names and valid schemes port optional both the uris since hdfs uri doesnt have port ',negative
'enable metric collection for hiveserver ',negative
'must check that not blank because otherwise you could get false positive the blank value was value you were legitimately testing see was the set ',negative
'dag specific counters ',negative
'could exponential notations ',negative
'first search the classpath ',negative
'many filesinkdescs are linked each other good idea keep track tasks for first filesinkdesc others can use ',negative
'positive number flip the first bit ',negative
'nothing default ',negative
'virtual ',negative
'pathchildrencache tried mkdir when the znode wasnt there and failed ',negative
'production double ',negative
'allow partial partition specification for nonscan since noscan fast ',negative
'create semijoin optimizations only for hinted columns ',negative
'ignore temporary tables ',negative
'use the row buffer size force lots rebuffering ',negative
'not from subquery ',negative
'both parts are scaling easy just check overflow ',negative
'exception from runtime that will show the full stack client ',negative
'revert false ',negative
'typecheckprocfactor expects typecheckctx have unparse translator ',negative
'finally try ',negative
'the number joins number input tables this not star join ',negative
'run minor compaction ',negative
'this how many bytes need store those additonal bits vint ',negative
'the index where the current char starts ',negative
'setup symbolfunction chain ',negative
'only dag failed killed the vertex status fetched from ',negative
'get the and for this branch ',negative
'clean trash ',negative
'map keep track which smb join operators and their information annotate their mapwork with ',negative
'needed for type parity ',negative
'create the parquet filterpredicate without including columns that not exist the schema such partition columns ',negative
'from txnid from txnid from txnid ',negative
'srsracquired lock are examining acquired can acquire because two shared reads can acquire together and there must ',negative
'insert filter operator between targetchild and inputparent ',negative
'here its nonacid schema file check from before table was marked transactional basexdeltaxx from load data ',negative
'found subdirectory depth less than number partition keys validate the partition directory name matches with the corresponding partition colname currentdepth ',negative
'full outer join ',negative
'the resulting privileges need filtered privilege type and username ',negative
'authorized perform action ',negative
'check optimizedonly hash table restrictions ',negative
'note the definitions what odbc and jdbc keywords exclude are different different places for now just return the odbc version here that excludes hive keywords that are also odbc reserved keywords could also exclude sql ',negative
'determine row schema for tsop ',negative
'configure the output key and value classes ',negative
'validationlevel ',negative
'test for setting the maximum partition count ',negative
'expr alias parses but only allowed for udtfs this check not needed and invalid when there transform the ',negative
'test random scan ',negative
'for partitioned table partitionvals are specified ',negative
'oid for spnego gssapi mechanism ',negative
'the aggregation batch vector needs know when start new batch ',negative
'sort ',negative
'integer digit fraction digits trailing zeroes are suppressed ',negative
'init lock manager ',negative
'retrieve from side ',negative
'dummy registry does not cache information and forwards all requests metastore ',negative
'are only trying convert bucketmapjoin sortbucketmapjoin ',negative
'use int outputtypeinfo ',negative
'calculate all the arguments ',negative
'note explicit format use throwable instead varargs ',negative
'handle synthetic row ids for the original files ',negative
'use task attempt number from conf provided ',negative
'grpset col needs constructed ',negative
'just insert the record the usual way default the simple behavior ',negative
'find the argument the operator which constant ',negative
'inverse wordshifted for accuracy shift back here ',negative
'required required required optional optional ',negative
'keeps track completed dags queryidentifiers need unique across applications ',negative
'well treat this the aggregate col stats for partpart tab col ',negative
'setters ',negative
'',negative
'that would block ',negative
'validate connection ',negative
'pick the correct parent only one the parents not reducesink that what are looking for ',negative
'check different encryption zones ',negative
'get table logical schema row type note table logical schema non partition cols partition cols virtual cols ',negative
'weve already obtained lock the table dont lock the partition too ',negative
'create list one ',negative
'unregister task from the known and running structures ',negative
'clean out previous contents ',negative
'note that update uses dynamic partitioning thus lock the table not partition ',negative
'default timestamp format still works ',negative
'read data from the znode for this server node this data could either config string new releases server end ',negative
'get mapping tables columns used ',negative
'need store this record not done yet case should produce result ',negative
'todo dont want some random jars unknown provenance sitting around care ideally should try reuse jars and verify using some checksum ',negative
'handle password ',negative
'will not try partial rewriting for nonrebuild incremental rewriting disabled ',negative
'the client should have been cached already for the common case otherwise this may actually introduce delay compilation for the first query ',negative
'job vars ',negative
'lets take look the operator memory requirements ',negative
'since the operator tree dag nodes with mutliple parents will visited more than once this can made configurable ',negative
'always inc the batch buffer index ',negative
'coltype ',negative
'failed something that was rendered irrelevant while were failing ',negative
'confirm the file really fixed and replace the old file ',negative
'stringexpr uses boyer moore horspool algorithm find faster threadsafe because holds final member instances only see ',negative
'add any input columns referenced windowfn args expressions ',negative
'the values from timestampgettime ',negative
'txn write txn write txn write txn write ',negative
'form result from lower and middle words ',negative
'input not rewritten produces correlated variables terminate rewrite ',negative
'between and ',negative
'filter timestamp against long seconds double seconds with fractional nanoseconds ',negative
'test second argument with nulls and repeating ',negative
'add nothing more ',negative
'out allocated columns ',negative
'checking var exists and its value right ',negative
'hive ',negative
'table name will lower case unless specified hbasetablename property ',negative
'open the next file ',negative
'maximum tolerable variance number partitions between cached node and our request ',negative
'drops partitions batches partnotinfs split into batches based batchsize and dropped the dropping will through retryutilities which will retry when there failure after reducing the batchsize decayingfactor retrying will cease when maxretries ',negative
'corresponding the columns its size should the same columns for example table has two columns key and value may mask value reversevalue then ',negative
'objectregistry available via the this setup part the tez processor construction that available whenever instance the objectcache created the assumption that tez will initialize the processor before anything else ',negative
'create selectop with granularity column ',negative
'directly invoke execdriver ',negative
'adjust negative result again doing what does ',negative
'',negative
'need check there are overflow digits the high word ',negative
'list registered applications ',negative
'write vint using our temporary byte buffer instead paying the thread local performance cost ',negative
'for varchar char type return the max length the type ',negative
'for the grouping set corresponding the rollup ',negative
'bgenjjtree senumdef ',negative
'set the related attributes according the keys and values ',negative
'handles cases where the query has predicate constantcolumnname ',negative
'note are overwriting the constant vector value ',negative
'not impersonation cli mode ',negative
'compression ',negative
'record column names that needed stats for but couldnt ',negative
'complete split futures ',negative
'check whether one the operators part work that input for the work the other operator work merge work work work work work ',negative
'currently the unions are not merged each union has only parents nway union will lead union operators ',negative
'original bucket files delta directories and previous base directory should have been cleaned ',negative
'return the fullyqualified path path resolving the path through any symlinks mount point ',negative
'create database and table ',negative
'simulate different filesystems returning different uri ',negative
'arraylist ',negative
'note ddl way alter partition use the msc api directly ',negative
'todo jdk ',negative
'the following code used collect column stats when ',negative
'sourcet this not strictly speaking invlaid but does ensure that all columns from target table are all null for every row this would make any when matched clause invalid since dont have rowid the when not matched could meaningful but its just data from source satisfying sourcet not worth the effort support this ',negative
'nodes stale after this ',negative
'here only register the whole table for postexec hook present the case will register writeentity movetask when the list dynamically created partitions are known ',negative
'move the result getcolumns forward match the columns the query ',negative
'this verify that does not revert default scheme information ',negative
'the background operation thread was cancelled ',negative
'note distinct expr can part key ',negative
'note although hiveproxy has method that allows check were being called from the metastore from the client dont have initialized hiveproxy till explicitly initialize being from the client side have chickenandegg problem now track whether not were running from clientside the sbap itself ',negative
'split identify partition parts ',negative
'first throw away digits below round digit ',negative
'add the path the list input paths ',negative
'stop the appenders for the operation log ',negative
'redact sensitive information before logging ',negative
'and ',negative
'sharing this state assumes splits will succeed fail get together same also start with null and only set true the first call would only the globaldisable thing the first failure wthe api error not any random failure ',negative
'load properties from hive configurations including both spark properties ',negative
'mark the original abandoned dont need anymore ',negative
'write record byte buffer ',negative
'denom product all ndvs except the least all ',negative
'order expedite things general case are not actually going reopen anything instead will try give out existing session from the pool and restart the problematic one background ',negative
'iterate through each day the year make sure datedatewritablev match ',negative
'serialize the output info into the configuration ',negative
'map table alias rowcontainer ',negative
'nonjavadoc see ',negative
'not async wasnt submitted for some reason failure etc ',negative
'int can happen cases where grouping used without grouping sets all other cases should long ',negative
'files created windows machines have different line endings than files created unixlinux windows uses carriage return and line feed line ending whereas unix uses just line feed ',negative
'loginfomodifying config values for acid write true these props are now enabled elsewhere see commit diffs would better instead throw they are not set for exmaple user has set for some reason well run query contrary what they wanted but throwing now would backwards incompatible ',negative
'',negative
'new transactions should allowed open ',negative
'txns overlap could replace wstxnid with txnid though any decent should infer this make sure rhs join only has rows just inserted part this committxn and lhs only has committed txns and conflict but not and dont currently track writeset all ',negative
'event alter stats update event ',negative
'whether cache current rdd ',negative
'null first ',negative
'the table was dropped before got around cleaning ',negative
'indicate the read buffer has data for example when reading data disk could pull ',negative
'tests multimap structure for parquet ',negative
'some other key collision keep probing ',negative
'safe cancel delegation tokens now ',negative
'update column expression map ',negative
'make almost sure get definite order touch blocks order large number times ',negative
'class builder ',negative
'event ',negative
'any bigtablecandidates from multisourced bigtablecandidates should ',negative
'',negative
'scan the output directory for existing files and add watches ',negative
'note its not clear that need track this unlike poolmanager dont have nonpool sessions the pool itself could internally track the ses sions gave out since ',negative
'mix functions ',negative
'create tables and load data ',negative
'are calling this here because expect the method completely async also dont want this call itself thread because want the percenttophysics conversion logic consistent between all the separate calls one master thread processing round note allocation manager does not have cluster state wont update anything when the ',negative
'bgenjjtree fieldlist ',negative
'default value set milliseconds for test purpose ',negative
'the autocommit mode always enabled for this connection per jdbc spec ',negative
'constant propagation optimizer ',negative
'augment source with col which has will cause update target otherwise ',negative
'queryparallelism ',negative
'only one digit add leading ',negative
'todo can support date int any types which would have fixed length value ',negative
'obtain relevant object inspector for this typeinfo ',negative
'the final ptf ptfchain can stream its output then set the its outputshape the returned the ',negative
'remember rhs table for semijoin ',negative
'note that illegal and common default when the value missing ',negative
'time zone found file metadata property name writertimezone convert the timestamp that writer time zone order emulate time zone agnostic behavior not then the file was written older version hive convert the timestamp the servers reader time zone for backwards compatibility reasons unless the session level configuration set true which case assume was written time zone agnostic writer dont convert ',negative
'memory manager uses cache policy trigger evictions create the policy first ',negative
'find whether exists local driver accept the url ',negative
'shrinked size for this split counter part this normal mode whats different that this evaluated unit row using recordreadergetpos and that evaluated unit split using inputsplitgetlength ',negative
'methods need not called many times ',negative
'string including invalid style literal characters ',negative
'run spark dynamic partition pruning ',negative
'and mark group keys repeating ',negative
'return true case one the children column expr ',negative
'invalid partition exception ',negative
'note closing stmt object are also reverting any session specific config changes ',negative
'query that should return nothing ',negative
'spark installation provided use the sparksubmit script otherwise call the sparksubmit class directly which has some caveats like having provide proper ',negative
'first try our cast method that will handle few special cases ',negative
'getbytes function says pos the ordinal position the first byte the blob value extracted the first byte position ',negative
'save original projection ',negative
'statstask require table already exist ',negative
'todo error handling distinguish ioconnection failures attribute appropriate spill output ',negative
'convert just the decimal digits dot sign etc into bytes this much faster than converting the biginteger value from unscaledvalue which ',negative
'the table names match check the partitions ',negative
'verify ',negative
'postpone the join processing for this pair also spilling this big table row ',negative
'and one without ',negative
'look for normal match ',negative
'check were operating the same database not move ',negative
'get the tables the default database ',negative
'compareto ',negative
'since can rely approx lastaccesstime but dont want performance hit ',negative
'only bits but long behave unsigned ',negative
'cache size ',negative
'done only for nonviews ',negative
'create planner with hook update the mapping tables when node copied when registered ',negative
'remove the location container tokens ',negative
'this point transport must contain client ugi doesnt then its old client ',negative
'get the error details from the underlying exception ',negative
'compare sorted strings rather than comparing exact strings ',negative
'filtering old authorizer ',negative
'change the selected vector ',negative
'this string constant used alterhandler figure out that should not attempt update stats set any clientside task which wishes signal that stats ',negative
'sum size all files the partition smaller than size required ',negative
'copy required allow incremental replication work correctly ',negative
'bgenjjtree typedouble ',negative
'getcolumn instead using directly ',negative
'parent can understand this expression ',negative
'child keys are null empty bail out ',negative
'noop when authtype nosasl ',negative
'vectorized implementation broundcol function ',negative
'stats are same need update ',negative
'now convert acid ',negative
'ignore and hope for the best ',negative
'prepare stringbuilders for lists use onetomany queries ',negative
'get the base values cache ',negative
'databases created ',negative
'have just obtained all needed splitting some block now need put the space remaining from that block into lower free lists well put most one block into each list since blocks can always combined make largerlevel block each bit the remaining targetsized blocks count one block list offset from targetsized list bit index the merges here too since the block just allocated could immediately moved out then the resulting free space abandoned ',negative
'the stack has been explored already till that level obtained cached string ',negative
'get current bucket file name ',negative
'combine not operator with the child operator otherwise the following optimization from bottom could lead incorrect result such notx and not null should not optimized notx but null ',negative
'reuse existing text member char writable ',negative
'considered using urlencoder but seemed too much ',negative
'max this table either the the big table cannot convert ',negative
'set port ',negative
'setvalue should able handle null input ',negative
'newparts ',negative
'hltxnid means its associated with transaction ',negative
'createdrop functions are marked admin functions usage available functions query are not restricted sql standard authorization ',negative
'hash bits dont match ',negative
'need make sure that the this hiveservers sessions sessionstate stored the thread local for the handler thread ',negative
'try appending non extendable shard spec ',negative
'the import specification asked for only particular partition loaded load only that and ignore all the others ',negative
'stripe will satisfy the predicate ',negative
'for use ddl statements that require exclusive lock ',negative
'ideally would like this check based the number splits the absence easy way get the number splits this based the total number files pessimistically assumming that ',negative
'tez below transactional get the following ekoifman tree ext hiveunionsubdir orcacidversion delta bucket hiveunionsubdir orcacidversion delta bucket hiveunionsubdir orcacidversion delta bucket directories files ',negative
'check results need emitted results only need emitted there nonnull entry table that preserved there are nonnull entries ',negative
'rename for nonlocal file will transfering the original file permissions from source the destination else case mvfile where copy from source destination will inherit the destinations parent group ownership ',negative
'parse out sentences using javas texthandling api ',negative
'authorize all droppedpartitions one shot ',negative
'class ',negative
'possible ',negative
'javaobject primitives javaarray ',negative
'read parameters ',negative
'have issues with stats just scale linearily ',negative
'mgr true ',negative
'user asked for mapside join ',negative
'add counter default rounding ',negative
'high word gets integer rounding middle and lower longwords are cleared ',negative
'get the destination and check table ',negative
'dont check version its dry run ',negative
'finally add sort clause this needed for the update toksortby toknullsfirst toktableorcol cmvbasetable ',negative
'allowing fallback default timestamp parsing custom patterns fail ',negative
'metastore returns object type such global global when object specified such privileges are not applicable this authorization mode ignore them ',negative
'use simpleentry save the offset and rowcount limit clause key simpleentry offset ',negative
'mergefilework subclass mapwork dont need distinguish here ',negative
'walk through window expressions construct rexnodes for those ',negative
'nothing unregister ',negative
'bootstrap skip current table update ',negative
'first two stripes will satisfy condition and hence single split ',negative
'nonjavadoc see javalangstring javalangstring javalangstring javautillist ',negative
'use version existing max segment generate new shard spec ',negative
'assume dag dag and that its connected add direct dependencies ',negative
'create the aggregate ',negative
'move pending the allocator can release ',negative
'joda datetime only has precision millis cut off any fractional portion ',negative
'add files compare the arguments list ',negative
'calculate window size ',negative
'initialize one columns array entries ',negative
'estimate the size each entry datatype with unknown size stringstruct etc assumed bytes for now ',negative
'see can arrive smaller number using distinct stats from key columns ',negative
'logger console ',negative
'add column info for non partion cols object inspector fields ',negative
'get the tag value ',negative
'when minor compaction runs collapse per statement delta files inside single transaction longer need statementid the file name ',negative
'note fetchone doesnt throw new not supported ',negative
'',negative
'spot check decimal scalarcolumn modulo ',negative
'test whether precision will fit within decimal bit signed long with decimal digits ',negative
'instantiate empty list that dont error out iterator fetching were here then the next check pos will show our caller that that weve exhausted our event supply ',negative
'use construct ',negative
'libjars should only set sqoop autoshipped ',negative
'verify data and layout ',negative
'get the sign the big decimal ',negative
'rather unexpected usage exception ',negative
'matter whether has acquired not cannot pass exclusive ',negative
'push down limit through outer join note run this after ppd support old style join syntax select from left outer join where rxrx and ',negative
'save work initialized later with smb information ',negative
'skewedcolvalues ',negative
'check should use delegation tokens authenticate the call below gets hold the tokens they are set hadoop this should happen the mapreduce tasks the client added the tokens into hadoops credential store the front end during job submission ',negative
'this default implementation locking only works for incremental maintenance which only works for transactional manager thus cannot acquire lock ',negative
'this ast has only one child then partition spec specified ',negative
'add into conditional task ',negative
'hashmap javaobject ',negative
'tolerance for long range bias and for short range bias ',negative
'hive ',negative
'dump information call took more than sec ',negative
'number rows means that statistics from metastore not reliable and means statistics gathering disabled estimate only num rows since could actual number rows ',negative
'ptf variables ',negative
'start iterating through the foreign keys this list might contain more than single foreign key and each foreign key might contain multiple columns the outer loop retrieves the information that common for single key table information while the inner loop checks adds information about each column ',negative
'just from the back and throw away everything think wrong skip last item the file ',negative
'compare the results ',negative
'create the recordreader ',negative
'remaining size needed after putting files the return path list ',negative
'the currently read token beginning array object move stream forward skipping any child tokens till were the corresponding endarray endobject token ',negative
'fetch the row inserted after schema altered and verify ',negative
'set the current ugi fake user ',negative
'first start the queries from the queue ',negative
'ignore ',negative
'now try invalid alter table ',negative
'copy value ',negative
'properties passed the client used execution hooks ',negative
'the datastructure doing the actual storage during mapjoins has some per row overhead ',negative
'split around the tab character ',negative
'for possible series equal keys ',negative
'mapreduce jobs will run locally based data size ',negative
'check isrepeating handling ',negative
'partitionlist not evaluated until the run phase ',negative
'the original exception lost not changing the interface maintain backward compatibility ',negative
'for simplicity always have parents while storing pools flat structure well first distribute them levels then add level level ',negative
'acquire txn batch ',negative
'all fields have been parsed bytes have been parsed need set the startposition fieldslength ensure can use the same formula calculate the length each field for missing fields their starting positions will all the same which will make their lengths and uncheckedgetfield will return these fields nulls ',negative
'all reducesinkoperators this subtree this set used when start remove unnecessary ',negative
'the operator stack the dispatcher generates the plan from the operator tree ',negative
'the rewritten where clause ',negative
'construct the astnode for the column that will join with the outerquery expression for select from where select from this will build outerqueryexpr ast returned call buildsqjoinexpr ',negative
'contains aliases from subquery ',negative
'nonjavadoc this provides lazydouble like class which can initialized from data stored binary format see int int ',negative
'apply join order optimizations reordering mst algorithm join optimizations failed because missing stats continue with ',negative
'sds probably view ',negative
'relycstr ',negative
'the same day the month ',negative
'lock are trying acquire exclusive ',negative
'assuming this means are not doing auth ',negative
'unsupported inmemory structure ',negative
'bgenjjtree typei ',negative
'stringval ',negative
'ddlsemanticanalyzer has already checked partial partition specs are allowed thus should not need check here ',negative
'create the dest directory not exist ',negative
'since addition commutative can add any order ',negative
'its marked too many aborted already know need compact ',negative
'add the parameter here cannot change runtime ',negative
'now that reordered qbjointrees update leftaliases all ',negative
'the target table exists and newer same current update based repllastid then just noop ',negative
'multiple distincts not supported with skew data ',negative
'part also virtual column but part col should not this list ',negative
'create parent does not exist recreation not error ',negative
'now need descratchify this location get rid any scratchdd from the location ',negative
'read the entire data back and see did everything right ',negative
'get table ',negative
'iterate through all the elements pig schema and validations dictated semantics consult hcatschema table when need helps with debug messages ',negative
'add this partition postexecution hook ',negative
'nonjavadoc see javaioreader long ',negative
'skip method finding the method name didnt change and class name didnt change ',negative
'create expression tree with type cast from string timestamp ',negative
'just copy the information since there nothing far ',negative
'iterate over all the fields picking the nested structs within them ',negative
'additional combinations for each the second and third arguments are omitted have coverage all the source templates already ',negative
'date and time parts ',negative
'store the position the argument for the function the input ',negative
'when uri instance initialized creates bunch private string fields never bothering about their possible duplication would best could tell uri constructor intern these strings right away without this option can only use reflection fix strings these fields after uri has been created ',negative
'the grouping set key present after the grouping keys before the distinct keys ',negative
'see javadoc need clean the cache data anymore ',negative
'have insert into fooab parser will enforce that columns are listed toktabcolname present ',negative
'move with our counts ',negative
'need initialize the lock manager ',negative
'verify the fetched logs from the beginning the log file ',negative
'final string query ',negative
'enabledisable bitpacking ',negative
'all must selected otherwise size would zero repeating property will not change ',negative
'verify the scratch directories has been cleaned ',negative
'bitsetwordsinuse transient force dumping into lower form ',negative
'subset counters that should interest for when one wants limit their publishing nondisplay names should used ',negative
'unscaled ',negative
'clear state from previous txn ',negative
'alternate ',negative
'dir ',negative
'turn metastoreside authorization ',negative
'marked where the projected expr coming from that the types will become nullable for the original projections which are now coming out ',negative
'set the system properties needed pig ',negative
'interrupt the current thread after second ',negative
'null gets stored into column which binary field ',negative
'since sortmerge join only follow the big table ',negative
'fully contained topoffset toplimit bottomlimit ',negative
'backtrack bucket columns crs prs any ',negative
'note never instantiate task without taskfactoryget youre not okay with equals breaking doing via taskfactoryget makes sure that generated and two tasks the same type dont show equal which important for things like iterating over array without this dta dtb and dtc would show one item the list children thus were instantiating via helper method that instantiates via taskfactoryget ',negative
'authorization header must have payload ',negative
'retain only valid intersections discard disjoint ranges ',negative
'param had scheme not url ',negative
'whether the native vectorized map join operator has performed its ',negative
'whether current batch has any forwarded keys mapping index lined wkeys index the batch mapping index the batch linear hash result size the current batch ',negative
'interface they aggregate the size the aggregation buffer ',negative
'the unassigned batchindex for the rows that have not received nonnull value yet temporary work array ',negative
'cross with outer join currently not merge ',negative
'build calcite rel ',negative
'nonacid transactional conversion property itself must mutexed prevent concurrent writes see hive for use cases ',negative
'writing this there case where this could false this just protection from possible future changes ',negative
'this flow usually taken for repl load our input the result files listing should expand out files ',negative
'for subqueries the and alias should appended since same aliases can reused within different subqueries for query like select select from where subq join select from where subq ',negative
'for kerberos setup ',negative
'interface for vector map join hash table which could hash map hash multiset hash set for single byte array key ',negative
'after ',negative
'create this doas that gets security context based passed ugi ',negative
'nonjavadoc see javautilcalendar ',negative
'why this even metastore ',negative
'remove dpp predicates ',negative
'such abc ',negative
'found expression that can try reduce ',negative
'set insert and update dont set off but delete does ',negative
'the existing table newer than our update dont allow the update ',negative
'precondition check verify whether the table created and data fetched correctly ',negative
'metaconf cleanup should trigger event listener ',negative
'bitwise the bitvector with the bitvector the agg buffer ',negative
'when new buffer fetched resultsetnext should called more times ',negative
'key doesnt contain its encoded value from previous iterator ',negative
'one ',negative
'this method gets the basic stats from metastore for tablepartitions this will make use the statistics from optimizer when available execution engine tez spark optimization applied only during physical compilation because dpp changing the stats such case will get the basic stats from metastore when statistics absent metastore will use the fallback ',negative
'preallocated member for storing index into the hashmapresults for each spilled row ',negative
'remove this alias from the alias list ',negative
'print out nth partition key for debugging ',negative
'the schema the same then bail out ',negative
'test that the server code exists and responds basic requests ',negative
'this statement will attempt move kvtxt out stickybitdir user foo expected return ',negative
'dpp now look ndvs both sides see the selectivity parent opsselgbrsgbrs ',negative
'additional bits beyond bits the secondssinceepoch part timestamp ',negative
'equals ',negative
'hive doesnt have autoincrement concept ',negative
'get list joins which cannot converted sort merge join only selects and filters operators are allowed between the table scan and join currently more operators can added the method ',negative
'have filled digits and have more room our limit precision fast decimal however since are processing fractional digits rounding away ',negative
'the summary query returns only one row ',negative
'couldnt think good way reuse the keys and value objects without even more allocations take the easy and safe approach ',negative
'compute distribution ',negative
'arena cannot change after have marked released ',negative
'set partition cols tsdesc ',negative
'the token file location should first argument pig ',negative
'extraction can subset columns this the projection the batch column numbers ',negative
'ignore shutting down anyway ',negative
'simulate the join driving the test big table data our test small table hashmap and create the expected output multiset testrow testrow and occurrence count ',negative
'type name difference adornment ',negative
'have stream for included column but future might have data streams its more like has least one column included that has index stream ',negative
'dynamic partitioning with custom path resolve the custom path using partition column values ',negative
'the first call markfailed should have removed the record from compactionqueue repeated call should fail ',negative
'this doesnt get used but its still necessary see ',negative
'first segment granularity has here ',negative
'this filter that should perform comparison for sorted searches ',negative
'list partitions ',negative
'base bucket bucket deletedelta bucket bucket delta bucket bucket delta bucket ',negative
'these data structures are needed create the new project ',negative
'add the value the vector ',negative
'decimal validation ',negative
'verify create tablefunction calls only add foreign key constraints table ',negative
'case with nulls ',negative
'change sessions default queue tezq and rerun test sequence ',negative
'after constant folding ',negative
'separator for open txns separator for aborted txns ',negative
'for top constraining sel ',negative
'pick the first host always weak attempt cache affinity ',negative
'the key given user ignored case parquet need supply null ',negative
'check table params ',negative
'test that adding file the remote context makes available executors ',negative
'set determines that task complete ',negative
'assume this the table are now ',negative
'generate new task ',negative
'read big value length wrote with the value ',negative
'compute the number values want read this page ',negative
'pattern identify errors related the client closing the socket early idea borrowed from netty sslhandler ',negative
'sql standard return null for zero elements ',negative
'all columns need least subset the parentofparents bucket cols ',negative
'replace table scan operator ',negative
'added the multi group optimization ',negative
'the test table has rows total query time should ',negative
'assume the latter pretty high dont check for now ',negative
'generate the beeline args per hive conf and execute the given script ',negative
'',negative
'tsimplejsonprotocol does not support deserialization isbinariesaddfalse ',negative
'was not present the cache maybe because was added another ',negative
'prefer left cause right might missing ',negative
'prettier error messages frontend ',negative
'ddddddddd hhmmssnnnnnnnnn ',negative
'there sortmerge join followed regular join the smbjoinoperator may not get initialized all consider the following query smb join for the mapper processing the smj not initialized need close either ',negative
'simple one long key map join benchmarks build with mvn clean install dskiptests pdistitests main hive directory from itestshivejmh directory run java jar targetbenchmarksjar inner innerbigonly leftsemi outer rowmodehashmap rowmodeoptimized vectorpassthrough nativevectorfast ',negative
'recursively remove nonparent task from its children ',negative
'write out serialized plan with counters log file ',negative
'object inspector corresponding the input parameter ',negative
'already existing database ',negative
'now all txns are removed from minhistorylevel all entries from txntowriteid would cleaned ',negative
'delta writer ',negative
'default origin given time zone when aligning multiperiod granularities ',negative
'destroy before returning the pool ',negative
'set the inferred bucket columns for the file this filesink produces ',negative
'the project trivial raw join ',negative
'all them were true return true ',negative
'todo also account for tezinternal session restarts ',negative
'jdk ',negative
'test partition listing with partial spec specified but not ',negative
'rename fails the file with same name already exist ',negative
'stats from metastore only once ',negative
'greater than equal ',negative
'dictionary ',negative
'nonjavadoc see ',negative
'this should fail ',negative
'check rightinputrel contains correlation ',negative
'extra capacity case overrun avoid resizing ',negative
'try another table ',negative
'set the lock object with dummy data and then set needed ',negative
'evaluate the predicate expression ',negative
'sumvcolc ',negative
'int not yet supported ',negative
'read data ',negative
'timeout after reset ',negative
'future ',negative
'call the process function ',negative
'all the other fks ',negative
'use them ',negative
'complete transaction simulate writing partitions ',negative
'never resize the pool assume this initialization that changes might have make the factory interface more complicated ',negative
'get the usergroups for checking permissions based the current ugi ',negative
'middle word gets integer rounding ',negative
'check udfs against the when none allow all udfs for ',negative
'true only for operators which produce atmost output row per input row this will allow the output column names directly translated input column names ',negative
'avoid that will remove the and inserted enforce bucketingsorting ',negative
'need factor this prevent overwhelming spark executormemory ',negative
'add all distinct params note distinct expr can not part key assume plan ',negative
'the canaccept part this log message does not account for this allocation ',negative
'another task higher priority may have come during the wait lookup the queue again pick the task the highest priority ',negative
'for outer joins since the small table key can null when there match must have physical scratch column for those keys cannot use the projection optimization used inner joins above ',negative
'bfs ',negative
'todo handle compound partition keys partition when multiple window clauses are present same select even predicate can not pushed past all them might still able push below some them select from select key value avgcint over partition key sumcfloat overpartition value from where value select from select key value avgcint over partition key from select key value sumcfloat overpartition value from where value ',negative
'note only run for columns command and assume basic stats means col stats ',negative
'tolerance for estimated count ',negative
'they all modify primordial rows ',negative
'there are fields the struct ',negative
'test path sql enabled and broken ',negative
'timeout ',negative
'once the eventid reaches then just increment sequentially this avoid longer values ',negative
'the output column type string initialize the buffer receive data ',negative
'jump table figure out whether wait acquire keep looking since java doesnt have function pointers grumble grumble store character that well use determine which function call the table maps the lock type the lock are looking acquire the lock type the lock are checking the lock state the lock ',negative
'ptf declarations ',negative
'load upgrade order for the given dbtype ',negative
'the optimizer will automatically convert maponly job ',negative
'construct path pattern find all dynamically generated paths ',negative
'the case proxy users the getcurrentuser will return the real user for oozie due the doas that happened just before the server started executing the method getdelegationtoken the metastore ',negative
'undone used tests ',negative
'authority use default one applies ',negative
'hardcoded for reproducibility ',negative
'ensure txn timesout ',negative
'unique the store func and out file name table our case ',negative
'path format segmentoutputpath ',negative
'create alias work which contains the merge operator ',negative
'template classname valuetype ',negative
'output keys and aggregates into the output batch ',negative
'check permisssion partition dirs and files created ',negative
'druid table not external table store the schema metadata store ',negative
'inserting hive variables ',negative
'set ours ',negative
'now read all the ranges from cache disk ',negative
'hiveserver metadata api types ends here ',negative
'sign and ',negative
'that can only process records ',negative
'read from the stream using the protocol for each column final schema ',negative
'get key columns from inputs those are the columns which will distribute also the columns will sort ',negative
'owner testowner ',negative
'dont enforce during test driver setup shutdown ',negative
'statementcancel after resultsetclose should noop ',negative
'for every pattern ',negative
'mrv job tag used identify templeton launcher child jobs each child job will tagged with the parent jobid that launcher task restart all previously running child jobs can killed before the child job launched again ',negative
'throw error default value isnt what hive allows ',negative
'start with empty priv set ',negative
'the ctas query does specify location use the table location else use the location ',negative
'',negative
'verify that the schematool ran preupgrade scripts and ignored errors ',negative
'use any nonnull values found remember the remaining unassigned ',negative
'restrict with any ranges found from where predicates ',negative
'first insert round ',negative
'over the tables and populate the related structures have materialize the table alias list since might ',negative
'arrays never change will just shallow assignment here instead copy ',negative
'fastisbyte returns false ',negative
'come ride the api rollercoaster the best part that ctx has teztaskattemptid inside ',negative
'having seen the root operator before means there was branch the operator graph theres typically two reasons for that muxdemux multi insert muxdemux will hit the same leaf again multi insert will result into vertex with multiple operators ',negative
'incremental repl with alters dbtablepartition ',negative
'generate the meta data for key index for key ',negative
'nothing for the zkcreate models ',negative
'timestamp columncolumn ',negative
'generate output column names ',negative
'cleanup ',negative
'assignment the last thing the try happen assume success ',negative
'order determine the sum field type precisionscale for modepartial and modefinal ',negative
'the base table for the group matches the skewed keys ',negative
'case substring from index with length ',negative
'swallow the exception and let the call determine what ',negative
'abstract class for hash multiset result ',negative
'hcatdriverrundrop table tblname hcatdriverruncreate table junitsemanalysis int partitioned string stored rcfile hcatdriverrunalter table tblname add partition liststring partvals new arrayliststring partvalsadd mapstringstring map tblname ',negative
'create standard settable struct object inspector ',negative
'acquire all locks for given query atomically blocks all into remain waiting state wait will undo any acquire which may have happened part this metastore transaction and then record which lock blocked the lock were testing info ',negative
'validate query materialization materialized views query results caching this check needs occur before constant folding which may remove some ',negative
'create new inputformat instance this the first time see this class ',negative
'hdfs scratch dir ',negative
'dynamic partitioning usecase ',negative
'val sign significand exponent this sign unscaledvalue scale make valthis need scale updown such that unscaledvalue significand exponent scale notice that must the scaling carefully check overflow and ',negative
'drop every table the default database ',negative
'parallel edge was found for the given mapjoin need down further skip this operator pipeline ',negative
'move the clock forward and check the delayed queue ',negative
'whether number open transactions reaches the threshold ',negative
'subscribe feeds from the movetask that movetask can forward the list ',negative
'merge those that can merged ',negative
'reducers not benefit from llap point printing ',negative
'test initial metadata count metrics ',negative
'test null both sides ',negative
'drop ',negative
'override public partitionid throw new runtimeexceptionnot applicable ',negative
'calculate hat estimation the next digit ',negative
'currently only give the initial event the task the first heartbeat given that the split ready seems pointless wait but thats how tez works ',negative
'got here means its acquire info lock ',negative
'want preserve columnname was original input query that rewrite ',negative
'the persistent function discarded try reload ',negative
'record repeating and nulls state restored later ',negative
'the session will restarted and return ',negative
'this doesnt create partition ',negative
'test long ',negative
'column numbers batch corresponding expression result arguments ',negative
'serialization fails will throw incompatible metastore error the client ',negative
'the regex changed make sure compile the regex again ',negative
'reprocess the spilled data ',negative
'key and value objects are created once initialize and then reused for every getcurrentkey and getcurrentvalue call this important since rcfile makes assumption this fact ',negative
'nonnull confvar only defined confvars ',negative
'expected ',negative
'fail the retry ',negative
'decimal precision trailing zeroes ',negative
'note metrics have not been initialized this will return null which means arent ',negative
'writeids allocated txns under txnhwm then find writehwm from nextwriteid ',negative
'renaming test make test framework skip ',negative
'optional string key ',negative
'will try reuse this but session queued before ',negative
'hmgettable result will not have privileges set does not retrieve that part from metastore unset privileges null before comparing ',negative
'handle the different map definition parquet definition has group repeated group map mapkeyvalue required binary key utf optional binary value utf definition has groups optional group map repeated group map mapkeyvalue required binary key utf optional binary value utf ',negative
'loginfosearching for dynpathspec ',negative
'ignore ',negative
'inserts are not tracked writeset ',negative
'filesink cannot simply cloned requires some special processing subqueries for the union will processed independent mapreduce jobs possibly running parallel those subqueries cannot write the same directory clone the filesink but create subdirectory the final path ',negative
'allow ',negative
'this dfs file ',negative
'this config contains all the configuration that master node wants provide the hcatalog ',negative
'the command has associated schema make sure gets printed use ',negative
'the group args are passed add the violating value the error msg ',negative
'test equals operator for strings and integers ',negative
'code borrowed from ',negative
'add the expression into the bloomfilter ',negative
'get the selectoperator ancestor ',negative
'rewrite the above plan correlaterelleft correlation condition true leftinputrel projecta rexnode aggregate groupby aggagg projectb may reference covar filter references corvar rightinputrel correlated reference ',negative
'compressed ',negative
'the securitycontext set authfilter ',negative
'check the sizes neededcolumns and partnames here either size aggrstats null after several retries thus can ',negative
'default monday beginning the week ',negative
'nlines read all lines log file ',negative
'same for char ',negative
'using shared instance the umbilical server ',negative
'try again with some different data values ',negative
'default should set true the spark config ',negative
'insert into should skip and increment partition number ',negative
'add nonnull parameters the schema ',negative
'static partitions ',negative
'skim off the exponent ',negative
'nonjavadoc see int int ',negative
'using old config value tests backwards compatibility ',negative
'operationcompleted ',negative
'whether the cache has been initialized not ',negative
'one these runs the output each reducer ',negative
'column was not found table schema its new column ',negative
'user does not specify queue use session default ',negative
'set global member indicating where store not vectorized information necessary ',negative
'run our value expressions over whole batch ',negative
'this called once per dont get the starting duck count here ',negative
'authorization checks are performed the used ',negative
'tokreplication replid ismetadataonly ',negative
'only used spillbigtablerow ',negative
'amreporter after the server that gets the correct address knows how deal with ',negative
'just logged exception with case jdo humongous callstack make new one ',negative
'merge the two closest bins into their average location weighted their heights the height the new bin the sum the heights the old bins double ',negative
'nothing new added the queue while analyze runs ',negative
'are running local mode then the amount memory used the child jvm can longer default the memory used the parent jvm ',negative
'empty out the file ',negative
'methods setreset gettable modifier ',negative
'orientation ',negative
'get session update session allocation kill query destroy session restart session return session back pool move session different pool ',negative
'prefix partition with something avoid being hidden file ',negative
'note all the fields are only modified master thread ',negative
'create the walker the rules dispatcher and the context create walker which walks the tree dfs manner while maintaining the operator stack the dispatcher ',negative
'were inside replication scope then the table not existing not error ',negative
'must deterministic order map see hive use instead mapsnewhashmap ',negative
'find the minmax based the offset and length and more for original ',negative
'parameter value not changed false connection int smallint throws exception ',negative
'columninfos for table alias ',negative
'',negative
'get all partitions that matches with the partition spec ',negative
'null does equal null here ',negative
'reducer already ',negative
'try with erroneously generated void ',negative
'here create project for the following reasons gby only accepts arg position the input however need sum vcolc this can better reuse the function ',negative
'have extracted the existence from the hash set result dont keep ',negative
'this method should called movetask when there are dynamic partitions generated ',negative
'try not leave any files open ',negative
'string value stringengetvalue does not apply variable expansion does variable expansion ',negative
'add negative self ',negative
'table location should not printed with hbase backed tables ',negative
'seems that not used anything ',negative
'repl metadata export has repllastid and replscopemetadata import repl metadata dump table metadata changed allows override has repllastid ',negative
'may need setup localdir for relocalization which usually setup environmentpwd used for relocalization add the user specified configuration confpbbinarystream ',negative
'set the thread local username thrifthttpservlet ',negative
'called from multiple parallel threads aclentries modifiable the method will not thread safe and could cause random concurrency issues this test case checks the aclentries returned from hadoopfilestatus threadsafe not ',negative
'directly deserialize with the caller reading fieldbyfield serialization format the caller responsible for calling the read method for the right type each field after calling readnextfield reading some fields require results object receive value information separate results object created the caller initialization per different field even for the same type some type values are reference either bytes the deserialization buffer other type specific buffers those references are only valid until the next time set called ',negative
'commenting part hive and are not supported for clobs tablenames filter short assertequals tablenamessize ',negative
'get nanos since epoch fromzone ',negative
'',negative
'change the plan this structure projecta replace corvar input ref from join join left condition true leftinputrel aggregategroupby singlevalue projectb everything from input plus literal true projinputrel ',negative
'lock are trying acquire shared read ',negative
'find all the agg expressions use list for all countdistinct ',negative
'check for partition ',negative
'index where the buffer minallocation units headers array ',negative
'sort the list get sorted deterministic output for ease testing ',negative
'capture stderr ',negative
'constant just return ',negative
'because zero zero need mention javadoc ',negative
'first just allocate just the output columns will using ',negative
'this case weve determined that theres too much data prune dynamically ',negative
'the return type will the concatenation input type and original values type ',negative
'insert overwrite ',negative
'call isnt equal type and has been determined that value generate might required should rather generate value generator ',negative
'convert the column the correct type when needed and set row obj ',negative
'this valid error message ',negative
'for nested subqueries the alias mapping not maintained currently ',negative
'since demuxoperator may appear multiple times muxoperators parents list use newchildindextag instead childoperatorstag example join mux gby demux this case the parent list mux demux gby demux need have two childoperatorstags the index this demuxoperator ',negative
'testing nulls and repeating ',negative
'non aggregate mode analyze union operator ',negative
'inside should not care about its access info ',negative
'',negative
'its important read the correct nulls truth the path needed for splitgrouper ',negative
'prefix extend start ',negative
'debug only ',negative
'run constant propagation twice because after predicate pushdown filter expressions are combined and may become eligible for reduction like not null filter ',negative
'uniqueconstraints ',negative
'assume this always comes from user operation that took the lock ',negative
'for password based authentication ',negative
'build druid query ',negative
'lock ids are unique across the system ',negative
'allow this form ',negative
'start index since the variable from table column ',negative
'vertices elapsed time ',negative
'restored the renamed tables ',negative
'can only flush after the updateaggregations done the potentially new entry aggs can flushed out the hash table ',negative
'whatever ',negative
'',negative
'assertassertequals statlength ',negative
'validate the create view statement this point the createvwdesc gets all the information for semanticcheck ',negative
'first field the row key ',negative
'move marker according delta change delta ',negative
'update stats for transactional tables full acid with overwrite even though are marking stats not being accurate ',negative
'export trivially retriable after clearing out the staging dir provided ',negative
'prevent instantiation ',negative
'now having clause can contain subquery predicate invoke genfilterplan handle subquery algebraic transformation just done for subquery predicates appearing the where clause ',negative
'udaf filter condition groupby caluse param funtion etc ',negative
'clauses need combined keeping only common elements ',negative
'format partitionpval add only when table partition colname matches ',negative
'create lock but send the heartbeat with long delay the lock will get expired ',negative
'all the integer types float and double string char varchar ',negative
'currently commit after selecting the txns abort whether serializable readcommitted the effect the same could use for update select from txns and the whole performtimeouts single huge transaction but the only benefit would make sure someone cannot heartbeat one these txns the same time the attempt heartbeat would block and fail immediately after its unblocked with current multiple txns implementation possible for someone send heartbeat the very end the expire interval and just after the select from txns made which case heartbeat will succeed but txn will still aborted solving this corner case not worth the perf penalty the client should heartbeat timely way ',negative
'check for column encoding specification ',negative
'set temp location ',negative
'outer key copying only used when are using the input bigtable batch the output ',negative
'generate the temporary file name ',negative
'there may multi distinct clauses for one column ',negative
'for debugging ',negative
'check this table sampled and needs more than input pruning ',negative
'set the session configuration ',negative
'optional bytes token ',negative
'serializes object text for transport byte headerb encoded like below bytes magic string identify serialized stream bytes numbitvectors because bitvectorsize bytes are enough hold positions ',negative
'null filters are supported simplify client code ',negative
'step function increase the polling timeout every sec starting from ',negative
'bytes the higherorder bits come from the second vint that follows the nanos field ',negative
'metadata ',negative
'the case that return type the genericudf not boolean and not all partition agree result make the node unknown they all agree replace the node ',negative
'the entry relevant aborted txns shouldnt removed from txntowriteid aborted txn would removed from txns only after the compaction also committed txn open txn retained ',negative
'merge the other estimation into the current one ',negative
'provide instance the code doesnt try make real instance just want test that fail before trying make connector with null password ',negative
'for example select deptno count sumbonus mindistinct sal from emp group deptno becomes select deptno sumcnt sumbonus minsal from select deptno count cnt sumbonus sal from emp group deptno sal aggregate ',negative
'positive number ',negative
'extract the delegation token from the ugi and add the job ',negative
'unpartitioned table row for delete inserts are not tracked writeset ',negative
'need total poolthreadcount threads start same there are poolthreadcount threads thread pool and another one which has started them the thread which sees atomic counter poolthreadcount the last thread join and wake all threads start all once ',negative
'sessionid ',negative
'shutdown hiveserver has been deregistered from zookeeper and has active sessions ',negative
'theres some bogus code that can modify the queue name forceset for pool sessions ',negative
'for avro type the serialization class parameter optional ',negative
'prepare ',negative
'our dbname null were interested all events ',negative
'one and only one ',negative
'init exec and set parameters included ',negative
'txn batch ',negative
'add rows corresponding the grouping sets for each row create rows one for each grouping set key since mapside aggregation has already been performed the number rows would have been reduced moreover the rows corresponding the grouping keys come together there higher chance finding the rows the hash table ',negative
'outerrr belongs outer query and required resolve correlated references ',negative
'set the operator plan ',negative
'top distinct can always merge whether bottom distinct not top all can only merge bottom also all that say should bail out top all and bottom distinct ',negative
'hdfs temp table space ',negative
'the big table bucket file names small tables ',negative
'this serious black magic the following lines nothing afaict but without them the subsequent call listpartitionvalues fails ',negative
'since getbuckethashcode uses this hivedecimal return the old much slower but compatible hash code ',negative
'operator that handles the output these joinoperator ',negative
'remove from the running list ',negative
'the non explain code path dont need track query rewrites all add fns during plan generation are noops the get rewrite methods are called thrown ',negative
'this mapping which big table columns input and keyvalue expressions will ',negative
'large cross product generate the vector optimization using repeating vectorized row batch optimization the overflow batch ',negative
'not public since must have column information ',negative
'create the parameter declaration string ',negative
'map may not contain all sources since input list may have been optimized out nonexistent tho such sources may still referenced the tablescanoperator its null then the partition probably doesnt exist lets use table permission ',negative
'the file still cached ',negative
'set recursive traversal case the cached query was union generated tez ',negative
'zero check ',negative
'make look like streaming api use case ',negative
'for vectorized reduceside operators getting inputs from reduce sink the row object inspector will get flattened version the object inspector where the nested keyvalue structs are replaced with single struct example key reducesinkkeyint value colint colint would get converted the following for vectorized input keyreducesinkkeyint valuecolint valuecolint the exprnodeevaluator initialzation below gets broken with the flattened object inpsectors convert back the form that contains the nested keyvalue structs ',negative
'keepalive information the client should informed and will have take care resubmitting the work some parts fault tolerance here ',negative
'object that can take set columns row vectorized row batch and serialized ',negative
'dont change the table object returned the metastore well mess with its caches ',negative
'skewedinfo ',negative
'create more deltaxx that compactor has dir file compact ',negative
'write key element ',negative
'decided treat this map regular object ',negative
'needmerge ',negative
'return the key itself since mapping was availablereturned ',negative
'nothing can here ',negative
'propagate null values for twoinput operator and set isrepeating and nonulls appropriately ',negative
'since local jobs are run sequentially all relevant information already available therefore need fetch job debug info asynchronously ',negative
'there are nulls null array entries are already initialized ',negative
'this combination the jar stuff from conf and not from conf ',negative
'dont allow for public ',negative
'boundary case require least one nonpartitioned column ',negative
'generate the new queryid needed ',negative
'fail trying set transactional true but doesnt satisfy bucketing and inputoutputformat requirement ',negative
'consider query like insert overwrite table select from join tkey tkey where and are sorted and bucketed key into the same number buckets dont need reducer enforce bucketing and sorting for the field below captures the fact that the reducer introduced enforce sorting bucketing has been removed this case sortmerge join needed and the sortmerge join between and ',negative
'opoutputvertexname may null ',negative
'should not happen take care all existing types ',negative
'sign whether interval positive negative ',negative
'depending filesystem implementation flush may may not anything ',negative
'sleeptime milliseconds between batches delegation tokens dropped ',negative
'later can extend this the union all case well ',negative
'implement this logic using replacechildren instead replacing the root node itself because windowing logic stores multiple pointers the ast and replacing root might lead some pointers leading nonrewritten version ',negative
'cannot send the ecb consumer discard whatever already there ',negative
'set create time ',negative
'compare start position ',negative
'keys reserved for updating listenerevent parameters this key will have the event identifier that processed during event this event identifier might shared across other implementations ',negative
'make sure the broken signature doesnt work ',negative
'invalid cases ',negative
'this time completes adding remaining partitions ',negative
'queryid the command time which lock was acquired mode the lock explicitlock ',negative
'shift one ',negative
'rebuild that more efficient than the full rebuild ',negative
'',negative
'methods setreset listpartitionnames modifier ',negative
'must send vectorptfoperator ',negative
'its false need close recordreader ',negative
'scratch batch that will used play back big table rows that were spilled ',negative
'int ',negative
'insert event partitioned table with dynamic addpartition ',negative
'arbitrary dont expect caller hang out for mins ',negative
'replace with the milliseconds conversion ',negative
'child the type the column ',negative
'distribute the available memory between the tasks ',negative
'are using test specific database then just drop the database and recreate ',negative
'write back the final null byte before the last fields ',negative
'need close the dummyops well the operator pipeline not considered closeddone unless all operators are done for broadcast joins that includes the dummy parents ',negative
'this simulates that cleaning thread will error out while cleaning the notifications ',negative
'todopc validate that there are types that refer this ',negative
'test after recovery ',negative
'sign zero dot digits support toformatstring which can add lot trailing zeroes ',negative
'sort keys are specified use edge that does not sort ',negative
'conversion functions take single parameter ',negative
'will break the uncompressed data the cache the chunks that are the size the prevalent orc compression buffer the default maximum allocation since cannot allocate bigger chunks whichever less ',negative
'resets the aggregation calculation variables ',negative
'chararray unbounded hivepig lossless ',negative
'could scheduling guaranteed task when higher priority task cannot scheduled try take duck away from lower priority task here ',negative
'close called udtfoperator ',negative
'get serializable details the destination tables ',negative
'function try fold ',negative
'checkh for exists and not exists the sub query must have more correlated predicates ',negative
'string should have been truncated ',negative
'param currentkey the current key param currentvalue the current value ',negative
'tries get lock and gets waiting state ',negative
'',negative
'same jobs running sidebyside ',negative
'finally evaluate the aggregators ',negative
'sort bucket names for the big table ',negative
'special date formatting functions ',negative
'see the indexes for colstats ',negative
'now read the bigendian compacted twos complement int parts compacted means they are stripped leading and xffs this why the intlengthpos tricks bellow length all the bytes have read after skip skip pos where start reading the current int intlength how many bytes read for the current int ',negative
'remove this entry from the table usage mappings ',negative
'the group spans vectorizedrowbatch swap the relevant columns into our batch buffers write the batch temporary storage ',negative
'location should created for views ',negative
'this can only happen once decompress time ',negative
'should update currently refers the source database name ',negative
'part virtual ',negative
'verify that getnextnotification returns all events ',negative
'update startindex ',negative
'the from and strings havent changed dont need preprocess again regenerate the mappings code points that need replaced deleted ',negative
'get the internal array structure ',negative
'schema provided user and the schema computed pig the time calling store must match ',negative
'exception for type checking for simplicity constructing the row objectinspector ',negative
'nonjavadoc see ',negative
'populate target ',negative
'not from cache still need hook the plans ',negative
'done with this part ',negative
'give value ',negative
'explicitly create immutable lists here forces the guava lib run the transformations and not them lazily the reason being the function class used for transformations additionally also creates the corresponding replcopytasks which cannot evaluated lazily since the query ',negative
'example from hivedecimaladd header comments ',negative
'rebuild inplace the selected array with rows destine forwarded ',negative
'the number spark tasks executed the hiveserver since the last restart ',negative
'test more than one lock can handled lock request ',negative
'repeated lintstring ',negative
'throw new not sort order and unique ',negative
'batch allocation should always happen atomically either write ids for all txns allocated none ',negative
'the probe failed must allocate set aggregation buffers and push the keywrapperbuffers pair into the hash very important clone the keywrapper the one have from our keywrappersbatch going resetreused next batch ',negative
'dpp work considered child because work needs finish for execute ',negative
'evict key that not from this batch initial intended ',negative
'newtable has exist this point compile ',negative
'using endpoint identification algorithm https enables ',negative
'reset the buffer ',negative
'need update the keys ',negative
'since compilation always blocking rpc call and schema ready after compilation can return when are the running state ',negative
'theoretically the key prefix could any unique string shared between tablescanoperator when publishing and statstask when aggregating here use dbnametablename partitionsec the prefix for easy read during explain and debugging ',negative
'this break tie insert delete given row done within the same txn that currentwriteid the same for both events and want the delete event sort since needs sent that inputsplit options options can skip ',negative
'add the entry the cache structures while under write lock ',negative
'remove any cached results from the previous test ',negative
'case select use fetch task instead move task the select from analyze table column rewrite dont create fetch task instead create column stats task later ',negative
'the task has started all operators within the task have started ',negative
'nothing can done ',negative
'read the altered partition via cachedstore ',negative
'read the tag ',negative
'partitions need update ',negative
'should key and reversevalue ',negative
'abstract class for hash set result ',negative
'columns down the dag the lvj will transform internal column names from something like key col because this need undo this transformation using the column expression map the column names propagate the dag ',negative
'common hash code routines ',negative
'todo parse converttoacidsql make sure has alter table defaulttflat set tblproperties transactionaltrue converttommsql make sure has alter table defaulttflattext set tblproperties transactionaltrue ',negative
'zero easy ',negative
'reuse the same type for all only ivy can return more than one probably all jars ',negative
'assert mapjoinpos ',negative
'fail some inserts that have records txncomponents ',negative
'rcfile read ',negative
'test string column char literal comparison ',negative
'return the multiset count for the lookup key ',negative
'credentials can change across dags ideally construct only once per dag ',negative
'initialize load path ',negative
'newpart ',negative
'some hive features depends several configuration legacy build and add these configuration jobconf here ',negative
'note for collapse false this just sets keyssame ',negative
'int length outputgetlength offset ',negative
'this column not included ',negative
'fail compaction that have failed records ',negative
'note for now dont have seterror here caller will seterror throw ',negative
'resultdec assertassertequals ',negative
'set the hook that will disallow creating nonwhitelisted udfs anywhere the plan are not using specific hook for genericudfbridge that doesnt work minillap because the daemon embedded the client also gets this hook and kryo brittle ',negative
'careful maintenance the flag ',negative
'checks the value contains any the passwordstrings and yes return true ',negative
'java calendar index starting ',negative
'ignore error ',negative
'reset conf vars ',negative
'total characters byte length ',negative
'can not have correct table stats then both the table stats and column stats are not useful ',negative
'returns absolute offset the match ',negative
'take topk closest neighbors and compute the bias corrected cardinality ',negative
'whether optimize union followed select followed filesink creates subdirectories the final output should not turned systems ',negative
'find all the valid cookies associated with the request ',negative
'unable parse the connect command ',negative
'extract the conditions that can useful ',negative
'now back bed until its time this again ',negative
'not using stats and this the first sink the path meaning that should use stats infer parallelism ',negative
'repeating ',negative
'nonjavadoc see ',negative
'cartesian product our ranges the child ranges ',negative
'temp macros are not allowed have qualified names ',negative
'compare among potentially multiple matches ',negative
'insert one more row this should trigger reached for ttp ',negative
'this function will merge csold into csnew ',negative
'bloomfilter objectbitset objectdata long numbits int numhashfunctions int ',negative
'functioncat functionschem functionname remarks ',negative
'throw exception the tablepartition bucketed one the columns ',negative
'this happens either when the input file the big table changed closeop needs fetch all the left data from the small tables and try join them ',negative
'run cbo ',negative
'case there are multiple columns referenced the same column name wont ',negative
'parsing necessary the end the parents end move past parent field separator ',negative
'returns number lines the printed throwable stack trace ',negative
'single source can process multiple columns and will send event for each them ',negative
'match was found add the clause the corresponding list ',negative
'projections are handled using generate not the load ',negative
'operators that belong each work ',negative
'restore the previous properties for framework name address etc ',negative
'filters ',negative
'tracks containerids and taskattemptids can kept independent the running dag ',negative
'nonjavadoc see javaioreader long ',negative
'cleanup basefsdir since can shared across tests ',negative
'show locks partition dstoday ',negative
'cant happen ',negative
'get the maxlen ',negative
'used hook unions ',negative
'utc default ',negative
'explictly ignoring getter visibility any for autojson serialization trigger based getters ',negative
'try deserialize using serde class our writable row objects created serializewrite ',negative
'joda pattern matching expects fractional seconds length match the number the pattern you want match you need different patterns with sss ',negative
'the first bounds check requires least one more byte beyond for int hence parse the first byte vintvlong determine the number bytes ',negative
'confget takes care parameter replacement iteratorvalue does not ',negative
'create table and load kvtxt ',negative
'used determine cleaner thread already running ',negative
'table aliased select for example ',negative
'try again ',negative
'read the record with different record reader and evolved schema ',negative
'',negative
'verify that unpartitioned table rename succeeded ',negative
'execute set command ',negative
'add credential provider password the child processs environment ',negative
'scalar queries should expect valuecount less than ',negative
'expect query return error state ',negative
'processbatch size currentbatchsize numcols batchnumcols selectedinuse batchselectedinuse ',negative
'ignore this multiple clients may trying create the same partition addpartitiondesc has ifexists flag but its not propagated and throws ',negative
'have some sort expression tree try sql filter pushdown ',negative
'tell reducerecordsource flush last record this reduce side smb ',negative
'time based counters dag done already dont update these counters ',negative
'again split for base ',negative
'transactions just the header row ',negative
'found delete events this location needs compacting ',negative
'determine this previous the last slice need read for this split ',negative
'get int view the buffer ',negative
'from precision maxs maxps scale maxs ',negative
'total characters byte length ',negative
'level create col col countc for each branch ',negative
'helper class generate mocked response ',negative
'the primarys nextrecord the next value return ',negative
'when processing dynamic partitioned hash joins some the small tables may not get processed before the mapjoins parents are removed during gentezworkprocess this keep ',negative
'bgenjjtree functiontype ',negative
'topic alan ',negative
'figure out the newlocalordinal relative the newinput ',negative
'this should never happen just added the lock ',negative
'check hive returns results correctly ',negative
'create struct clause ',negative
'plan maybe null driverclose called another thread for the same driver object ',negative
'column attributes provided create list null attributes ',negative
'this means that update set something something can whatever supported select something ',negative
'check that src exists and also checks permissions necessary rename src dest ',negative
'this should fail once finds out the threshold has been reached ',negative
'throw some canfinish variations just for fun ',negative
'now copy over the data where isnullindex false ',negative
'byteminvalue ',negative
'first start with high message size limit this should allow connections ',negative
'this the data copy ',negative
'settabletreereader that can avoid this check ',negative
'should throw ',negative
'tasks finished but some failed ',negative
'try infer common primitive category ',negative
'types are different need check whether can convert them ',negative
'with repeating ',negative
'subexpr the list containing generated clauses result this optimization ',negative
'decimal column vectors gets the same weight long column vectors ',negative
'not failing the job due failure constructing the log url ',negative
'different from sql compat mode ',negative
'hold the aggregation results for each row the partition number rows processed the partition ',negative
'the buffer has nsync bytes ',negative
'infotype ',negative
'resultset serialization settings ',negative
'this method necessary synchronize lazycreation the timers ',negative
'ignore priority ',negative
'here build aux structure that used verify that the foreign key that declared actually referencing valid primary key unique key also check that the types ',negative
'should only one object ',negative
'make array with qbjointree outer most inner mostn ',negative
'cannot estimated sample runtime ',negative
'the write ids returned for the transaction batch also sequential ',negative
'total rows generate rows cache most percentile extra rows generate different thread ',negative
'finally decompress data map per and return caller ',negative
'from here out choose whether want run llap ',negative
'since only have single distinct call ',negative
'equals ',negative
'returned the fractional part not set ',negative
'see there are additional knownfragments there are more fragments came after this cleanup was scheduled and theres nothing done ',negative
'the query here insertinto and the target immutable table verify that our destination empty before proceeding ',negative
'set setting read column ids with empty list ',negative
'read from dumpfile and instantiate self ',negative
'check that all correlated refs the filter condition are ',negative
'calculate number different entries and evaluate ',negative
'alter table can change the type partition key now check the column name only ',negative
'rely the fact that poll checks interrupt even when theres something the queue the structure replaced with smth that doesnt must check interrupt here because hive operators rely recordreader handle task interruption and unlike most rrs ',negative
'for multiinsert queries thus nodeofinterest the from clause ',negative
'read the null terminator ',negative
'test when third argument has nulls ',negative
'skip writing tags when feeding into mapjoin hashtable whether this can forward records directly instead shufflingsorting ',negative
'array hash multiset results can lookups the whole batch before output result ',negative
'find the root all custom paths from custom pattern the root the largest prefix input pattern string that doesnt match custompathpattern ',negative
'shorter than the required pattern ',negative
'the session taken out the pool but waiting for registration ',negative
'last work weve processed order hook the current ',negative
'test various set methods and copy constructors ',negative
'yarn property spark yarn mode ',negative
'task kill while the request still pending state means the request should retried ',negative
'build the exprnodefuncdesc with recursively built children ',negative
'sum all nonnull long column values for avg maintain isgroupresultnull after last row last group batch compute the group avg when sum nonnull ',negative
'allocate little extra space limit need reallocate ',negative
'serializerclass ',negative
'find the absolute minimum transaction ',negative
'null null null ',negative
'increment cursor for elements per innot clause ',negative
'the passed argument matches somewhat closely with accepted argument ',negative
'the following members have context information for the current partition file being read ',negative
'row format terminated clause ',negative
'propertyfile file ',negative
'incorrect precision expected but was ',negative
'get the root operator ',negative
'filtering has been applied yet selectedinuse false meaning that all rows qualify true then the selected array records the offsets qualifying rows ',negative
'created hint skip ',negative
'are composing query that returns single row update happened after ',negative
'dimension ',negative
'includes the columns that have data ',negative
'always using the rowid column ',negative
'keeps track vertices from which events are expected ',negative
'nonjavadoc see ',negative
'create connection user ',negative
'the binary data ',negative
'return nulls for conversion operators ',negative
'cannot backtrack the expression bail out ',negative
'cant infer type ',negative
'measured nanoseconds from the epoch ',negative
'tokskewedlocations ',negative
'create batch with one string bytes column ',negative
'did find the filedir itself ',negative
'either there was nothing which could pushed down size there were complex predicates which dont support yet currently supported are one the form key size key size key and key size add residual ',negative
'aggexpr part distinct key ',negative
'support for dynamic partitions can added later ',negative
'now the operation with java bigdecimal ',negative
'optimize set add both ',negative
'semi join specific ',negative
'reset grace hashjoin context that there state maintained when operatorwork ',negative
'the jobtracker signalled that the threshold not exceeded ',negative
'check they are operating the same partition not move ',negative
'flush final partial batch ',negative
'check for substruct validity ',negative
'drop the tables when were done this makes the test work inside ide ',negative
'full stop byte ',negative
'create partitions ',negative
'this can only possible there merge work followed the union ',negative
'problem use new name ',negative
'last character ends token there are quotes all the text between the quotes considered single token this can happen for timestamp with local timezone ',negative
'input javawritable conversion needed ',negative
'right input positions are shifted newleftfieldcount ',negative
'value for the flag ',negative
'applydistinct ',negative
'case the expression regex col this can only happen without clause dont allow this for exprresolver the group case ',negative
'fileread writes the writer which writes orcwriter which writes cachewriter ',negative
'uses sampling which means its not bucketed ',negative
'this point everything that can consumed from appstatusbuilder has been consumed ',negative
'the absolute offset the beginning the key within the writebuffers ',negative
'table could the big table there need convert ',negative
'filter longdouble ',negative
'file ownershippermission checks should done the new table path ',negative
'read the record with different record reader ',negative
'execute process ',negative
'the last char escape char read the actual char ',negative
'referenced later ',negative
'getallwork returns topologically sorted list which use make sure that vertices are created before they are used edges ',negative
'debug instrumenter useful finding which fns get called and how often ',negative
'test readfully ',negative
'first check temp tables ',negative
'make sure qualify the name from the outset theres ambiguity ',negative
'column family primitive type for mapkey value can stored binary format pass the qualifier prefix cherry pick the qualifiers that match the prefix instead picking everything ',negative
'partitioned table query has only pruning filters ',negative
'index into list ',negative
'container affinity can implemented host affinity for llap not required until edges are used hive ',negative
'check parameter set validity public method ',negative
'didnt have predicate pushdown read everything ',negative
'load property file ',negative
'string comparison good enough since its form dateyyyymmdd ',negative
'main path found increfed ',negative
'such abcde ',negative
'reached this condition had replication state record for the object but its replacement has state disallow replacement ',negative
'sets the env variable value defined sets property simulate default credential ',negative
'setup our inner join specific members ',negative
'are going fail expensive stuff ranges are broken play safe ',negative
'total merge cost ',negative
'weve seen this already ',negative
'the fallback from failed sql jdo not possible ',negative
'some joins might null see processnode for leafnode clean them ',negative
'not modify the header here the caller will use this space ',negative
'skip leading zeroes word ',negative
'success ',negative
'and the new join rel ',negative
'note this path should specific concatenate never executed select query modify the existing move task already the candidate running tasks ',negative
'utils ',negative
'spill previously loaded tables make more room ',negative
'could not heartbeat the lock the operation has finished hence interrupt this work ',negative
'these will handled the output the table instead ',negative
'copy the tezsessionstate from the old clisessionstate ',negative
'closing the underlying bytestream should have effect the data should still accessible ',negative
'fetch the first group for all small table aliases ',negative
'stores mappings from local hdfs location for all resource types ',negative
'deserializerclass ',negative
'store the type for later retrieval ',negative
'the operator and need determine any the children are final candidates ',negative
'remove unnecessary expressions ',negative
'some debug information ',negative
'get parent schema ',negative
'string ',negative
'and must have locations within the table directory ',negative
'not much can about ',negative
'the max value number zeroes for bit hash can encoded using only bits will disable bit packing for any values ',negative
'will true there nonnull entry ',negative
'',negative
'the structure inside cte like this tokcte toksubquery may refer toksubquery ',negative
'obtain udaf name ',negative
'preempt only specific hosts preemptions already exist those ',negative
'partitionlist ',negative
'there grouping key and row came this operator ',negative
'parentcatalogname ',negative
'throw away middle and lowest words ',negative
'takes place during optimization ',negative
'bootstrap load which should also replicate the aborted write ids both tables ',negative
'default separators are indexed instead indexed thus the separator offset byte the separator for the hive row for the row struct and the maps and ',negative
'optional queryidentifier ',negative
'friday august ',negative
'read through all values ',negative
'reset everything ',negative
'populate the load file work that columnstatstask can work ',negative
'update join statistics ',negative
'dplb ',negative
'operator wants some work the end group ',negative
'there should only dummy object the rowcontainer ',negative
'combined lastaccess and ownertester paramparam ',negative
'skip the partitions progress and the ones for which stats update disabled could filter the skipped partititons out part the initial names query ',negative
'input file has header footer cannot splitted ',negative
'check that the output done ',negative
'create filter sqcountcheckcount instead project because relfieldtrimmer ends getting rid project ',negative
'read operation mode ',negative
'change could not retrieve for all partitions ',negative
'fileids ',negative
'parse the response message authzid utfnul authcid utfnul passwd ',negative
'disconnect the connection union work and connect merge work ',negative
'convert the search condition into restriction the hbase scan ',negative
'should merge join ',negative
'cmapintstring ',negative
'for the second one explicitly set location make sure ends the specified place ',negative
'array byte ',negative
'now make the select produce regular columnsdynamic partition columns with ',negative
'nop ',negative
'test that opening jdbc connection nonexistent database throws hivesqlexception ',negative
'first allocation write should add the table the nextwriteid meta table the initial value for write should and hence add with number write ids allocated here ',negative
'events insert last repl repldumpidxy ',negative
'since new statistics derived from all relations involved ',negative
'force locality ',negative
'add more complex types ',negative
'set the stats key prefix the same directory name the directory name can changed the optimizer but the key should not changed ',negative
'underflows large ',negative
'change choose the appropriate file system ',negative
'null result from all ranges ',negative
'there joinvalue joinkey has all null elements ',negative
'nonjavadoc see ',negative
'since have create space for find expired node will remove ',negative
'useful when the type column vector has not determined yet ',negative
'this parameter left for compatibility when reading existing configs removed druid ',negative
'note query priorities add them might here ',negative
'seems these two operators can merged check that plan meets some preconditions before doing particular the presence map joins the upstream plan cannot exceed the noconditional task size and already merged the big table cannot merge the broadcast ',negative
'when inittrue combine with will generate resolved parse tree from syntax tree readentity created under these conditions should all relevant the syntax tree even the ones without parents set mergeisdirect true here ',negative
'decimalformat longformatter new decimalformat ',negative
'read once gain access key and value objects ',negative
'check the contents second row ',negative
'trigger bootstrap dump which just creates table and other tables and constraints not loaded ',negative
'alter table commands require table ownership there should not output object but just case the table incorrectly added ',negative
'add key for reduce sink ',negative
'evaluate the aggregation over one the groups batches ',negative
'intentionally nothing ',negative
'idempotent case and just return ',negative
'clear out any rows the batch from previous partition since are going change the repeating partition column values ',negative
'todo see comments under blacklistnode ',negative
'ensures maps can deserialized when see for why that might used ',negative
'',negative
'sorting makes tests easier write since file names and rowids depend statementid this makes file name data mapping stable ',negative
'batching not enabled try add all the partitions one call ',negative
'for nonnative table property storagehandler should retained ',negative
'the length the scratch buffer that needs passed tobytes toformatbytes ',negative
'will inherit the name and status from the plan are replacing ',negative
'never call this function without first calling heartbeatlong long ',negative
'look for tables with empty pattern ',negative
'see committxn for more info this inequality ',negative
'table dropped after repl dump ',negative
'cache not yet prewarmed add this set which the prewarm thread can check that the prewarm thread does not add back ',negative
'future can reuse this conversion ',negative
'nonjavadoc see ',negative
'current split from the same file preceding split and the preceding split has footerbuffer ',negative
'subquery was only condition where clause ',negative
'filter projection need stage ',negative
'can make this configurable ',negative
'the start the split was the middle the previous slice ',negative
'create new union operator ',negative
'set different uri one the criteria deciding whether return the same client not uris are checked for string equivalence even spaces make them different ',negative
'get the hashtable file and path ',negative
'get conf from user payload ',negative
'used please ',negative
'replace the default input output file format with those found ',negative
'the number single value rows that were generated the big table batch ',negative
'hit the end after getting optional integer and optional dot and optional blank padding ',negative
'positive number ',negative
'for given row put into proper partition based its hash value when memory threshold reached the biggest hash table memory will spilled disk the hash table specific partition already disk all later rows will put into row container for later use ',negative
'just creating orc reader going sanity checks make sure its valid orc file ',negative
'retrieve primary key constraints cannot null ',negative
'table metadata ',negative
'numfalses ',negative
'finally not reduce the size input enough bail out ',negative
'drop tables ',negative
'since integer always some products here are not included ',negative
'same primitive category but different qualifiers rely sort out the type params ',negative
'may need peel off the genericudfbridge that added cbo user ',negative
'for other usually not used types just quote the value ',negative
'names ',negative
'table write for this operation ',negative
'return new project ',negative
'note schema evolution currently does not support column index changes ',negative
'this next section repeats the tests with maxlength parameter that exactly the number current characters the string this shouldnt affect the trim ',negative
'verify the output ',negative
'evaluate children only scalar false ',negative
'input fulltablename format dbnametablename ',negative
'when registered the system registry ',negative
'deserialize and append new row using the current batch size the index ',negative
'construct aggregation function info ',negative
'the the tracking node must sequential node ',negative
'one dead session with dryrun ',negative
'trailing spaces are significant ',negative
'right child ',negative
'',negative
'copy the data ',negative
'apply best effort fetch the correct table alias not found fallback old logic ',negative
'nonjavadoc see ',negative
'emitted from join operator will depend this factor ',negative
'note longer call addtasklocalfiles because all the resources are correctly updated the session resource lists now and thus added vertices something breaks might need called here ',negative
'prepare output descriptors for the input opt ',negative
'when kerberos enabled have add the accumulo delegation token the job that gets passed down the yarntez task ',negative
'all the sessions use that were not destroyed returned with failed update now die ',negative
'create table sets empty non null structures ',negative
'test repeating case nulls ',negative
'have cycle ',negative
'the sizes match prefer the table with fewer partitions ',negative
'partspec ordered hash map number dynamic partition columns number static partition columns path name corresponding columns the root path columns paths start from number buckets each partition ',negative
'asynchronously shutdown this instance hiveserver there are active client sessions ',negative
'repl status should return null ',negative
'need figure out the current transaction number and the list open transactions avoid needing transaction the underlying database well look the current transaction number first subsequently shows the open list thats ',negative
'note that this doesnt include appid assume that all the subsequent instances the same usercluster are logically the same all the paths will reused all the security tokensetc should transition between them etc ',negative
'insert reduceside ',negative
'checkpermissions returns false query not found throws failure ',negative
'set confoverlay parameters ',negative
'validation will later exclude vectorization virtual columns usage necessary ',negative
'output columns ',negative
'reference vectorization description needed for explain vectorization hash table loading etc ',negative
'files with write ids may not valid may affect snapshot isolation for ongoing txns well ',negative
'copy workdir ',negative
'the acl apis also expect the tradition usergroupother permission the form acl ',negative
'cleanup pathtoaliases ',negative
'the node may have been blacklisted this point which means may not the activenodelist ',negative
'iterate through any records because our read offset was past the stripe offset the rows from the last stripe will ',negative
'insert entries txntowriteid for newly allocated write ids ',negative
'adding keys pita theres way plug into timed rolling just create new fsm ',negative
'partcolsisnull ',negative
'owner ',negative
'prevent construction ',negative
'check for conditions that will lead local copy checks are are testing hive either source destination local filesystemfile aggregate filesize all source pathscan directory file less than configured size number files all source pathscan directory file less than configured size ',negative
'singleline ',negative
'setting statementid makes compacted delta files use ',negative
'depend linux openssl exit codes ',negative
'todo dont anything for now just log this for debugging may able make use this later for workload management ',negative
'update the lru node from what weve seen far ',negative
'for each input file ',negative
'retest waiting locks both have same ext ',negative
'operator needs invoke specific cleanup that operator can override ',negative
'the ptned table will not dumped gettable will return null ',negative
'widening cast not change ndv min max ',negative
'test validate that all tables exist the hms metastore ',negative
'how many rows this split ',negative
'match ',negative
'check for existence table ',negative
'update nondistinct groupby key value aggregations keycolx valuecolx ',negative
'hive though there are not restrictions hive table property key and could any combination the letters digits and even punctuations support conventional property name webhcat prepery name starting with letter digit probably with period underscore and hyphen only the middle like autopurge lastmodifiedby etc ',negative
'consider looked the possibility faster decimal double conversion using some their lower level logic that extracts the various parts out double the difficulty javas rounding rules are byzantine ',negative
'the reduce plan inputs have tags add all inputs that have tags ',negative
'number bits address registers ',negative
'are allowed interface has been added for the same ',negative
'already accounted for ',negative
'',negative
'current partitioned table with last partition replicated denoted ',negative
'key token start index ',negative
'try appending segment with conflicting interval ',negative
'first operator reduce task ',negative
'remove old join from child set all the rss ',negative
'methods that return scalar expressions ',negative
'',negative
'the job not empty but runs fast have wait until all the taskendjobend ',negative
'are currently performing binary search ',negative
'log final state consolelogger ',negative
'decompose join condition number leaf predicates ',negative
'the union operator has been processed ',negative
'constant list projection known length ',negative
'could have this protected method wno class but half hive static there ',negative
'less frequently set parameter not passing param ',negative
'bail out ',negative
'determine the size small table inputs ',negative
'this will make the object completely unusable semantics clear are not defined ',negative
'',negative
'must ',negative
'this method should called subclasses beforeclass initializer ',negative
'directly serialize fieldbyfield the binarysortable format this alternative way serialize than what provided binarysortableserde ',negative
'the following evaluate methods since are supporting scratch column reuse must assume the column may have nonulls false and some isnull entries true proper assignments ',negative
'hashcode should positive flip all the bits its negative ',negative
'immutable ',negative
'outer join specific ',negative
'schemapattern null means that the schemapattern value should not used narrow the search ',negative
'for now just adding true condition where clause can remove the where clause from the ast requires moving all subsequent children left ',negative
'smallintvalue ',negative
'this basically the same except that dont use base encode binary data because were using printable string delimiter consider such row straq str string the delimiter and ',negative
'set not implemented ignore ',negative
'they will file urls ',negative
'look because driverrun committed ',negative
'need check with instanceof instead just checking vectorized because the row can vectorizedrowbatch when fetchoptimizer kicks even the operator pipeline not vectorized ',negative
'change the children the original join operator point the map join operator ',negative
'the number keys with sequential duplicates collapsed both null and nonnull the batch ',negative
'more timedout txns ',negative
'write record parquet format ',negative
'done with all the things ',negative
'whitespace characters ',negative
'could renewed return that information ',negative
'returns true trailing slash needed appended the url ',negative
'task provided location preference got host since host full and only host left random pool ',negative
'delayed due temporary resource availability ',negative
'spot check decimal colscalar modulo ',negative
'log classpaths ',negative
'schedule task invalidate cache entry and remove from lookup ',negative
'need and check any the tasks running llap mode ',negative
'contains results from last processed input record ',negative
'remove the operators till certain depth ',negative
'the user has set false the idea was the number reducers are few the number files anyway are small however with this optimization are increasing the number files possibly big margin merge aggresively ',negative
'only worry about getting schema are dealing with avro ',negative
'full precision ',negative
'this assumes llap cluster owner always the user ',negative
'the number files for the table should same number buckets ',negative
'note this noop for custom udfs ',negative
'set marker that this conf has been processed ',negative
'dont instantiate ',negative
'get the kind expression ',negative
'',negative
'get unique skewed value list ',negative
'currently expressions are not allowed cluster distribute order sort clause for each the above clause types check ',negative
'generate unique for temp table path this path will fixed for the life the temp table ',negative
'with nulls and selected ',negative
'have tell apart partitions resulting from spec with different prefix lengths already have smth for the same prefix length can the two ',negative
'see hive for details ',negative
'the update has failed but the state has changed since then retry needed ',negative
'now the dumped path can one three things can dump which case expect set dirs each with name and with metadata file each and table dirs inside that can table dump dir which case expect metadata dump table question the dir and individual ptn dir hierarchy dump can incremental dump which means have several subdirs each which have the evid the dir name and each which correspond eventlevel dump currently only createtable and addpartition are handled all these dumps will tableptn level ',negative
'all inserts should basereader for normal read this should always delete delta not compacting ',negative
'patch the projection list for updates putting back the original set expressions ',negative
'pick any system properties that start with hive and set them our config this way can properly pull any hive values from the environment without needing know all ',negative
'for nonnative tables need exact match avoid hive the table location contains files and the string representation its path does not have trailing slash ',negative
'beeline ',negative
'comparison operations ',negative
'only our parent class can call this ',negative
'acid off there cant any acid tables nothing compact ',negative
'check and update partition cols necessary ideally this should done createvalue the partition constant per split but since hive uses and this does not call createvalue for each new recordreader creates this check required next ',negative
'function was properly called but threw its own exception unwrap and pass ',negative
'this child demuxoperator does not use tag just set the oldtag ',negative
'parameter was array primitives make sure the primitives are strings ',negative
'skip map processing for the first path element root array ',negative
'ignore ',negative
'only uses transactional and acid tables ',negative
'the same operator present times ',negative
'set some base data then stream some number partitions ',negative
'test that data inserted through hcatoutputformat readable from hive ',negative
'effect since ',negative
'have base the original files are obsolete ',negative
'formatteroff ',negative
'close the writer ',negative
'after bootstrap dump all the opened txns should aborted verify ',negative
'which field are start with consistent style with ',negative
'optional int aint ',negative
'base scan only ',negative
'valid merge register set size gets bigger also items ',negative
'dealing with string type ',negative
'move past union separator ',negative
'implementation notes since only local file systems are supported there need use hadoop version path class javanio package provides modern implementation file and directory operations which better then the traditional javaio are using here particular supports atomic creation temporary files with specified permissions the specified directory this also avoids various attacks possible when temp file name generated first followed file creation see for the description nio api and for the description interoperability between legacy api nio api avoid race conditions with readers the metrics file the implementation dumps metrics temporary file the same directory the actual metrics file and then renames the destination since both are located the same filesystem this rename likely atomic long the underlying support atomic renames note this reporter very similar would good unify the two ',negative
'scale fractional digits dot integer digits ',negative
'long count and double sum ',negative
'cant assume jdk implementing this explicitly return integercomparex integerminvalue integerminvalue ',negative
'not group across files case side work because there only reader per grouped split this would affect smb joins where want find the smallest key all the bucket files ',negative
'populate the groupby keys with the remapped arguments for aggregate the top groupset basically identity first fields aggregate ',negative
'containing the archived version the files ',negative
'objective here ensure that when exceptions are thrown hivemetastore api methods they bubble and are stored the objects ',negative
'replace ',negative
'evaluation the bytes constant vector expression after the vector ',negative
'consider validate type information ',negative
'use cbo and may apply maskingfiltering policies create copy the ast the reason that the generation the operator tree may modify the initial ast but need parse for second time would like parse the unmodified ast ',negative
'fields ',negative
'number entries store before being merged sparse map ',negative
'same file offset different lengths ',negative
'pig datetime can map date timestamp see which controlled hive target table information ',negative
'must call makeliteral not have the rexbuilderroundtime logic kick ',negative
'nonjavadoc see setting true correct only for special internal functions ',negative
'bucketing ',negative
'also remove the after the prefix ',negative
'destination table any true for full acid table and table should the destination table written using acid ',negative
'followed select star completely removed ',negative
'this point have arrived the level where need all the data and the ',negative
'otherwise the planner will throw exception different planners ',negative
'transaction manager ',negative
'numpartitionfields means random partitioning ',negative
'null input ',negative
'map values can primitive complex ',negative
'when splitupdate not enabled then all the deltas the current directories should considered usual ',negative
'retrievecd false not need deep retrieval the table column descriptor for instance this the case when are creating the table ',negative
'this not rebuild retrieve all the materializations turn not need force the materialization contents uptodate this not rebuild and apply the user parameters instead ',negative
'partvals ',negative
'get all the variable names being converted regex hiveconf using reflection ',negative
'theory the include path should come from the configuration ',negative
'this should connection ',negative
'required required required required optional optional ',negative
'trim the clob value max length int can hold ',negative
'test string ',negative
'number partitions for the chosen big table ',negative
'files size for splits ',negative
'hence the uncovered buckets not have any relevant data and can just ignore them ',negative
'the update options for outside the lock see below the synchronized block ',negative
'nonhive catalogs should not transactional ',negative
'start this server ',negative
'only scale adjustment needed ',negative
'iterate the global map and emit row for each key ',negative
'also allow lowercase versions all the keywords ',negative
'run query against nonacid table and shouldnt have txn logged conf ',negative
'used for ',negative
'bucketed just that get files ',negative
'update changed properties stats ',negative
'add ops existing collection ',negative
'event ',negative
'start log cleaner the start each test ',negative
'boolean that says whether the data distribution uniform hash not java hashcode ',negative
'have kerberos credentials ',negative
'table like tableacidtbl ',negative
'input provides the definition correlated variable ',negative
'copy uncompressed data cache put call moves position forward the size the data ',negative
'configuration ',negative
'not rexcall bail out ',negative
'date conversions supported genericudfdecimal ',negative
'call listlocatedstatus mockmocktable call check existence side file for mockmocktable call open mockmocktable call check existence side file for mockmocktable call open mockmocktable ',negative
'issupported ',negative
'add the previous nextkvreader back queue ',negative
'set host name conf ',negative
'verify that the provided object inspector can pull out these same values ',negative
'reset the previously supplied buffer that will receive the serialized data ',negative
'test the length first most cases avoid doing byte comparison ',negative
'overwrite ',negative
'checked for overflow based the outputtypeinfo ',negative
'init conf ',negative
'indexes during process call ',negative
'each iterator creates level encoding ',negative
'check the input projrel aggregate the entire input ',negative
'the table was not initially created with specified location ',negative
'add table via objectstore ',negative
'unexpected error placeholder tag not found throw ',negative
'add permanent udfs being used ',negative
'return hivesite jobconf ',negative
'createtable event with multiple partitions ',negative
'cancel the kills any avoid killing the returned sessions also sets the count for the async initialization ',negative
'load hash table for bucket mapjoin ',negative
'check cor var references are valid ',negative
'and the logs ',negative
'output instead input adding owner requirement output will catch that well ',negative
'initialize the rowbatchcontext ',negative
'check there data the resultset ',negative
'initialization ',negative
'still working ',negative
'input operator not the same position ',negative
'call the metastore get the currently queued and running compactions ',negative
'projection pruning this introduces select above hence needs run last due ',negative
'nonjavadoc see int javalangstring ',negative
'this total free memory available per executor case llap ',negative
'add interceptor that adds the xforwardedfor header with given ips ',negative
'not needed without semijoin reduction mapjoins when semijoins are enabled for parallel mapjoins ',negative
'cleanup the dag lock here since may have been created after the query completed ',negative
'the reader schema always comes without acid columns ',negative
'hightesttxnid ',negative
'this splitupdate initialize delete delta file path anticipation that they would write updatedelete events that separate file this writes file directory which starts with deletedelta the actual initialization writer only happens any delete events are written avoid empty files ',negative
'check explicit pool specifications valid cases where priority changed ',negative
'extract value from commaseparated keyvalue pairs ',negative
'attach for storage handlers ',negative
'for alter partition events ',negative
'both inputs repeat ',negative
'fieldtype means that this faked thrift field which does not serialize the field the stream result the only way get the field fall back the position the intention this hack fieldtype make real thrift prototype but there are lot additional work fulfill that and that protocol inherently does not support ',negative
'may resized later ',negative
'singleaggrel produces nullable type create the new ',negative
'let create more partitions total without any triggers ',negative
'extract the information necessary create the predicate for the new filter ',negative
'error stream always uses the default serde with single column ',negative
'hive requires this taskattemptid unique mrs taskattemptid composed the counterpart for spark should when therere multiple attempts for task hive will rely the partitionid figure out the data are duplicate not when collecting the final outputs see ',negative
'multiple newtags can point the same child when the child joinoperator first check contains the key childindex ',negative
'noarg ctor required for kyro serialization ',negative
'drop named primary key ',negative
'prefix the form dbnametblname ',negative
'check source partition exists ',negative
'singleton behaviour create the cache instance required ',negative
'this yields empty because starting index out bounds ',negative
'inspect the output type each key expression and remember the output columns ',negative
'try isrepeating path left input only nulls ',negative
'add direction token ',negative
'does any operator the tree stop the task from being converted conditional task ',negative
'single long key hash map optimized for vector map join ',negative
'grantinfo ',negative
'element for key byte hash table hashmap ',negative
'results ',negative
'map max nesting level one less because uses additional separator ',negative
'project can also generate constants need include them ',negative
'first argument charcount which consumed here ',negative
'the correct plugin ',negative
'hive doesnt have enum type were going treat them strings during the stage well check for enumness and ',negative
'lock used for synchronizing the state transition and its associated resource releases ',negative
'get all parents reduce sink ',negative
'give some time then dont delay shutdown too much ',negative
'column cannot push the predicate ',negative
'deallocated now will deallocated later ',negative
'group key ',negative
'field present partition but not table ',negative
'count cares not about nulls nor selection ',negative
'for clause ',negative
'end cleanup ',negative
'nothing currently ',negative
'update col stats map with col stats for columns from left side ',negative
'functiontype ',negative
'small table hts but since its idempotent should ',negative
'unknown type ',negative
'the table different dfs than the partition ',negative
'remove the last ',negative
'for comparison purposes can scale away those digits and can not scale since that could overflow ',negative
'simplifies things just add default ones for partitions ',negative
'order ',negative
'nothing matched see comment top ',negative
'scaled value might not equal but after scaling should ',negative
'ignore nsoe because that means theres nothing drop ',negative
'pending task which not finishable ',negative
'recurse ',negative
'deprecated hive values that are keeping for backwards compatibility ',negative
'assuming this only being done for join keys result shouldnt have recursively check any nested child expressions because the result the expression should exist ',negative
'only fetch the table have listener that needs ',negative
'removed this becomes performance issue ',negative
'must reuse super infogetpassword not accessible ',negative
'the value before the offset make byte segment reference absolute ',negative
'dont have file cache will add this one ',negative
'support for null constant object ',negative
'not date ',negative
'reenable the node preempted ',negative
'deadlock possible extreme cases not handled this will detected heartbeat ',negative
'regardless acquired waiting one shared write cannot pass another ',negative
'not empty means are creating basework whose operator tree contains union operators this case need save these baseworks and remove ',negative
'need spill from write buffer disk ',negative
'replicating then the partition already existing means need replace maybe the destination ptns repllastid older than the replacements ',negative
'str empty string ',negative
'remove the limit operator ',negative
'only used bucket map join ',negative
'there should delta dirs plus base dir the location ',negative
'this call deleting partitions that are already missing from filesystem parameter deletedata set false msck doing clean hms for some reason the partition already ',negative
'either delayedresources delayedlocality with unknown requested host request for preemption theres none pending single preemption pending and this the next task assigned will assigned once that slot becomes available ',negative
'the mapfield test multiple level map definition ',negative
'use the colfam and colqual get the value ',negative
'get evaluator for simple field expression ',negative
'the output has some extra fields set them null ',negative
'get the total number columns selected and for each output column store the base table points for insert overwrite table select tkey tkey udftvalue tvalue from join tkey tkey and tkey tkey the following arrays are created table mapping ',negative
'there are separate configuration parameters control whether merge for maponly job ',negative
'put existing column new list make sure the right position ',negative
'can never have more than this elements ',negative
'loj join preserves lhs types ',negative
'files size for splits ',negative
'perform some sanity checks the arguments ',negative
'file operations failed ',negative
'leave this ctor around for backward compat ',negative
'the value will have already been set before were called dont overwrite ',negative
'reader using blocking socket interrupt ',negative
'agentinfo ',negative
'heartbeatcount ',negative
'later the properties have come from the partition opposed from the table order support versioning ',negative
'remove the tag from key coming out reducer and store separate variable ',negative
'this first section repeats the tests with large maxlength parameter ',negative
'creates objects recursive manner ',negative
'since left integer always some products here are not included ',negative
'change the value for the next instance ',negative
'though intnum handed byte hcat the map will emit ',negative
'temporary ',negative
'ensure that both the partitions are the complete list ',negative
'guaranteed that there only list within this list because reduce sink always brings down the bucketing cols single list ',negative
'base only deltas ',negative
'marker comment look stats read ops ',negative
'safety check are merging join operators and there are postfiltering conditions they cannot outer joins ',negative
'since were provided qualifier prefix only accept qualifiers that start with this prefix ',negative
'',negative
'not external table ',negative
'used only for insert events this the number rows held memory before flush invoked ',negative
'remove env var that would default child jvm use parents memory default child jvm would use default memory for hadoop client ',negative
'elements ',negative
'set state current thread ',negative
'could verify precisely write time but just approximate allocation time ',negative
'write element element from the list ',negative
'because already confirm that the stats accurate impossible that the column types have been changed while the column stats still accurate ',negative
'timeout ',negative
'get all the values from getxxx methods ',negative
'map string string ',negative
'add the token the clientugi for securely talking the metastore ',negative
'colaccessinfo set only case semanticanalyzer ',negative
'exceptional use case for avro ',negative
'lsquareindex expression ',negative
'there are grouping keys grouping sets cannot present ',negative
'tokenidentifier ',negative
'ifexists currently verified ddlsemanticanalyzer ',negative
'strip off the file type any dont make gzcopy ',negative
'',negative
'the dests can have different nondistinct aggregations have iterate over all ',negative
'',negative
'there should valid txns newer list that are not also older all values oldinvalidids should also newinvalidids oldhwm newhwm then all ids between oldhwm newhwm should exist newinvalidtxns gap the sequence means committed txn newer list lists are not equivalent ',negative
'derby oracle ',negative
'noop ',negative
'test success exception caught ',negative
'the method will update vectorgroupbydesc ',negative
'add these values mixedup green null ',negative
'right pad longer strings with multibyte characters ',negative
'objectname ',negative
'rewrite logic change the collations field reference the new input ',negative
'set output column vector entry since have one output column the logical index ',negative
'more than digits ',negative
'third time ',negative
'get all parent tasks ',negative
'check the columns well value types the partition clause are valid ',negative
'setup timeouts for various services ',negative
'declared cursor ',negative
'fields populated from builder ',negative
'simple implementation for now currently parquet uses heap buffers ',negative
'start the cachedstore update service ',negative
'note returns decimal strings with more than digits ',negative
'this method should not synchronized can lead deadlocks since calls sync method meanwhile the scheduler could try updating states via synchronized method ',negative
'gets lock ',negative
'table properties not match currently not merge ',negative
'build map map the original filesinkoperator and the cloned filesinkoperators ',negative
'optional bytes historytext ',negative
'load data into table note filepath has local the hive server ',negative
'challenge how the math get this raw binary back our decimal form briefly for the middle and upper binary words convert the middleupper word into decimal long words and then multiply those the binary words power and add the multiply results into the result decimal longwords ',negative
'since only have one table with data dont compact tables ',negative
'peel off otherwise return itself ',negative
'not valid range add residual ',negative
'gets lock ',negative
'total characters byte length ',negative
'test with schema evolution and include ',negative
'also creates the root directory ',negative
'admin has already customized this list honor that ',negative
'try with supplementary characters ',negative
'the max column width too large reset max allowed column width ',negative
'gather references from original query this map from aliases references keep all references will need modify them after creating ',negative
'already verified that should have the rowid mapping ',negative
'add this task into task tree set all parent tasks ',negative
'this filter generated one predicates need not extracted ',negative
'two int one double two random ',negative
'since partval constant safe cast exprnodedesc its value should normalized format leading zero integer date format yyyymmdd etc ',negative
'select count ',negative
'essential properties that shouldnt overridden users ',negative
'catalogname ',negative
'dont generate for nullsafes ',negative
'temporary map only create one partition context entry ',negative
'for minor compaction there progress report and dont filter deltas ',negative
'this the functionality matches ',negative
'gets lock ',negative
'add writertimezone property file metadata ',negative
'add all function arguments map ',negative
'set for single threading ',negative
'just write out the value asis ',negative
'now read the relative offset next record next record always before the ',negative
'this may happen for queries like select source table ',negative
'can now retry adding keyvalue into hash which flushed but for simplicity just forward them ',negative
'create lazybinary initialed with inputba ',negative
'weird but need placeholder otherwise rename cannot move file the right place ',negative
'check for recreated ',negative
'catch make sure locks can released when the query cancelled ',negative
'read tbl via cachedstore ',negative
'should never reach here unless there were failed tasks ',negative
'todo some thoughts here have current todo move some these methods over messagefactory instead being here can override them but before move them over should keep the following mind should return iterables not lists that makes sure that can memorysafe when implementing rather than forcing ourselves down path wherein returning list part our interface and then people use size somesuch which makes need materialize the entire list and not change also returning iterables allows things like iterablestransform for some these should not have magic names like tableobjjson because that breaks expectation couple things firstly that serialization format although that fine for this jsonmessagefactory and secondly that makes just have number mappings one for each obj type and sometimes the case with alter have multiples also any eventspecific item belongs that event message event itself opposed the factory its okay have utility accessor methods here that are used each the messages provide accessors adding couple those here ',negative
'',negative
'ids used ensure two taskinfos are different without using the underlying task instance ',negative
'lazystring has socalled null sequence the value empty string not ',negative
'default return earliest possible state ',negative
'they are different throw error ',negative
'check for timed out remote workers ',negative
'returns the result file object which will contain the query results ',negative
'try merge multiple ranges together ',negative
'its void change the type byte because once the types are run through getcommonclass byte and any other type will resolve type ',negative
'methods setreset caller checker ',negative
'semishared for updatedelete ',negative
'extrapolation not needed ',negative
'colstats ',negative
'the vertex that this operator output ',negative
'assume here nobody will try get session before open returns ',negative
'create list topop nodes ',negative
'implicitly handles users providing invalid authorizations ',negative
'storage instantiated the constructor ',negative
'the list expressions after select select transform ',negative
'quotes use for quoting tables where necessary ',negative
'handle all the getreuse requests wont actually give out anything here but merely map all the requests and place them appropriate order pool queues the only exception the reuse without queue contention can granted immediately cant reuse the session immediately will convert the reuse normal get because ',negative
'for orc parquet all the following statements are the same analyze table partition compute statistics analyze table partition compute statistics noscan ',negative
'group stats colname for each partition ',negative
'fingerprint ',negative
'run this optimization early since expanding the operator pipeline ',negative
'the inputops this vertex ',negative
'myenumstringlistmap ',negative
'read database auth calls for each authorization provider ',negative
'returning either vectorized nonvectorized reader from the same call requires breaking ',negative
'need fill mapwork with local work and bucket information for smb join ',negative
'count how many fields are there ',negative
'prevents under subscription ',negative
'this may happen are not projecting any column from current operator think count where are projecting rows without any columns such case estimate empty row size empty java object ',negative
'use large prime number seed the random number generator javas random number generator uses the linear congruential generator generate random numbers using the following recurrence relation mod where the seed java implementation uses this problematic because not prime number and hence the set numbers from dont form finite field these numbers dont come from finite field any give and may not pair wise independent however empirically passing prime numbers seeds seems work better than when passing composite numbers seeds ideally javas random should pick such that prime ',negative
'initialize indexbuilder for deleteevents hive ',negative
'sampling predicates can merged with predicates from children because ppdppr already applied but clarify the intention sampling just skips merging ',negative
'dont make any calls but catalog calls until the catalog has been created just told ',negative
'relative end position the windowing can negative ',negative
'isolate query conf ',negative
'show partition level privileges ',negative
'the way this works such originalcolumnnames the equivalent getneededcolumns from tsop they are assumed the same order the columns orc file and they are assumed equivalent the columns includedcolumns because was generated from the same column list some point the past minus the subtype columns therefore when thru all the top level orc file columns that are included order they match originalcolumnnames this way not depend names stored inside orc for sarg leaf column name resolution see mapsargcolumns method ',negative
'create after dropping needed events ',negative
'this submit blocks background threads are available run this operation ',negative
'easier read logs and for assumption done replication flow ',negative
'load data ',negative
'expect all the levels have items ',negative
'returnpath and hivetestmode bail ',negative
'use the flajoletmartin estimator estimate the number distinct valuesfm uses the location the least significant zero estimate logphindvs ',negative
'use maxmin range ndv gets scaled selectivity ',negative
'add uri entity for transform script script assumed local unless downloadable ',negative
'tablecat tableschem tablename columnname datatype typename columnsize bufferlength unused decimaldigits numprecradix ',negative
'todo could actually store bit flag ref indicating whether this hash match probe and the former case use hash bits for first few resizes int hashcodeorpart oldslot newhashbitcount ',negative
'input aliases this for join used for ppd ',negative
'close the previous fsp longer needed ',negative
'decide skewed value directory selection ',negative
'minidfscluster litters files and folders all over the place ',negative
'new logic ',negative
'sessionscope compile lock ',negative
'for addition for subtraction ',negative
'helper class set chunkedinputoutput stream for testing ',negative
'merge currtask from multiple topops ',negative
'tblname can null cases helper being used higher abstraction level such with datbases ',negative
'one cannot simply reuse the session there are other queries waiting maintain fairness well try take query slot instantly and that fails well return this session back the pool and give the user new session later ',negative
'array level ',negative
'needs set these values should the work detangle this ',negative
'test for aborted transactions ',negative
'map from new tags indices children demuxoperator the first operator the ',negative
'helper function create vertex from mapwork ',negative
'bgenjjtree constmapcontents ',negative
'the table has implemented the project the obvious way creating project with fields strip away and create our own project with one field ',negative
'escaped byte unescape ',negative
'new merge ',negative
'are going use this counter pseudorandom number for the start the search this avoid churning the beginning the arena all the time ',negative
'cascade only occurs table level then cascade partition level ',negative
'remember any matching rows matchs matchsize the end the loop selected batchsize will represent both matching and nonmatching rows for outer join only deferred rows will have been removed from selected ',negative
'pool min ',negative
'check for nulls just safe ',negative
'bgenjjtree function ',negative
'todo check view references too ',negative
'special handling for count similar ',negative
'index has primitive ',negative
'get all partitions ',negative
'',negative
'parent current pipeline ',negative
'generate the mapside groupbyoperator for the query block the new groupbyoperator will child the inputoperatorinfo param mode the mode the aggregation hash param not null this function will store the mapping from aggregation stringtree the this parameter can used the nextstage groupby aggregations return the new groupbyoperator ',negative
'meanwhile the init succeeds ',negative
'save away the original ast ',negative
'end serves final separator ',negative
'safely sorting ',negative
'logger with int base ',negative
'not should value ',negative
'dont try operate with less than minsize allocator space will just give you grief ',negative
'get the number retries ',negative
'isexternal set false here can overwritten the import stmt ',negative
'this map type ',negative
'truncate table the wrong catalog ',negative
'zero special case ',negative
'unique key the filterinputrel ',negative
'already visited ',negative
'allocate map bucket grouped splits ',negative
'see the javadoc and ',negative
'when comparing the compressedowid the one with the lesser value smaller ',negative
'materialized views ',negative
'the name the hive column ',negative
'update partition column info descriptor ',negative
'current code version datas version datas version ',negative
'need flatten ',negative
'this can reasonable for empty txn startcommit readonly txn also iud with that didnt match any rows ',negative
'impossible get ranges for row aaa and row bbb ',negative
'whether the subdirectory has any file ',negative
'rename the partition directory not external table ',negative
'create the mapping between the output the old correlation rel ',negative
'scale length value ',negative
'have connection error the jdo connection url hook might ',negative
'with this option were assuming that the external application using the jdbc driver has done jaas kerberos login already ',negative
'naked ',negative
'setting the comparison greater the search should use the block ',negative
'output type hiveintervaldaytime ',negative
'these members hold the current value that was read when readnextfield return false ',negative
'add filters for each the uris supported templeton added the entire substructure using the mapreduce notification cannot give the callback templeton secure mode this because mapreduce does not use secure credentials for callbacks jetty would fail the request unauthorized ',negative
'turn off clientside authorization ',negative
'required required required required required required required required required required ',negative
'foreigntablename ',negative
'handled later only struct will supported ',negative
'this starts the reader the background ',negative
'nonjavadoc serializes one int part into the given link bytebuffer considering twos complement for negatives ',negative
'the sorted columns cant all found the values then the data only sorted the columns seen until now ',negative
'for each bucket file only keep its base files and store into new list ',negative
'wait until either all messages are processed maximum time limit reached ',negative
'this time fails when try load the foreign key constraints all other constraints are loaded ',negative
'create the additional vectorization ptf information needed the vectorptfoperator during execution ',negative
'there anything check here ',negative
'combine all predicates into single expression ',negative
'lru cache using linked hash map ',negative
'throwexception ',negative
'get groupby keys and store reducekeys ',negative
'bgenjjtree async ',negative
'todo make work for jdk use cleanerutil from ',negative
'serdeinfo ',negative
'timeout before reset ',negative
'probably the app does not exist ',negative
'this should not happen cannot merge ',negative
'apply this optimization the input query there cannot exist any order bysort clause thus existsordering should false there cannot exist any distribute clause thus existspartitioning should false there cannot exist any cluster clause thus ',negative
'create table with smallinttinyint columns load data and query from hive ',negative
'load conf files ',negative
'construct pattern the form where partval either the escaped partition value given input regex the form this works because the and separating key names and partition keyvalues are not escaped ',negative
'transport would have got closed via clientshutdown dont need this but just case make this call ',negative
'fetchfirst fetch again from the same operation handle with fetchfirst orientation ',negative
'guaranteed flag inconsistent based heartbeat another message should send ',negative
'note for now dont actually pass the queryforcbo cbo because accepts not ast and can also access all the private stuff rely the fact that cbo ignores the unknown tokens create table destination the query otherwise ',negative
'match multijoin join ',negative
'missing database the query ',negative
'',negative
'case the servers idletimeout set lower value might close its side ',negative
'init closeable utils case register not called see hive ',negative
'where null same where false ',negative
'the dispatcher fires the processor corresponding the closest matching rule and passes the context along ',negative
'typically targettmp typically target ',negative
'return further processing needed ',negative
'once the lines the log file have been fed into the errorheuristics see they have detected anything any has record what errorandsolution gave can later return the most ',negative
'type checking and implicit type conversion for join keys ',negative
'supported ',negative
'recursively remove task from its children tasks this task doesnt have any parent task ',negative
'this plugable policy chose the candidate mapjoin table for converting join sort merge join the policy can decide the big table position some the existing policies decide the big table based size position the tables ',negative
'true prune ',negative
'this will return null the metastore not being accessed from metastore thrift server the ttransport being used connect not instance tsocket kereberos ',negative
'filter ',negative
'adjust right collation ',negative
'aggregation buffer methods wrap aggregation buffer ',negative
'the partitions used ',negative
'write delta file partition ',negative
'set hivelockmgr null just case this invalid manager got set next querys ctx ',negative
'session string supposed unique its got some reasonable size ',negative
'put records into compactionqueue and nothing ',negative
'last split ',negative
'location json file ',negative
'closed state not interesting state before finished error ',negative
'distributed attribute with distinct values ',negative
'case failure send back whatever constructed far which would from the appreport ',negative
'load the version stored the metastore ',negative
'mapjoin ',negative
'create the parents first ',negative
'set the default ',negative
'store partition key expr mapwork ',negative
'set the extra fields null ',negative
'',negative
'are doing this both load table and load partitions ',negative
'using vint instead bytes parse the first byte vintvlong determine the number bytes ',negative
'poll the tasks see which one completed ',negative
'generate column access stats required wait until column pruning ',negative
'use ddlexclusive cause lock prevent races between concurrent add partition calls with not exists this concurrent calls add the same partition may both add data since for transactional tables creating partition metadata and moving data there are separate actions ',negative
'null out the remaining columns ',negative
'endreason shows other for containertimeout ',negative
'first calls resultsetnext should return true ',negative
'parent statistics null then that branch the tree not walked yet dont update the stats until all branches are walked ',negative
'bgenjjtree ',negative
'retrieve stats from metastore ',negative
'aborted terminal state nothing about the txn can change after that read committed sufficient ',negative
'while scan the css also get the densityavg lowerbound and ',negative
'processing directories ',negative
'test set random multiplications high precision ',negative
'enable read authorization metastore ',negative
'fetch the xml tag dogcxxxdogc ',negative
'destination hash partition has just spilled ',negative
'validate mainly for includes excludes working they should ',negative
'could rewrite into subquery ',negative
'check there are ioexceptions ',negative
'rightinputrel has this shape filter references corvar ',negative
'total free memory maxmemory used memory ',negative
'notice that command line options take precedence over the deprecated old style naked args ',negative
'update top project positions ',negative
'second input parameter but column ',negative
'refer flajoletmartin for the value phi ',negative
'doublecheck ',negative
'process singlecolumn long leftsemi join vectorized row batch ',negative
'range register index bits ',negative
'empty keyset basically ',negative
'from zookeepermain private method ',negative
'lump all partitions outside the tablepath into one partspec ',negative
'drop table from the wrong catalog ',negative
'the table name can potentially dotformat one with column names specified part the table name abc where column and field the objectcolumn etc for authorization purposes should use only the first part the dotted name format ',negative
'the following fields specify the criteria objects for this priv required ',negative
'the session will with the new mapping check ',negative
'first lets try connecting using the last successful url that fails then error out ',negative
'use construct ',negative
'set alias size mapping this can used determine one table chosen big table whats the total size left tables which ',negative
'set some conf parameters ',negative
'empty regex should one warn message ',negative
'invariant ksize ksize ',negative
'generate split strategy for nonacid schema original files any ',negative
'add the group expressions ',negative
'delete unneeded directories that were replaced other ones via reopen ',negative
'verify output ',negative
'class ',negative
'which affects the locality matching ',negative
'nonjavadoc see ',negative
'return false any input column nonrepeating otherwise true this returns false all the arguments are constant there are zero arguments possible future optimization set the output isrepeating for cases allconstant arguments for deterministic functions ',negative
'modifying the meastoreuri property ',negative
'output job properties ',negative
'start with the destination bucketedsorted position the source the column corresponding that position key which maps column which also bucketedsorted into the same ',negative
'vectorization should the last optimization because doesnt modify the plan any operators makes very low level transformation the expressions ',negative
'analyze create view command ',negative
'matches only groupbyoperators which are reducers rather than map group operators ',negative
'check the local work ',negative
'this changed under tmp dir not sure this will have any effect ',negative
'just pass everything that syntax supports ',negative
'spill ',negative
'the lock may have multiple components dbhivelock hence need check for each them ',negative
'check that the error code present the error description ',negative
'encountered column field which cannot found sources ',negative
'todo hive ',negative
'txnid ',negative
'mgby already contains key remove distinct and change all the others ',negative
'todo allow defaults for scheduling policy ',negative
'someone using this buffer eventually will evicted ',negative
'cost cost writing intermediary results local cost reading from local for transferring join ',negative
'this time completes adding just constraints for table ',negative
'for uncompressed case need some special processing before read basically are trying create artificial consistent ranges cache there are cbs uncompressed file the end this processing the list would contain either cache buffers buffers allocated and not cached are only reading parts the data for some ranges and dont want cache both are represented ',negative
'recursively going into objectinspector structure ',negative
'set global separator member next level ',negative
'generate split for any buckets that werent covered this happens the case where bucket just has deltas and ',negative
'introduce some randomness and avoid hammering the metastore the same time same logic dbtxnmanager ',negative
'convert everything writable types arguments are the same but objectinspectors are different ',negative
'first separated substring would txnid and the rest are ',negative
'rerun insert into but this time new partitions will created there will violation ',negative
'object that added the cache ',negative
'multi group optimization specific operators ',negative
'drop case leftover from unsuccessful run ',negative
'string ',negative
'because null literal doesnt work for all dbs ',negative
'get new location ',negative
'were looking for the udf with the smallest maximum numeric type ',negative
'partition can archived during recovery ',negative
'encoding still sparse use linear counting with increase accuracy use pprime bits for register index ',negative
'test the udf adaptor for generic udf opposed legacy udf ',negative
'annotation tree with statistics ',negative
'add the reducer ',negative
'flush here the memory usage too high after that have the entire ',negative
'are provided with prefix ',negative
'create external table ',negative
'test that adding jar the remote context makes show the classpath ',negative
'open txn which allocate write and remain open state ',negative
'map mode run iff work map work ',negative
'xmx specified ',negative
'the current txn either open aborted state ',negative
'stub out mocked helper instance ',negative
'prep ',negative
'call reducesinkoperator with new input inspector ',negative
'set timezone based user timezone origin not already set default hive time semantics consider user timezone ',negative
'reconstruct the sparse map from delta encoded and varint input stream ',negative
'for each field ',negative
'stats are not available just assume its useful edge ',negative
'lets see doubles work ',negative
'java ',negative
'currrecord numrecords have already fetched the top numrecords ',negative
'alias operator map from the semantic analyzer ',negative
'output batch scratch columns for the small table portion ',negative
'all the parents are locked shared mode ',negative
'backup task ',negative
'connect using token via beeline with inputstream ',negative
'this test session temporary files are cleaned after hive ',negative
'tokdestination toktab toktabname materializationname ',negative
'the caller needs gurrantee that they are the same type based numbitvectors ',negative
'',negative
'action ',negative
'once sessionstate for thread set clidriver picks conf from ',negative
'minute granularity ',negative
'max ',negative
'only first call throws exception ',negative
'isdead only set internally this class link markdeadboolean will abort all remaining txns make this noop make sure that wellbehaved client that calls aborttransaction error doesnt get misleading errors ',negative
'handle secure connection specified ',negative
'test normal retriable client ',negative
'walk relnode graph note from where gby nodes ',negative
'take look see escaped ',negative
'updating several local structures ',negative
'unwrap the tuple ',negative
'now rewrite the plan projecta all lhs plus transformed original projections replacing references count with case statement correlatorleft correlation condition true leftinputrel aggregate groupby agg agg ',negative
'all the inputs for the tez processor ',negative
'write fourth byte header ',negative
'deserializes bit decimals the maximum bit precision decimal digits note major assumption the input decimal has already been bounds checked and least has precision not bounds check here for better performance ',negative
'unregister from the amreporter since the task now running ',negative
'just trigger auto creation needed metastore tables ',negative
'not partitioned ',negative
'txnid ',negative
'make sure not the default that are testing tblproperties indeed propagate ',negative
'this assumes that the llap cluster and session are both running under user ',negative
'column stats will inaccurate ',negative
'remove map extra level ',negative
'make sure reduce task environment points ',negative
'this what expect disk ekoifmanwarehouse ekoifman tree nonacidpart nonacidpart hiveunionsubdir hiveunionsubdir hiveunionsubdir directories files ',negative
'sum input and output are decimal any mode partial partial final complete ',negative
'finally add the event broadcast operator ',negative
'means that expression cant pushed either because value group ',negative
'convert input arguments text necessary ',negative
'table write already allocated for the given transaction then just use ',negative
'once have decided the map join the tree would transform from join mapjoin big table small table for spark ',negative
'read from the new table ',negative
'rest the types date char varchar etc are already registered ',negative
'traverse the leaf nodes the tree the stack entries indicate the existing leaf ',negative
'the byte length the scratch byte array that needs passed ',negative
'event ',negative
'reset the aggregations for distincts optimization with sortingbucketing perform partial aggregation ',negative
'config logj with customized files ',negative
'the new session will also now ',negative
'get the serialization object and the class being deserialized ',negative
'kill the second ctrlc ',negative
'let the job retry several times which eventually lead failure ',negative
'prscrscgbyr map aggregation prscgbyrcomplete revert expressions cgbyr that crs ',negative
'check dpp branches are equal ',negative
'configure http client for kerberospassword based authentication ',negative
'allow for improved schemes ',negative
'already called verify there input split thus for groupbyoperator summary row set finaldirs and add dummy split here ',negative
'discard all the locked blocks ',negative
'closeop can overriden ',negative
'triggername ',negative
'its possible that user session closed while creating spark client ',negative
'zone part ',negative
'acid and tables support load data with transactional semantics this will allow load data txn assuming can determine the target suitable table type ',negative
'all the operators need initialized before process ',negative
'bug throws exception ',negative
'reduce side gby dont know the grouping set was present not get from map side gby ',negative
'secure the web server with kerberos ',negative
'use the new faster hash code since are hashing memory objects ',negative
'should not allowed after query complete received ',negative
'whether pattern sel gby dpp ',negative
'float ',negative
'otherwise return the expression ',negative
'txnid ',negative
'open the original path weve been given and find the list original buckets ',negative
'determine input type info ',negative
'first get the appropriate field schema for this field ',negative
'call the actual operator initialization function ',negative
'from ',negative
'the set virtual columns that vectorized readers may support ',negative
'verify that environment context has statsgenerated set task ',negative
'extract innerrecord field refs ',negative
'pmod calculation can overflow based the type arguments casting the arguments according outputtypeinfo that the results match with genericudfposmod implementation ',negative
'change connector ssl used ',negative
'skip leading zeroes word ',negative
'need rollback because did select that acquired locks but didnt actually update anything also may have locked some locks acquired that now want not acquire its rollback because once see one wait were done wont look for more only rollback savepoint because want commit our heartbeat changes ',negative
'teardown the cluster ',negative
'since may split current task use preorder walker ',negative
'setstring can override this ',negative
'check whether the have the same schema ',negative
'put the exe context into all the operators ',negative
'first row determines isgroupresultnull and decimal firstvalue stream fill result repeated ',negative
'first get the utc midnight for that day which always exists small island sanity ',negative
'since old orc format doesnt support binary statistics ',negative
'bytes are same case was longer ',negative
'create test tables with partitions ',negative
'delete delta file with delete events ',negative
'save ',negative
'failures will not retried avoid fork exec running sysctl command ',negative
'decompose the incoming text row into fields ',negative
'was internal column lets try get name from columnexprmap ',negative
'deserializes from string fastbitset creates object and returns ',negative
'case column stats grouping sets ',negative
'read the relative offset word the beginning and beyond records ',negative
'disable backtracking ',negative
'operator file sink reduce sink something that forces ',negative
'obtain table props query ',negative
'this stream for entire stripe and needed for every uncompress once and reuse ',negative
'coming from reducesink the aggregations would the form valuecol valuecol ',negative
'construct setop output using original left right input ',negative
'preallocated member for storing index into the hashsetresults for each spilled row ',negative
'topn will cause shortcircuit dont need any initialization ',negative
'doublevalue ',negative
'whether this rebuild rewritten expression ',negative
'add the output dir the watch set scan and cancel current watch ',negative
'its corresponding tablescanoperator ',negative
'logically each bucket consists copy copyn etc dont know priori this true then the current split from copyn file its needed correctly set maxkey particular set maxkeynull this split the tail the last file for this logical bucket include all deltas written after nonacid acid table conversion todo hive also see comments link originalreaderpair about unbucketed tables ',negative
'set and parse the row ',negative
'finally are going use ',negative
'retry once ',negative
'propagate the cluster name the script ',negative
'craete table and check dir ownership ',negative
'check can merge mapjointask into that child ',negative
'making sure this not initialized unless needed ',negative
'nonjavadoc see ',negative
'authorize the grant ',negative
'same idea only set for nonnative tables ',negative
'the next parseddelta may have everything equal the prev parseddelta except the path this may happen when have split update and have two types delta directories deltaxy and deletedeltaxy for the same txn range ',negative
'nonjavadoc see int int int ',negative
'increment the createdfiles counter ',negative
'update has failed should try task ',negative
'may statementid ',negative
'does not change the output ordering from the inputs ',negative
'process column constraint ',negative
'this input the big table contained the big candidates set and either have not chosen big table yet has been chosen the big table above the cumulative cardinality for this input higher ',negative
'assumes stored data schema acid ',negative
'process the current data point ',negative
'and ',negative
'have essentially deallocated this ',negative
'highvalue ',negative
'default not wait ',negative
'create acid table with dbtxnmanager ',negative
'data stream could empty stream already reached end stream before present stream ',negative
'only populate corrupt ids for the things couldnt deserialize are not using ppd assume that ppd makes sure the cached values are correct fails otherwise also dont use the footers ppd case ',negative
'bitvectors ',negative
'how handle different scales ',negative
'skip word also ',negative
'try rewrite countx into count not nullable remove duplicate aggregate calls well ',negative
'replace the partitions dfs with the tables dfs ',negative
'cant use equals because the walker depends them being object equal the default graph walker processes node after its kids have been processed that comparison needs ',negative
'lock operations not controlled for now ',negative
'import will mark the parent writeentity thus ensuring that check for table creation privileges ',negative
'assume here wont lower maybe should just read and not guess ',negative
'conflict ',negative
'',negative
'catchall call cases like those with ctas onto unpartitioned table see hive ',negative
'dont bust existing setups ',negative
'interrupt the cli thread stop the current statement and return ',negative
'call when nextreadindex nextreadcount ',negative
'orc table restrict reordering columns will break schema evolution ',negative
'direct access ',negative
'increment dropkey get new key for hash map ',negative
'lowest word gets integer rounding ',negative
'length utf string fixed width bytes serializing binary format ',negative
'note for now llap only supported tez tasks will never come others may added here although this only necessary have extra debug information ',negative
'expand list correct size ',negative
'filter didnt anything ',negative
'when getpos called should return the same value signaling the end the search the search should continue linearly and should sync the beginning the block ',negative
'hive join tests fail tez when have more than join the same key and there outer join down the join tree that requires filtertag disable this conversion map join here now need emulate the behavior create new operation able support this this seems like corner case enough special case this for now ',negative
'use thrift transportable formatter ',negative
'for metadataonly empty rows optimizations nullonerow input format can selected ',negative
'not supported ',negative
'because rexinputrefs represent ref expr corresponding value inputrefs used get corresponding index ',negative
'use clip the name because this method will return corrupted value when ',negative
'the user didnt specify serde use the default ',negative
'optional optional optional optional ',negative
'required required required required required optional ',negative
'',negative
'wrong type here ',negative
'viewexpandedtext ',negative
'production listfieldtype ',negative
'nothing stop ',negative
'technically methods run threadpool that created externally with the ugi however that brittle wed save the ugi explicitly here ',negative
'try with types that have type params ',negative
'drop the table but not its data ',negative
'build calcite rel node for project using converted projections col ',negative
'perform conversion null map values ',negative
'show locks ',negative
'initialize hcatoutputformat ',negative
'for backwards compatibility with old metastore persistence ',negative
'get the counters for the task ',negative
'element map ',negative
'nonjavadoc see ',negative
'get the lru key value ',negative
'get new client ',negative
'bgenjjtree enum ',negative
'the path string contains the dag identifier ',negative
'used for buffer columns values ',negative
'drop tablepartition corresponding records compactionqueue and should disappear ',negative
'get column name from custom path matcher and column value from dynamic path matcher ',negative
'private static minicluster cluster ',negative
'now add cache the dummy colstats for these partitions ',negative
'varchar ',negative
'show locks ',negative
'join which takes place separate task ',negative
'tasklist ',negative
'comparesupported returns false because union can contain ',negative
'required optional optional optional optional optional optional ',negative
'thus use flag identify have finished pushing the sort past union ',negative
'nonjavadoc see ',negative
'create table select should not return resultset ',negative
'map operators only all operators launch containers user code etc prevents running inside llap operators try running everything llap fail that not possible non blessed user code script etc please hive choose for ',negative
'',negative
'this not the first table and are not using big table ',negative
'are getting session from tezsessionpool have the session but doesnt have registry info yet have the session with registry info have failed the master thread has canceled this and will never look again ',negative
'bgenjjtree definition ',negative
'always configure storage handler with before calling any methods ',negative
'get the ast tree ',negative
'cleanup method called run cleanup tasks job state failed default cleanup provided ',negative
'changing the sortmerge join mapjoin ',negative
'finalize the headers ',negative
'check whether exception thrown when fetching log from closed operation ',negative
'ratio greater than then number rows increases this can happen when some operators like groupby duplicates the input rows which case number distincts should not change update the distinct count only when the output number rows less than input number rows ',negative
'load into existing empty table ',negative
'not update metrics see above ',negative
'validate resource plan ',negative
'segments load still need honer overwrite ',negative
'list operation states measure duration ',negative
'this semijoin need add the condition ',negative
'execute select statement and verify that aborted insert statement not counted ',negative
'add these values red green null ',negative
'the size the array equal the number selected columns ',negative
'loop through the partitions and form the expression ',negative
'test longlong version ',negative
'nonjavadoc see javaioreader long ',negative
'the key column not column then dont apply this optimization this will fixed part for type conversion udfs ',negative
'txinid ',negative
'additional information about stats virtual column number ',negative
'example from hivedecimalsubtract header comments ',negative
'initialize the transaction manager this must done before analyze called ',negative
'after dedup should left with locks path exclusive ',negative
'load the expected results ',negative
'test read ',negative
'set fetch size session conf map ',negative
'write the data type ',negative
'table directory which includes the partition directory has already been moved just update the partition location the metastore ',negative
'this only executed for outer joins with residual filters ',negative
'retrieve information about cache usage for the query ',negative
'believe not some tools generate queries with limit and than expect query run quickly lets meet their requirement ',negative
'determine which columns requires cast leftright input calcite ',negative
'currently acid requires table bucketed ',negative
'the vectorization context for creating the vectorizedrowbatch for the node ',negative
'referencing correlated variables ',negative
'update existing stat objects field ',negative
'for tables other than the big table need fetch more data until reach new group done ',negative
'first incremental load ',negative
'',negative
'short year should work ',negative
'',negative
'separator for multiple tables validwriteidlist also skip for last entry ',negative
'check the ouput specs only storage handler native tabless outputformats does not set the jobs output properties correctly ',negative
'and thus valid both times flowed the same pace congratulate ourselves and bail ',negative
'longvalue ',negative
'for incremental repl will have individual events which can other things like roles and fns well this point all dump dirs should contain dumpmetadata file that tells what inside that dumpdir ',negative
'fill the prefix bytes with deterministic data based the actual meaningful data ',negative
'including parameters passed the query ',negative
'reset the behaviour ',negative
'when have operators that have multiple parents not clear which parents traits need propagate forward ',negative
'sum input timestamp and output double just modes partial complete ',negative
'get view column authorization ',negative
'note deos not allow upgrading read lock write lock care must taken while under read lock make sure not perform any actions which attempt take write lock ',negative
'private final mapstring integer columnmap ',negative
'serdeinfo ',negative
'all index ',negative
'filterg does date parsing for quoted strings wed need verify theres ',negative
'for each column are converting the row column object ',negative
'verify can get from cache ',negative
'array hash set results can lookups the whole batch before output result ',negative
'extract the integer portion get the quotient ',negative
'for static partition may not exist when set true ',negative
'extract information about the old value ',negative
'find theres any dpp sink branch the branchingop that equivalent ',negative
'use scratch buffer with the hivedecimalwritable tobytes method dont incur poor performance creating string result ',negative
'there are some partitions with state didnt fetch any state update the stats with empty list reflect that the stateinitialize structures ',negative
'use default configuration for noauth mode ',negative
'skip leading zeros and compute number digits magnitude ',negative
'indicates read buffer has data number rows the temporary read buffer cursor during reading total number pairs output ',negative
'system include path ',negative
'add two decimals ',negative
'ctxgetcurrtask roottasks should removed ',negative
'below method returns the dependencies for the passed query explain the dependencies are the set input tables and partitions and are provided back json output for the explain command example output defaulttestsambaviv tabletype table input ',negative
'obtain delegation token from accumulo ',negative
'check hiveconfdir defined ',negative
'but can backported disable setupcleanup all versions ',negative
'have received new directory information make split strategies ',negative
'mapoperator out sparkwork use bridge spark transformation and hive operators sparkwork ',negative
'alter the table missing either due droprename which follows the alter the existing table newer than our update ',negative
'serializer then buffers rows certain limit and serializes the whole batch when the buffer full the serialize returns null the buffer not full the size buffer kept track the ',negative
'based whitelistblacklist ',negative
'the same server ',negative
'todo try using set ',negative
'close also calls flush ',negative
'failure occurs here the directory containing the original files ',negative
'remember case need connect additional work later ',negative
'its not exception caused auth check ignore ',negative
'generate the second reducesinkoperator for the group plan parseinfogetxxxdest the new reducesinkoperator will child groupbyoperatorinfo the second reducesinkoperator will put the group keys the mapreduce sort key and put the partial aggregation results the mapreduce value param numpartitionfields the number fields the mapreduce partition key this should always the same the number group keys should able remove this parameter since this phase there distinct any more return the new reducesinkoperator throws semanticexception ',negative
'sort state acquired waiting and then locktype then ',negative
'some prime numbers spaced about powers magnitude ',negative
'the output this udf constant dont even bother evaluating ',negative
'int offset outputgetlength ',negative
'break immediately timeout ',negative
'final preds ',negative
'only right input repeating and has nulls ',negative
'time part ',negative
'the delta directory should also have only bucket file bucket ',negative
'everything prefixed unittests everything prefixed ',negative
'enforce uniqueness column names ',negative
'hive history disabled create noop proxy ',negative
'null means all for show grants global for grantrevoke ',negative
'verify tables and partitions destination for equivalence ',negative
'required ',negative
'clear existing cookie ',negative
'cast timestamp ',negative
'constant string projection select hello from table ',negative
'the produces object due the limitations the traversal interface which requires interpretation that object into ranges changes the return object from the must also represent change the ',negative
'this setups auth filtering build ',negative
'the result null throw exception this can caught calling code the vectorized code path and made yield sql null value ',negative
'only one result column verify the system generated column name ',negative
'constants and nulls are ',negative
'for now dont know which virtual columns are going included well add them ',negative
'need preserve currentgroups ',negative
'metrics will have already been initialized were using them since hmshandler ',negative
'are not calling superseek since handle the present stream differently ',negative
'add row chain except case unb preceding only max needs tracked current max will never become out range can only replaced larger max ',negative
'need make sure that null operator lim fil present all branches multiinsert query before applying the optimization this method does full tree traversal starting from and will return true only finds target null operator each branch ',negative
'need set null data entries because the input nan values will automatically propagate the output ',negative
'hashmap javafieldref primitives hashmapentry javafieldref ',negative
'create path hivecontrib jar local filesystem ',negative
'the final move ',negative
'column qualifier with colon ',negative
'adjust the number reducers this correlation based ',negative
'disconnect the reduce work from its child this should produce two isolated sub graphs ',negative
'wait seconds too case exception not end busy waiting for the solution for this exception ',negative
'load data local inpath doesnt delete source files clean here ',negative
'element for key long hash table hashmap ',negative
'derived from the alias vvvt ',negative
'lock are trying acquire shared write ',negative
'',negative
'the reducer needs restored consider query like select count from bucketbig join bucketsmall akey bkey ',negative
'nonjavadoc see javasqlblob ',negative
'always start the running state requests for state updates will sent out after registration ',negative
'optional int fragmentnumber ',negative
'nway join ',negative
'now change its child ',negative
'input expression for count ',negative
'expect query completed now ',negative
'for partial and final objectinspectors for partial aggregations list doubles ',negative
'will transform gbrsgby ',negative
'for backward compatibility ',negative
'grouping same but category not ',negative
'decrease check that the pool shrinks incl killing the unused and returned sessions ',negative
'case custom dynamic partitions cant just move the subtree partition root directory since the partitions location contain regex pattern need first find the final destination each partition and move its output ',negative
'verify true have acid table are producing the table schema from orc the vectorizer class assures this ',negative
'store the results produced the dispatcher ',negative
'this assumes all paths are bucket names which means lookup needed ',negative
'according hivetypetosqltype possible options are ',negative
'more batches read exhausted the reader ',negative
'the record with valid cqid has disappeared this sign something wrong ',negative
'detect udtfs nested select group etc they arent supported ',negative
'estimate needed underlying aggbuffer for results for intermediates results underlying wdwsz intermediates underlying wdwsz ',negative
'surprisingly these privs are already granted ',negative
'lower case role names for case insensitive behavior ',negative
'check log files look ',negative
'set the range bytes deserialized ',negative
'the has access method can try defaultfileaccess ',negative
'bgenjjtree typeset ',negative
'there more than argument specified different natural language locale being specified ',negative
'internal input format class for ',negative
'specified the query ',negative
'row ',negative
'expected failure ',negative
'assume all columns are null except the dummy column always nonnull ',negative
'compare the field types ',negative
'arithmetic two type intervaldaytime storing nanosecond interval ',negative
'todo currently way test alter table this interface doesnt support alter table ',negative
'sanity check ',negative
'empty ',negative
'since cannot directly set the private byte field inside text ',negative
'did not reduce check the children nodes ',negative
'test the sequence validation functionality ',negative
'binary search only works know the size the split and the recordreader rcfilerecordreader ',negative
'show user level privileges ',negative
'must release the connection before call other methods ',negative
'adopted hadoop calling new sequencefilereader leaves ',negative
'todo ',negative
'operator tree now done ',negative
'get the connection properties from user specific config file ',negative
'parent guaranteed have single list because reduce sink ',negative
'check whether session log dir deleted after session closed ',negative
'nothing aggregate ',negative
'read event from notification ',negative
'just return stats gathering should not block the main query ',negative
'greater than equal and less than equal ',negative
'should return all ',negative
'test select absrootcolb from table testroot ',negative
'ignore exception ',negative
'succeed transactional set true and the table bucketed and uses orc ',negative
'the current position the key series ',negative
'partition column ',negative
'predicates without field references can pushed both inputs ',negative
'files size for splits ',negative
'check standard out terminal ',negative
'get the session specified class loader from sessionstate ',negative
'confusing directories export ',negative
'get the key store map ',negative
'match ',negative
'construct using ',negative
'adds databasename dbname the filter ',negative
'specialcasing for adminlevel operations that not require object checking ',negative
'need attempt merge the files again ',negative
'check there are privileges filtered ',negative
'free list indices each unallocated block for quick lookup ',negative
'add hive operator level statistics ',negative
'appropriate operators the ',negative
'hbase stuff ',negative
'isrepeating and there are nulls ',negative
'remove the dead session dir ',negative
'entire batch filtered out ',negative
'check column number ',negative
'set the output string entry the contents text object null object reference record that the value sql null ',negative
'walkaround tez ',negative
'initialize input ',negative
'all values are null none qualify ',negative
'since schedule can called from multiple threads peek the wait queue try scheduling the task and then remove the task scheduling successful this ',negative
'skip leading zeroes word ',negative
'replace all toktabref with fully qualified table name not already fully qualified ',negative
'nodes that need see siblings for and sibling levels ',negative
'alteroptype null case stats update ',negative
'just need initialize the proxyusers for the first time given that the conf will not change the fly ',negative
'contains functionality that helps with understanding how subquery was rewritten ',negative
'could also have one metricssource for all the pools and add all the pools the collector its getmetrics call separate records not clear thats supported also wed have initialize the metrics ourselves instead using metric annotation ',negative
'set max possible value ',negative
'set the thread local username used for doas true ',negative
'check the original partitions the dest table ',negative
'for direct connections dont yet support reestablishing connections ',negative
'childtask merge nothing there are more than one childtasks which case dont want anything ',negative
'valid events this batch but were still not done processing events ',negative
'split work into multiple branches one for each childwork childworks ',negative
'generate types for column mapping ',negative
'table descriptor the final ',negative
'the verifyandset this case expected fail with the ',negative
'fetchfirst execute sql and fetch its sql operation log expected value ',negative
'validate after compaction ',negative
'fixed doesnt exist hive fixeds lists bytes out ',negative
'files size for splits ',negative
'the code should account for the bug and update the iterators the split ',negative
'files size for splits ',negative
'should not happen default value set ',negative
'the scaling down during multiplication avoid unnecessary overflow note that even this could overflow newscale too small ',negative
'note some code uses this list correlate with column names and yet these lists may contain duplicates which this call will remove and the other wont far can tell code will actually use these two methods together all good the code gets the list without relying this method maybe just works magic ',negative
'output will same both partial full aggregation modes ',negative
'flag control that always threads are initialized only once ',negative
'this materialized view this stores the view descriptor ',negative
'setup connection information ',negative
'add the index expr reducekeys distinctindices ',negative
'null evicted task means offer accepted evictedtask not equal taskwrapper means current task accepted and evicted ',negative
'true the logs should removed after the operation should used only test mode ',negative
'see this explicit cast ',negative
'nonimpersonation mode map scheduler queue current user fair scheduler configured ',negative
'column constant ',negative
'nonjavadoc see long ',negative
'get completed attempts from jobtasksjsp ',negative
'already set ',negative
'clean ',negative
'scale down right and compare ',negative
'returns map ',negative
'add field with comment ',negative
'can have unaliased and one aliased mapping column ',negative
'test that can search correctly using buffer and pulling sequence bytes out the middle this case ',negative
'test null right ',negative
'increment count values seen far ',negative
'the directory might dbtablepartition ',negative
'get our singlecolumn long hash set information for this specialized class ',negative
'groupbyoperators ',negative
'after the timeout just force abort the open txns ',negative
'fastscale ',negative
'the processing thread will switch between these two objects ',negative
'set true the operator tree below ',negative
'this struct type ',negative
'param statusdir directory statusdir defined for the webhcat job supposed contain stdoutstderrsyslog for the webhcat controller job param jobtype currently support mapreduce the specific parser will parse the log the controller job and retrieve jobid all mapreduce jobs launches the generic mapreduce parser works when the program use jobclientrunjob submit the job but the program use other api generic mapreduce parser not guaranteed find the jobid param conf configuration for webhcat ',negative
'nothing with nulls ',negative
'partition columns the table level schema ',negative
'insert event partitioned table existing partition ',negative
'must have already been called ',negative
'set invoking hmshandler threadlocal this will used later notify ',negative
'the state for guaranteed task tracking synchronized this addition isguaranteed only modified under the epic lock because involves modifying the corresponding structures that contain the task objects the same time ',negative
'nonjavadoc see javasqltime javautilcalendar ',negative
'dump and load only first insert record ',negative
'try populate correlation variables using local fields ',negative
'ctas cannot part multitxn stmt ',negative
'now the job initialized reason rjgetjobstate again and not want extra rpc call ',negative
'know how handle dpp sinks ',negative
'nothing implicitconversions ',negative
'note critical this prior initializing logj otherwise ',negative
'check basic operation ',negative
'extract all the inputformatclass names for each chunk the combinedsplit ',negative
'from script need load history and need completer either ',negative
'check the partitions dont exist the desttable ',negative
'need convert ',negative
'load required jdbc driver ',negative
'change the resource plan use fifo policy ',negative
'columnvector entry byte array representing serialized bloomfilter does simple byte oring which should faster than deserializemerge ',negative
'allocate free and optionally allocate ',negative
'because percentile really quantile values should generally strictly between and ',negative
'the user may have passed list files comma separated ',negative
'nonjavadoc see ',negative
'heap not full add the buffer the heap and restore heap property ',negative
'public string rid specific zemanta use ',negative
'put the jks file the current test path only for test purpose ',negative
'found lazysimpleserdes nullsequence ',negative
'serializer then recordvalue null the buffer not full the size buffer kept track the serde ',negative
'once have read the from the file there are two kinds cases for which might have discard rows from the batch case when the row created transaction that not valid case when the row has been deleted will through the batch discover rows which match any the cases and specifically remove them from the selected vector course selectedinuse should also set ',negative
'createtable truncate insert the result just one record ',negative
'test char literal string column comparison ',negative
'there conflicting lock the same object with lower sequence number ',negative
'final final mixing bit values abc into pairs abc values differing only few bits will usually produce values that look totally different this was tested for pairs that differed one bit two bits any combination top bits abc any combination bottom bits abc differ defined for and transformed the output delta gray code string commonly produced subtraction look like single bit difference the base values were pseudorandom all zero but one bit set all zero plus counter that starts zero these constants passed and these came close define finalabc rotb rotc rota rotb rotc rota rotb ',negative
'tracks total pending preemptions ',negative
'this one has the mixedsize chars ',negative
'update the table column stats for table cache ',negative
'write header ',negative
'make sure escape separator char prop values ',negative
'checking for status table ',negative
'test few field names ',negative
'unique identity for this instance ',negative
'were here proxy user set ',negative
'recheck got verified another thread while were waiting ',negative
'convert input row standard objects ',negative
'the rel which being visited ',negative
'whatever ',negative
'check the rest command specified explicitly use hcatalog says that implicitly using the pig usehcatalog arg ',negative
'there was error adding partitions rollback copy and rethrow ',negative
'the volcanoplanner from apply both planners need use the correct executor ',negative
'newpath the basedelta dir ',negative
'update the metadata for the materialized view ',negative
'the partitioning columns the parent are more specific than those the child ',negative
'nonjavadoc see ',negative
'longer part the hivedecimal representation anymore string then bytes ',negative
'',negative
'pass through group key indicators present ',negative
'year month day hour minute second ',negative
'periodic task time out submitted tasks that have not been updated with umbilical heartbeat ',negative
'found old valid block for this key the cache ',negative
'continue reading from the input stream until the desired number byte has been read ',negative
'test that the server code exists ',negative
'modifiedrowcount ',negative
'this will also invoked for tasks which have been killed rejected the daemon informing the daemon becomes necessary once the llapscheduler supports preemption andor starts attempting kill tasks which may running node ',negative
'get base type since type string may parameterized ',negative
'once enough rows have been output there need generate more output ',negative
'twice returns not cleaned cache ',negative
'add select expression ',negative
'all accesses synchronized the object itself could replaced with cas ',negative
'using minidfs the permissions dont work properly because the current user gets treated superuser for this test specify different nonsuper user ',negative
'break out the loop fast watchmode disabled ',negative
'bail out first missed column ',negative
'stage waiting for inputslots complete ',negative
'remove the last ',negative
'block semantic analysis check activecalls ',negative
'create schema object containing the give column ',negative
'the final match intend return ',negative
'limit below milliseconds only ',negative
'match found match found and the current row will dropped the current row has been spilled disk the join postponed ',negative
'construct without map field ',negative
'store the given version and comment the metastore ',negative
'just digits ',negative
'convert seconds since the epoch with fraction nanoseconds long integer ',negative
'can fail with ',negative
'the delta dirs should have been cleaned ',negative
'prepare data for the source table ',negative
'get the submap ',negative
'seqnumber ',negative
'now add any scratch columns needed for children operators ',negative
'because its using write ',negative
'restore the old out stream ',negative
'initialize listcolumnvector for keys and values ',negative
'nonjavadoc see ',negative
'request accepted request rejected wait queue full request accepted but evicted other low priority task ',negative
'the tail each task chain ',negative
'can the join operator converted bucket mapmerge join operator ',negative
'dont measure data generation execution cost generate the big table into memory first ',negative
'load the list partitions and return the list partition specs ',negative
'save results the cache for future queries use ',negative
'use construct ',negative
'drop messages for the dropped partitions ',negative
'link filesink that will write the same final location ',negative
'this class populates the following operator traits for the entire operator tree bucketing columns table pruned partitions bucketing columns refer not the bucketing columns from the table object but instead the dynamic bucketing done operators such reduce sinks and groupbys all the operators have translation from their input names the output names corresponding the bucketing column the colexprmap that part every operator used this transformation the table object used for the basecase mapreduce when deciding perform bucket map join this object used the bucketmapjoinproc find number files for the table correspond the number buckets specified the meta data the pruned partition information has the same purpose the table object the moment the traits sortedness etc can populated well for future optimizations make use ',negative
'this makes sure can use the same formula compute the ',negative
'this should validated change time lets fall back default here ',negative
'this set all move tasks the end multiinsert query will only begin once all ',negative
'not able push anything down ',negative
'test with table name which does not exists ',negative
'belongs target table strictly speaking there maybe ambiguous ref but this will caught later when multiinsert parsed ',negative
'need prune the select operator ',negative
'for hdfs could avoid serializing file and just replace the path with inodebased path however that breaks bunch stuff because hive later looks things split path ',negative
'now consolidate all the events that happenned during the objdump into the objdump ',negative
'not adding the registry service since need control when initialized conf used pickup properties ',negative
'prepare the expression filter the columns ',negative
'need process here ',negative
'create new session ctx object with client type ',negative
'corvar offset should point the leftinput currentrel ',negative
'reducer autoparallelism unset fixed uniform parallel ',negative
'null constant could typed need check the value ',negative
'remove parent for the big table branch ',negative
'otherwise recurse ',negative
'end ',negative
'release them the same time ',negative
'hosts per host requests the same priority first host next host last with host third request host should not allocated immediately ',negative
'tests returns the first file present the lookup order when files are present the lookup order ',negative
'are going small tables ',negative
'read result over ssl ',negative
'split the partition predicate identify column and value ',negative
'recursively through expression and make sure the following ',negative
'this exception indicates that code record could not parsed and the caller can decide whether drop send dead letter queue rolling back the txn and retrying wontable help since the tuple will exactly the same when its replayed ',negative
'the reader that currently has the lowest key ',negative
'since integer always some products here are not included ',negative
'todo verify that this works for systems using ugidoas oozie ',negative
'need update event operators with the cloned table scan ',negative
'can trick the inputformat into using mockinstance ',negative
'otherwise just removed lockedinvalid item from heap continue ',negative
'ideally test like this should qfile test however the explain output from qfile always ',negative
'nothing changes cache ',negative
'network cost dphj ',negative
'new part ',negative
'the lock has single components simplehivelock zookeeperhivelock pos lock paths array contains dbname pos contains tblname ',negative
'nothing intern ',negative
'just check for writing permissions fails with then means the location may readonly ',negative
'the parser should not allow this ',negative
'first data dir contains files ',negative
'set some reasonable defaults ',negative
'since are going creating new table should mark that write entity that the auth framework can work there ',negative
'remove from the residual predicate ',negative
'the max age task allowed ',negative
'build reloptabstracttable ',negative
'signal new failure mapreduce ',negative
'for special modes that case sessionstateget empty ',negative
'create new ugi and add map ',negative
'set job ',negative
'print out the vertex dependency root stage ',negative
'the join key ',negative
'this used multiple places among others ',negative
'loop through the bits that are set true and mark those rows false their ',negative
'use lazysimpleserde for note lazysimpleserde does not support tables with single column col type arraystring this happens when the table created using ',negative
'note columnstypes missing all columns will string type ',negative
'new input inspector ',negative
'template classname valuetype ifdefined ',negative
'user supplied data for that object ',negative
'middle word gets integer rounding lower longword cleared ',negative
'job ',negative
'create another dynamicpartitionctx which has different inputtodp column mapping ',negative
'also allow constraint creation only and foreign key creation fails ',negative
'clean history ',negative
'check each mapjoin and shufflejoin operator see they are performing cross product yes output warning the sessions console the checks made are the following shuffle join check the parent reducesinkop the joinop its keys list size then this cross product the parent reducesinkop the mapwork for the same stage mapjoin the keys expr list the mapjoin desc empty list for any input this implies cross product tez shuffle join check the parent reducesinkop the joinop its keys list size then this cross product the parent reducesinkop checked based the map the reducework that contains the joinop tez map join the keys expr list the mapjoin desc empty list for any input this implies cross product ',negative
'applied the table ',negative
'tablecolumn alias ',negative
'move unprocessed remainder beginning buffer ',negative
'nonjavadoc see ',negative
'create too many partitions just enough validate over limit requests ',negative
'inscriptional yodh bytes ',negative
'this may happen when enablebitvector false ',negative
'importing into existing table fileformat checked checktable ',negative
'row mode will not catch this input file format then not enabled ',negative
'map aggregation ',negative
'send failover request minihs and make sure minihs takes over returning back leader test listeners ',negative
'test query where timeout kicks ',negative
'lookup udf class failed ',negative
'notify the master thread and the user ',negative
'try read the dropped after cache update ',negative
'set clear the rest the reading variables based vectorrow deserialization ',negative
'generics this how vectorization currently works ',negative
'store the differing configuration for each alias the job ',negative
'error crosses threshold inside close want ',negative
'this unnecessary check and forced configuration the property file maybe replace with enforced empty value string ',negative
'reverse the list since checked the part from leaf dir tables base dir ',negative
'user hasnt specify partition spec generate from tables partition spec this only insertinsert intoinsert overwriteanalyze ',negative
'see comment dumping rows via sql for why this doesnt work for all types ',negative
'copy without retry ',negative
'repeat and drop partition without purge ',negative
'obsolete list should include the two original bucket files and the old base dir ',negative
'replace the edge manager for all vertices which have routing type custom ',negative
'not included the input collations but can propagated this join might enforce ',negative
'will download into fnscoped subdirectories avoid name collisions assume there are collisions within the same that doesnt mean download for every ',negative
'get all the sessions validate cluster fractions ',negative
'mapping from task the number failures ',negative
'default file system which may may not online during pure metadata operations ',negative
'deterministic ease testing ',negative
'this unionoperator inside the reduce side job generated correlation optimizer which means all inputs this unionoperator are from demuxoperator should not touch this unionoperator genmapredtasks ',negative
'ambiguous ',negative
